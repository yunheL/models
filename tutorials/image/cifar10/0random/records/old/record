(tensorflow) ➜  cifar10 git:(master) python cifar10_train.py 
>> Downloading cifar-10-binary.tar.gz 100.0%
Successfully downloaded cifar-10-binary.tar.gz 170052171 bytes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-04-01 16:46:09.062105: step 0, loss = 4.68 (352.1 examples/sec; 0.364 sec/batch)
2017-04-01 16:46:13.516046: step 10, loss = 4.59 (287.4 examples/sec; 0.445 sec/batch)
2017-04-01 16:46:17.925275: step 20, loss = 4.45 (290.3 examples/sec; 0.441 sec/batch)
2017-04-01 16:46:22.595843: step 30, loss = 4.40 (274.1 examples/sec; 0.467 sec/batch)
2017-04-01 16:46:27.900686: step 40, loss = 4.34 (241.3 examples/sec; 0.530 sec/batch)
2017-04-01 16:46:32.642094: step 50, loss = 4.24 (270.0 examples/sec; 0.474 sec/batch)
2017-04-01 16:46:37.330230: step 60, loss = 4.33 (273.0 examples/sec; 0.469 sec/batch)
2017-04-01 16:46:41.865209: step 70, loss = 4.24 (282.3 examples/sec; 0.453 sec/batch)
2017-04-01 16:46:46.431843: step 80, loss = 4.15 (280.3 examples/sec; 0.457 sec/batch)
2017-04-01 16:46:50.900624: step 90, loss = 4.13 (286.4 examples/sec; 0.447 sec/batch)
2017-04-01 16:46:55.466571: step 100, loss = 4.07 (280.3 examples/sec; 0.457 sec/batch)
2017-04-01 16:46:59.916598: step 110, loss = 4.45 (287.6 examples/sec; 0.445 sec/batch)
2017-04-01 16:47:04.367679: step 120, loss = 4.10 (287.6 examples/sec; 0.445 sec/batch)
2017-04-01 16:47:08.781641: step 130, loss = 3.97 (290.0 examples/sec; 0.441 sec/batch)
2017-04-01 16:47:13.186630: step 140, loss = 4.10 (290.6 examples/sec; 0.440 sec/batch)
2017-04-01 16:47:17.668083: step 150, loss = 3.88 (285.6 examples/sec; 0.448 sec/batch)
2017-04-01 16:47:22.664525: step 160, loss = 3.91 (256.2 examples/sec; 0.500 sec/batch)
2017-04-01 16:47:27.284929: step 170, loss = 4.21 (277.0 examples/sec; 0.462 sec/batch)
2017-04-01 16:47:31.929226: step 180, loss = 3.86 (275.6 examples/sec; 0.464 sec/batch)
2017-04-01 16:47:36.358219: step 190, loss = 3.91 (289.0 examples/sec; 0.443 sec/batch)
2017-04-01 16:47:40.866642: step 200, loss = 3.55 (283.9 examples/sec; 0.451 sec/batch)
2017-04-01 16:47:45.768214: step 210, loss = 3.87 (261.1 examples/sec; 0.490 sec/batch)
2017-04-01 16:47:50.171486: step 220, loss = 3.80 (290.7 examples/sec; 0.440 sec/batch)
2017-04-01 16:47:54.547935: step 230, loss = 3.69 (292.5 examples/sec; 0.438 sec/batch)
2017-04-01 16:47:58.904731: step 240, loss = 3.66 (293.8 examples/sec; 0.436 sec/batch)
2017-04-01 16:48:03.256861: step 250, loss = 3.78 (294.1 examples/sec; 0.435 sec/batch)
2017-04-01 16:48:07.604439: step 260, loss = 3.59 (294.4 examples/sec; 0.435 sec/batch)
2017-04-01 16:48:11.959900: step 270, loss = 3.64 (293.9 examples/sec; 0.436 sec/batch)
2017-04-01 16:48:16.284718: step 280, loss = 3.51 (296.0 examples/sec; 0.432 sec/batch)
2017-04-01 16:48:20.675922: step 290, loss = 3.78 (291.5 examples/sec; 0.439 sec/batch)
2017-04-01 16:48:25.081887: step 300, loss = 3.37 (290.5 examples/sec; 0.441 sec/batch)
2017-04-01 16:48:29.417597: step 310, loss = 3.43 (295.2 examples/sec; 0.434 sec/batch)
2017-04-01 16:48:33.814620: step 320, loss = 3.49 (291.1 examples/sec; 0.440 sec/batch)
2017-04-01 16:48:38.164100: step 330, loss = 3.48 (294.3 examples/sec; 0.435 sec/batch)
2017-04-01 16:48:42.518990: step 340, loss = 3.49 (293.9 examples/sec; 0.435 sec/batch)
2017-04-01 16:48:46.852910: step 350, loss = 3.87 (295.3 examples/sec; 0.433 sec/batch)
2017-04-01 16:48:51.213294: step 360, loss = 3.57 (293.6 examples/sec; 0.436 sec/batch)
2017-04-01 16:48:55.587397: step 370, loss = 3.39 (292.6 examples/sec; 0.437 sec/batch)
2017-04-01 16:49:00.041782: step 380, loss = 3.48 (287.4 examples/sec; 0.445 sec/batch)
2017-04-01 16:49:04.724013: step 390, loss = 3.47 (273.4 examples/sec; 0.468 sec/batch)
2017-04-01 16:49:09.350909: step 400, loss = 3.40 (276.6 examples/sec; 0.463 sec/batch)
2017-04-01 16:49:13.792292: step 410, loss = 3.30 (288.2 examples/sec; 0.444 sec/batch)
2017-04-01 16:49:18.180272: step 420, loss = 3.42 (291.7 examples/sec; 0.439 sec/batch)
2017-04-01 16:49:22.532333: step 430, loss = 3.31 (294.1 examples/sec; 0.435 sec/batch)
2017-04-01 16:49:26.889367: step 440, loss = 3.24 (293.8 examples/sec; 0.436 sec/batch)
2017-04-01 16:49:31.244544: step 450, loss = 3.37 (293.9 examples/sec; 0.436 sec/batch)
2017-04-01 16:49:35.630957: step 460, loss = 3.25 (291.8 examples/sec; 0.439 sec/batch)
2017-04-01 16:49:40.008338: step 470, loss = 3.22 (292.4 examples/sec; 0.438 sec/batch)
2017-04-01 16:49:44.495184: step 480, loss = 3.35 (285.3 examples/sec; 0.449 sec/batch)
2017-04-01 16:49:48.925781: step 490, loss = 3.16 (288.9 examples/sec; 0.443 sec/batch)
2017-04-01 16:49:53.443015: step 500, loss = 3.19 (283.4 examples/sec; 0.452 sec/batch)
2017-04-01 16:49:57.914246: step 510, loss = 3.23 (286.3 examples/sec; 0.447 sec/batch)
2017-04-01 16:50:02.367359: step 520, loss = 2.95 (287.4 examples/sec; 0.445 sec/batch)
2017-04-01 16:50:06.738804: step 530, loss = 3.12 (292.8 examples/sec; 0.437 sec/batch)
2017-04-01 16:50:11.178900: step 540, loss = 2.99 (288.3 examples/sec; 0.444 sec/batch)
2017-04-01 16:50:15.556398: step 550, loss = 3.12 (292.4 examples/sec; 0.438 sec/batch)
2017-04-01 16:50:19.917123: step 560, loss = 3.05 (293.5 examples/sec; 0.436 sec/batch)
2017-04-01 16:50:24.314241: step 570, loss = 2.92 (291.1 examples/sec; 0.440 sec/batch)
2017-04-01 16:50:28.755744: step 580, loss = 3.00 (288.2 examples/sec; 0.444 sec/batch)
2017-04-01 16:50:33.247689: step 590, loss = 3.01 (285.0 examples/sec; 0.449 sec/batch)
2017-04-01 16:50:37.749492: step 600, loss = 2.92 (284.3 examples/sec; 0.450 sec/batch)
2017-04-01 16:50:42.243744: step 610, loss = 2.91 (284.8 examples/sec; 0.449 sec/batch)
2017-04-01 16:50:46.730805: step 620, loss = 3.03 (285.3 examples/sec; 0.449 sec/batch)
2017-04-01 16:50:51.174477: step 630, loss = 2.86 (288.1 examples/sec; 0.444 sec/batch)
2017-04-01 16:50:55.591994: step 640, loss = 3.05 (289.8 examples/sec; 0.442 sec/batch)
2017-04-01 16:50:59.955516: step 650, loss = 2.83 (293.3 examples/sec; 0.436 sec/batch)
2017-04-01 16:51:04.391406: step 660, loss = 2.95 (288.6 examples/sec; 0.444 sec/batch)
2017-04-01 16:51:08.800554: step 670, loss = 3.05 (290.3 examples/sec; 0.441 sec/batch)
2017-04-01 16:51:13.226017: step 680, loss = 2.90 (289.2 examples/sec; 0.443 sec/batch)
2017-04-01 16:51:17.611995: step 690, loss = 2.86 (291.8 examples/sec; 0.439 sec/batch)
2017-04-01 16:51:22.239775: step 700, loss = 2.82 (276.6 examples/sec; 0.463 sec/batch)
2017-04-01 16:51:26.948824: step 710, loss = 2.83 (271.8 examples/sec; 0.471 sec/batch)
2017-04-01 16:51:31.903582: step 720, loss = 2.82 (258.3 examples/sec; 0.495 sec/batch)
2017-04-01 16:51:36.395188: step 730, loss = 2.73 (285.0 examples/sec; 0.449 sec/batch)
2017-04-01 16:51:41.282034: step 740, loss = 2.75 (261.9 examples/sec; 0.489 sec/batch)
2017-04-01 16:51:45.877375: step 750, loss = 2.71 (278.5 examples/sec; 0.460 sec/batch)
2017-04-01 16:51:50.349449: step 760, loss = 2.68 (286.2 examples/sec; 0.447 sec/batch)
2017-04-01 16:51:54.774632: step 770, loss = 2.64 (289.3 examples/sec; 0.443 sec/batch)
2017-04-01 16:51:59.147408: step 780, loss = 2.80 (292.7 examples/sec; 0.437 sec/batch)
2017-04-01 16:52:03.618803: step 790, loss = 2.63 (286.3 examples/sec; 0.447 sec/batch)
2017-04-01 16:52:08.072312: step 800, loss = 2.58 (287.4 examples/sec; 0.445 sec/batch)
2017-04-01 16:52:12.390069: step 810, loss = 2.70 (296.5 examples/sec; 0.432 sec/batch)
2017-04-01 16:52:16.729328: step 820, loss = 2.75 (295.0 examples/sec; 0.434 sec/batch)
2017-04-01 16:52:21.093707: step 830, loss = 2.71 (293.3 examples/sec; 0.436 sec/batch)
2017-04-01 16:52:25.632860: step 840, loss = 2.49 (282.0 examples/sec; 0.454 sec/batch)
2017-04-01 16:52:30.008503: step 850, loss = 2.63 (292.5 examples/sec; 0.438 sec/batch)
2017-04-01 16:52:34.736489: step 860, loss = 2.61 (270.7 examples/sec; 0.473 sec/batch)
2017-04-01 16:52:39.335807: step 870, loss = 2.58 (278.3 examples/sec; 0.460 sec/batch)
2017-04-01 16:52:44.177831: step 880, loss = 2.61 (264.4 examples/sec; 0.484 sec/batch)
2017-04-01 16:52:49.549177: step 890, loss = 2.64 (238.3 examples/sec; 0.537 sec/batch)
2017-04-01 16:52:55.187669: step 900, loss = 2.66 (227.0 examples/sec; 0.564 sec/batch)
2017-04-01 16:52:59.815487: step 910, loss = 2.74 (276.6 examples/sec; 0.463 sec/batch)
2017-04-01 16:53:04.229918: step 920, loss = 2.42 (290.0 examples/sec; 0.441 sec/batch)
2017-04-01 16:53:08.636509: step 930, loss = 2.44 (290.5 examples/sec; 0.441 sec/batch)
2017-04-01 16:53:13.180447: step 940, loss = 2.64 (281.7 examples/sec; 0.454 sec/batch)
2017-04-01 16:53:17.884279: step 950, loss = 2.76 (272.1 examples/sec; 0.470 sec/batch)
2017-04-01 16:53:22.322830: step 960, loss = 2.60 (288.4 examples/sec; 0.444 sec/batch)
2017-04-01 16:53:26.710753: step 970, loss = 2.63 (291.7 examples/sec; 0.439 sec/batch)
2017-04-01 16:53:31.161560: step 980, loss = 2.50 (287.6 examples/sec; 0.445 sec/batch)
2017-04-01 16:53:35.537263: step 990, loss = 2.51 (292.5 examples/sec; 0.438 sec/batch)
2017-04-01 16:53:39.960088: step 1000, loss = 2.20 (289.4 examples/sec; 0.442 sec/batch)
2017-04-01 16:53:44.397937: step 1010, loss = 2.54 (288.4 examples/sec; 0.444 sec/batch)
2017-04-01 16:53:49.131735: step 1020, loss = 2.59 (270.4 examples/sec; 0.473 sec/batch)
2017-04-01 16:53:54.134974: step 1030, loss = 2.33 (255.8 examples/sec; 0.500 sec/batch)
2017-04-01 16:53:58.956405: step 1040, loss = 2.19 (265.5 examples/sec; 0.482 sec/batch)
2017-04-01 16:54:04.193882: step 1050, loss = 2.30 (244.4 examples/sec; 0.524 sec/batch)
2017-04-01 16:54:09.145252: step 1060, loss = 2.46 (258.5 examples/sec; 0.495 sec/batch)
2017-04-01 16:54:13.769262: step 1070, loss = 2.32 (276.8 examples/sec; 0.462 sec/batch)
2017-04-01 16:54:19.103948: step 1080, loss = 2.28 (239.9 examples/sec; 0.533 sec/batch)
2017-04-01 16:54:23.980539: step 1090, loss = 2.28 (262.5 examples/sec; 0.488 sec/batch)
2017-04-01 16:54:28.961590: step 1100, loss = 2.37 (257.0 examples/sec; 0.498 sec/batch)
2017-04-01 16:54:33.854325: step 1110, loss = 2.26 (261.6 examples/sec; 0.489 sec/batch)
2017-04-01 16:54:38.685256: step 1120, loss = 2.30 (265.0 examples/sec; 0.483 sec/batch)
2017-04-01 16:54:43.567123: step 1130, loss = 2.36 (262.2 examples/sec; 0.488 sec/batch)
2017-04-01 16:54:48.360129: step 1140, loss = 2.20 (267.1 examples/sec; 0.479 sec/batch)
2017-04-01 16:54:53.043157: step 1150, loss = 2.27 (273.3 examples/sec; 0.468 sec/batch)
2017-04-01 16:54:57.938950: step 1160, loss = 2.25 (261.4 examples/sec; 0.490 sec/batch)
2017-04-01 16:55:02.728750: step 1170, loss = 2.29 (267.2 examples/sec; 0.479 sec/batch)
2017-04-01 16:55:07.706037: step 1180, loss = 2.18 (257.2 examples/sec; 0.498 sec/batch)
2017-04-01 16:55:12.510154: step 1190, loss = 2.39 (266.4 examples/sec; 0.480 sec/batch)
2017-04-01 16:55:17.500574: step 1200, loss = 2.22 (256.5 examples/sec; 0.499 sec/batch)
2017-04-01 16:55:22.887037: step 1210, loss = 2.34 (237.6 examples/sec; 0.539 sec/batch)
2017-04-01 16:55:28.340994: step 1220, loss = 2.06 (234.7 examples/sec; 0.545 sec/batch)
2017-04-01 16:55:33.176020: step 1230, loss = 2.14 (264.7 examples/sec; 0.484 sec/batch)
2017-04-01 16:55:37.982663: step 1240, loss = 2.17 (266.3 examples/sec; 0.481 sec/batch)
2017-04-01 16:55:42.747780: step 1250, loss = 2.14 (268.6 examples/sec; 0.477 sec/batch)
2017-04-01 16:55:48.302966: step 1260, loss = 2.21 (230.4 examples/sec; 0.556 sec/batch)
2017-04-01 16:55:53.753928: step 1270, loss = 2.25 (234.8 examples/sec; 0.545 sec/batch)
2017-04-01 16:55:59.253619: step 1280, loss = 2.09 (232.7 examples/sec; 0.550 sec/batch)
2017-04-01 16:56:04.733649: step 1290, loss = 2.40 (233.6 examples/sec; 0.548 sec/batch)
2017-04-01 16:56:10.258993: step 1300, loss = 2.10 (231.7 examples/sec; 0.553 sec/batch)
2017-04-01 16:56:15.783349: step 1310, loss = 2.15 (231.7 examples/sec; 0.552 sec/batch)
2017-04-01 16:56:21.279862: step 1320, loss = 2.02 (232.9 examples/sec; 0.550 sec/batch)
2017-04-01 16:56:26.685052: step 1330, loss = 2.22 (236.8 examples/sec; 0.541 sec/batch)
2017-04-01 16:56:32.184990: step 1340, loss = 2.05 (232.7 examples/sec; 0.550 sec/batch)
2017-04-01 16:56:37.604475: step 1350, loss = 2.05 (236.2 examples/sec; 0.542 sec/batch)
2017-04-01 16:56:43.086314: step 1360, loss = 2.00 (233.5 examples/sec; 0.548 sec/batch)
2017-04-01 16:56:48.557896: step 1370, loss = 1.95 (233.9 examples/sec; 0.547 sec/batch)
2017-04-01 16:56:53.985084: step 1380, loss = 1.93 (235.8 examples/sec; 0.543 sec/batch)
2017-04-01 16:56:59.364794: step 1390, loss = 2.09 (237.9 examples/sec; 0.538 sec/batch)
2017-04-01 16:57:03.980149: step 1400, loss = 2.20 (277.3 examples/sec; 0.462 sec/batch)
2017-04-01 16:57:09.376303: step 1410, loss = 1.89 (237.2 examples/sec; 0.540 sec/batch)
2017-04-01 16:57:14.786542: step 1420, loss = 2.07 (236.6 examples/sec; 0.541 sec/batch)
2017-04-01 16:57:19.473498: step 1430, loss = 2.20 (273.1 examples/sec; 0.469 sec/batch)
2017-04-01 16:57:23.998435: step 1440, loss = 2.18 (282.9 examples/sec; 0.452 sec/batch)
2017-04-01 16:57:28.871841: step 1450, loss = 1.98 (262.6 examples/sec; 0.487 sec/batch)
2017-04-01 16:57:33.617687: step 1460, loss = 1.87 (269.7 examples/sec; 0.475 sec/batch)
2017-04-01 16:57:38.206061: step 1470, loss = 2.03 (279.0 examples/sec; 0.459 sec/batch)
2017-04-01 16:57:43.041103: step 1480, loss = 1.89 (264.7 examples/sec; 0.484 sec/batch)
2017-04-01 16:57:47.804851: step 1490, loss = 1.93 (268.7 examples/sec; 0.476 sec/batch)
2017-04-01 16:57:52.797824: step 1500, loss = 2.19 (256.4 examples/sec; 0.499 sec/batch)
2017-04-01 16:57:57.756336: step 1510, loss = 1.92 (258.1 examples/sec; 0.496 sec/batch)
2017-04-01 16:58:02.460923: step 1520, loss = 1.97 (272.1 examples/sec; 0.470 sec/batch)
2017-04-01 16:58:07.294931: step 1530, loss = 1.88 (264.8 examples/sec; 0.483 sec/batch)
2017-04-01 16:58:12.029379: step 1540, loss = 2.03 (270.4 examples/sec; 0.473 sec/batch)
2017-04-01 16:58:16.868059: step 1550, loss = 1.99 (264.5 examples/sec; 0.484 sec/batch)
2017-04-01 16:58:21.523783: step 1560, loss = 1.82 (274.9 examples/sec; 0.466 sec/batch)
2017-04-01 16:58:26.328261: step 1570, loss = 1.82 (266.4 examples/sec; 0.480 sec/batch)
2017-04-01 16:58:30.918451: step 1580, loss = 1.85 (278.9 examples/sec; 0.459 sec/batch)
2017-04-01 16:58:35.468558: step 1590, loss = 1.74 (281.3 examples/sec; 0.455 sec/batch)
2017-04-01 16:58:39.983613: step 1600, loss = 1.91 (283.5 examples/sec; 0.452 sec/batch)
2017-04-01 16:58:44.440593: step 1610, loss = 1.91 (287.2 examples/sec; 0.446 sec/batch)
2017-04-01 16:58:49.102955: step 1620, loss = 2.10 (274.5 examples/sec; 0.466 sec/batch)
2017-04-01 16:58:53.534374: step 1630, loss = 1.76 (288.8 examples/sec; 0.443 sec/batch)
2017-04-01 17:00:14.060457: step 1650, loss = 1.81 (21.5 examples/sec; 5.967 sec/batch)
2017-04-01 17:00:19.467620: step 1660, loss = 1.85 (236.7 examples/sec; 0.541 sec/batch)
2017-04-01 17:00:26.020603: step 1670, loss = 1.88 (195.3 examples/sec; 0.655 sec/batch)
2017-04-01 17:00:32.129221: step 1680, loss = 1.78 (209.5 examples/sec; 0.611 sec/batch)
2017-04-01 17:00:37.645063: step 1690, loss = 2.03 (232.1 examples/sec; 0.552 sec/batch)
2017-04-01 17:00:43.465294: step 1700, loss = 1.98 (219.9 examples/sec; 0.582 sec/batch)
2017-04-01 17:00:49.629897: step 1710, loss = 1.89 (207.6 examples/sec; 0.616 sec/batch)
2017-04-01 17:00:55.937309: step 1720, loss = 1.72 (202.9 examples/sec; 0.631 sec/batch)
2017-04-01 17:01:01.761723: step 1730, loss = 1.86 (219.8 examples/sec; 0.582 sec/batch)
2017-04-01 17:01:08.704911: step 1740, loss = 1.83 (184.4 examples/sec; 0.694 sec/batch)
2017-04-01 17:01:15.076011: step 1750, loss = 1.71 (200.9 examples/sec; 0.637 sec/batch)
2017-04-01 17:01:21.268946: step 1760, loss = 1.78 (206.7 examples/sec; 0.619 sec/batch)
2017-04-01 17:01:27.517667: step 1770, loss = 1.75 (204.8 examples/sec; 0.625 sec/batch)
2017-04-01 17:01:32.002907: step 1780, loss = 1.88 (285.4 examples/sec; 0.449 sec/batch)
2017-04-01 17:01:37.601624: step 1790, loss = 1.94 (228.6 examples/sec; 0.560 sec/batch)
2017-04-01 17:01:43.250440: step 1800, loss = 1.76 (226.6 examples/sec; 0.565 sec/batch)
2017-04-01 17:01:48.615671: step 1810, loss = 1.75 (238.6 examples/sec; 0.537 sec/batch)
2017-04-01 17:01:53.961780: step 1820, loss = 1.66 (239.4 examples/sec; 0.535 sec/batch)
2017-04-01 17:01:59.498362: step 1830, loss = 1.85 (231.2 examples/sec; 0.554 sec/batch)
2017-04-01 17:02:04.966487: step 1840, loss = 1.53 (234.1 examples/sec; 0.547 sec/batch)
2017-04-01 17:02:10.383715: step 1850, loss = 1.65 (236.3 examples/sec; 0.542 sec/batch)
2017-04-01 17:02:15.917525: step 1860, loss = 1.81 (231.3 examples/sec; 0.553 sec/batch)
2017-04-01 17:02:21.382154: step 1870, loss = 1.61 (234.2 examples/sec; 0.546 sec/batch)
2017-04-01 17:02:26.779096: step 1880, loss = 1.84 (237.2 examples/sec; 0.540 sec/batch)
2017-04-01 17:02:32.243100: step 1890, loss = 1.65 (234.3 examples/sec; 0.546 sec/batch)
2017-04-01 17:02:37.700177: step 1900, loss = 1.66 (234.6 examples/sec; 0.546 sec/batch)
2017-04-01 17:02:43.086582: step 1910, loss = 1.79 (237.6 examples/sec; 0.539 sec/batch)
2017-04-01 17:02:48.473142: step 1920, loss = 1.67 (237.6 examples/sec; 0.539 sec/batch)
2017-04-01 17:02:53.885530: step 1930, loss = 1.64 (236.5 examples/sec; 0.541 sec/batch)
2017-04-01 17:02:59.576915: step 1940, loss = 1.56 (224.9 examples/sec; 0.569 sec/batch)
2017-04-01 17:03:05.016597: step 1950, loss = 1.69 (235.3 examples/sec; 0.544 sec/batch)
^Z
[1]  + 31255 suspended  python cifar10_train.py
(tensorflow) ➜  cifar10 git:(master) vim cifar10_eval.py
(tensorflow) ➜  cifar10 git:(master) python cifar10_eval.py
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-04-01 17:03:45.705129: precision @ 1 = 0.644
^Z
[2]  + 32147 suspended  python cifar10_eval.py
(tensorflow) ➜  cifar10 git:(master) ps 
  PID TTY          TIME CMD
 9297 pts/62   00:00:00 zsh
31255 pts/62   00:47:13 python
32147 pts/62   00:00:31 python
32276 pts/62   00:00:00 ps
(tensorflow) ➜  cifar10 git:(master) fg
[2]  - 32147 continued  python cifar10_eval.py
^Z
[2]  + 32147 suspended  python cifar10_eval.py
(tensorflow) ➜  cifar10 git:(master) fg 1
fg: job not found: 1
(tensorflow) ➜  cifar10 git:(master) fg [1]
zsh: no matches found: [1]
(tensorflow) ➜  cifar10 git:(master) fg 31255
fg: job not found: 31255
(tensorflow) ➜  cifar10 git:(master) fg 32147
fg: job not found: 32147
(tensorflow) ➜  cifar10 git:(master) jobs -l
[1]  - 31255 suspended  python cifar10_train.py
[2]  + 32147 suspended  python cifar10_eval.py
(tensorflow) ➜  cifar10 git:(master) fg 31255
fg: job not found: 31255
(tensorflow) ➜  cifar10 git:(master) fg - 31255
fg: job not found: -
(tensorflow) ➜  cifar10 git:(master) fg -31255 
fg: job not found: -31255
(tensorflow) ➜  cifar10 git:(master) fg 1     
fg: job not found: 1
(tensorflow) ➜  cifar10 git:(master) jobs
[1]  - suspended  python cifar10_train.py
[2]  + suspended  python cifar10_eval.py
(tensorflow) ➜  cifar10 git:(master) fg [1]
zsh: no matches found: [1]
(tensorflow) ➜  cifar10 git:(master) bash  
groups: cannot find name for group ID 1104738684
bash: /s/synopsys/@sys/synopsys_env.sh: No such file or directory
[yunhe@rockhopper-06] (1)$ jobs
[yunhe@rockhopper-06] (2)$ jobs
[yunhe@rockhopper-06] (3)$ exit
exit
(tensorflow) ➜  cifar10 git:(master) jobs
[1]  - suspended  python cifar10_train.py
[2]  + suspended  python cifar10_eval.py
(tensorflow) ➜  cifar10 git:(master) fg -
fg: job not found: -
(tensorflow) ➜  cifar10 git:(master) jobs -l
[1]  - 31255 suspended  python cifar10_train.py
[2]  + 32147 suspended  python cifar10_eval.py
(tensorflow) ➜  cifar10 git:(master) fg 31255
fg: job not found: 31255
(tensorflow) ➜  cifar10 git:(master) fg
[2]  - 32147 continued  python cifar10_eval.py
^Z
[2]  + 32147 suspended  python cifar10_eval.py
(tensorflow) ➜  cifar10 git:(master) fg %1    
[1]  - 31255 continued  python cifar10_train.py
2017-04-01 17:12:34.651268: step 1960, loss = 1.61 (2.2 examples/sec; 56.963 sec/batch)
2017-04-01 17:12:39.956149: step 1970, loss = 1.80 (241.3 examples/sec; 0.530 sec/batch)
2017-04-01 17:12:45.268633: step 1980, loss = 1.60 (240.9 examples/sec; 0.531 sec/batch)
2017-04-01 17:12:50.315760: step 1990, loss = 1.59 (253.6 examples/sec; 0.505 sec/batch)
2017-04-01 17:12:55.465543: step 2000, loss = 1.72 (248.6 examples/sec; 0.515 sec/batch)
2017-04-01 17:13:00.691856: step 2010, loss = 1.53 (244.9 examples/sec; 0.523 sec/batch)
2017-04-01 17:13:06.157767: step 2020, loss = 1.74 (234.2 examples/sec; 0.547 sec/batch)
2017-04-01 17:13:12.342798: step 2030, loss = 1.49 (206.9 examples/sec; 0.619 sec/batch)
2017-04-01 17:13:18.005747: step 2040, loss = 1.49 (226.0 examples/sec; 0.566 sec/batch)
2017-04-01 17:13:23.454851: step 2050, loss = 1.50 (234.9 examples/sec; 0.545 sec/batch)
2017-04-01 17:13:28.953055: step 2060, loss = 1.66 (232.8 examples/sec; 0.550 sec/batch)
2017-04-01 17:13:34.689873: step 2070, loss = 1.55 (223.1 examples/sec; 0.574 sec/batch)
2017-04-01 17:13:40.688480: step 2080, loss = 1.75 (213.4 examples/sec; 0.600 sec/batch)
2017-04-01 17:13:46.086412: step 2090, loss = 1.44 (237.1 examples/sec; 0.540 sec/batch)
2017-04-01 17:13:51.154199: step 2100, loss = 1.44 (252.6 examples/sec; 0.507 sec/batch)
2017-04-01 17:13:56.210158: step 2110, loss = 1.51 (253.2 examples/sec; 0.506 sec/batch)
2017-04-01 17:14:00.955265: step 2120, loss = 1.64 (269.8 examples/sec; 0.475 sec/batch)
2017-04-01 17:14:06.571283: step 2130, loss = 1.54 (227.9 examples/sec; 0.562 sec/batch)
2017-04-01 17:14:11.943710: step 2140, loss = 1.74 (238.3 examples/sec; 0.537 sec/batch)
2017-04-01 17:14:17.311220: step 2150, loss = 1.61 (238.5 examples/sec; 0.537 sec/batch)
2017-04-01 17:14:23.048056: step 2160, loss = 1.31 (223.1 examples/sec; 0.574 sec/batch)
2017-04-01 17:14:29.934389: step 2170, loss = 1.65 (185.9 examples/sec; 0.689 sec/batch)
2017-04-01 17:14:36.799830: step 2180, loss = 1.62 (186.4 examples/sec; 0.687 sec/batch)
2017-04-01 17:14:42.348167: step 2190, loss = 1.58 (230.7 examples/sec; 0.555 sec/batch)
2017-04-01 17:14:47.796919: step 2200, loss = 1.56 (234.9 examples/sec; 0.545 sec/batch)
2017-04-01 17:14:53.118749: step 2210, loss = 1.58 (240.5 examples/sec; 0.532 sec/batch)
2017-04-01 17:14:58.935548: step 2220, loss = 1.49 (220.1 examples/sec; 0.582 sec/batch)
2017-04-01 17:15:04.886458: step 2230, loss = 1.44 (215.1 examples/sec; 0.595 sec/batch)
2017-04-01 17:15:10.376215: step 2240, loss = 1.51 (233.2 examples/sec; 0.549 sec/batch)
2017-04-01 17:15:15.754796: step 2250, loss = 1.76 (238.0 examples/sec; 0.538 sec/batch)
2017-04-01 17:15:21.089214: step 2260, loss = 1.49 (240.0 examples/sec; 0.533 sec/batch)
2017-04-01 17:15:26.496528: step 2270, loss = 1.51 (236.7 examples/sec; 0.541 sec/batch)
2017-04-01 17:15:31.921983: step 2280, loss = 1.48 (235.9 examples/sec; 0.543 sec/batch)
2017-04-01 17:15:37.269650: step 2290, loss = 1.48 (239.4 examples/sec; 0.535 sec/batch)
2017-04-01 17:15:42.756605: step 2300, loss = 1.42 (233.3 examples/sec; 0.549 sec/batch)
2017-04-01 17:15:48.292139: step 2310, loss = 1.42 (231.2 examples/sec; 0.554 sec/batch)
2017-04-01 17:15:53.647534: step 2320, loss = 1.56 (239.0 examples/sec; 0.536 sec/batch)
2017-04-01 17:15:58.971610: step 2330, loss = 1.50 (240.4 examples/sec; 0.532 sec/batch)
2017-04-01 17:16:04.271380: step 2340, loss = 1.55 (241.5 examples/sec; 0.530 sec/batch)
2017-04-01 17:16:09.548368: step 2350, loss = 1.39 (242.6 examples/sec; 0.528 sec/batch)
2017-04-01 17:16:14.753473: step 2360, loss = 1.30 (245.9 examples/sec; 0.521 sec/batch)
2017-04-01 17:16:20.082893: step 2370, loss = 1.45 (240.2 examples/sec; 0.533 sec/batch)
2017-04-01 17:16:25.547486: step 2380, loss = 1.46 (234.2 examples/sec; 0.546 sec/batch)
^Z
[1]  + 31255 suspended  python cifar10_train.py
(tensorflow) ➜  cifar10 git:(master) jobs 
[1]  + suspended  python cifar10_train.py
[2]  - suspended  python cifar10_eval.py
(tensorflow) ➜  cifar10 git:(master) fg %2
[2]  - 32147 continued  python cifar10_eval.py
2017-04-01 17:20:51.836949: precision @ 1 = 0.701
^Z
[2]  + 32147 suspended  python cifar10_eval.py
(tensorflow) ➜  cifar10 git:(master) fg %1
[1]  - 31255 continued  python cifar10_train.py
2017-04-01 17:21:28.135197: step 2390, loss = 1.56 (4.2 examples/sec; 30.259 sec/batch)
2017-04-01 17:21:32.511402: step 2400, loss = 1.56 (292.5 examples/sec; 0.438 sec/batch)
2017-04-01 17:21:36.817642: step 2410, loss = 1.46 (297.2 examples/sec; 0.431 sec/batch)
2017-04-01 17:21:41.114162: step 2420, loss = 1.50 (297.9 examples/sec; 0.430 sec/batch)
2017-04-01 17:21:45.460008: step 2430, loss = 1.42 (294.5 examples/sec; 0.435 sec/batch)
2017-04-01 17:21:49.726932: step 2440, loss = 1.38 (300.0 examples/sec; 0.427 sec/batch)
2017-04-01 17:21:54.085543: step 2450, loss = 1.28 (293.7 examples/sec; 0.436 sec/batch)
2017-04-01 17:21:58.416419: step 2460, loss = 1.55 (295.6 examples/sec; 0.433 sec/batch)
2017-04-01 17:22:02.782470: step 2470, loss = 1.56 (293.2 examples/sec; 0.437 sec/batch)
2017-04-01 17:22:07.122528: step 2480, loss = 1.46 (294.9 examples/sec; 0.434 sec/batch)
2017-04-01 17:22:11.408808: step 2490, loss = 1.27 (298.6 examples/sec; 0.429 sec/batch)
2017-04-01 17:22:15.962287: step 2500, loss = 1.67 (281.1 examples/sec; 0.455 sec/batch)
2017-04-01 17:22:20.417065: step 2510, loss = 1.46 (287.3 examples/sec; 0.445 sec/batch)
2017-04-01 17:22:24.832628: step 2520, loss = 1.58 (289.9 examples/sec; 0.442 sec/batch)
2017-04-01 17:22:29.217138: step 2530, loss = 1.40 (291.9 examples/sec; 0.438 sec/batch)
2017-04-01 17:22:33.669942: step 2540, loss = 1.39 (287.5 examples/sec; 0.445 sec/batch)
2017-04-01 17:22:37.966575: step 2550, loss = 1.51 (297.9 examples/sec; 0.430 sec/batch)
2017-04-01 17:22:42.271871: step 2560, loss = 1.41 (297.3 examples/sec; 0.431 sec/batch)
2017-04-01 17:22:46.622619: step 2570, loss = 1.65 (294.2 examples/sec; 0.435 sec/batch)
2017-04-01 17:22:50.904719: step 2580, loss = 1.35 (298.9 examples/sec; 0.428 sec/batch)
2017-04-01 17:22:55.224445: step 2590, loss = 1.54 (296.3 examples/sec; 0.432 sec/batch)
2017-04-01 17:22:59.576104: step 2600, loss = 1.39 (294.1 examples/sec; 0.435 sec/batch)
2017-04-01 17:23:03.876034: step 2610, loss = 1.29 (297.7 examples/sec; 0.430 sec/batch)
2017-04-01 17:23:08.194078: step 2620, loss = 1.51 (296.4 examples/sec; 0.432 sec/batch)
2017-04-01 17:23:12.505797: step 2630, loss = 1.29 (296.9 examples/sec; 0.431 sec/batch)
2017-04-01 17:23:16.882971: step 2640, loss = 1.52 (292.4 examples/sec; 0.438 sec/batch)
2017-04-01 17:23:21.221485: step 2650, loss = 1.33 (295.0 examples/sec; 0.434 sec/batch)
2017-04-01 17:23:25.565586: step 2660, loss = 1.33 (294.7 examples/sec; 0.434 sec/batch)
2017-04-01 17:23:29.918901: step 2670, loss = 1.30 (294.0 examples/sec; 0.435 sec/batch)
2017-04-01 17:23:34.254080: step 2680, loss = 1.36 (295.3 examples/sec; 0.434 sec/batch)
2017-04-01 17:23:38.584377: step 2690, loss = 1.13 (295.6 examples/sec; 0.433 sec/batch)
2017-04-01 17:23:42.990443: step 2700, loss = 1.38 (290.5 examples/sec; 0.441 sec/batch)
2017-04-01 17:23:47.308754: step 2710, loss = 1.43 (296.4 examples/sec; 0.432 sec/batch)
2017-04-01 17:23:51.638984: step 2720, loss = 1.27 (295.6 examples/sec; 0.433 sec/batch)
2017-04-01 17:23:55.985034: step 2730, loss = 1.43 (294.5 examples/sec; 0.435 sec/batch)
2017-04-01 17:24:00.262347: step 2740, loss = 1.23 (299.3 examples/sec; 0.428 sec/batch)
2017-04-01 17:24:04.611686: step 2750, loss = 1.31 (294.3 examples/sec; 0.435 sec/batch)
2017-04-01 17:24:09.029476: step 2760, loss = 1.33 (289.7 examples/sec; 0.442 sec/batch)
2017-04-01 17:24:13.362680: step 2770, loss = 1.10 (295.4 examples/sec; 0.433 sec/batch)
2017-04-01 17:24:17.691825: step 2780, loss = 1.29 (295.7 examples/sec; 0.433 sec/batch)
2017-04-01 17:24:21.977125: step 2790, loss = 1.38 (298.7 examples/sec; 0.429 sec/batch)
2017-04-01 17:24:26.408791: step 2800, loss = 1.17 (288.8 examples/sec; 0.443 sec/batch)
2017-04-01 17:24:30.780601: step 2810, loss = 1.27 (292.8 examples/sec; 0.437 sec/batch)
2017-04-01 17:24:35.163375: step 2820, loss = 1.43 (292.1 examples/sec; 0.438 sec/batch)
2017-04-01 17:24:39.574837: step 2830, loss = 1.58 (290.2 examples/sec; 0.441 sec/batch)
2017-04-01 17:24:43.960970: step 2840, loss = 1.30 (291.8 examples/sec; 0.439 sec/batch)
2017-04-01 17:24:48.301560: step 2850, loss = 1.36 (294.9 examples/sec; 0.434 sec/batch)
2017-04-01 17:24:52.692589: step 2860, loss = 1.34 (291.5 examples/sec; 0.439 sec/batch)
2017-04-01 17:24:57.019790: step 2870, loss = 1.39 (295.8 examples/sec; 0.433 sec/batch)
2017-04-01 17:25:01.402621: step 2880, loss = 1.34 (292.0 examples/sec; 0.438 sec/batch)
2017-04-01 17:25:05.765935: step 2890, loss = 1.35 (293.4 examples/sec; 0.436 sec/batch)
2017-04-01 17:25:10.186750: step 2900, loss = 1.31 (289.5 examples/sec; 0.442 sec/batch)
2017-04-01 17:25:14.538767: step 2910, loss = 1.31 (294.1 examples/sec; 0.435 sec/batch)
2017-04-01 17:25:18.888915: step 2920, loss = 1.21 (294.2 examples/sec; 0.435 sec/batch)
2017-04-01 17:25:23.285423: step 2930, loss = 1.29 (291.1 examples/sec; 0.440 sec/batch)
2017-04-01 17:25:27.666159: step 2940, loss = 1.45 (292.2 examples/sec; 0.438 sec/batch)
2017-04-01 17:25:32.037736: step 2950, loss = 1.24 (292.8 examples/sec; 0.437 sec/batch)
2017-04-01 17:25:36.354311: step 2960, loss = 1.30 (296.5 examples/sec; 0.432 sec/batch)
2017-04-01 17:25:40.653467: step 2970, loss = 1.33 (297.7 examples/sec; 0.430 sec/batch)
2017-04-01 17:25:45.043743: step 2980, loss = 1.19 (291.6 examples/sec; 0.439 sec/batch)
2017-04-01 17:25:49.907297: step 2990, loss = 1.22 (263.2 examples/sec; 0.486 sec/batch)
2017-04-01 17:25:54.876476: step 3000, loss = 1.23 (257.6 examples/sec; 0.497 sec/batch)
2017-04-01 17:25:59.376718: step 3010, loss = 1.33 (284.4 examples/sec; 0.450 sec/batch)
2017-04-01 17:26:03.810686: step 3020, loss = 1.36 (288.7 examples/sec; 0.443 sec/batch)
2017-04-01 17:26:08.224051: step 3030, loss = 1.20 (290.0 examples/sec; 0.441 sec/batch)
2017-04-01 17:26:12.618432: step 3040, loss = 1.17 (291.3 examples/sec; 0.439 sec/batch)
2017-04-01 17:26:16.969433: step 3050, loss = 1.28 (294.2 examples/sec; 0.435 sec/batch)
2017-04-01 17:26:21.338443: step 3060, loss = 1.36 (293.0 examples/sec; 0.437 sec/batch)
2017-04-01 17:26:25.729216: step 3070, loss = 1.27 (291.5 examples/sec; 0.439 sec/batch)
2017-04-01 17:26:30.119621: step 3080, loss = 1.15 (291.5 examples/sec; 0.439 sec/batch)
2017-04-01 17:26:34.523834: step 3090, loss = 1.25 (290.6 examples/sec; 0.440 sec/batch)
2017-04-01 17:26:38.949966: step 3100, loss = 1.26 (289.2 examples/sec; 0.443 sec/batch)
2017-04-01 17:26:43.363807: step 3110, loss = 1.26 (290.0 examples/sec; 0.441 sec/batch)
2017-04-01 17:26:47.692879: step 3120, loss = 1.16 (295.7 examples/sec; 0.433 sec/batch)
2017-04-01 17:26:52.099701: step 3130, loss = 1.32 (290.5 examples/sec; 0.441 sec/batch)
2017-04-01 17:26:56.484863: step 3140, loss = 1.35 (291.9 examples/sec; 0.439 sec/batch)
2017-04-01 17:27:01.142073: step 3150, loss = 1.27 (274.8 examples/sec; 0.466 sec/batch)
2017-04-01 17:27:05.509248: step 3160, loss = 1.38 (293.1 examples/sec; 0.437 sec/batch)
2017-04-01 17:27:10.085238: step 3170, loss = 1.29 (279.7 examples/sec; 0.458 sec/batch)
2017-04-01 17:27:14.557461: step 3180, loss = 1.27 (286.2 examples/sec; 0.447 sec/batch)
2017-04-01 17:27:19.044503: step 3190, loss = 1.30 (285.3 examples/sec; 0.449 sec/batch)
2017-04-01 17:27:23.557363: step 3200, loss = 1.27 (283.6 examples/sec; 0.451 sec/batch)
2017-04-01 17:27:27.927075: step 3210, loss = 1.18 (292.9 examples/sec; 0.437 sec/batch)
2017-04-01 17:27:32.336261: step 3220, loss = 1.25 (290.3 examples/sec; 0.441 sec/batch)
2017-04-01 17:27:36.701542: step 3230, loss = 1.10 (293.2 examples/sec; 0.437 sec/batch)
2017-04-01 17:27:41.062822: step 3240, loss = 1.10 (293.5 examples/sec; 0.436 sec/batch)
2017-04-01 17:27:45.463349: step 3250, loss = 1.21 (290.9 examples/sec; 0.440 sec/batch)
2017-04-01 17:27:49.901335: step 3260, loss = 1.07 (288.4 examples/sec; 0.444 sec/batch)
2017-04-01 17:27:54.353830: step 3270, loss = 1.15 (287.5 examples/sec; 0.445 sec/batch)
2017-04-01 17:27:58.719147: step 3280, loss = 1.32 (293.2 examples/sec; 0.437 sec/batch)
2017-04-01 17:28:03.135987: step 3290, loss = 1.18 (289.8 examples/sec; 0.442 sec/batch)
2017-04-01 17:28:07.621932: step 3300, loss = 1.29 (285.3 examples/sec; 0.449 sec/batch)
2017-04-01 17:28:12.010036: step 3310, loss = 1.37 (291.7 examples/sec; 0.439 sec/batch)
2017-04-01 17:28:16.403208: step 3320, loss = 1.21 (291.4 examples/sec; 0.439 sec/batch)
2017-04-01 17:28:20.803995: step 3330, loss = 1.35 (290.9 examples/sec; 0.440 sec/batch)
2017-04-01 17:28:25.451306: step 3340, loss = 1.13 (275.4 examples/sec; 0.465 sec/batch)
2017-04-01 17:28:29.898515: step 3350, loss = 1.25 (287.8 examples/sec; 0.445 sec/batch)
2017-04-01 17:28:34.217637: step 3360, loss = 1.11 (296.4 examples/sec; 0.432 sec/batch)
2017-04-01 17:28:38.617331: step 3370, loss = 1.26 (290.9 examples/sec; 0.440 sec/batch)
2017-04-01 17:28:43.005169: step 3380, loss = 1.18 (291.7 examples/sec; 0.439 sec/batch)
2017-04-01 17:28:47.405333: step 3390, loss = 1.37 (290.9 examples/sec; 0.440 sec/batch)
2017-04-01 17:28:51.892605: step 3400, loss = 1.08 (285.3 examples/sec; 0.449 sec/batch)
2017-04-01 17:28:56.254245: step 3410, loss = 1.08 (293.5 examples/sec; 0.436 sec/batch)
2017-04-01 17:29:00.744115: step 3420, loss = 1.23 (285.1 examples/sec; 0.449 sec/batch)
2017-04-01 17:29:05.171800: step 3430, loss = 1.24 (289.1 examples/sec; 0.443 sec/batch)
2017-04-01 17:29:09.591631: step 3440, loss = 1.14 (289.6 examples/sec; 0.442 sec/batch)
2017-04-01 17:29:14.086332: step 3450, loss = 1.25 (284.8 examples/sec; 0.449 sec/batch)
2017-04-01 17:29:18.490095: step 3460, loss = 1.10 (290.7 examples/sec; 0.440 sec/batch)
2017-04-01 17:29:22.877143: step 3470, loss = 1.16 (291.8 examples/sec; 0.439 sec/batch)
2017-04-01 17:29:27.285032: step 3480, loss = 1.28 (290.4 examples/sec; 0.441 sec/batch)
2017-04-01 17:29:31.730977: step 3490, loss = 1.13 (287.9 examples/sec; 0.445 sec/batch)
2017-04-01 17:29:36.234749: step 3500, loss = 1.21 (284.2 examples/sec; 0.450 sec/batch)
2017-04-01 17:29:40.637984: step 3510, loss = 1.17 (290.7 examples/sec; 0.440 sec/batch)
2017-04-01 17:29:45.074778: step 3520, loss = 1.09 (288.5 examples/sec; 0.444 sec/batch)
2017-04-01 17:29:49.537818: step 3530, loss = 1.19 (286.8 examples/sec; 0.446 sec/batch)
2017-04-01 17:29:53.914616: step 3540, loss = 1.06 (292.5 examples/sec; 0.438 sec/batch)
2017-04-01 17:29:58.276875: step 3550, loss = 1.27 (293.4 examples/sec; 0.436 sec/batch)
2017-04-01 17:30:02.716796: step 3560, loss = 1.00 (288.3 examples/sec; 0.444 sec/batch)
2017-04-01 17:30:08.068387: step 3570, loss = 1.12 (239.2 examples/sec; 0.535 sec/batch)
2017-04-01 17:30:12.509995: step 3580, loss = 1.18 (288.2 examples/sec; 0.444 sec/batch)
2017-04-01 17:30:16.882503: step 3590, loss = 1.24 (292.7 examples/sec; 0.437 sec/batch)
2017-04-01 17:30:21.342794: step 3600, loss = 1.12 (287.0 examples/sec; 0.446 sec/batch)
2017-04-01 17:30:25.713485: step 3610, loss = 1.26 (292.9 examples/sec; 0.437 sec/batch)
2017-04-01 17:30:30.085719: step 3620, loss = 1.27 (292.8 examples/sec; 0.437 sec/batch)
2017-04-01 17:30:34.493822: step 3630, loss = 1.15 (290.4 examples/sec; 0.441 sec/batch)
2017-04-01 17:30:38.813045: step 3640, loss = 1.01 (296.3 examples/sec; 0.432 sec/batch)
2017-04-01 17:30:43.207139: step 3650, loss = 1.12 (291.3 examples/sec; 0.439 sec/batch)
2017-04-01 17:30:47.565048: step 3660, loss = 1.14 (293.7 examples/sec; 0.436 sec/batch)
2017-04-01 17:30:51.933455: step 3670, loss = 1.13 (293.0 examples/sec; 0.437 sec/batch)
2017-04-01 17:30:56.276694: step 3680, loss = 1.28 (294.7 examples/sec; 0.434 sec/batch)
2017-04-01 17:31:00.679446: step 3690, loss = 1.33 (290.7 examples/sec; 0.440 sec/batch)
2017-04-01 17:31:05.130958: step 3700, loss = 1.05 (287.5 examples/sec; 0.445 sec/batch)
2017-04-01 17:31:09.536079: step 3710, loss = 1.39 (290.6 examples/sec; 0.441 sec/batch)
2017-04-01 17:31:13.927744: step 3720, loss = 1.32 (291.5 examples/sec; 0.439 sec/batch)
2017-04-01 17:31:18.384693: step 3730, loss = 1.07 (287.2 examples/sec; 0.446 sec/batch)
2017-04-01 17:31:22.750733: step 3740, loss = 0.98 (293.2 examples/sec; 0.437 sec/batch)
2017-04-01 17:31:27.093738: step 3750, loss = 1.11 (294.7 examples/sec; 0.434 sec/batch)
2017-04-01 17:31:31.487243: step 3760, loss = 1.12 (291.3 examples/sec; 0.439 sec/batch)
2017-04-01 17:31:35.851744: step 3770, loss = 1.23 (293.3 examples/sec; 0.436 sec/batch)
2017-04-01 17:31:40.175174: step 3780, loss = 1.25 (296.1 examples/sec; 0.432 sec/batch)
2017-04-01 17:31:44.529183: step 3790, loss = 1.18 (294.0 examples/sec; 0.435 sec/batch)
2017-04-01 17:31:48.987361: step 3800, loss = 1.09 (287.1 examples/sec; 0.446 sec/batch)
2017-04-01 17:31:53.303046: step 3810, loss = 1.12 (296.6 examples/sec; 0.432 sec/batch)
2017-04-01 17:31:57.660630: step 3820, loss = 1.01 (293.7 examples/sec; 0.436 sec/batch)
2017-04-01 17:32:02.031243: step 3830, loss = 0.94 (292.9 examples/sec; 0.437 sec/batch)
2017-04-01 17:32:06.480480: step 3840, loss = 1.11 (287.7 examples/sec; 0.445 sec/batch)
2017-04-01 17:32:10.926125: step 3850, loss = 1.11 (287.9 examples/sec; 0.445 sec/batch)
2017-04-01 17:32:15.391493: step 3860, loss = 1.05 (286.7 examples/sec; 0.447 sec/batch)
2017-04-01 17:32:19.917945: step 3870, loss = 1.09 (282.8 examples/sec; 0.453 sec/batch)
2017-04-01 17:32:24.372659: step 3880, loss = 1.00 (287.3 examples/sec; 0.445 sec/batch)
2017-04-01 17:32:28.724581: step 3890, loss = 1.12 (294.1 examples/sec; 0.435 sec/batch)
2017-04-01 17:32:33.210856: step 3900, loss = 1.30 (285.3 examples/sec; 0.449 sec/batch)
2017-04-01 17:32:37.592274: step 3910, loss = 1.19 (292.1 examples/sec; 0.438 sec/batch)
2017-04-01 17:32:41.949952: step 3920, loss = 1.23 (293.7 examples/sec; 0.436 sec/batch)
2017-04-01 17:32:46.328402: step 3930, loss = 0.96 (292.3 examples/sec; 0.438 sec/batch)
2017-04-01 17:32:50.633724: step 3940, loss = 1.13 (297.3 examples/sec; 0.431 sec/batch)
2017-04-01 17:32:54.939498: step 3950, loss = 1.13 (297.3 examples/sec; 0.431 sec/batch)
2017-04-01 17:32:59.276175: step 3960, loss = 1.16 (295.2 examples/sec; 0.434 sec/batch)
2017-04-01 17:33:03.639112: step 3970, loss = 1.17 (293.4 examples/sec; 0.436 sec/batch)
2017-04-01 17:33:07.991825: step 3980, loss = 1.03 (294.1 examples/sec; 0.435 sec/batch)
2017-04-01 17:33:12.348889: step 3990, loss = 1.06 (293.8 examples/sec; 0.436 sec/batch)
2017-04-01 17:33:16.817040: step 4000, loss = 1.01 (286.5 examples/sec; 0.447 sec/batch)
2017-04-01 17:33:21.079953: step 4010, loss = 1.08 (300.3 examples/sec; 0.426 sec/batch)
2017-04-01 17:33:25.398255: step 4020, loss = 1.11 (296.4 examples/sec; 0.432 sec/batch)
2017-04-01 17:33:29.723011: step 4030, loss = 1.26 (296.0 examples/sec; 0.432 sec/batch)
2017-04-01 17:33:33.980532: step 4040, loss = 0.97 (300.6 examples/sec; 0.426 sec/batch)
2017-04-01 17:33:38.289218: step 4050, loss = 1.09 (297.1 examples/sec; 0.431 sec/batch)
2017-04-01 17:33:42.630136: step 4060, loss = 1.27 (294.9 examples/sec; 0.434 sec/batch)
2017-04-01 17:33:46.975632: step 4070, loss = 1.16 (294.6 examples/sec; 0.435 sec/batch)
2017-04-01 17:33:51.343374: step 4080, loss = 1.18 (293.1 examples/sec; 0.437 sec/batch)
2017-04-01 17:33:55.647115: step 4090, loss = 1.19 (297.4 examples/sec; 0.430 sec/batch)
2017-04-01 17:34:00.000767: step 4100, loss = 1.06 (294.0 examples/sec; 0.435 sec/batch)
2017-04-01 17:34:04.303677: step 4110, loss = 0.94 (297.5 examples/sec; 0.430 sec/batch)
2017-04-01 17:34:08.646710: step 4120, loss = 1.00 (294.7 examples/sec; 0.434 sec/batch)
2017-04-01 17:34:12.994690: step 4130, loss = 1.15 (294.4 examples/sec; 0.435 sec/batch)
2017-04-01 17:34:17.317698: step 4140, loss = 1.18 (296.1 examples/sec; 0.432 sec/batch)
2017-04-01 17:34:21.662418: step 4150, loss = 1.04 (294.6 examples/sec; 0.434 sec/batch)
2017-04-01 17:34:26.035958: step 4160, loss = 1.29 (292.7 examples/sec; 0.437 sec/batch)
2017-04-01 17:34:30.385147: step 4170, loss = 1.10 (294.3 examples/sec; 0.435 sec/batch)
2017-04-01 17:34:34.753742: step 4180, loss = 1.08 (293.0 examples/sec; 0.437 sec/batch)
2017-04-01 17:34:39.041495: step 4190, loss = 1.14 (298.5 examples/sec; 0.429 sec/batch)
2017-04-01 17:34:43.427934: step 4200, loss = 1.12 (291.8 examples/sec; 0.439 sec/batch)
2017-04-01 17:34:47.799276: step 4210, loss = 1.14 (292.8 examples/sec; 0.437 sec/batch)
2017-04-01 17:34:52.135025: step 4220, loss = 1.10 (295.2 examples/sec; 0.434 sec/batch)
2017-04-01 17:34:56.494445: step 4230, loss = 1.26 (293.6 examples/sec; 0.436 sec/batch)
2017-04-01 17:35:00.908298: step 4240, loss = 0.97 (290.0 examples/sec; 0.441 sec/batch)
2017-04-01 17:35:05.378623: step 4250, loss = 1.00 (286.3 examples/sec; 0.447 sec/batch)
2017-04-01 17:35:09.834149: step 4260, loss = 0.90 (287.3 examples/sec; 0.446 sec/batch)
2017-04-01 17:35:14.315552: step 4270, loss = 1.10 (285.6 examples/sec; 0.448 sec/batch)
2017-04-01 17:35:18.720757: step 4280, loss = 1.11 (290.6 examples/sec; 0.441 sec/batch)
2017-04-01 17:35:23.060703: step 4290, loss = 1.09 (294.9 examples/sec; 0.434 sec/batch)
2017-04-01 17:35:27.493373: step 4300, loss = 1.14 (288.8 examples/sec; 0.443 sec/batch)
2017-04-01 17:35:31.921486: step 4310, loss = 1.12 (289.1 examples/sec; 0.443 sec/batch)
2017-04-01 17:35:36.348441: step 4320, loss = 1.19 (289.1 examples/sec; 0.443 sec/batch)
2017-04-01 17:35:41.283446: step 4330, loss = 0.98 (259.4 examples/sec; 0.494 sec/batch)
2017-04-01 17:35:46.140806: step 4340, loss = 1.14 (263.5 examples/sec; 0.486 sec/batch)
2017-04-01 17:35:50.898011: step 4350, loss = 1.12 (269.1 examples/sec; 0.476 sec/batch)
2017-04-01 17:35:55.450341: step 4360, loss = 1.14 (281.2 examples/sec; 0.455 sec/batch)
2017-04-01 17:36:00.184873: step 4370, loss = 1.32 (270.4 examples/sec; 0.473 sec/batch)
2017-04-01 17:36:04.480603: step 4380, loss = 1.06 (298.0 examples/sec; 0.430 sec/batch)
2017-04-01 17:36:08.821063: step 4390, loss = 1.10 (294.9 examples/sec; 0.434 sec/batch)
2017-04-01 17:36:13.598890: step 4400, loss = 1.02 (267.9 examples/sec; 0.478 sec/batch)
2017-04-01 17:36:17.916780: step 4410, loss = 1.14 (296.4 examples/sec; 0.432 sec/batch)
2017-04-01 17:36:22.306859: step 4420, loss = 1.01 (291.6 examples/sec; 0.439 sec/batch)
2017-04-01 17:36:26.686809: step 4430, loss = 1.15 (292.2 examples/sec; 0.438 sec/batch)
2017-04-01 17:36:30.994690: step 4440, loss = 0.89 (297.1 examples/sec; 0.431 sec/batch)
2017-04-01 17:36:35.384805: step 4450, loss = 1.07 (291.6 examples/sec; 0.439 sec/batch)
2017-04-01 17:36:39.771378: step 4460, loss = 0.89 (291.8 examples/sec; 0.439 sec/batch)
2017-04-01 17:36:44.150014: step 4470, loss = 0.89 (292.3 examples/sec; 0.438 sec/batch)
2017-04-01 17:36:48.515867: step 4480, loss = 0.98 (293.2 examples/sec; 0.437 sec/batch)
2017-04-01 17:36:52.885973: step 4490, loss = 1.00 (292.9 examples/sec; 0.437 sec/batch)
2017-04-01 17:36:57.334622: step 4500, loss = 0.91 (287.7 examples/sec; 0.445 sec/batch)
2017-04-01 17:37:01.955876: step 4510, loss = 0.98 (277.0 examples/sec; 0.462 sec/batch)
2017-04-01 17:37:06.405815: step 4520, loss = 1.03 (287.6 examples/sec; 0.445 sec/batch)
2017-04-01 17:37:10.808304: step 4530, loss = 0.94 (290.7 examples/sec; 0.440 sec/batch)
2017-04-01 17:37:15.444006: step 4540, loss = 0.89 (276.1 examples/sec; 0.464 sec/batch)
2017-04-01 17:37:19.804685: step 4550, loss = 0.95 (293.5 examples/sec; 0.436 sec/batch)
2017-04-01 17:37:24.133301: step 4560, loss = 1.06 (295.7 examples/sec; 0.433 sec/batch)
2017-04-01 17:37:28.466397: step 4570, loss = 1.07 (295.4 examples/sec; 0.433 sec/batch)
2017-04-01 17:37:32.822760: step 4580, loss = 1.14 (293.8 examples/sec; 0.436 sec/batch)
2017-04-01 17:37:37.236137: step 4590, loss = 1.02 (290.0 examples/sec; 0.441 sec/batch)
2017-04-01 17:37:41.635806: step 4600, loss = 1.11 (290.9 examples/sec; 0.440 sec/batch)
2017-04-01 17:37:46.013891: step 4610, loss = 1.04 (292.4 examples/sec; 0.438 sec/batch)
2017-04-01 17:37:50.327507: step 4620, loss = 0.91 (296.7 examples/sec; 0.431 sec/batch)
2017-04-01 17:37:54.647164: step 4630, loss = 1.09 (296.3 examples/sec; 0.432 sec/batch)
2017-04-01 17:37:58.932475: step 4640, loss = 1.20 (298.7 examples/sec; 0.429 sec/batch)
2017-04-01 17:38:03.260697: step 4650, loss = 0.96 (295.7 examples/sec; 0.433 sec/batch)
2017-04-01 17:38:07.603319: step 4660, loss = 1.13 (294.8 examples/sec; 0.434 sec/batch)
2017-04-01 17:38:12.221769: step 4670, loss = 1.20 (277.1 examples/sec; 0.462 sec/batch)
2017-04-01 17:38:16.566278: step 4680, loss = 1.04 (294.6 examples/sec; 0.434 sec/batch)
2017-04-01 17:38:20.899298: step 4690, loss = 0.97 (295.4 examples/sec; 0.433 sec/batch)
2017-04-01 17:38:25.360737: step 4700, loss = 1.10 (286.9 examples/sec; 0.446 sec/batch)
2017-04-01 17:38:29.735697: step 4710, loss = 0.99 (292.6 examples/sec; 0.437 sec/batch)
2017-04-01 17:38:34.052674: step 4720, loss = 1.10 (296.5 examples/sec; 0.432 sec/batch)
2017-04-01 17:38:38.364341: step 4730, loss = 1.04 (296.9 examples/sec; 0.431 sec/batch)
2017-04-01 17:38:42.676460: step 4740, loss = 0.91 (296.8 examples/sec; 0.431 sec/batch)
2017-04-01 17:38:46.986271: step 4750, loss = 0.91 (297.0 examples/sec; 0.431 sec/batch)
2017-04-01 17:38:51.331004: step 4760, loss = 1.04 (294.6 examples/sec; 0.434 sec/batch)
2017-04-01 17:38:55.625976: step 4770, loss = 1.08 (298.0 examples/sec; 0.429 sec/batch)
2017-04-01 17:38:59.963137: step 4780, loss = 1.03 (295.1 examples/sec; 0.434 sec/batch)
2017-04-01 17:39:04.342237: step 4790, loss = 0.94 (292.3 examples/sec; 0.438 sec/batch)
2017-04-01 17:39:08.737947: step 4800, loss = 1.10 (291.2 examples/sec; 0.440 sec/batch)
2017-04-01 17:39:13.063030: step 4810, loss = 0.94 (295.9 examples/sec; 0.433 sec/batch)
2017-04-01 17:39:17.393581: step 4820, loss = 1.18 (295.6 examples/sec; 0.433 sec/batch)
2017-04-01 17:39:21.699994: step 4830, loss = 1.04 (297.2 examples/sec; 0.431 sec/batch)
2017-04-01 17:39:26.454297: step 4840, loss = 1.08 (269.2 examples/sec; 0.475 sec/batch)
2017-04-01 17:39:30.835744: step 4850, loss = 1.09 (292.1 examples/sec; 0.438 sec/batch)
2017-04-01 17:39:35.552573: step 4860, loss = 0.93 (271.4 examples/sec; 0.472 sec/batch)
2017-04-01 17:39:40.168966: step 4870, loss = 0.92 (277.3 examples/sec; 0.462 sec/batch)
2017-04-01 17:39:44.513026: step 4880, loss = 1.08 (294.7 examples/sec; 0.434 sec/batch)
2017-04-01 17:39:49.161235: step 4890, loss = 0.98 (275.4 examples/sec; 0.465 sec/batch)
2017-04-01 17:39:53.747353: step 4900, loss = 1.28 (279.1 examples/sec; 0.459 sec/batch)
2017-04-01 17:39:58.045726: step 4910, loss = 0.90 (297.8 examples/sec; 0.430 sec/batch)
2017-04-01 17:40:02.577823: step 4920, loss = 1.23 (282.4 examples/sec; 0.453 sec/batch)
2017-04-01 17:40:07.776162: step 4930, loss = 1.15 (246.2 examples/sec; 0.520 sec/batch)
2017-04-01 17:40:13.179971: step 4940, loss = 0.96 (236.9 examples/sec; 0.540 sec/batch)
2017-04-01 17:40:18.621856: step 4950, loss = 0.98 (235.2 examples/sec; 0.544 sec/batch)
2017-04-01 17:40:23.927507: step 4960, loss = 1.09 (241.3 examples/sec; 0.531 sec/batch)
2017-04-01 17:40:29.221751: step 4970, loss = 1.00 (241.8 examples/sec; 0.529 sec/batch)
2017-04-01 17:40:34.577063: step 4980, loss = 1.16 (239.0 examples/sec; 0.536 sec/batch)
2017-04-01 17:40:39.877059: step 4990, loss = 0.94 (241.5 examples/sec; 0.530 sec/batch)
2017-04-01 17:40:45.163281: step 5000, loss = 1.14 (242.1 examples/sec; 0.529 sec/batch)
2017-04-01 17:40:49.585675: step 5010, loss = 1.13 (289.4 examples/sec; 0.442 sec/batch)
2017-04-01 17:40:54.234555: step 5020, loss = 1.18 (275.3 examples/sec; 0.465 sec/batch)
2017-04-01 17:40:59.416602: step 5030, loss = 1.04 (247.0 examples/sec; 0.518 sec/batch)
2017-04-01 17:41:04.044679: step 5040, loss = 1.05 (276.6 examples/sec; 0.463 sec/batch)
2017-04-01 17:41:08.476308: step 5050, loss = 0.95 (288.8 examples/sec; 0.443 sec/batch)
2017-04-01 17:41:12.849225: step 5060, loss = 1.02 (292.7 examples/sec; 0.437 sec/batch)
2017-04-01 17:41:17.196552: step 5070, loss = 1.15 (294.4 examples/sec; 0.435 sec/batch)
2017-04-01 17:41:21.515397: step 5080, loss = 0.79 (296.4 examples/sec; 0.432 sec/batch)
2017-04-01 17:41:25.835194: step 5090, loss = 0.94 (296.3 examples/sec; 0.432 sec/batch)
2017-04-01 17:41:30.234341: step 5100, loss = 1.03 (291.0 examples/sec; 0.440 sec/batch)
2017-04-01 17:41:34.621741: step 5110, loss = 0.88 (291.7 examples/sec; 0.439 sec/batch)
2017-04-01 17:41:38.970865: step 5120, loss = 0.92 (294.3 examples/sec; 0.435 sec/batch)
2017-04-01 17:41:43.305119: step 5130, loss = 0.99 (295.3 examples/sec; 0.433 sec/batch)
^Z
[1]  + 31255 suspended  python cifar10_train.py
(tensorflow) ➜  cifar10 git:(master) fg %2  
[2]  - 32147 continued  python cifar10_eval.py
2017-04-01 17:46:32.970563: precision @ 1 = 0.764
^Z
[2]  + 32147 suspended  python cifar10_eval.py
(tensorflow) ➜  cifar10 git:(master) fg %1            
[1]  - 31255 continued  python cifar10_train.py
2017-04-01 17:48:46.608930: step 5140, loss = 0.93 (3.0 examples/sec; 42.330 sec/batch)
2017-04-01 17:48:50.892474: step 5150, loss = 0.93 (298.8 examples/sec; 0.428 sec/batch)
2017-04-01 17:48:55.209824: step 5160, loss = 1.29 (296.5 examples/sec; 0.432 sec/batch)
2017-04-01 17:48:59.547957: step 5170, loss = 0.86 (295.1 examples/sec; 0.434 sec/batch)
2017-04-01 17:49:03.888835: step 5180, loss = 0.98 (294.9 examples/sec; 0.434 sec/batch)
2017-04-01 17:49:08.243617: step 5190, loss = 1.14 (293.9 examples/sec; 0.435 sec/batch)
2017-04-01 17:49:12.643509: step 5200, loss = 1.04 (290.9 examples/sec; 0.440 sec/batch)
2017-04-01 17:49:17.002469: step 5210, loss = 0.89 (293.6 examples/sec; 0.436 sec/batch)
2017-04-01 17:49:21.384741: step 5220, loss = 0.83 (292.1 examples/sec; 0.438 sec/batch)
2017-04-01 17:49:25.805273: step 5230, loss = 1.12 (289.6 examples/sec; 0.442 sec/batch)
2017-04-01 17:49:30.601789: step 5240, loss = 1.12 (266.9 examples/sec; 0.480 sec/batch)
2017-04-01 17:49:35.584716: step 5250, loss = 0.83 (256.9 examples/sec; 0.498 sec/batch)
2017-04-01 17:49:40.009906: step 5260, loss = 0.91 (289.3 examples/sec; 0.443 sec/batch)
2017-04-01 17:49:44.433076: step 5270, loss = 0.92 (289.4 examples/sec; 0.442 sec/batch)
2017-04-01 17:49:49.069116: step 5280, loss = 0.92 (276.1 examples/sec; 0.464 sec/batch)
2017-04-01 17:49:54.786481: step 5290, loss = 1.04 (223.9 examples/sec; 0.572 sec/batch)
2017-04-01 17:50:00.220799: step 5300, loss = 0.92 (235.5 examples/sec; 0.543 sec/batch)
2017-04-01 17:50:05.493662: step 5310, loss = 1.08 (242.8 examples/sec; 0.527 sec/batch)
2017-04-01 17:50:10.829968: step 5320, loss = 0.92 (239.9 examples/sec; 0.534 sec/batch)
2017-04-01 17:50:16.513607: step 5330, loss = 0.82 (225.2 examples/sec; 0.568 sec/batch)
2017-04-01 17:50:22.303303: step 5340, loss = 1.15 (221.1 examples/sec; 0.579 sec/batch)
2017-04-01 17:50:28.214808: step 5350, loss = 0.94 (216.5 examples/sec; 0.591 sec/batch)
2017-04-01 17:50:33.296911: step 5360, loss = 0.98 (251.9 examples/sec; 0.508 sec/batch)
2017-04-01 17:50:38.104965: step 5370, loss = 1.20 (266.2 examples/sec; 0.481 sec/batch)
2017-04-01 17:50:42.927482: step 5380, loss = 1.13 (265.4 examples/sec; 0.482 sec/batch)
2017-04-01 17:50:48.105580: step 5390, loss = 0.89 (247.2 examples/sec; 0.518 sec/batch)
2017-04-01 17:50:53.602306: step 5400, loss = 0.95 (232.9 examples/sec; 0.550 sec/batch)
2017-04-01 17:50:58.941434: step 5410, loss = 1.07 (239.7 examples/sec; 0.534 sec/batch)
2017-04-01 17:51:05.000825: step 5420, loss = 0.88 (211.2 examples/sec; 0.606 sec/batch)
2017-04-01 17:51:10.374578: step 5430, loss = 0.97 (238.2 examples/sec; 0.537 sec/batch)
2017-04-01 17:51:15.655113: step 5440, loss = 1.05 (242.4 examples/sec; 0.528 sec/batch)
2017-04-01 17:51:20.472955: step 5450, loss = 1.17 (265.7 examples/sec; 0.482 sec/batch)
2017-04-01 17:51:25.141259: step 5460, loss = 1.13 (274.2 examples/sec; 0.467 sec/batch)
2017-04-01 17:51:29.444677: step 5470, loss = 1.00 (297.4 examples/sec; 0.430 sec/batch)
2017-04-01 17:51:33.747017: step 5480, loss = 1.08 (297.5 examples/sec; 0.430 sec/batch)
2017-04-01 17:51:38.040501: step 5490, loss = 1.11 (298.1 examples/sec; 0.429 sec/batch)
2017-04-01 17:51:42.428052: step 5500, loss = 0.94 (291.7 examples/sec; 0.439 sec/batch)
2017-04-01 17:51:46.793950: step 5510, loss = 1.01 (293.2 examples/sec; 0.437 sec/batch)
2017-04-01 17:51:51.255550: step 5520, loss = 1.11 (286.9 examples/sec; 0.446 sec/batch)
2017-04-01 17:51:55.703850: step 5530, loss = 1.07 (287.7 examples/sec; 0.445 sec/batch)
2017-04-01 17:52:00.063434: step 5540, loss = 0.81 (293.6 examples/sec; 0.436 sec/batch)
2017-04-01 17:52:04.533850: step 5550, loss = 0.83 (286.3 examples/sec; 0.447 sec/batch)
2017-04-01 17:52:08.953005: step 5560, loss = 1.16 (289.6 examples/sec; 0.442 sec/batch)
2017-04-01 17:52:14.349647: step 5570, loss = 0.94 (237.2 examples/sec; 0.540 sec/batch)
2017-04-01 17:52:19.558735: step 5580, loss = 0.93 (245.7 examples/sec; 0.521 sec/batch)
2017-04-01 17:52:25.617131: step 5590, loss = 1.04 (211.3 examples/sec; 0.606 sec/batch)
2017-04-01 17:52:30.853809: step 5600, loss = 0.94 (244.4 examples/sec; 0.524 sec/batch)
2017-04-01 17:52:37.038657: step 5610, loss = 0.91 (207.0 examples/sec; 0.618 sec/batch)
2017-04-01 17:52:42.333279: step 5620, loss = 0.86 (241.8 examples/sec; 0.529 sec/batch)
2017-04-01 17:52:48.116392: step 5630, loss = 0.96 (221.3 examples/sec; 0.578 sec/batch)
2017-04-01 17:52:52.989093: step 5640, loss = 1.09 (262.7 examples/sec; 0.487 sec/batch)
2017-04-01 17:52:57.584390: step 5650, loss = 0.98 (278.5 examples/sec; 0.460 sec/batch)
2017-04-01 17:53:01.946397: step 5660, loss = 0.89 (293.4 examples/sec; 0.436 sec/batch)
2017-04-01 17:53:06.309928: step 5670, loss = 0.84 (293.3 examples/sec; 0.436 sec/batch)
2017-04-01 17:53:10.834799: step 5680, loss = 1.05 (282.9 examples/sec; 0.452 sec/batch)
2017-04-01 17:53:15.492325: step 5690, loss = 0.97 (274.8 examples/sec; 0.466 sec/batch)
2017-04-01 17:53:20.801101: step 5700, loss = 1.04 (241.1 examples/sec; 0.531 sec/batch)
2017-04-01 17:53:26.558288: step 5710, loss = 1.11 (222.3 examples/sec; 0.576 sec/batch)
2017-04-01 17:53:31.978296: step 5720, loss = 1.05 (236.2 examples/sec; 0.542 sec/batch)
2017-04-01 17:53:37.600633: step 5730, loss = 0.91 (227.7 examples/sec; 0.562 sec/batch)
2017-04-01 17:53:42.136991: step 5740, loss = 0.75 (282.2 examples/sec; 0.454 sec/batch)
2017-04-01 17:53:47.069058: step 5750, loss = 1.00 (259.5 examples/sec; 0.493 sec/batch)
2017-04-01 17:53:52.966619: step 5760, loss = 1.00 (217.0 examples/sec; 0.590 sec/batch)
2017-04-01 17:53:57.990018: step 5770, loss = 1.00 (254.8 examples/sec; 0.502 sec/batch)
2017-04-01 17:54:03.259681: step 5780, loss = 0.83 (242.9 examples/sec; 0.527 sec/batch)
2017-04-01 17:54:08.787757: step 5790, loss = 1.06 (231.5 examples/sec; 0.553 sec/batch)
2017-04-01 17:54:14.510101: step 5800, loss = 1.03 (223.7 examples/sec; 0.572 sec/batch)
2017-04-01 17:54:19.952429: step 5810, loss = 1.05 (235.2 examples/sec; 0.544 sec/batch)
2017-04-01 17:54:25.385356: step 5820, loss = 1.00 (235.6 examples/sec; 0.543 sec/batch)
2017-04-01 17:54:29.721794: step 5830, loss = 0.81 (295.2 examples/sec; 0.434 sec/batch)
2017-04-01 17:54:34.021646: step 5840, loss = 1.22 (297.7 examples/sec; 0.430 sec/batch)
2017-04-01 17:54:38.363186: step 5850, loss = 0.97 (294.8 examples/sec; 0.434 sec/batch)
2017-04-01 17:54:42.703927: step 5860, loss = 0.95 (294.9 examples/sec; 0.434 sec/batch)
2017-04-01 17:54:47.066720: step 5870, loss = 1.00 (293.4 examples/sec; 0.436 sec/batch)
2017-04-01 17:54:51.445575: step 5880, loss = 0.99 (292.3 examples/sec; 0.438 sec/batch)
2017-04-01 17:54:55.967951: step 5890, loss = 1.01 (283.0 examples/sec; 0.452 sec/batch)
2017-04-01 17:55:00.418225: step 5900, loss = 0.91 (287.6 examples/sec; 0.445 sec/batch)
2017-04-01 17:55:04.819408: step 5910, loss = 0.90 (290.8 examples/sec; 0.440 sec/batch)
2017-04-01 17:55:09.204078: step 5920, loss = 0.91 (291.9 examples/sec; 0.438 sec/batch)
2017-04-01 17:55:13.580746: step 5930, loss = 0.81 (292.5 examples/sec; 0.438 sec/batch)
2017-04-01 17:55:17.920839: step 5940, loss = 1.01 (294.9 examples/sec; 0.434 sec/batch)
2017-04-01 17:55:22.250329: step 5950, loss = 0.93 (295.6 examples/sec; 0.433 sec/batch)
2017-04-01 17:55:26.696092: step 5960, loss = 1.03 (287.9 examples/sec; 0.445 sec/batch)
2017-04-01 17:55:31.060578: step 5970, loss = 1.04 (293.3 examples/sec; 0.436 sec/batch)
2017-04-01 17:55:35.412009: step 5980, loss = 0.99 (294.2 examples/sec; 0.435 sec/batch)
2017-04-01 17:55:39.808905: step 5990, loss = 0.92 (291.1 examples/sec; 0.440 sec/batch)
2017-04-01 17:55:44.334029: step 6000, loss = 1.06 (282.9 examples/sec; 0.453 sec/batch)
2017-04-01 17:55:48.699833: step 6010, loss = 0.83 (293.2 examples/sec; 0.437 sec/batch)
2017-04-01 17:55:53.076996: step 6020, loss = 1.01 (292.4 examples/sec; 0.438 sec/batch)
2017-04-01 17:55:57.632261: step 6030, loss = 0.89 (281.0 examples/sec; 0.456 sec/batch)
2017-04-01 17:56:02.124471: step 6040, loss = 0.91 (284.9 examples/sec; 0.449 sec/batch)
2017-04-01 17:56:06.633567: step 6050, loss = 1.09 (283.9 examples/sec; 0.451 sec/batch)
2017-04-01 17:56:11.011827: step 6060, loss = 1.01 (292.4 examples/sec; 0.438 sec/batch)
2017-04-01 17:56:15.353471: step 6070, loss = 0.97 (294.8 examples/sec; 0.434 sec/batch)
2017-04-01 17:56:19.700005: step 6080, loss = 1.04 (294.5 examples/sec; 0.435 sec/batch)
2017-04-01 17:56:24.025441: step 6090, loss = 0.98 (295.9 examples/sec; 0.433 sec/batch)
2017-04-01 17:56:28.412513: step 6100, loss = 0.92 (291.8 examples/sec; 0.439 sec/batch)
2017-04-01 17:56:32.751039: step 6110, loss = 1.10 (295.0 examples/sec; 0.434 sec/batch)
2017-04-01 17:56:37.071853: step 6120, loss = 1.01 (296.2 examples/sec; 0.432 sec/batch)
2017-04-01 17:56:41.420021: step 6130, loss = 0.95 (294.4 examples/sec; 0.435 sec/batch)
2017-04-01 17:56:46.257184: step 6140, loss = 0.90 (264.6 examples/sec; 0.484 sec/batch)
2017-04-01 17:56:51.121546: step 6150, loss = 0.87 (263.1 examples/sec; 0.486 sec/batch)
2017-04-01 17:56:56.130360: step 6160, loss = 1.05 (255.5 examples/sec; 0.501 sec/batch)
2017-04-01 17:57:01.182438: step 6170, loss = 0.99 (253.4 examples/sec; 0.505 sec/batch)
2017-04-01 17:57:06.835929: step 6180, loss = 0.95 (226.4 examples/sec; 0.565 sec/batch)
2017-04-01 17:57:11.891941: step 6190, loss = 0.85 (253.2 examples/sec; 0.506 sec/batch)
2017-04-01 17:57:16.753967: step 6200, loss = 0.91 (263.3 examples/sec; 0.486 sec/batch)
2017-04-01 17:57:21.106388: step 6210, loss = 0.88 (294.1 examples/sec; 0.435 sec/batch)
2017-04-01 17:57:25.635060: step 6220, loss = 0.95 (282.6 examples/sec; 0.453 sec/batch)
2017-04-01 17:57:30.075213: step 6230, loss = 0.94 (288.3 examples/sec; 0.444 sec/batch)
2017-04-01 17:57:34.481136: step 6240, loss = 1.11 (290.5 examples/sec; 0.441 sec/batch)
2017-04-01 17:57:38.858804: step 6250, loss = 0.88 (292.4 examples/sec; 0.438 sec/batch)
2017-04-01 17:57:43.173332: step 6260, loss = 1.14 (296.7 examples/sec; 0.431 sec/batch)
2017-04-01 17:57:47.522061: step 6270, loss = 1.16 (294.3 examples/sec; 0.435 sec/batch)
2017-04-01 17:57:51.898636: step 6280, loss = 0.81 (292.5 examples/sec; 0.438 sec/batch)
2017-04-01 17:57:56.331125: step 6290, loss = 1.02 (288.8 examples/sec; 0.443 sec/batch)
2017-04-01 17:58:01.528519: step 6300, loss = 0.99 (246.3 examples/sec; 0.520 sec/batch)
2017-04-01 17:58:05.886961: step 6310, loss = 1.01 (293.7 examples/sec; 0.436 sec/batch)
2017-04-01 17:58:10.852916: step 6320, loss = 0.88 (257.8 examples/sec; 0.497 sec/batch)
2017-04-01 17:58:16.476800: step 6330, loss = 1.00 (227.6 examples/sec; 0.562 sec/batch)
2017-04-01 17:58:21.651342: step 6340, loss = 0.88 (247.4 examples/sec; 0.517 sec/batch)
2017-04-01 17:58:26.894677: step 6350, loss = 0.79 (244.1 examples/sec; 0.524 sec/batch)
2017-04-01 17:58:31.615532: step 6360, loss = 0.77 (271.1 examples/sec; 0.472 sec/batch)
2017-04-01 17:58:36.693194: step 6370, loss = 0.99 (252.1 examples/sec; 0.508 sec/batch)
2017-04-01 17:58:41.110385: step 6380, loss = 0.93 (289.8 examples/sec; 0.442 sec/batch)
2017-04-01 17:58:45.565361: step 6390, loss = 0.89 (287.3 examples/sec; 0.445 sec/batch)
2017-04-01 17:58:50.044396: step 6400, loss = 1.05 (285.8 examples/sec; 0.448 sec/batch)
2017-04-01 17:58:54.511972: step 6410, loss = 0.89 (286.5 examples/sec; 0.447 sec/batch)
2017-04-01 17:58:58.902495: step 6420, loss = 0.89 (291.5 examples/sec; 0.439 sec/batch)
2017-04-01 17:59:03.347543: step 6430, loss = 0.86 (288.0 examples/sec; 0.445 sec/batch)
2017-04-01 17:59:07.747837: step 6440, loss = 0.96 (290.9 examples/sec; 0.440 sec/batch)
2017-04-01 17:59:12.185233: step 6450, loss = 0.84 (288.5 examples/sec; 0.444 sec/batch)
2017-04-01 17:59:16.564260: step 6460, loss = 0.77 (292.3 examples/sec; 0.438 sec/batch)
2017-04-01 17:59:21.237098: step 6470, loss = 1.05 (273.9 examples/sec; 0.467 sec/batch)
2017-04-01 17:59:25.702346: step 6480, loss = 1.02 (286.7 examples/sec; 0.447 sec/batch)
2017-04-01 17:59:30.100045: step 6490, loss = 0.97 (291.1 examples/sec; 0.440 sec/batch)
2017-04-01 17:59:34.633673: step 6500, loss = 1.03 (282.3 examples/sec; 0.453 sec/batch)
2017-04-01 17:59:38.995568: step 6510, loss = 0.88 (293.5 examples/sec; 0.436 sec/batch)
2017-04-01 17:59:43.843981: step 6520, loss = 1.14 (264.0 examples/sec; 0.485 sec/batch)
2017-04-01 17:59:48.418761: step 6530, loss = 1.05 (279.8 examples/sec; 0.457 sec/batch)
2017-04-01 17:59:53.122321: step 6540, loss = 0.90 (272.1 examples/sec; 0.470 sec/batch)
2017-04-01 17:59:57.820191: step 6550, loss = 0.89 (272.5 examples/sec; 0.470 sec/batch)
2017-04-01 18:00:02.220604: step 6560, loss = 0.94 (290.9 examples/sec; 0.440 sec/batch)
2017-04-01 18:00:06.603521: step 6570, loss = 1.05 (292.0 examples/sec; 0.438 sec/batch)
2017-04-01 18:00:10.992294: step 6580, loss = 1.13 (291.7 examples/sec; 0.439 sec/batch)
2017-04-01 18:00:15.432086: step 6590, loss = 0.81 (288.3 examples/sec; 0.444 sec/batch)
2017-04-01 18:00:20.430025: step 6600, loss = 0.90 (256.1 examples/sec; 0.500 sec/batch)
2017-04-01 18:00:24.935332: step 6610, loss = 0.89 (284.1 examples/sec; 0.451 sec/batch)
2017-04-01 18:00:29.611104: step 6620, loss = 0.81 (273.8 examples/sec; 0.468 sec/batch)
2017-04-01 18:00:34.117516: step 6630, loss = 0.91 (284.0 examples/sec; 0.451 sec/batch)
2017-04-01 18:00:38.631348: step 6640, loss = 1.01 (283.6 examples/sec; 0.451 sec/batch)
2017-04-01 18:00:42.990018: step 6650, loss = 1.06 (293.7 examples/sec; 0.436 sec/batch)
2017-04-01 18:00:47.364046: step 6660, loss = 0.78 (292.6 examples/sec; 0.437 sec/batch)
2017-04-01 18:00:51.714035: step 6670, loss = 0.88 (294.3 examples/sec; 0.435 sec/batch)
2017-04-01 18:00:56.084382: step 6680, loss = 0.90 (292.9 examples/sec; 0.437 sec/batch)
2017-04-01 18:01:00.397042: step 6690, loss = 0.85 (296.8 examples/sec; 0.431 sec/batch)
2017-04-01 18:01:04.810176: step 6700, loss = 0.88 (290.0 examples/sec; 0.441 sec/batch)
2017-04-01 18:01:09.135907: step 6710, loss = 0.84 (295.9 examples/sec; 0.433 sec/batch)
2017-04-01 18:01:13.435896: step 6720, loss = 0.99 (297.7 examples/sec; 0.430 sec/batch)
2017-04-01 18:01:18.156275: step 6730, loss = 1.10 (271.2 examples/sec; 0.472 sec/batch)
2017-04-01 18:01:22.707918: step 6740, loss = 0.84 (281.2 examples/sec; 0.455 sec/batch)
2017-04-01 18:01:27.005230: step 6750, loss = 1.00 (297.9 examples/sec; 0.430 sec/batch)
2017-04-01 18:01:31.327122: step 6760, loss = 1.04 (296.2 examples/sec; 0.432 sec/batch)
2017-04-01 18:01:35.585425: step 6770, loss = 1.02 (300.6 examples/sec; 0.426 sec/batch)
2017-04-01 18:01:39.887280: step 6780, loss = 0.93 (297.5 examples/sec; 0.430 sec/batch)
2017-04-01 18:01:44.233754: step 6790, loss = 0.89 (294.5 examples/sec; 0.435 sec/batch)
2017-04-01 18:01:49.211509: step 6800, loss = 0.95 (257.1 examples/sec; 0.498 sec/batch)
2017-04-01 18:01:53.598455: step 6810, loss = 0.88 (291.8 examples/sec; 0.439 sec/batch)
2017-04-01 18:01:57.916207: step 6820, loss = 0.96 (296.5 examples/sec; 0.432 sec/batch)
2017-04-01 18:02:02.504570: step 6830, loss = 0.93 (279.0 examples/sec; 0.459 sec/batch)
2017-04-01 18:02:07.050536: step 6840, loss = 0.99 (281.6 examples/sec; 0.455 sec/batch)
2017-04-01 18:02:12.221008: step 6850, loss = 1.07 (247.6 examples/sec; 0.517 sec/batch)
2017-04-01 18:02:17.337380: step 6860, loss = 0.94 (250.2 examples/sec; 0.512 sec/batch)
2017-04-01 18:02:22.072109: step 6870, loss = 1.02 (270.3 examples/sec; 0.473 sec/batch)
2017-04-01 18:02:26.511444: step 6880, loss = 0.82 (288.3 examples/sec; 0.444 sec/batch)
2017-04-01 18:02:30.976977: step 6890, loss = 1.36 (286.6 examples/sec; 0.447 sec/batch)
2017-04-01 18:02:35.399052: step 6900, loss = 0.75 (289.5 examples/sec; 0.442 sec/batch)
2017-04-01 18:02:39.728020: step 6910, loss = 0.96 (295.7 examples/sec; 0.433 sec/batch)
2017-04-01 18:02:44.096750: step 6920, loss = 0.94 (293.0 examples/sec; 0.437 sec/batch)
2017-04-01 18:02:48.690303: step 6930, loss = 0.92 (278.7 examples/sec; 0.459 sec/batch)
2017-04-01 18:02:53.779928: step 6940, loss = 0.91 (251.5 examples/sec; 0.509 sec/batch)
2017-04-01 18:02:58.901023: step 6950, loss = 0.81 (249.9 examples/sec; 0.512 sec/batch)
2017-04-01 18:03:03.685898: step 6960, loss = 0.99 (267.5 examples/sec; 0.478 sec/batch)
2017-04-01 18:03:09.129882: step 6970, loss = 0.98 (235.1 examples/sec; 0.544 sec/batch)
2017-04-01 18:03:14.735740: step 6980, loss = 0.83 (228.3 examples/sec; 0.561 sec/batch)
2017-04-01 18:03:19.368529: step 6990, loss = 0.93 (276.3 examples/sec; 0.463 sec/batch)
2017-04-01 18:03:24.050414: step 7000, loss = 0.84 (273.4 examples/sec; 0.468 sec/batch)
2017-04-01 18:03:29.384598: step 7010, loss = 0.86 (240.0 examples/sec; 0.533 sec/batch)
2017-04-01 18:03:34.809438: step 7020, loss = 0.77 (236.0 examples/sec; 0.542 sec/batch)
2017-04-01 18:03:40.104609: step 7030, loss = 1.12 (241.7 examples/sec; 0.530 sec/batch)
2017-04-01 18:03:45.937149: step 7040, loss = 1.00 (219.5 examples/sec; 0.583 sec/batch)
2017-04-01 18:03:51.657805: step 7050, loss = 0.88 (223.8 examples/sec; 0.572 sec/batch)
2017-04-01 18:03:57.025753: step 7060, loss = 1.15 (238.5 examples/sec; 0.537 sec/batch)
2017-04-01 18:04:02.359664: step 7070, loss = 0.93 (240.0 examples/sec; 0.533 sec/batch)
2017-04-01 18:04:07.694605: step 7080, loss = 1.09 (239.9 examples/sec; 0.533 sec/batch)
2017-04-01 18:04:13.049682: step 7090, loss = 0.75 (239.0 examples/sec; 0.536 sec/batch)
2017-04-01 18:04:18.490896: step 7100, loss = 0.95 (235.2 examples/sec; 0.544 sec/batch)
2017-04-01 18:04:23.817009: step 7110, loss = 1.03 (240.3 examples/sec; 0.533 sec/batch)
2017-04-01 18:04:29.194240: step 7120, loss = 0.99 (238.0 examples/sec; 0.538 sec/batch)
2017-04-01 18:04:34.560460: step 7130, loss = 0.91 (238.5 examples/sec; 0.537 sec/batch)
2017-04-01 18:04:39.952770: step 7140, loss = 0.89 (237.4 examples/sec; 0.539 sec/batch)
2017-04-01 18:04:45.302425: step 7150, loss = 0.92 (239.3 examples/sec; 0.535 sec/batch)
2017-04-01 18:04:50.744800: step 7160, loss = 0.98 (235.2 examples/sec; 0.544 sec/batch)
2017-04-01 18:04:56.016305: step 7170, loss = 0.92 (242.8 examples/sec; 0.527 sec/batch)
2017-04-01 18:05:01.339682: step 7180, loss = 0.90 (240.4 examples/sec; 0.532 sec/batch)
2017-04-01 18:05:06.676617: step 7190, loss = 0.76 (239.8 examples/sec; 0.534 sec/batch)
2017-04-01 18:05:12.134410: step 7200, loss = 0.77 (234.5 examples/sec; 0.546 sec/batch)
2017-04-01 18:05:17.458932: step 7210, loss = 0.87 (240.4 examples/sec; 0.532 sec/batch)
2017-04-01 18:05:22.740808: step 7220, loss = 0.79 (242.3 examples/sec; 0.528 sec/batch)
2017-04-01 18:05:28.000956: step 7230, loss = 0.79 (243.3 examples/sec; 0.526 sec/batch)
2017-04-01 18:05:33.333371: step 7240, loss = 0.96 (240.0 examples/sec; 0.533 sec/batch)
2017-04-01 18:05:38.616294: step 7250, loss = 0.86 (242.3 examples/sec; 0.528 sec/batch)
2017-04-01 18:05:43.927691: step 7260, loss = 0.87 (241.0 examples/sec; 0.531 sec/batch)
2017-04-01 18:05:49.396096: step 7270, loss = 0.86 (234.1 examples/sec; 0.547 sec/batch)
2017-04-01 18:05:54.675993: step 7280, loss = 0.95 (242.4 examples/sec; 0.528 sec/batch)
2017-04-01 18:06:00.021433: step 7290, loss = 1.07 (239.5 examples/sec; 0.535 sec/batch)
2017-04-01 18:06:05.706928: step 7300, loss = 1.00 (225.1 examples/sec; 0.569 sec/batch)
2017-04-01 18:06:11.786723: step 7310, loss = 0.98 (210.5 examples/sec; 0.608 sec/batch)
2017-04-01 18:06:18.153624: step 7320, loss = 1.05 (201.0 examples/sec; 0.637 sec/batch)
2017-04-01 18:06:23.220438: step 7330, loss = 1.01 (252.6 examples/sec; 0.507 sec/batch)
2017-04-01 18:06:28.791142: step 7340, loss = 0.87 (229.8 examples/sec; 0.557 sec/batch)
2017-04-01 18:06:34.513172: step 7350, loss = 1.03 (223.7 examples/sec; 0.572 sec/batch)
2017-04-01 18:06:39.786626: step 7360, loss = 0.86 (242.7 examples/sec; 0.527 sec/batch)
2017-04-01 18:06:45.153983: step 7370, loss = 0.84 (238.5 examples/sec; 0.537 sec/batch)
2017-04-01 18:06:50.489606: step 7380, loss = 0.94 (239.9 examples/sec; 0.534 sec/batch)
2017-04-01 18:06:55.854252: step 7390, loss = 1.07 (238.6 examples/sec; 0.536 sec/batch)
2017-04-01 18:07:01.294233: step 7400, loss = 0.96 (235.3 examples/sec; 0.544 sec/batch)
2017-04-01 18:07:06.784458: step 7410, loss = 0.81 (233.1 examples/sec; 0.549 sec/batch)
2017-04-01 18:07:11.615669: step 7420, loss = 0.93 (264.9 examples/sec; 0.483 sec/batch)
2017-04-01 18:07:17.145315: step 7430, loss = 0.85 (231.5 examples/sec; 0.553 sec/batch)
2017-04-01 18:07:23.841952: step 7440, loss = 0.87 (191.1 examples/sec; 0.670 sec/batch)
2017-04-01 18:07:28.384705: step 7450, loss = 0.89 (281.8 examples/sec; 0.454 sec/batch)
2017-04-01 18:07:33.511141: step 7460, loss = 1.07 (249.7 examples/sec; 0.513 sec/batch)
2017-04-01 18:07:38.788309: step 7470, loss = 1.06 (242.6 examples/sec; 0.528 sec/batch)
2017-04-01 18:07:43.077826: step 7480, loss = 1.05 (298.4 examples/sec; 0.429 sec/batch)
2017-04-01 18:07:47.366341: step 7490, loss = 0.72 (298.5 examples/sec; 0.429 sec/batch)
2017-04-01 18:07:51.695919: step 7500, loss = 1.16 (295.6 examples/sec; 0.433 sec/batch)
2017-04-01 18:07:55.975568: step 7510, loss = 0.85 (299.1 examples/sec; 0.428 sec/batch)
2017-04-01 18:08:00.282783: step 7520, loss = 0.88 (297.2 examples/sec; 0.431 sec/batch)
2017-04-01 18:08:04.590033: step 7530, loss = 0.84 (297.2 examples/sec; 0.431 sec/batch)
2017-04-01 18:08:08.915275: step 7540, loss = 0.94 (295.9 examples/sec; 0.433 sec/batch)
2017-04-01 18:08:13.286307: step 7550, loss = 0.99 (292.8 examples/sec; 0.437 sec/batch)
2017-04-01 18:08:17.635054: step 7560, loss = 0.89 (294.3 examples/sec; 0.435 sec/batch)
2017-04-01 18:08:21.963009: step 7570, loss = 0.90 (295.8 examples/sec; 0.433 sec/batch)
2017-04-01 18:08:26.268236: step 7580, loss = 1.12 (297.3 examples/sec; 0.431 sec/batch)
2017-04-01 18:08:30.570960: step 7590, loss = 0.75 (297.5 examples/sec; 0.430 sec/batch)
2017-04-01 18:08:34.933617: step 7600, loss = 0.73 (293.4 examples/sec; 0.436 sec/batch)
2017-04-01 18:08:39.275973: step 7610, loss = 0.83 (294.8 examples/sec; 0.434 sec/batch)
2017-04-01 18:08:43.558141: step 7620, loss = 0.99 (298.9 examples/sec; 0.428 sec/batch)
2017-04-01 18:08:47.903820: step 7630, loss = 0.98 (294.5 examples/sec; 0.435 sec/batch)
2017-04-01 18:08:52.151919: step 7640, loss = 0.80 (301.3 examples/sec; 0.425 sec/batch)
2017-04-01 18:08:56.466011: step 7650, loss = 0.83 (296.7 examples/sec; 0.431 sec/batch)
2017-04-01 18:09:00.874134: step 7660, loss = 1.03 (290.4 examples/sec; 0.441 sec/batch)
2017-04-01 18:09:05.310476: step 7670, loss = 0.90 (288.5 examples/sec; 0.444 sec/batch)
2017-04-01 18:09:09.699130: step 7680, loss = 0.96 (291.7 examples/sec; 0.439 sec/batch)
2017-04-01 18:09:13.969054: step 7690, loss = 0.93 (299.8 examples/sec; 0.427 sec/batch)
2017-04-01 18:09:18.298597: step 7700, loss = 0.82 (295.6 examples/sec; 0.433 sec/batch)
2017-04-01 18:09:22.580376: step 7710, loss = 0.99 (298.9 examples/sec; 0.428 sec/batch)
2017-04-01 18:09:26.915808: step 7720, loss = 0.88 (295.2 examples/sec; 0.434 sec/batch)
2017-04-01 18:09:31.253308: step 7730, loss = 0.87 (295.1 examples/sec; 0.434 sec/batch)
2017-04-01 18:09:35.914805: step 7740, loss = 1.02 (274.6 examples/sec; 0.466 sec/batch)
2017-04-01 18:09:40.660871: step 7750, loss = 0.93 (269.7 examples/sec; 0.475 sec/batch)
2017-04-01 18:09:44.950167: step 7760, loss = 0.88 (298.4 examples/sec; 0.429 sec/batch)
2017-04-01 18:09:49.288358: step 7770, loss = 1.05 (295.1 examples/sec; 0.434 sec/batch)
2017-04-01 18:09:53.765603: step 7780, loss = 0.88 (285.9 examples/sec; 0.448 sec/batch)
2017-04-01 18:09:58.035321: step 7790, loss = 1.01 (299.8 examples/sec; 0.427 sec/batch)
2017-04-01 18:10:02.450475: step 7800, loss = 0.89 (289.9 examples/sec; 0.442 sec/batch)
2017-04-01 18:10:06.807871: step 7810, loss = 0.83 (293.8 examples/sec; 0.436 sec/batch)
2017-04-01 18:10:11.173363: step 7820, loss = 1.14 (293.2 examples/sec; 0.437 sec/batch)
2017-04-01 18:10:15.602356: step 7830, loss = 0.70 (289.0 examples/sec; 0.443 sec/batch)
2017-04-01 18:10:19.984800: step 7840, loss = 0.88 (292.1 examples/sec; 0.438 sec/batch)
2017-04-01 18:10:24.333962: step 7850, loss = 0.88 (294.3 examples/sec; 0.435 sec/batch)
2017-04-01 18:10:28.669177: step 7860, loss = 0.78 (295.3 examples/sec; 0.434 sec/batch)
2017-04-01 18:10:33.034490: step 7870, loss = 0.87 (293.2 examples/sec; 0.437 sec/batch)
2017-04-01 18:10:37.334670: step 7880, loss = 0.81 (297.7 examples/sec; 0.430 sec/batch)
2017-04-01 18:10:41.653904: step 7890, loss = 0.93 (296.3 examples/sec; 0.432 sec/batch)
2017-04-01 18:10:46.016032: step 7900, loss = 0.94 (293.4 examples/sec; 0.436 sec/batch)
2017-04-01 18:10:50.397517: step 7910, loss = 0.96 (292.1 examples/sec; 0.438 sec/batch)
2017-04-01 18:10:55.059269: step 7920, loss = 0.80 (274.6 examples/sec; 0.466 sec/batch)
2017-04-01 18:10:59.587354: step 7930, loss = 0.89 (282.7 examples/sec; 0.453 sec/batch)
2017-04-01 18:11:04.146396: step 7940, loss = 1.03 (280.8 examples/sec; 0.456 sec/batch)
2017-04-01 18:11:08.666735: step 7950, loss = 0.78 (283.2 examples/sec; 0.452 sec/batch)
2017-04-01 18:11:12.989232: step 7960, loss = 0.91 (296.1 examples/sec; 0.432 sec/batch)
2017-04-01 18:11:17.343792: step 7970, loss = 0.80 (293.9 examples/sec; 0.435 sec/batch)
2017-04-01 18:11:21.768147: step 7980, loss = 0.92 (289.3 examples/sec; 0.442 sec/batch)
2017-04-01 18:11:26.177345: step 7990, loss = 0.98 (290.3 examples/sec; 0.441 sec/batch)
2017-04-01 18:11:30.687543: step 8000, loss = 0.97 (283.8 examples/sec; 0.451 sec/batch)
2017-04-01 18:11:35.100678: step 8010, loss = 0.87 (290.0 examples/sec; 0.441 sec/batch)
2017-04-01 18:11:39.426727: step 8020, loss = 0.81 (295.9 examples/sec; 0.433 sec/batch)
2017-04-01 18:11:43.797437: step 8030, loss = 0.75 (292.9 examples/sec; 0.437 sec/batch)
2017-04-01 18:11:48.167071: step 8040, loss = 0.89 (292.9 examples/sec; 0.437 sec/batch)
2017-04-01 18:11:52.583087: step 8050, loss = 0.92 (289.9 examples/sec; 0.442 sec/batch)
2017-04-01 18:11:56.915835: step 8060, loss = 0.81 (295.4 examples/sec; 0.433 sec/batch)
2017-04-01 18:12:01.874130: step 8070, loss = 0.78 (258.2 examples/sec; 0.496 sec/batch)
2017-04-01 18:12:07.278149: step 8080, loss = 0.84 (236.9 examples/sec; 0.540 sec/batch)
2017-04-01 18:12:11.869643: step 8090, loss = 0.91 (278.8 examples/sec; 0.459 sec/batch)
2017-04-01 18:12:16.533815: step 8100, loss = 0.90 (274.4 examples/sec; 0.466 sec/batch)
2017-04-01 18:12:20.887206: step 8110, loss = 0.94 (294.0 examples/sec; 0.435 sec/batch)
2017-04-01 18:12:25.521611: step 8120, loss = 0.98 (276.2 examples/sec; 0.463 sec/batch)
2017-04-01 18:12:30.365001: step 8130, loss = 0.97 (264.3 examples/sec; 0.484 sec/batch)
2017-04-01 18:12:34.892256: step 8140, loss = 0.81 (282.7 examples/sec; 0.453 sec/batch)
2017-04-01 18:12:39.281056: step 8150, loss = 1.01 (291.7 examples/sec; 0.439 sec/batch)
2017-04-01 18:12:43.776296: step 8160, loss = 0.84 (284.7 examples/sec; 0.450 sec/batch)
2017-04-01 18:12:48.141400: step 8170, loss = 0.90 (293.2 examples/sec; 0.437 sec/batch)
2017-04-01 18:12:52.454770: step 8180, loss = 1.00 (296.8 examples/sec; 0.431 sec/batch)
2017-04-01 18:12:56.947964: step 8190, loss = 0.79 (284.9 examples/sec; 0.449 sec/batch)
2017-04-01 18:13:01.456271: step 8200, loss = 0.87 (283.9 examples/sec; 0.451 sec/batch)
2017-04-01 18:13:05.931332: step 8210, loss = 0.86 (286.0 examples/sec; 0.448 sec/batch)
2017-04-01 18:13:10.281922: step 8220, loss = 1.00 (294.2 examples/sec; 0.435 sec/batch)
2017-04-01 18:13:14.736820: step 8230, loss = 0.86 (287.3 examples/sec; 0.445 sec/batch)
2017-04-01 18:13:19.289017: step 8240, loss = 0.94 (281.2 examples/sec; 0.455 sec/batch)
2017-04-01 18:13:23.708851: step 8250, loss = 0.80 (289.6 examples/sec; 0.442 sec/batch)
2017-04-01 18:13:28.221811: step 8260, loss = 0.87 (283.6 examples/sec; 0.451 sec/batch)
2017-04-01 18:13:32.587924: step 8270, loss = 0.99 (293.2 examples/sec; 0.437 sec/batch)
2017-04-01 18:13:37.085821: step 8280, loss = 0.95 (284.6 examples/sec; 0.450 sec/batch)
2017-04-01 18:13:41.646928: step 8290, loss = 0.97 (280.6 examples/sec; 0.456 sec/batch)
2017-04-01 18:13:46.152670: step 8300, loss = 0.86 (284.1 examples/sec; 0.451 sec/batch)
2017-04-01 18:13:50.858188: step 8310, loss = 0.98 (272.0 examples/sec; 0.471 sec/batch)
2017-04-01 18:13:55.462319: step 8320, loss = 1.02 (278.0 examples/sec; 0.460 sec/batch)
2017-04-01 18:14:00.085794: step 8330, loss = 0.89 (276.8 examples/sec; 0.462 sec/batch)
2017-04-01 18:14:04.554136: step 8340, loss = 0.96 (286.5 examples/sec; 0.447 sec/batch)
2017-04-01 18:14:08.921850: step 8350, loss = 0.99 (293.1 examples/sec; 0.437 sec/batch)
2017-04-01 18:14:13.782028: step 8360, loss = 0.90 (263.4 examples/sec; 0.486 sec/batch)
2017-04-01 18:14:18.501183: step 8370, loss = 1.07 (271.2 examples/sec; 0.472 sec/batch)
2017-04-01 18:14:22.948062: step 8380, loss = 0.68 (287.8 examples/sec; 0.445 sec/batch)
2017-04-01 18:14:27.866374: step 8390, loss = 0.85 (260.3 examples/sec; 0.492 sec/batch)
2017-04-01 18:14:32.420559: step 8400, loss = 0.93 (281.1 examples/sec; 0.455 sec/batch)
2017-04-01 18:14:36.830726: step 8410, loss = 0.66 (290.2 examples/sec; 0.441 sec/batch)
2017-04-01 18:14:41.385312: step 8420, loss = 0.70 (281.0 examples/sec; 0.455 sec/batch)
2017-04-01 18:14:45.864436: step 8430, loss = 0.88 (285.8 examples/sec; 0.448 sec/batch)
2017-04-01 18:14:50.404130: step 8440, loss = 1.03 (282.0 examples/sec; 0.454 sec/batch)
2017-04-01 18:14:54.729859: step 8450, loss = 1.02 (295.9 examples/sec; 0.433 sec/batch)
2017-04-01 18:14:59.154313: step 8460, loss = 0.99 (289.3 examples/sec; 0.442 sec/batch)
2017-04-01 18:15:03.568639: step 8470, loss = 0.90 (290.0 examples/sec; 0.441 sec/batch)
2017-04-01 18:15:08.421525: step 8480, loss = 0.74 (263.8 examples/sec; 0.485 sec/batch)
2017-04-01 18:15:12.843034: step 8490, loss = 0.91 (289.5 examples/sec; 0.442 sec/batch)
2017-04-01 18:15:17.233744: step 8500, loss = 0.94 (291.5 examples/sec; 0.439 sec/batch)
2017-04-01 18:15:21.563064: step 8510, loss = 0.80 (295.7 examples/sec; 0.433 sec/batch)
2017-04-01 18:15:26.120435: step 8520, loss = 0.84 (280.9 examples/sec; 0.456 sec/batch)
2017-04-01 18:15:30.531331: step 8530, loss = 0.77 (290.2 examples/sec; 0.441 sec/batch)
2017-04-01 18:15:34.953000: step 8540, loss = 0.77 (289.5 examples/sec; 0.442 sec/batch)
2017-04-01 18:15:39.384561: step 8550, loss = 0.94 (288.8 examples/sec; 0.443 sec/batch)
2017-04-01 18:15:43.953769: step 8560, loss = 0.99 (280.1 examples/sec; 0.457 sec/batch)
2017-04-01 18:15:48.421594: step 8570, loss = 0.92 (286.5 examples/sec; 0.447 sec/batch)
2017-04-01 18:15:53.166624: step 8580, loss = 0.80 (269.8 examples/sec; 0.475 sec/batch)
2017-04-01 18:15:58.526247: step 8590, loss = 0.98 (238.8 examples/sec; 0.536 sec/batch)
2017-04-01 18:16:03.802427: step 8600, loss = 0.81 (242.6 examples/sec; 0.528 sec/batch)
2017-04-01 18:16:08.769060: step 8610, loss = 0.91 (257.7 examples/sec; 0.497 sec/batch)
2017-04-01 18:16:13.445848: step 8620, loss = 0.74 (273.7 examples/sec; 0.468 sec/batch)
2017-04-01 18:16:18.816789: step 8630, loss = 0.93 (238.3 examples/sec; 0.537 sec/batch)
2017-04-01 18:16:24.183438: step 8640, loss = 0.98 (238.5 examples/sec; 0.537 sec/batch)
2017-04-01 18:16:29.576026: step 8650, loss = 0.92 (237.4 examples/sec; 0.539 sec/batch)
2017-04-01 18:16:34.897421: step 8660, loss = 0.79 (240.5 examples/sec; 0.532 sec/batch)
2017-04-01 18:16:39.948058: step 8670, loss = 0.72 (253.4 examples/sec; 0.505 sec/batch)
2017-04-01 18:16:44.873989: step 8680, loss = 0.94 (259.8 examples/sec; 0.493 sec/batch)
2017-04-01 18:16:50.077732: step 8690, loss = 0.95 (246.0 examples/sec; 0.520 sec/batch)
2017-04-01 18:16:56.184137: step 8700, loss = 0.90 (209.6 examples/sec; 0.611 sec/batch)
2017-04-01 18:17:01.867381: step 8710, loss = 0.98 (225.2 examples/sec; 0.568 sec/batch)
2017-04-01 18:17:07.640777: step 8720, loss = 1.08 (221.7 examples/sec; 0.577 sec/batch)
2017-04-01 18:17:12.506174: step 8730, loss = 0.92 (263.1 examples/sec; 0.487 sec/batch)
2017-04-01 18:17:16.846296: step 8740, loss = 0.72 (294.9 examples/sec; 0.434 sec/batch)
2017-04-01 18:17:21.175778: step 8750, loss = 0.82 (295.6 examples/sec; 0.433 sec/batch)
2017-04-01 18:17:25.490758: step 8760, loss = 0.98 (296.6 examples/sec; 0.431 sec/batch)
2017-04-01 18:17:29.820108: step 8770, loss = 0.86 (295.7 examples/sec; 0.433 sec/batch)
2017-04-01 18:17:34.166566: step 8780, loss = 1.05 (294.5 examples/sec; 0.435 sec/batch)
2017-04-01 18:17:39.335996: step 8790, loss = 0.88 (247.6 examples/sec; 0.517 sec/batch)
2017-04-01 18:17:45.276579: step 8800, loss = 1.09 (215.5 examples/sec; 0.594 sec/batch)
2017-04-01 18:17:49.911252: step 8810, loss = 0.92 (276.2 examples/sec; 0.463 sec/batch)
2017-04-01 18:17:54.223702: step 8820, loss = 0.87 (296.8 examples/sec; 0.431 sec/batch)
2017-04-01 18:17:58.557545: step 8830, loss = 0.91 (295.3 examples/sec; 0.433 sec/batch)
2017-04-01 18:18:02.937143: step 8840, loss = 1.04 (292.3 examples/sec; 0.438 sec/batch)
2017-04-01 18:18:07.314320: step 8850, loss = 0.96 (292.4 examples/sec; 0.438 sec/batch)
2017-04-01 18:18:11.640081: step 8860, loss = 0.92 (295.9 examples/sec; 0.433 sec/batch)
2017-04-01 18:18:16.023695: step 8870, loss = 0.98 (292.0 examples/sec; 0.438 sec/batch)
2017-04-01 18:18:20.339164: step 8880, loss = 1.12 (296.6 examples/sec; 0.432 sec/batch)
2017-04-01 18:18:24.712203: step 8890, loss = 0.90 (292.7 examples/sec; 0.437 sec/batch)
2017-04-01 18:18:29.045436: step 8900, loss = 0.95 (295.4 examples/sec; 0.433 sec/batch)
2017-04-01 18:18:33.308974: step 8910, loss = 1.10 (300.2 examples/sec; 0.426 sec/batch)
2017-04-01 18:18:37.614679: step 8920, loss = 0.94 (297.3 examples/sec; 0.431 sec/batch)
2017-04-01 18:18:41.894442: step 8930, loss = 0.68 (299.1 examples/sec; 0.428 sec/batch)
2017-04-01 18:18:46.322220: step 8940, loss = 0.85 (289.1 examples/sec; 0.443 sec/batch)
2017-04-01 18:18:50.699657: step 8950, loss = 0.98 (292.4 examples/sec; 0.438 sec/batch)
2017-04-01 18:18:55.039161: step 8960, loss = 0.95 (295.0 examples/sec; 0.434 sec/batch)
2017-04-01 18:18:59.371542: step 8970, loss = 0.92 (295.4 examples/sec; 0.433 sec/batch)
2017-04-01 18:19:03.803774: step 8980, loss = 0.84 (288.8 examples/sec; 0.443 sec/batch)
2017-04-01 18:19:08.203591: step 8990, loss = 0.77 (290.9 examples/sec; 0.440 sec/batch)
2017-04-01 18:19:12.578406: step 9000, loss = 0.85 (292.6 examples/sec; 0.437 sec/batch)
2017-04-01 18:19:16.940626: step 9010, loss = 0.99 (293.4 examples/sec; 0.436 sec/batch)
2017-04-01 18:19:21.271253: step 9020, loss = 0.90 (295.6 examples/sec; 0.433 sec/batch)
2017-04-01 18:19:25.585321: step 9030, loss = 0.94 (296.7 examples/sec; 0.431 sec/batch)
2017-04-01 18:19:29.923426: step 9040, loss = 0.81 (295.1 examples/sec; 0.434 sec/batch)
2017-04-01 18:19:34.220709: step 9050, loss = 0.79 (297.9 examples/sec; 0.430 sec/batch)
2017-04-01 18:19:38.535273: step 9060, loss = 0.97 (296.7 examples/sec; 0.431 sec/batch)
2017-04-01 18:19:42.898772: step 9070, loss = 0.94 (293.3 examples/sec; 0.436 sec/batch)
2017-04-01 18:19:47.292780: step 9080, loss = 0.85 (291.3 examples/sec; 0.439 sec/batch)
2017-04-01 18:19:51.625051: step 9090, loss = 0.76 (295.5 examples/sec; 0.433 sec/batch)
2017-04-01 18:19:56.065894: step 9100, loss = 0.90 (288.2 examples/sec; 0.444 sec/batch)
2017-04-01 18:20:00.433911: step 9110, loss = 0.95 (293.0 examples/sec; 0.437 sec/batch)
2017-04-01 18:20:04.773691: step 9120, loss = 0.88 (294.9 examples/sec; 0.434 sec/batch)
2017-04-01 18:20:09.177889: step 9130, loss = 0.88 (290.6 examples/sec; 0.440 sec/batch)
2017-04-01 18:20:13.635900: step 9140, loss = 0.95 (287.1 examples/sec; 0.446 sec/batch)
2017-04-01 18:20:18.171030: step 9150, loss = 0.83 (282.2 examples/sec; 0.454 sec/batch)
2017-04-01 18:20:22.764493: step 9160, loss = 0.94 (278.7 examples/sec; 0.459 sec/batch)
2017-04-01 18:20:27.113952: step 9170, loss = 1.11 (294.3 examples/sec; 0.435 sec/batch)
2017-04-01 18:20:31.529477: step 9180, loss = 0.80 (289.9 examples/sec; 0.442 sec/batch)
2017-04-01 18:20:36.233621: step 9190, loss = 0.97 (272.1 examples/sec; 0.470 sec/batch)
2017-04-01 18:20:41.412417: step 9200, loss = 0.93 (247.2 examples/sec; 0.518 sec/batch)
2017-04-01 18:20:46.809715: step 9210, loss = 1.01 (237.2 examples/sec; 0.540 sec/batch)
2017-04-01 18:20:51.698200: step 9220, loss = 0.84 (261.8 examples/sec; 0.489 sec/batch)
2017-04-01 18:20:55.541040: step 9230, loss = 0.93 (333.1 examples/sec; 0.384 sec/batch)
2017-04-01 18:20:59.479942: step 9240, loss = 1.01 (325.0 examples/sec; 0.394 sec/batch)
2017-04-01 18:21:03.413524: step 9250, loss = 0.74 (325.4 examples/sec; 0.393 sec/batch)
2017-04-01 18:21:07.305663: step 9260, loss = 0.84 (328.9 examples/sec; 0.389 sec/batch)
2017-04-01 18:21:11.148493: step 9270, loss = 0.82 (333.1 examples/sec; 0.384 sec/batch)
2017-04-01 18:21:14.998592: step 9280, loss = 1.02 (332.5 examples/sec; 0.385 sec/batch)
2017-04-01 18:21:18.838035: step 9290, loss = 0.90 (333.4 examples/sec; 0.384 sec/batch)
2017-04-01 18:21:22.761407: step 9300, loss = 0.86 (326.3 examples/sec; 0.392 sec/batch)
2017-04-01 18:21:26.588413: step 9310, loss = 0.75 (334.5 examples/sec; 0.383 sec/batch)
2017-04-01 18:21:30.462357: step 9320, loss = 0.81 (330.4 examples/sec; 0.387 sec/batch)
2017-04-01 18:21:34.710420: step 9330, loss = 0.91 (301.3 examples/sec; 0.425 sec/batch)
2017-04-01 18:21:39.494952: step 9340, loss = 0.90 (267.5 examples/sec; 0.478 sec/batch)
2017-04-01 18:21:43.357550: step 9350, loss = 0.78 (331.4 examples/sec; 0.386 sec/batch)
2017-04-01 18:21:47.221314: step 9360, loss = 0.94 (331.3 examples/sec; 0.386 sec/batch)

