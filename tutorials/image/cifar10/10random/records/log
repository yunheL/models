2017-04-02 21:45:23.669929: step 0, loss = 4.68 (315.6 examples/sec; 0.406 sec/batch)
2017-04-02 21:45:27.645568: step 10, loss = 4.67 (322.0 examples/sec; 0.398 sec/batch)
2017-04-02 21:45:31.583257: step 20, loss = 4.65 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 21:45:35.530536: step 30, loss = 4.63 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 21:45:39.448360: step 40, loss = 4.61 (326.7 examples/sec; 0.392 sec/batch)
2017-04-02 21:45:43.406907: step 50, loss = 4.58 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 21:45:47.335156: step 60, loss = 4.57 (325.8 examples/sec; 0.393 sec/batch)
2017-04-02 21:45:51.230287: step 70, loss = 4.55 (328.6 examples/sec; 0.390 sec/batch)
2017-04-02 21:45:55.166996: step 80, loss = 4.54 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 21:45:59.093515: step 90, loss = 4.51 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 21:46:03.088525: step 100, loss = 4.50 (320.4 examples/sec; 0.400 sec/batch)
2017-04-02 21:46:06.968654: step 110, loss = 4.47 (329.9 examples/sec; 0.388 sec/batch)
2017-04-02 21:46:10.895615: step 120, loss = 4.46 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 21:46:14.812758: step 130, loss = 4.44 (326.8 examples/sec; 0.392 sec/batch)
2017-04-02 21:46:18.737902: step 140, loss = 4.43 (326.1 examples/sec; 0.393 sec/batch)
2017-04-02 21:46:22.665518: step 150, loss = 4.41 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 21:46:26.585436: step 160, loss = 4.39 (326.5 examples/sec; 0.392 sec/batch)
2017-04-02 21:46:30.538573: step 170, loss = 4.37 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 21:46:34.479697: step 180, loss = 4.36 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 21:46:38.403280: step 190, loss = 4.35 (326.2 examples/sec; 0.392 sec/batch)
2017-04-02 21:46:42.396040: step 200, loss = 4.33 (320.6 examples/sec; 0.399 sec/batch)
2017-04-02 21:46:46.323704: step 210, loss = 4.31 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 21:46:50.262193: step 220, loss = 4.30 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 21:46:54.189367: step 230, loss = 4.27 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 21:46:58.136962: step 240, loss = 4.27 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 21:47:02.040636: step 250, loss = 4.25 (327.9 examples/sec; 0.390 sec/batch)
2017-04-02 21:47:05.980564: step 260, loss = 4.23 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 21:47:09.905323: step 270, loss = 4.21 (326.1 examples/sec; 0.392 sec/batch)
2017-04-02 21:47:13.826822: step 280, loss = 4.20 (326.4 examples/sec; 0.392 sec/batch)
2017-04-02 21:47:17.757418: step 290, loss = 4.19 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 21:47:21.758938: step 300, loss = 4.17 (319.9 examples/sec; 0.400 sec/batch)
2017-04-02 21:47:25.683202: step 310, loss = 4.15 (326.2 examples/sec; 0.392 sec/batch)
2017-04-02 21:47:29.630694: step 320, loss = 4.14 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 21:47:33.580017: step 330, loss = 4.13 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 21:47:37.505877: step 340, loss = 4.11 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 21:47:41.431506: step 350, loss = 4.10 (326.1 examples/sec; 0.393 sec/batch)
2017-04-02 21:47:45.361858: step 360, loss = 4.08 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 21:47:49.284328: step 370, loss = 4.07 (326.3 examples/sec; 0.392 sec/batch)
2017-04-02 21:47:53.201939: step 380, loss = 4.06 (326.7 examples/sec; 0.392 sec/batch)
2017-04-02 21:47:57.165930: step 390, loss = 4.04 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 21:48:01.163614: step 400, loss = 4.03 (320.2 examples/sec; 0.400 sec/batch)
2017-04-02 21:48:05.117799: step 410, loss = 4.01 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 21:48:09.065753: step 420, loss = 4.00 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 21:48:13.019701: step 430, loss = 3.99 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 21:48:16.969876: step 440, loss = 3.97 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 21:48:20.908750: step 450, loss = 3.96 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 21:48:24.823770: step 460, loss = 3.94 (326.9 examples/sec; 0.392 sec/batch)
2017-04-02 21:48:28.771142: step 470, loss = 3.93 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 21:48:32.718855: step 480, loss = 3.92 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 21:48:36.642003: step 490, loss = 3.91 (326.3 examples/sec; 0.392 sec/batch)
2017-04-02 21:48:40.653931: step 500, loss = 3.90 (319.0 examples/sec; 0.401 sec/batch)
2017-04-02 21:48:44.573559: step 510, loss = 3.89 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 21:48:48.529803: step 520, loss = 3.86 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 21:48:52.456411: step 530, loss = 3.85 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 21:48:56.406751: step 540, loss = 3.84 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 21:49:00.375939: step 550, loss = 3.84 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 21:49:04.316109: step 560, loss = 3.82 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 21:49:08.286355: step 570, loss = 3.81 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 21:49:12.231857: step 580, loss = 3.79 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 21:49:16.170055: step 590, loss = 3.78 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 21:49:20.172087: step 600, loss = 3.77 (319.8 examples/sec; 0.400 sec/batch)
2017-04-02 21:49:24.144589: step 610, loss = 3.76 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 21:49:28.101152: step 620, loss = 3.75 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 21:49:32.020768: step 630, loss = 3.74 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 21:49:35.946976: step 640, loss = 3.72 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 21:49:39.879743: step 650, loss = 3.72 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 21:49:43.841923: step 660, loss = 3.70 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 21:49:47.805331: step 670, loss = 3.69 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 21:49:51.742557: step 680, loss = 3.68 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 21:49:55.667531: step 690, loss = 3.67 (326.1 examples/sec; 0.392 sec/batch)
2017-04-02 21:49:59.667981: step 700, loss = 3.66 (320.0 examples/sec; 0.400 sec/batch)
2017-04-02 21:50:03.643191: step 710, loss = 3.65 (322.0 examples/sec; 0.398 sec/batch)
2017-04-02 21:50:07.594303: step 720, loss = 3.64 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 21:50:11.529954: step 730, loss = 3.63 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 21:50:15.490190: step 740, loss = 3.62 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 21:50:19.414068: step 750, loss = 3.61 (326.2 examples/sec; 0.392 sec/batch)
2017-04-02 21:50:23.335286: step 760, loss = 3.60 (326.4 examples/sec; 0.392 sec/batch)
2017-04-02 21:50:27.302938: step 770, loss = 3.59 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 21:50:31.237761: step 780, loss = 3.58 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 21:50:35.207003: step 790, loss = 3.57 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 21:50:39.245290: step 800, loss = 3.56 (317.0 examples/sec; 0.404 sec/batch)
2017-04-02 21:50:43.169412: step 810, loss = 3.54 (326.2 examples/sec; 0.392 sec/batch)
2017-04-02 21:50:47.093561: step 820, loss = 3.53 (326.2 examples/sec; 0.392 sec/batch)
2017-04-02 21:50:51.018759: step 830, loss = 3.52 (326.1 examples/sec; 0.393 sec/batch)
2017-04-02 21:50:54.941998: step 840, loss = 3.51 (326.3 examples/sec; 0.392 sec/batch)
2017-04-02 21:50:58.845761: step 850, loss = 3.51 (327.9 examples/sec; 0.390 sec/batch)
2017-04-02 21:51:02.800096: step 860, loss = 3.50 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 21:51:06.763708: step 870, loss = 3.49 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 21:51:10.723561: step 880, loss = 3.48 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 21:51:14.664422: step 890, loss = 3.47 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 21:51:18.708326: step 900, loss = 3.46 (316.5 examples/sec; 0.404 sec/batch)
2017-04-02 21:51:22.678573: step 910, loss = 3.45 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 21:51:26.633379: step 920, loss = 3.44 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 21:51:30.617734: step 930, loss = 3.43 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 21:51:34.549424: step 940, loss = 3.42 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 21:51:38.488073: step 950, loss = 3.41 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 21:51:42.394000: step 960, loss = 3.40 (327.7 examples/sec; 0.391 sec/batch)
2017-04-02 21:51:46.348607: step 970, loss = 3.40 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 21:51:50.290148: step 980, loss = 3.39 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 21:51:54.228851: step 990, loss = 3.38 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 21:51:58.243733: step 1000, loss = 3.37 (318.8 examples/sec; 0.401 sec/batch)
2017-04-02 21:52:02.153433: step 1010, loss = 3.36 (327.4 examples/sec; 0.391 sec/batch)
2017-04-02 21:52:06.105823: step 1020, loss = 3.35 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 21:52:10.101099: step 1030, loss = 3.34 (320.4 examples/sec; 0.400 sec/batch)
2017-04-02 21:52:14.026857: step 1040, loss = 3.34 (326.1 examples/sec; 0.393 sec/batch)
2017-04-02 21:52:17.990295: step 1050, loss = 3.32 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 21:52:21.943616: step 1060, loss = 3.32 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 21:52:25.882381: step 1070, loss = 3.31 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 21:52:29.842273: step 1080, loss = 3.30 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 21:52:33.783082: step 1090, loss = 3.29 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 21:52:37.792200: step 1100, loss = 3.29 (319.3 examples/sec; 0.401 sec/batch)
2017-04-02 21:52:41.766406: step 1110, loss = 3.28 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 21:52:45.721034: step 1120, loss = 3.27 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 21:52:49.661332: step 1130, loss = 3.26 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 21:52:53.593058: step 1140, loss = 3.26 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 21:52:57.569297: step 1150, loss = 3.25 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 21:53:01.509922: step 1160, loss = 3.24 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 21:53:05.493865: step 1170, loss = 3.24 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 21:53:09.437332: step 1180, loss = 3.22 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 21:53:13.415773: step 1190, loss = 3.22 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 21:53:17.448079: step 1200, loss = 3.21 (317.4 examples/sec; 0.403 sec/batch)
2017-04-02 21:53:21.389501: step 1210, loss = 3.20 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 21:53:25.375455: step 1220, loss = 3.20 (321.1 examples/sec; 0.399 sec/batch)
2017-04-02 21:53:29.327808: step 1230, loss = 3.19 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 21:53:33.285447: step 1240, loss = 3.19 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 21:53:37.242955: step 1250, loss = 3.18 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 21:53:41.188169: step 1260, loss = 3.17 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 21:53:45.162986: step 1270, loss = 3.16 (322.0 examples/sec; 0.397 sec/batch)
2017-04-02 21:53:49.145319: step 1280, loss = 3.15 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 21:53:53.142666: step 1290, loss = 3.15 (320.2 examples/sec; 0.400 sec/batch)
2017-04-02 21:53:57.179627: step 1300, loss = 3.14 (317.1 examples/sec; 0.404 sec/batch)
2017-04-02 21:54:01.146211: step 1310, loss = 3.14 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 21:54:05.091352: step 1320, loss = 3.13 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 21:54:09.046673: step 1330, loss = 3.12 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 21:54:13.010890: step 1340, loss = 3.12 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 21:54:16.975380: step 1350, loss = 3.11 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 21:54:20.905453: step 1360, loss = 3.10 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 21:54:24.863041: step 1370, loss = 3.10 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 21:54:28.827549: step 1380, loss = 3.09 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 21:54:32.780426: step 1390, loss = 3.09 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 21:54:36.795683: step 1400, loss = 3.08 (318.8 examples/sec; 0.402 sec/batch)
2017-04-02 21:54:40.758742: step 1410, loss = 3.07 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 21:54:44.702426: step 1420, loss = 3.07 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 21:54:48.653694: step 1430, loss = 3.06 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 21:54:52.595634: step 1440, loss = 3.05 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 21:54:56.547671: step 1450, loss = 3.05 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 21:55:00.518472: step 1460, loss = 3.04 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 21:55:04.434605: step 1470, loss = 3.04 (326.9 examples/sec; 0.392 sec/batch)
2017-04-02 21:55:08.391209: step 1480, loss = 3.03 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 21:55:12.344682: step 1490, loss = 3.02 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 21:55:16.350962: step 1500, loss = 3.02 (319.5 examples/sec; 0.401 sec/batch)
2017-04-02 21:55:20.299128: step 1510, loss = 3.01 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 21:55:24.966887: step 1520, loss = 3.01 (274.2 examples/sec; 0.467 sec/batch)
2017-04-02 21:55:28.951980: step 1530, loss = 3.00 (321.2 examples/sec; 0.399 sec/batch)
2017-04-02 21:55:32.958517: step 1540, loss = 2.99 (319.5 examples/sec; 0.401 sec/batch)
2017-04-02 21:55:36.947467: step 1550, loss = 2.99 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 21:55:40.901674: step 1560, loss = 2.99 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 21:55:44.882807: step 1570, loss = 2.98 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 21:55:48.828762: step 1580, loss = 2.97 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 21:55:52.795789: step 1590, loss = 2.97 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 21:55:56.815574: step 1600, loss = 2.96 (318.4 examples/sec; 0.402 sec/batch)
2017-04-02 21:56:00.744784: step 1610, loss = 2.96 (325.8 examples/sec; 0.393 sec/batch)
2017-04-02 21:56:04.709898: step 1620, loss = 2.95 (322.8 examples/sec; 0.397 sec/batch)
2017-04-02 21:56:08.691008: step 1630, loss = 2.95 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 21:56:12.655786: step 1640, loss = 2.95 (322.8 examples/sec; 0.396 sec/batch)
2017-04-02 21:56:16.634938: step 1650, loss = 2.94 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 21:56:20.565641: step 1660, loss = 2.93 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 21:56:24.541152: step 1670, loss = 2.93 (322.0 examples/sec; 0.398 sec/batch)
2017-04-02 21:56:28.509071: step 1680, loss = 2.92 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 21:56:32.441192: step 1690, loss = 2.92 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 21:56:36.470145: step 1700, loss = 2.91 (317.7 examples/sec; 0.403 sec/batch)
2017-04-02 21:56:40.434982: step 1710, loss = 2.91 (322.8 examples/sec; 0.396 sec/batch)
2017-04-02 21:56:44.367267: step 1720, loss = 2.90 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 21:56:48.352475: step 1730, loss = 2.90 (321.2 examples/sec; 0.399 sec/batch)
2017-04-02 21:56:52.351369: step 1740, loss = 2.89 (320.1 examples/sec; 0.400 sec/batch)
2017-04-02 21:56:56.315247: step 1750, loss = 2.89 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 21:57:00.275420: step 1760, loss = 2.88 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 21:57:04.234350: step 1770, loss = 2.88 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 21:57:08.204010: step 1780, loss = 2.87 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 21:57:12.149039: step 1790, loss = 2.87 (324.5 examples/sec; 0.395 sec/batch)
2017-04-02 21:57:16.154915: step 1800, loss = 2.87 (319.5 examples/sec; 0.401 sec/batch)
2017-04-02 21:57:20.082685: step 1810, loss = 2.86 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 21:57:24.028989: step 1820, loss = 2.85 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 21:57:27.943695: step 1830, loss = 2.85 (327.0 examples/sec; 0.391 sec/batch)
2017-04-02 21:57:31.901353: step 1840, loss = 2.85 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 21:57:35.883014: step 1850, loss = 2.84 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 21:57:39.857598: step 1860, loss = 2.84 (322.0 examples/sec; 0.397 sec/batch)
2017-04-02 21:57:43.818498: step 1870, loss = 2.83 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 21:57:47.769797: step 1880, loss = 2.83 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 21:57:51.729263: step 1890, loss = 2.83 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 21:57:55.730384: step 1900, loss = 2.82 (319.9 examples/sec; 0.400 sec/batch)
2017-04-02 21:57:59.680444: step 1910, loss = 2.82 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 21:58:03.628464: step 1920, loss = 2.81 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 21:58:07.608215: step 1930, loss = 2.81 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 21:58:11.528071: step 1940, loss = 2.81 (326.5 examples/sec; 0.392 sec/batch)
2017-04-02 21:58:15.485183: step 1950, loss = 2.80 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 21:58:19.445543: step 1960, loss = 2.80 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 21:58:23.433999: step 1970, loss = 2.79 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 21:58:27.370674: step 1980, loss = 2.79 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 21:58:31.314775: step 1990, loss = 2.79 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 21:58:35.368352: step 2000, loss = 2.78 (315.8 examples/sec; 0.405 sec/batch)
2017-04-02 21:58:39.341150: step 2010, loss = 2.78 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 21:58:43.337822: step 2020, loss = 2.77 (320.3 examples/sec; 0.400 sec/batch)
2017-04-02 21:58:47.296505: step 2030, loss = 2.77 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 21:59:49.963460: precision @ 1 = 0.101

2017-04-02 22:00:34.880368: step 2040, loss = 2.77 (11.9 examples/sec; 10.758 sec/batch)
2017-04-02 22:00:38.850021: step 2050, loss = 2.76 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 22:00:42.830922: step 2060, loss = 2.76 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 22:00:46.790080: step 2070, loss = 2.76 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 22:00:50.773738: step 2080, loss = 2.75 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 22:00:54.725999: step 2090, loss = 2.75 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 22:00:58.754607: step 2100, loss = 2.74 (317.7 examples/sec; 0.403 sec/batch)
2017-04-02 22:01:02.727876: step 2110, loss = 2.74 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 22:01:06.683658: step 2120, loss = 2.74 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 22:01:10.656514: step 2130, loss = 2.74 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 22:01:14.613537: step 2140, loss = 2.73 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:01:18.590406: step 2150, loss = 2.73 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 22:01:22.529904: step 2160, loss = 2.73 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 22:01:26.524374: step 2170, loss = 2.72 (320.4 examples/sec; 0.399 sec/batch)
2017-04-02 22:01:30.486651: step 2180, loss = 2.72 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 22:01:34.447871: step 2190, loss = 2.71 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 22:01:38.464409: step 2200, loss = 2.71 (318.7 examples/sec; 0.402 sec/batch)
2017-04-02 22:01:42.406189: step 2210, loss = 2.71 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 22:01:46.382100: step 2220, loss = 2.71 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 22:01:50.372697: step 2230, loss = 2.70 (320.8 examples/sec; 0.399 sec/batch)
2017-04-02 22:01:54.329622: step 2240, loss = 2.70 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:01:58.298927: step 2250, loss = 2.69 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 22:02:02.258553: step 2260, loss = 2.69 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 22:02:06.243956: step 2270, loss = 2.69 (321.2 examples/sec; 0.399 sec/batch)
2017-04-02 22:02:10.182699: step 2280, loss = 2.69 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 22:02:14.139845: step 2290, loss = 2.68 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:02:18.160936: step 2300, loss = 2.68 (318.3 examples/sec; 0.402 sec/batch)
2017-04-02 22:02:22.117102: step 2310, loss = 2.68 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:02:26.057120: step 2320, loss = 2.67 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 22:02:30.005498: step 2330, loss = 2.67 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 22:02:33.923414: step 2340, loss = 2.66 (326.7 examples/sec; 0.392 sec/batch)
2017-04-02 22:02:37.886050: step 2350, loss = 2.66 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 22:02:41.846720: step 2360, loss = 2.66 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 22:02:45.806377: step 2370, loss = 2.66 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 22:02:49.789135: step 2380, loss = 2.65 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 22:02:53.741585: step 2390, loss = 2.65 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 22:02:57.780122: step 2400, loss = 2.65 (316.9 examples/sec; 0.404 sec/batch)
2017-04-02 22:03:01.741892: step 2410, loss = 2.65 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 22:03:05.687632: step 2420, loss = 2.64 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 22:03:09.646284: step 2430, loss = 2.64 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 22:03:13.559629: step 2440, loss = 2.64 (327.1 examples/sec; 0.391 sec/batch)
2017-04-02 22:03:17.485719: step 2450, loss = 2.64 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 22:03:21.451990: step 2460, loss = 2.64 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 22:03:25.397428: step 2470, loss = 2.63 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 22:03:29.341401: step 2480, loss = 2.63 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 22:03:33.295982: step 2490, loss = 2.63 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 22:03:37.322746: step 2500, loss = 2.62 (317.9 examples/sec; 0.403 sec/batch)
2017-04-02 22:03:41.258277: step 2510, loss = 2.62 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 22:03:45.206516: step 2520, loss = 2.62 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 22:03:49.135695: step 2530, loss = 2.62 (325.8 examples/sec; 0.393 sec/batch)
2017-04-02 22:03:53.086473: step 2540, loss = 2.61 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 22:03:57.027404: step 2550, loss = 2.61 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 22:04:00.979685: step 2560, loss = 2.61 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 22:04:04.894142: step 2570, loss = 2.60 (327.0 examples/sec; 0.391 sec/batch)
2017-04-02 22:04:08.815036: step 2580, loss = 2.60 (326.5 examples/sec; 0.392 sec/batch)
2017-04-02 22:04:12.758080: step 2590, loss = 2.61 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 22:04:16.779895: step 2600, loss = 2.60 (318.3 examples/sec; 0.402 sec/batch)
2017-04-02 22:04:20.729916: step 2610, loss = 2.60 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 22:04:24.667254: step 2620, loss = 2.60 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 22:04:28.639972: step 2630, loss = 2.59 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 22:04:32.601017: step 2640, loss = 2.59 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 22:04:36.528483: step 2650, loss = 2.59 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 22:04:40.473887: step 2660, loss = 2.59 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 22:04:44.466074: step 2670, loss = 2.58 (320.6 examples/sec; 0.399 sec/batch)
2017-04-02 22:04:48.419328: step 2680, loss = 2.58 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 22:04:52.349649: step 2690, loss = 2.57 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 22:04:56.323753: step 2700, loss = 2.58 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 22:05:00.265408: step 2710, loss = 2.57 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 22:05:04.205719: step 2720, loss = 2.57 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 22:05:08.146456: step 2730, loss = 2.57 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 22:05:12.062304: step 2740, loss = 2.56 (326.9 examples/sec; 0.392 sec/batch)
2017-04-02 22:05:16.001884: step 2750, loss = 2.57 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 22:05:19.936765: step 2760, loss = 2.56 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 22:05:24.558520: step 2770, loss = 2.56 (277.0 examples/sec; 0.462 sec/batch)
2017-04-02 22:05:28.491111: step 2780, loss = 2.56 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 22:05:32.443913: step 2790, loss = 2.56 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 22:05:36.472782: step 2800, loss = 2.56 (317.7 examples/sec; 0.403 sec/batch)
2017-04-02 22:05:40.423252: step 2810, loss = 2.55 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 22:05:44.356683: step 2820, loss = 2.55 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 22:05:48.329705: step 2830, loss = 2.55 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 22:05:52.283155: step 2840, loss = 2.55 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 22:05:56.224958: step 2850, loss = 2.54 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 22:06:00.186235: step 2860, loss = 2.54 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 22:06:04.118661: step 2870, loss = 2.54 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 22:06:08.045459: step 2880, loss = 2.54 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 22:06:11.992182: step 2890, loss = 2.54 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 22:06:15.978263: step 2900, loss = 2.54 (321.1 examples/sec; 0.399 sec/batch)
2017-04-02 22:06:19.935341: step 2910, loss = 2.53 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:06:23.885965: step 2920, loss = 2.53 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 22:06:27.886830: step 2930, loss = 2.53 (319.9 examples/sec; 0.400 sec/batch)
2017-04-02 22:06:31.841406: step 2940, loss = 2.53 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 22:06:35.786262: step 2950, loss = 2.53 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 22:06:39.742440: step 2960, loss = 2.53 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:06:43.705455: step 2970, loss = 2.52 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 22:06:47.677988: step 2980, loss = 2.52 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 22:06:51.671324: step 2990, loss = 2.52 (320.5 examples/sec; 0.399 sec/batch)
2017-04-02 22:06:55.696161: step 3000, loss = 2.52 (318.0 examples/sec; 0.402 sec/batch)
2017-04-02 22:06:59.615611: step 3010, loss = 2.52 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 22:07:03.561033: step 3020, loss = 2.52 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 22:07:07.494476: step 3030, loss = 2.52 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 22:07:11.435747: step 3040, loss = 2.51 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 22:07:15.370573: step 3050, loss = 2.51 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 22:07:19.331832: step 3060, loss = 2.51 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 22:07:23.287466: step 3070, loss = 2.50 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 22:07:27.232141: step 3080, loss = 2.51 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 22:07:31.178256: step 3090, loss = 2.50 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 22:07:35.195569: step 3100, loss = 2.50 (318.6 examples/sec; 0.402 sec/batch)
2017-04-02 22:07:39.151605: step 3110, loss = 2.50 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 22:07:43.126324: step 3120, loss = 2.50 (322.0 examples/sec; 0.397 sec/batch)
2017-04-02 22:07:47.067381: step 3130, loss = 2.49 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 22:07:51.031183: step 3140, loss = 2.50 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 22:07:54.975807: step 3150, loss = 2.49 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 22:07:58.939711: step 3160, loss = 2.49 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 22:08:02.905295: step 3170, loss = 2.49 (322.8 examples/sec; 0.397 sec/batch)
2017-04-02 22:08:06.881914: step 3180, loss = 2.49 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 22:08:10.857982: step 3190, loss = 2.49 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 22:08:14.885017: step 3200, loss = 2.49 (317.9 examples/sec; 0.403 sec/batch)
2017-04-02 22:08:18.875925: step 3210, loss = 2.49 (320.7 examples/sec; 0.399 sec/batch)
2017-04-02 22:08:22.843303: step 3220, loss = 2.48 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 22:08:26.798329: step 3230, loss = 2.48 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 22:08:30.749397: step 3240, loss = 2.48 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 22:08:34.699097: step 3250, loss = 2.48 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 22:08:38.629455: step 3260, loss = 2.47 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 22:08:42.578606: step 3270, loss = 2.47 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 22:08:46.544390: step 3280, loss = 2.48 (322.8 examples/sec; 0.397 sec/batch)
2017-04-02 22:08:50.490753: step 3290, loss = 2.47 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 22:08:54.507126: step 3300, loss = 2.47 (318.7 examples/sec; 0.402 sec/batch)
2017-04-02 22:08:58.452287: step 3310, loss = 2.47 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 22:09:02.394028: step 3320, loss = 2.47 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 22:09:06.351779: step 3330, loss = 2.46 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 22:09:10.335321: step 3340, loss = 2.47 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 22:09:14.319697: step 3350, loss = 2.46 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 22:09:18.286695: step 3360, loss = 2.46 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 22:09:22.248366: step 3370, loss = 2.47 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 22:09:26.230964: step 3380, loss = 2.46 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 22:09:30.203344: step 3390, loss = 2.46 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 22:09:34.233506: step 3400, loss = 2.46 (317.6 examples/sec; 0.403 sec/batch)
2017-04-02 22:09:38.178442: step 3410, loss = 2.46 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 22:09:42.140289: step 3420, loss = 2.46 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 22:09:46.102971: step 3430, loss = 2.45 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 22:09:50.058968: step 3440, loss = 2.45 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 22:09:54.006061: step 3450, loss = 2.45 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 22:09:57.988600: step 3460, loss = 2.45 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 22:10:01.974609: step 3470, loss = 2.45 (321.1 examples/sec; 0.399 sec/batch)
2017-04-02 22:10:05.957938: step 3480, loss = 2.45 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 22:10:09.945988: step 3490, loss = 2.45 (321.0 examples/sec; 0.399 sec/batch)
2017-04-02 22:10:13.963858: step 3500, loss = 2.44 (318.6 examples/sec; 0.402 sec/batch)
2017-04-02 22:10:17.935154: step 3510, loss = 2.45 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 22:10:21.921899: step 3520, loss = 2.44 (321.1 examples/sec; 0.399 sec/batch)
2017-04-02 22:10:25.867893: step 3530, loss = 2.44 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 22:10:29.784062: step 3540, loss = 2.44 (326.9 examples/sec; 0.392 sec/batch)
2017-04-02 22:10:33.743504: step 3550, loss = 2.44 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 22:10:37.706256: step 3560, loss = 2.44 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 22:10:41.696361: step 3570, loss = 2.44 (320.8 examples/sec; 0.399 sec/batch)
2017-04-02 22:10:45.671275: step 3580, loss = 2.45 (322.0 examples/sec; 0.397 sec/batch)
2017-04-02 22:10:49.610775: step 3590, loss = 2.44 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 22:10:53.659074: step 3600, loss = 2.43 (316.2 examples/sec; 0.405 sec/batch)
2017-04-02 22:10:57.639218: step 3610, loss = 2.43 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 22:11:01.617384: step 3620, loss = 2.44 (321.8 examples/sec; 0.398 sec/batch)
2017-04-02 22:11:05.622356: step 3630, loss = 2.44 (319.6 examples/sec; 0.400 sec/batch)
2017-04-02 22:11:09.617789: step 3640, loss = 2.43 (320.4 examples/sec; 0.400 sec/batch)
2017-04-02 22:11:13.616658: step 3650, loss = 2.43 (320.1 examples/sec; 0.400 sec/batch)
2017-04-02 22:11:17.581051: step 3660, loss = 2.43 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 22:11:21.541805: step 3670, loss = 2.43 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 22:11:25.524304: step 3680, loss = 2.43 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 22:11:29.491451: step 3690, loss = 2.43 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 22:11:33.525994: step 3700, loss = 2.43 (317.3 examples/sec; 0.403 sec/batch)
2017-04-02 22:11:37.491031: step 3710, loss = 2.43 (322.8 examples/sec; 0.397 sec/batch)
2017-04-02 22:11:41.478302: step 3720, loss = 2.42 (321.0 examples/sec; 0.399 sec/batch)
2017-04-02 22:11:45.447506: step 3730, loss = 2.43 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 22:11:49.399881: step 3740, loss = 2.42 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 22:11:53.366492: step 3750, loss = 2.43 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 22:11:57.373560: step 3760, loss = 2.42 (319.4 examples/sec; 0.401 sec/batch)
2017-04-02 22:12:01.346287: step 3770, loss = 2.42 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 22:12:05.343938: step 3780, loss = 2.42 (320.2 examples/sec; 0.400 sec/batch)
2017-04-02 22:12:09.339419: step 3790, loss = 2.42 (320.4 examples/sec; 0.400 sec/batch)
2017-04-02 22:12:13.370529: step 3800, loss = 2.42 (317.5 examples/sec; 0.403 sec/batch)
2017-04-02 22:12:17.366062: step 3810, loss = 2.41 (320.4 examples/sec; 0.400 sec/batch)
2017-04-02 22:12:21.324566: step 3820, loss = 2.41 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 22:12:25.268105: step 3830, loss = 2.42 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 22:12:29.238214: step 3840, loss = 2.41 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 22:12:33.217520: step 3850, loss = 2.41 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 22:12:37.215322: step 3860, loss = 2.41 (320.2 examples/sec; 0.400 sec/batch)
2017-04-02 22:12:41.207098: step 3870, loss = 2.41 (320.7 examples/sec; 0.399 sec/batch)
2017-04-02 22:12:45.186032: step 3880, loss = 2.41 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 22:12:49.158672: step 3890, loss = 2.41 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 22:12:53.206862: step 3900, loss = 2.41 (316.2 examples/sec; 0.405 sec/batch)
2017-04-02 22:12:57.192421: step 3910, loss = 2.41 (321.2 examples/sec; 0.399 sec/batch)
2017-04-02 22:13:01.166965: step 3920, loss = 2.41 (322.0 examples/sec; 0.397 sec/batch)
2017-04-02 22:13:05.150915: step 3930, loss = 2.40 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 22:13:09.132450: step 3940, loss = 2.40 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 22:13:13.122812: step 3950, loss = 2.40 (320.8 examples/sec; 0.399 sec/batch)
2017-04-02 22:13:17.106411: step 3960, loss = 2.40 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 22:13:21.102032: step 3970, loss = 2.40 (320.4 examples/sec; 0.400 sec/batch)
2017-04-02 22:13:25.088282: step 3980, loss = 2.40 (321.1 examples/sec; 0.399 sec/batch)
2017-04-02 22:13:29.085633: step 3990, loss = 2.40 (320.2 examples/sec; 0.400 sec/batch)
2017-04-02 22:13:33.207254: step 4000, loss = 2.40 (310.6 examples/sec; 0.412 sec/batch)
2017-04-02 22:13:37.194317: step 4010, loss = 2.40 (321.0 examples/sec; 0.399 sec/batch)
2017-04-02 22:13:41.203512: step 4020, loss = 2.40 (319.3 examples/sec; 0.401 sec/batch)
2017-04-02 22:13:45.190240: step 4030, loss = 2.40 (321.1 examples/sec; 0.399 sec/batch)
2017-04-02 22:13:49.166689: step 4040, loss = 2.39 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 22:13:53.167157: step 4050, loss = 2.39 (320.0 examples/sec; 0.400 sec/batch)
2017-04-02 22:13:57.169360: step 4060, loss = 2.39 (319.8 examples/sec; 0.400 sec/batch)
2017-04-02 22:14:01.152521: step 4070, loss = 2.39 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 22:14:05.158495: step 4080, loss = 2.39 (319.5 examples/sec; 0.401 sec/batch)
2017-04-02 22:14:09.152239: step 4090, loss = 2.39 (320.5 examples/sec; 0.399 sec/batch)
2017-04-02 22:14:13.179470: step 4100, loss = 2.39 (317.8 examples/sec; 0.403 sec/batch)
2017-04-02 22:14:17.177486: step 4110, loss = 2.39 (320.2 examples/sec; 0.400 sec/batch)
2017-04-02 22:14:21.161341: step 4120, loss = 2.39 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 22:14:25.169505: step 4130, loss = 2.39 (319.3 examples/sec; 0.401 sec/batch)
2017-04-02 22:14:29.167290: step 4140, loss = 2.39 (320.2 examples/sec; 0.400 sec/batch)
2017-04-02 22:14:33.204967: step 4150, loss = 2.39 (317.0 examples/sec; 0.404 sec/batch)
2017-04-02 22:14:37.184694: step 4160, loss = 2.39 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 22:14:41.177747: step 4170, loss = 2.39 (320.6 examples/sec; 0.399 sec/batch)
2017-04-02 22:14:45.150022: step 4180, loss = 2.39 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 22:14:49.118950: step 4190, loss = 2.39 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 22:14:53.169502: step 4200, loss = 2.38 (316.0 examples/sec; 0.405 sec/batch)
2017-04-02 22:14:57.142012: step 4210, loss = 2.38 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 22:15:01.103859: step 4220, loss = 2.38 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 22:15:05.039743: step 4230, loss = 2.38 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 22:15:09.009443: step 4240, loss = 2.38 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 22:15:12.983082: step 4250, loss = 2.38 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 22:15:16.952083: step 4260, loss = 2.38 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 22:15:20.903783: step 4270, loss = 2.38 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 22:15:25.711623: step 4280, loss = 2.38 (266.2 examples/sec; 0.481 sec/batch)
2017-04-02 22:15:29.694211: step 4290, loss = 2.38 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 22:15:33.727473: step 4300, loss = 2.38 (317.4 examples/sec; 0.403 sec/batch)
2017-04-02 22:15:37.695389: step 4310, loss = 2.38 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 22:15:41.689594: step 4320, loss = 2.37 (320.5 examples/sec; 0.399 sec/batch)
2017-04-02 22:15:45.655801: step 4330, loss = 2.38 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 22:15:49.599025: step 4340, loss = 2.38 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 22:17:54.644172: precision @ 1 = 0.105

2017-04-02 22:18:33.797597: step 4350, loss = 2.38 (7.8 examples/sec; 16.420 sec/batch)
2017-04-02 22:18:37.737814: step 4360, loss = 2.37 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 22:18:41.683031: step 4370, loss = 2.38 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 22:18:45.653505: step 4380, loss = 2.37 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 22:18:49.593734: step 4390, loss = 2.37 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 22:18:53.609735: step 4400, loss = 2.37 (318.7 examples/sec; 0.402 sec/batch)
2017-04-02 22:18:57.574486: step 4410, loss = 2.37 (322.8 examples/sec; 0.396 sec/batch)
2017-04-02 22:19:01.542483: step 4420, loss = 2.37 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 22:19:05.522939: step 4430, loss = 2.37 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 22:19:09.447864: step 4440, loss = 2.37 (326.1 examples/sec; 0.392 sec/batch)
2017-04-02 22:19:13.442518: step 4450, loss = 2.37 (320.4 examples/sec; 0.399 sec/batch)
2017-04-02 22:19:17.407404: step 4460, loss = 2.37 (322.8 examples/sec; 0.396 sec/batch)
2017-04-02 22:19:21.347996: step 4470, loss = 2.37 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 22:19:25.282517: step 4480, loss = 2.37 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 22:19:29.235959: step 4490, loss = 2.37 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 22:19:33.276298: step 4500, loss = 2.37 (316.8 examples/sec; 0.404 sec/batch)
2017-04-02 22:19:37.227421: step 4510, loss = 2.37 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 22:19:41.189258: step 4520, loss = 2.37 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 22:19:45.166788: step 4530, loss = 2.37 (321.8 examples/sec; 0.398 sec/batch)
2017-04-02 22:19:49.131732: step 4540, loss = 2.37 (322.8 examples/sec; 0.396 sec/batch)
2017-04-02 22:19:53.108091: step 4550, loss = 2.37 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 22:19:57.075737: step 4560, loss = 2.36 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 22:20:01.029475: step 4570, loss = 2.36 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 22:20:04.983681: step 4580, loss = 2.36 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 22:20:08.925669: step 4590, loss = 2.37 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 22:20:12.950573: step 4600, loss = 2.36 (318.0 examples/sec; 0.402 sec/batch)
2017-04-02 22:20:16.899945: step 4610, loss = 2.36 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 22:20:20.891532: step 4620, loss = 2.36 (320.7 examples/sec; 0.399 sec/batch)
2017-04-02 22:20:24.803282: step 4630, loss = 2.36 (327.2 examples/sec; 0.391 sec/batch)
2017-04-02 22:20:28.757387: step 4640, loss = 2.36 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 22:20:32.729008: step 4650, loss = 2.36 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 22:20:36.674267: step 4660, loss = 2.36 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 22:20:40.640324: step 4670, loss = 2.36 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 22:20:44.570768: step 4680, loss = 2.36 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 22:20:48.537682: step 4690, loss = 2.36 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 22:20:52.554813: step 4700, loss = 2.36 (318.6 examples/sec; 0.402 sec/batch)
2017-04-02 22:20:56.511916: step 4710, loss = 2.36 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:21:00.485045: step 4720, loss = 2.36 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 22:21:04.467704: step 4730, loss = 2.36 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 22:21:08.434385: step 4740, loss = 2.36 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 22:21:12.410479: step 4750, loss = 2.35 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 22:21:16.347346: step 4760, loss = 2.35 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 22:21:20.281737: step 4770, loss = 2.36 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 22:21:24.235211: step 4780, loss = 2.35 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 22:21:28.220804: step 4790, loss = 2.36 (321.2 examples/sec; 0.399 sec/batch)
2017-04-02 22:21:32.229539: step 4800, loss = 2.36 (319.3 examples/sec; 0.401 sec/batch)
2017-04-02 22:21:36.183551: step 4810, loss = 2.36 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 22:21:40.167739: step 4820, loss = 2.36 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 22:21:44.118918: step 4830, loss = 2.35 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 22:21:48.086010: step 4840, loss = 2.35 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 22:21:52.087438: step 4850, loss = 2.35 (319.9 examples/sec; 0.400 sec/batch)
2017-04-02 22:21:56.087872: step 4860, loss = 2.35 (320.0 examples/sec; 0.400 sec/batch)
2017-04-02 22:22:00.088076: step 4870, loss = 2.35 (320.0 examples/sec; 0.400 sec/batch)
2017-04-02 22:22:04.060809: step 4880, loss = 2.35 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 22:22:08.055232: step 4890, loss = 2.35 (320.4 examples/sec; 0.399 sec/batch)
2017-04-02 22:22:12.103265: step 4900, loss = 2.35 (316.2 examples/sec; 0.405 sec/batch)
2017-04-02 22:22:16.072454: step 4910, loss = 2.35 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 22:22:20.044506: step 4920, loss = 2.35 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 22:22:24.022333: step 4930, loss = 2.35 (321.8 examples/sec; 0.398 sec/batch)
2017-04-02 22:22:28.025124: step 4940, loss = 2.35 (319.8 examples/sec; 0.400 sec/batch)
2017-04-02 22:22:31.994457: step 4950, loss = 2.35 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 22:22:35.987581: step 4960, loss = 2.35 (320.6 examples/sec; 0.399 sec/batch)
2017-04-02 22:22:39.947520: step 4970, loss = 2.35 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 22:22:43.935902: step 4980, loss = 2.35 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 22:22:47.902528: step 4990, loss = 2.35 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 22:22:51.946237: step 5000, loss = 2.35 (316.5 examples/sec; 0.404 sec/batch)
2017-04-02 22:22:55.907577: step 5010, loss = 2.35 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 22:22:59.887804: step 5020, loss = 2.35 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 22:23:03.845350: step 5030, loss = 2.34 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 22:23:07.793374: step 5040, loss = 2.34 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 22:23:11.779190: step 5050, loss = 2.34 (321.1 examples/sec; 0.399 sec/batch)
2017-04-02 22:23:15.735527: step 5060, loss = 2.34 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:23:19.685654: step 5070, loss = 2.34 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 22:23:23.655462: step 5080, loss = 2.34 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 22:23:27.631389: step 5090, loss = 2.35 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 22:23:31.656584: step 5100, loss = 2.35 (318.0 examples/sec; 0.403 sec/batch)
2017-04-02 22:23:35.647853: step 5110, loss = 2.34 (320.7 examples/sec; 0.399 sec/batch)
2017-04-02 22:23:39.603653: step 5120, loss = 2.34 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 22:23:43.576937: step 5130, loss = 2.34 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 22:23:47.549886: step 5140, loss = 2.34 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 22:23:51.510323: step 5150, loss = 2.34 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 22:23:55.487222: step 5160, loss = 2.34 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 22:23:59.448550: step 5170, loss = 2.34 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 22:24:03.420189: step 5180, loss = 2.34 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 22:24:07.403716: step 5190, loss = 2.34 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 22:24:11.434729: step 5200, loss = 2.34 (317.5 examples/sec; 0.403 sec/batch)
2017-04-02 22:24:15.378504: step 5210, loss = 2.34 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 22:24:19.363302: step 5220, loss = 2.34 (321.2 examples/sec; 0.398 sec/batch)
2017-04-02 22:24:23.297134: step 5230, loss = 2.34 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 22:24:27.265732: step 5240, loss = 2.34 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 22:24:31.249449: step 5250, loss = 2.34 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 22:24:35.215219: step 5260, loss = 2.34 (322.8 examples/sec; 0.397 sec/batch)
2017-04-02 22:24:39.234531: step 5270, loss = 2.34 (318.5 examples/sec; 0.402 sec/batch)
2017-04-02 22:24:43.207221: step 5280, loss = 2.33 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 22:24:47.153365: step 5290, loss = 2.34 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 22:24:51.183474: step 5300, loss = 2.34 (317.6 examples/sec; 0.403 sec/batch)
2017-04-02 22:24:55.149254: step 5310, loss = 2.34 (322.8 examples/sec; 0.397 sec/batch)
2017-04-02 22:24:59.150967: step 5320, loss = 2.34 (319.9 examples/sec; 0.400 sec/batch)
2017-04-02 22:25:03.140854: step 5330, loss = 2.34 (320.8 examples/sec; 0.399 sec/batch)
2017-04-02 22:25:07.118441: step 5340, loss = 2.34 (321.8 examples/sec; 0.398 sec/batch)
2017-04-02 22:25:11.088079: step 5350, loss = 2.34 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 22:25:15.063937: step 5360, loss = 2.34 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 22:25:19.055196: step 5370, loss = 2.34 (320.7 examples/sec; 0.399 sec/batch)
2017-04-02 22:25:23.031531: step 5380, loss = 2.33 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 22:25:27.880403: step 5390, loss = 2.34 (264.0 examples/sec; 0.485 sec/batch)
2017-04-02 22:25:31.899278: step 5400, loss = 2.33 (318.5 examples/sec; 0.402 sec/batch)
2017-04-02 22:25:35.854539: step 5410, loss = 2.33 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 22:25:39.802288: step 5420, loss = 2.33 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 22:25:43.778689: step 5430, loss = 2.33 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 22:25:47.779272: step 5440, loss = 2.34 (320.0 examples/sec; 0.400 sec/batch)
2017-04-02 22:25:51.778627: step 5450, loss = 2.33 (320.1 examples/sec; 0.400 sec/batch)
2017-04-02 22:25:55.752093: step 5460, loss = 2.33 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 22:25:59.752493: step 5470, loss = 2.33 (320.0 examples/sec; 0.400 sec/batch)
2017-04-02 22:26:03.743730: step 5480, loss = 2.33 (320.7 examples/sec; 0.399 sec/batch)
2017-04-02 22:26:07.722363: step 5490, loss = 2.33 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 22:26:11.757427: step 5500, loss = 2.33 (317.2 examples/sec; 0.404 sec/batch)
2017-04-02 22:26:15.769036: step 5510, loss = 2.33 (319.1 examples/sec; 0.401 sec/batch)
2017-04-02 22:26:19.744819: step 5520, loss = 2.33 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 22:26:23.774072: step 5530, loss = 2.33 (317.7 examples/sec; 0.403 sec/batch)
2017-04-02 22:26:27.777978: step 5540, loss = 2.33 (319.7 examples/sec; 0.400 sec/batch)
2017-04-02 22:26:31.772405: step 5550, loss = 2.33 (320.4 examples/sec; 0.399 sec/batch)
2017-04-02 22:26:35.737152: step 5560, loss = 2.33 (322.8 examples/sec; 0.396 sec/batch)
2017-04-02 22:26:39.750979: step 5570, loss = 2.33 (318.9 examples/sec; 0.401 sec/batch)
2017-04-02 22:26:43.718150: step 5580, loss = 2.33 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 22:26:47.681478: step 5590, loss = 2.33 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 22:26:51.734830: step 5600, loss = 2.33 (315.8 examples/sec; 0.405 sec/batch)
2017-04-02 22:26:55.719373: step 5610, loss = 2.33 (321.2 examples/sec; 0.398 sec/batch)
2017-04-02 22:26:59.691548: step 5620, loss = 2.33 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 22:27:03.683483: step 5630, loss = 2.33 (320.6 examples/sec; 0.399 sec/batch)
2017-04-02 22:27:07.692365: step 5640, loss = 2.33 (319.3 examples/sec; 0.401 sec/batch)
2017-04-02 22:27:11.642540: step 5650, loss = 2.33 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 22:27:15.597836: step 5660, loss = 2.33 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 22:27:19.559769: step 5670, loss = 2.33 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 22:27:23.506708: step 5680, loss = 2.33 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 22:27:27.482789: step 5690, loss = 2.33 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 22:27:31.527898: step 5700, loss = 2.33 (316.4 examples/sec; 0.405 sec/batch)
2017-04-02 22:27:35.498323: step 5710, loss = 2.33 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 22:27:39.488368: step 5720, loss = 2.33 (320.8 examples/sec; 0.399 sec/batch)
2017-04-02 22:27:43.477816: step 5730, loss = 2.33 (320.8 examples/sec; 0.399 sec/batch)
2017-04-02 22:27:47.462720: step 5740, loss = 2.32 (321.2 examples/sec; 0.398 sec/batch)
2017-04-02 22:27:51.462546: step 5750, loss = 2.33 (320.0 examples/sec; 0.400 sec/batch)
2017-04-02 22:27:55.388926: step 5760, loss = 2.33 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 22:27:59.331782: step 5770, loss = 2.33 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 22:28:03.314537: step 5780, loss = 2.32 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 22:28:07.295129: step 5790, loss = 2.33 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 22:28:11.340412: step 5800, loss = 2.33 (316.4 examples/sec; 0.405 sec/batch)
2017-04-02 22:28:15.342831: step 5810, loss = 2.33 (319.8 examples/sec; 0.400 sec/batch)
2017-04-02 22:28:19.323909: step 5820, loss = 2.33 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 22:28:23.273316: step 5830, loss = 2.33 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 22:28:27.218319: step 5840, loss = 2.32 (324.5 examples/sec; 0.395 sec/batch)
2017-04-02 22:28:31.174451: step 5850, loss = 2.33 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:28:35.106075: step 5860, loss = 2.32 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 22:28:39.046789: step 5870, loss = 2.32 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 22:28:43.057053: step 5880, loss = 2.32 (319.2 examples/sec; 0.401 sec/batch)
2017-04-02 22:28:47.037911: step 5890, loss = 2.32 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 22:28:51.085968: step 5900, loss = 2.32 (316.2 examples/sec; 0.405 sec/batch)
2017-04-02 22:28:55.059274: step 5910, loss = 2.33 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 22:28:59.014353: step 5920, loss = 2.33 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 22:29:02.959267: step 5930, loss = 2.33 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 22:29:06.952901: step 5940, loss = 2.32 (320.5 examples/sec; 0.399 sec/batch)
2017-04-02 22:29:10.902630: step 5950, loss = 2.33 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 22:29:14.858387: step 5960, loss = 2.32 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 22:29:18.810658: step 5970, loss = 2.32 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 22:29:22.780409: step 5980, loss = 2.32 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 22:29:26.727050: step 5990, loss = 2.33 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 22:29:30.786176: step 6000, loss = 2.32 (315.3 examples/sec; 0.406 sec/batch)
2017-04-02 22:29:34.738599: step 6010, loss = 2.32 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 22:29:38.684333: step 6020, loss = 2.32 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 22:29:42.633864: step 6030, loss = 2.32 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 22:29:46.578550: step 6040, loss = 2.32 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 22:29:50.539598: step 6050, loss = 2.32 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 22:29:54.513292: step 6060, loss = 2.32 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 22:29:58.471633: step 6070, loss = 2.32 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 22:30:02.429538: step 6080, loss = 2.32 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 22:31:15.815329: precision @ 1 = 0.100

2017-04-02 22:31:53.631664: step 6090, loss = 2.32 (11.5 examples/sec; 11.120 sec/batch)
2017-04-02 22:31:57.643729: step 6100, loss = 2.32 (319.0 examples/sec; 0.401 sec/batch)
2017-04-02 22:32:01.615041: step 6110, loss = 2.32 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 22:32:05.557926: step 6120, loss = 2.32 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 22:32:09.510118: step 6130, loss = 2.32 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 22:32:13.501344: step 6140, loss = 2.32 (320.7 examples/sec; 0.399 sec/batch)
2017-04-02 22:32:17.465713: step 6150, loss = 2.32 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 22:32:21.437806: step 6160, loss = 2.32 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 22:32:25.379962: step 6170, loss = 2.32 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 22:32:29.344112: step 6180, loss = 2.32 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 22:32:33.307621: step 6190, loss = 2.32 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 22:32:37.323234: step 6200, loss = 2.32 (318.8 examples/sec; 0.402 sec/batch)
2017-04-02 22:32:41.282737: step 6210, loss = 2.32 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 22:32:45.233905: step 6220, loss = 2.32 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 22:32:49.162442: step 6230, loss = 2.32 (325.8 examples/sec; 0.393 sec/batch)
2017-04-02 22:32:53.114165: step 6240, loss = 2.32 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 22:32:57.054183: step 6250, loss = 2.32 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 22:33:01.020764: step 6260, loss = 2.32 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 22:33:04.994522: step 6270, loss = 2.32 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 22:33:08.934076: step 6280, loss = 2.32 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 22:33:12.878125: step 6290, loss = 2.32 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 22:33:16.888015: step 6300, loss = 2.32 (319.2 examples/sec; 0.401 sec/batch)
2017-04-02 22:33:20.826592: step 6310, loss = 2.32 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 22:33:24.765490: step 6320, loss = 2.32 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 22:33:28.743260: step 6330, loss = 2.32 (321.8 examples/sec; 0.398 sec/batch)
2017-04-02 22:33:32.698813: step 6340, loss = 2.32 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 22:33:36.655289: step 6350, loss = 2.32 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:33:40.627121: step 6360, loss = 2.32 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 22:33:44.651382: step 6370, loss = 2.32 (318.1 examples/sec; 0.402 sec/batch)
2017-04-02 22:33:48.570010: step 6380, loss = 2.32 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 22:33:52.515677: step 6390, loss = 2.32 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 22:33:56.544712: step 6400, loss = 2.32 (317.7 examples/sec; 0.403 sec/batch)
2017-04-02 22:34:00.518097: step 6410, loss = 2.32 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 22:34:04.515158: step 6420, loss = 2.31 (320.2 examples/sec; 0.400 sec/batch)
2017-04-02 22:34:08.477584: step 6430, loss = 2.32 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 22:34:12.451613: step 6440, loss = 2.31 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 22:34:16.387318: step 6450, loss = 2.32 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 22:34:20.355813: step 6460, loss = 2.32 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 22:34:24.328976: step 6470, loss = 2.32 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 22:34:28.266590: step 6480, loss = 2.31 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 22:34:32.215495: step 6490, loss = 2.31 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 22:34:36.234817: step 6500, loss = 2.32 (318.5 examples/sec; 0.402 sec/batch)
2017-04-02 22:34:40.185841: step 6510, loss = 2.32 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 22:34:44.125787: step 6520, loss = 2.32 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 22:34:48.057969: step 6530, loss = 2.32 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 22:34:52.017066: step 6540, loss = 2.32 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 22:34:55.961793: step 6550, loss = 2.32 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 22:34:59.907919: step 6560, loss = 2.32 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 22:35:03.876881: step 6570, loss = 2.31 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 22:35:07.817204: step 6580, loss = 2.32 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 22:35:11.763822: step 6590, loss = 2.32 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 22:35:15.779225: step 6600, loss = 2.31 (318.8 examples/sec; 0.402 sec/batch)
2017-04-02 22:35:19.715135: step 6610, loss = 2.31 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 22:35:24.447838: step 6620, loss = 2.31 (270.5 examples/sec; 0.473 sec/batch)
2017-04-02 22:35:28.392465: step 6630, loss = 2.31 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 22:35:32.344210: step 6640, loss = 2.31 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 22:35:36.324175: step 6650, loss = 2.31 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 22:35:40.280510: step 6660, loss = 2.31 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:35:44.215919: step 6670, loss = 2.32 (325.3 examples/sec; 0.394 sec/batch)
2017-04-02 22:35:48.139991: step 6680, loss = 2.31 (326.2 examples/sec; 0.392 sec/batch)
2017-04-02 22:35:52.041784: step 6690, loss = 2.32 (328.1 examples/sec; 0.390 sec/batch)
2017-04-02 22:35:56.042969: step 6700, loss = 2.32 (319.9 examples/sec; 0.400 sec/batch)
2017-04-02 22:35:59.968863: step 6710, loss = 2.32 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 22:36:03.915242: step 6720, loss = 2.31 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 22:36:07.853569: step 6730, loss = 2.31 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 22:36:11.800324: step 6740, loss = 2.32 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 22:36:15.731491: step 6750, loss = 2.31 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 22:36:19.684924: step 6760, loss = 2.31 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 22:36:23.634521: step 6770, loss = 2.31 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 22:36:27.570194: step 6780, loss = 2.31 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 22:36:31.519335: step 6790, loss = 2.31 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 22:36:35.523943: step 6800, loss = 2.31 (319.6 examples/sec; 0.400 sec/batch)
2017-04-02 22:36:39.461833: step 6810, loss = 2.31 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 22:36:43.417994: step 6820, loss = 2.31 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:36:47.356615: step 6830, loss = 2.31 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 22:36:51.292661: step 6840, loss = 2.31 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 22:36:55.227193: step 6850, loss = 2.31 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 22:36:59.171760: step 6860, loss = 2.31 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 22:37:03.125025: step 6870, loss = 2.31 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 22:37:07.064912: step 6880, loss = 2.31 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 22:37:11.009642: step 6890, loss = 2.31 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 22:37:14.998914: step 6900, loss = 2.31 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 22:37:18.924442: step 6910, loss = 2.31 (326.1 examples/sec; 0.393 sec/batch)
2017-04-02 22:37:22.851127: step 6920, loss = 2.31 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 22:37:26.814246: step 6930, loss = 2.31 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 22:37:30.779696: step 6940, loss = 2.31 (322.8 examples/sec; 0.397 sec/batch)
2017-04-02 22:37:34.738320: step 6950, loss = 2.31 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 22:37:38.681805: step 6960, loss = 2.31 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 22:37:42.614822: step 6970, loss = 2.31 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 22:37:46.553562: step 6980, loss = 2.32 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 22:37:50.497003: step 6990, loss = 2.31 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 22:37:54.499315: step 7000, loss = 2.31 (319.8 examples/sec; 0.400 sec/batch)
2017-04-02 22:37:58.461686: step 7010, loss = 2.31 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 22:38:02.421962: step 7020, loss = 2.31 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 22:38:06.372101: step 7030, loss = 2.31 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 22:38:10.313383: step 7040, loss = 2.31 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 22:38:14.261767: step 7050, loss = 2.31 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 22:38:18.247125: step 7060, loss = 2.31 (321.2 examples/sec; 0.399 sec/batch)
2017-04-02 22:38:22.209979: step 7070, loss = 2.31 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 22:38:26.178125: step 7080, loss = 2.31 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 22:38:30.133357: step 7090, loss = 2.31 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 22:38:34.134071: step 7100, loss = 2.31 (319.9 examples/sec; 0.400 sec/batch)
2017-04-02 22:38:38.107520: step 7110, loss = 2.31 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 22:38:42.082764: step 7120, loss = 2.31 (322.0 examples/sec; 0.398 sec/batch)
2017-04-02 22:38:46.032139: step 7130, loss = 2.31 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 22:38:49.994737: step 7140, loss = 2.31 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 22:38:53.943898: step 7150, loss = 2.31 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 22:38:57.890278: step 7160, loss = 2.31 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 22:39:01.850632: step 7170, loss = 2.31 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 22:39:05.864516: step 7180, loss = 2.31 (318.9 examples/sec; 0.401 sec/batch)
2017-04-02 22:39:09.818030: step 7190, loss = 2.31 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 22:39:13.856582: step 7200, loss = 2.31 (316.9 examples/sec; 0.404 sec/batch)
2017-04-02 22:39:17.811216: step 7210, loss = 2.31 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 22:39:21.751477: step 7220, loss = 2.31 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 22:39:25.686117: step 7230, loss = 2.31 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 22:39:29.645422: step 7240, loss = 2.31 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 22:39:33.643483: step 7250, loss = 2.31 (320.2 examples/sec; 0.400 sec/batch)
2017-04-02 22:39:37.594208: step 7260, loss = 2.31 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 22:39:41.574110: step 7270, loss = 2.31 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 22:39:45.512308: step 7280, loss = 2.31 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 22:39:49.480336: step 7290, loss = 2.31 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 22:39:53.488349: step 7300, loss = 2.31 (319.4 examples/sec; 0.401 sec/batch)
2017-04-02 22:39:57.418337: step 7310, loss = 2.31 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 22:40:01.382556: step 7320, loss = 2.31 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 22:40:05.332595: step 7330, loss = 2.31 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 22:40:09.305925: step 7340, loss = 2.31 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 22:40:13.278414: step 7350, loss = 2.31 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 22:40:17.186165: step 7360, loss = 2.31 (327.6 examples/sec; 0.391 sec/batch)
2017-04-02 22:40:21.108268: step 7370, loss = 2.31 (326.4 examples/sec; 0.392 sec/batch)
2017-04-02 22:40:25.053080: step 7380, loss = 2.31 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 22:40:29.012775: step 7390, loss = 2.31 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 22:40:33.046615: step 7400, loss = 2.31 (317.3 examples/sec; 0.403 sec/batch)
2017-04-02 22:40:36.999604: step 7410, loss = 2.31 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 22:40:40.949170: step 7420, loss = 2.30 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 22:40:44.917366: step 7430, loss = 2.31 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 22:40:48.870242: step 7440, loss = 2.31 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 22:40:52.863162: step 7450, loss = 2.31 (320.6 examples/sec; 0.399 sec/batch)
2017-04-02 22:40:56.851097: step 7460, loss = 2.31 (321.0 examples/sec; 0.399 sec/batch)
2017-04-02 22:41:00.799367: step 7470, loss = 2.31 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 22:41:04.747310: step 7480, loss = 2.31 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 22:41:08.674528: step 7490, loss = 2.31 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 22:41:12.658808: step 7500, loss = 2.31 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 22:41:16.643654: step 7510, loss = 2.31 (321.2 examples/sec; 0.398 sec/batch)
2017-04-02 22:41:20.571260: step 7520, loss = 2.31 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 22:41:24.505513: step 7530, loss = 2.31 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 22:41:28.456986: step 7540, loss = 2.31 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 22:41:32.433584: step 7550, loss = 2.31 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 22:41:36.399720: step 7560, loss = 2.31 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 22:41:40.349663: step 7570, loss = 2.31 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 22:41:44.290438: step 7580, loss = 2.31 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 22:41:48.241462: step 7590, loss = 2.31 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 22:41:52.267618: step 7600, loss = 2.31 (317.9 examples/sec; 0.403 sec/batch)
2017-04-02 22:41:56.224409: step 7610, loss = 2.31 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:42:00.163279: step 7620, loss = 2.31 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 22:42:04.084582: step 7630, loss = 2.31 (326.4 examples/sec; 0.392 sec/batch)
2017-04-02 22:42:08.009998: step 7640, loss = 2.31 (326.1 examples/sec; 0.393 sec/batch)
2017-04-02 22:42:11.939850: step 7650, loss = 2.31 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 22:42:15.882113: step 7660, loss = 2.31 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 22:42:19.820744: step 7670, loss = 2.31 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 22:42:23.743642: step 7680, loss = 2.31 (326.3 examples/sec; 0.392 sec/batch)
2017-04-02 22:42:27.704225: step 7690, loss = 2.31 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 22:42:31.696160: step 7700, loss = 2.31 (320.6 examples/sec; 0.399 sec/batch)
2017-04-02 22:42:35.617665: step 7710, loss = 2.31 (326.4 examples/sec; 0.392 sec/batch)
2017-04-02 22:42:39.577989: step 7720, loss = 2.31 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 22:42:43.515279: step 7730, loss = 2.31 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 22:42:47.473756: step 7740, loss = 2.31 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 22:42:51.392509: step 7750, loss = 2.30 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 22:42:55.323016: step 7760, loss = 2.31 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 22:42:59.267408: step 7770, loss = 2.31 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 22:43:03.212644: step 7780, loss = 2.30 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 22:43:07.186694: step 7790, loss = 2.31 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 22:43:11.168356: step 7800, loss = 2.31 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 22:43:15.100962: step 7810, loss = 2.31 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 22:43:19.059020: step 7820, loss = 2.31 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 22:43:22.984853: step 7830, loss = 2.31 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 22:43:26.924922: step 7840, loss = 2.31 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 22:43:30.883720: step 7850, loss = 2.31 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 22:43:34.816250: step 7860, loss = 2.31 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 22:43:38.787590: step 7870, loss = 2.31 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 22:43:42.753816: step 7880, loss = 2.31 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 22:43:46.718450: step 7890, loss = 2.31 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 22:43:50.751076: step 7900, loss = 2.31 (317.4 examples/sec; 0.403 sec/batch)
2017-04-02 22:43:54.682518: step 7910, loss = 2.31 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 22:43:58.631142: step 7920, loss = 2.31 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 22:44:02.569293: step 7930, loss = 2.31 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 22:44:06.534265: step 7940, loss = 2.31 (322.8 examples/sec; 0.396 sec/batch)
2017-04-02 22:44:10.483011: step 7950, loss = 2.31 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 22:44:14.441417: step 7960, loss = 2.31 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 22:44:18.376063: step 7970, loss = 2.31 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 22:44:22.327041: step 7980, loss = 2.31 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 22:44:26.269826: step 7990, loss = 2.31 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 22:44:30.261691: step 8000, loss = 2.31 (320.7 examples/sec; 0.399 sec/batch)
2017-04-02 22:44:34.168121: step 8010, loss = 2.31 (327.7 examples/sec; 0.391 sec/batch)
2017-04-02 22:44:38.081499: step 8020, loss = 2.31 (327.1 examples/sec; 0.391 sec/batch)
2017-04-02 22:44:42.039335: step 8030, loss = 2.31 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 22:46:01.063745: precision @ 1 = 0.105

2017-04-02 22:47:14.434791: step 8040, loss = 2.30 (8.4 examples/sec; 15.240 sec/batch)
2017-04-02 22:47:18.366346: step 8050, loss = 2.31 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 22:47:22.326411: step 8060, loss = 2.31 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 22:47:26.246917: step 8070, loss = 2.31 (326.5 examples/sec; 0.392 sec/batch)
2017-04-02 22:47:30.183595: step 8080, loss = 2.31 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 22:47:34.131655: step 8090, loss = 2.31 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 22:47:38.140750: step 8100, loss = 2.31 (319.3 examples/sec; 0.401 sec/batch)
2017-04-02 22:47:42.067441: step 8110, loss = 2.31 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 22:47:46.020863: step 8120, loss = 2.31 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 22:47:49.976758: step 8130, loss = 2.31 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 22:47:53.902954: step 8140, loss = 2.30 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 22:47:57.824344: step 8150, loss = 2.30 (326.4 examples/sec; 0.392 sec/batch)
2017-04-02 22:48:01.779984: step 8160, loss = 2.31 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 22:48:05.719144: step 8170, loss = 2.31 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 22:48:09.658977: step 8180, loss = 2.31 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 22:48:13.614851: step 8190, loss = 2.31 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 22:48:17.619874: step 8200, loss = 2.31 (319.6 examples/sec; 0.401 sec/batch)
2017-04-02 22:48:21.566088: step 8210, loss = 2.31 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 22:48:25.514356: step 8220, loss = 2.31 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 22:48:29.449097: step 8230, loss = 2.31 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 22:48:33.402191: step 8240, loss = 2.31 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 22:48:37.364563: step 8250, loss = 2.30 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 22:48:41.311873: step 8260, loss = 2.31 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 22:48:45.266764: step 8270, loss = 2.31 (323.6 examples/sec; 0.395 sec/batch)
2017-04-02 22:48:49.236606: step 8280, loss = 2.31 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 22:48:53.186632: step 8290, loss = 2.31 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 22:48:57.209287: step 8300, loss = 2.31 (318.2 examples/sec; 0.402 sec/batch)
2017-04-02 22:49:01.163957: step 8310, loss = 2.30 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 22:49:05.096461: step 8320, loss = 2.31 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 22:49:09.014504: step 8330, loss = 2.31 (326.7 examples/sec; 0.392 sec/batch)
2017-04-02 22:49:12.955636: step 8340, loss = 2.31 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 22:49:16.861650: step 8350, loss = 2.31 (327.7 examples/sec; 0.391 sec/batch)
2017-04-02 22:49:20.812597: step 8360, loss = 2.31 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 22:49:24.761632: step 8370, loss = 2.30 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 22:49:28.691902: step 8380, loss = 2.31 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 22:49:32.655459: step 8390, loss = 2.31 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 22:49:36.634202: step 8400, loss = 2.31 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 22:49:40.580103: step 8410, loss = 2.31 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 22:49:44.532157: step 8420, loss = 2.31 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 22:49:48.474622: step 8430, loss = 2.31 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 22:49:52.421493: step 8440, loss = 2.30 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 22:49:56.333946: step 8450, loss = 2.30 (327.2 examples/sec; 0.391 sec/batch)
2017-04-02 22:50:00.279965: step 8460, loss = 2.30 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 22:50:04.231732: step 8470, loss = 2.30 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 22:50:08.187710: step 8480, loss = 2.30 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 22:50:12.138702: step 8490, loss = 2.30 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 22:50:16.144394: step 8500, loss = 2.31 (319.5 examples/sec; 0.401 sec/batch)
2017-04-02 22:50:20.069674: step 8510, loss = 2.30 (326.1 examples/sec; 0.393 sec/batch)
2017-04-02 22:50:24.004462: step 8520, loss = 2.30 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 22:50:27.949468: step 8530, loss = 2.30 (324.5 examples/sec; 0.395 sec/batch)
2017-04-02 22:50:31.854210: step 8540, loss = 2.31 (327.8 examples/sec; 0.390 sec/batch)
2017-04-02 22:50:35.784741: step 8550, loss = 2.30 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 22:50:39.705935: step 8560, loss = 2.30 (326.4 examples/sec; 0.392 sec/batch)
2017-04-02 22:50:43.659342: step 8570, loss = 2.31 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 22:50:47.594674: step 8580, loss = 2.30 (325.3 examples/sec; 0.394 sec/batch)
2017-04-02 22:50:51.540713: step 8590, loss = 2.30 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 22:50:55.566846: step 8600, loss = 2.31 (317.9 examples/sec; 0.403 sec/batch)
2017-04-02 22:50:59.479187: step 8610, loss = 2.30 (327.2 examples/sec; 0.391 sec/batch)
2017-04-02 22:51:03.425857: step 8620, loss = 2.31 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 22:51:07.386378: step 8630, loss = 2.30 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 22:51:11.304393: step 8640, loss = 2.30 (326.7 examples/sec; 0.392 sec/batch)
2017-04-02 22:51:15.227637: step 8650, loss = 2.30 (326.3 examples/sec; 0.392 sec/batch)
2017-04-02 22:51:19.172327: step 8660, loss = 2.31 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 22:51:23.120926: step 8670, loss = 2.30 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 22:51:27.073781: step 8680, loss = 2.31 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 22:51:31.027544: step 8690, loss = 2.31 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 22:51:35.042062: step 8700, loss = 2.30 (318.8 examples/sec; 0.401 sec/batch)
2017-04-02 22:51:39.070642: step 8710, loss = 2.30 (317.7 examples/sec; 0.403 sec/batch)
2017-04-02 22:51:43.012365: step 8720, loss = 2.31 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 22:51:46.982222: step 8730, loss = 2.31 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 22:51:50.928553: step 8740, loss = 2.30 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 22:51:54.861608: step 8750, loss = 2.31 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 22:51:58.832517: step 8760, loss = 2.30 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 22:52:02.803362: step 8770, loss = 2.31 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 22:52:06.746540: step 8780, loss = 2.30 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 22:52:10.678894: step 8790, loss = 2.31 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 22:52:14.704944: step 8800, loss = 2.30 (317.9 examples/sec; 0.403 sec/batch)
2017-04-02 22:52:18.653138: step 8810, loss = 2.30 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 22:52:22.571242: step 8820, loss = 2.30 (326.7 examples/sec; 0.392 sec/batch)
2017-04-02 22:52:26.498676: step 8830, loss = 2.31 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 22:52:30.430236: step 8840, loss = 2.30 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 22:52:34.363078: step 8850, loss = 2.31 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 22:52:38.314614: step 8860, loss = 2.31 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 22:52:42.270973: step 8870, loss = 2.31 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:52:46.235791: step 8880, loss = 2.31 (322.8 examples/sec; 0.396 sec/batch)
2017-04-02 22:52:50.201892: step 8890, loss = 2.30 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 22:52:54.182848: step 8900, loss = 2.31 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 22:52:58.156225: step 8910, loss = 2.30 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 22:53:02.094638: step 8920, loss = 2.30 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 22:53:06.026015: step 8930, loss = 2.30 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 22:53:09.998510: step 8940, loss = 2.30 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 22:53:13.943620: step 8950, loss = 2.30 (324.5 examples/sec; 0.395 sec/batch)
2017-04-02 22:53:17.873917: step 8960, loss = 2.31 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 22:53:21.852239: step 8970, loss = 2.31 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 22:53:25.792722: step 8980, loss = 2.30 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 22:53:29.744243: step 8990, loss = 2.31 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 22:53:33.735060: step 9000, loss = 2.30 (320.7 examples/sec; 0.399 sec/batch)
2017-04-02 22:53:37.674043: step 9010, loss = 2.31 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 22:53:41.618799: step 9020, loss = 2.30 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 22:53:45.606509: step 9030, loss = 2.30 (321.0 examples/sec; 0.399 sec/batch)
2017-04-02 22:53:49.525989: step 9040, loss = 2.31 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 22:53:53.473580: step 9050, loss = 2.30 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 22:53:57.436585: step 9060, loss = 2.31 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 22:54:01.369814: step 9070, loss = 2.30 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 22:54:05.297702: step 9080, loss = 2.30 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 22:54:09.236324: step 9090, loss = 2.31 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 22:54:13.266436: step 9100, loss = 2.31 (317.6 examples/sec; 0.403 sec/batch)
2017-04-02 22:54:17.175705: step 9110, loss = 2.31 (327.4 examples/sec; 0.391 sec/batch)
2017-04-02 22:54:21.115714: step 9120, loss = 2.31 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 22:54:25.035385: step 9130, loss = 2.31 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 22:54:28.957973: step 9140, loss = 2.31 (326.3 examples/sec; 0.392 sec/batch)
2017-04-02 22:54:32.883196: step 9150, loss = 2.31 (326.1 examples/sec; 0.393 sec/batch)
2017-04-02 22:54:36.823584: step 9160, loss = 2.30 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 22:54:40.767445: step 9170, loss = 2.31 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 22:54:44.717030: step 9180, loss = 2.30 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 22:54:48.679321: step 9190, loss = 2.31 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 22:54:52.660325: step 9200, loss = 2.30 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 22:54:56.609591: step 9210, loss = 2.30 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 22:55:00.583424: step 9220, loss = 2.30 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 22:55:04.495509: step 9230, loss = 2.30 (327.2 examples/sec; 0.391 sec/batch)
2017-04-02 22:55:08.442936: step 9240, loss = 2.31 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 22:55:12.371816: step 9250, loss = 2.31 (325.8 examples/sec; 0.393 sec/batch)
2017-04-02 22:55:16.312542: step 9260, loss = 2.30 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 22:55:20.236835: step 9270, loss = 2.31 (326.2 examples/sec; 0.392 sec/batch)
2017-04-02 22:55:24.170997: step 9280, loss = 2.31 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 22:55:28.142598: step 9290, loss = 2.30 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 22:55:32.161245: step 9300, loss = 2.31 (318.5 examples/sec; 0.402 sec/batch)
2017-04-02 22:55:36.119455: step 9310, loss = 2.31 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 22:55:40.047843: step 9320, loss = 2.31 (325.8 examples/sec; 0.393 sec/batch)
2017-04-02 22:55:44.015478: step 9330, loss = 2.30 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 22:55:47.944656: step 9340, loss = 2.30 (325.8 examples/sec; 0.393 sec/batch)
2017-04-02 22:55:51.875799: step 9350, loss = 2.30 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 22:55:55.815934: step 9360, loss = 2.31 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 22:55:59.737472: step 9370, loss = 2.31 (326.4 examples/sec; 0.392 sec/batch)
2017-04-02 22:56:03.673096: step 9380, loss = 2.30 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 22:56:07.609053: step 9390, loss = 2.30 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 22:56:11.588165: step 9400, loss = 2.31 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 22:56:15.527975: step 9410, loss = 2.30 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 22:56:19.468960: step 9420, loss = 2.30 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 22:56:23.417257: step 9430, loss = 2.31 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 22:56:27.351257: step 9440, loss = 2.30 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 22:56:31.274946: step 9450, loss = 2.30 (326.2 examples/sec; 0.392 sec/batch)
2017-04-02 22:56:35.206965: step 9460, loss = 2.30 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 22:56:39.139698: step 9470, loss = 2.31 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 22:56:43.073829: step 9480, loss = 2.30 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 22:56:47.015670: step 9490, loss = 2.30 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 22:56:51.038470: step 9500, loss = 2.31 (318.2 examples/sec; 0.402 sec/batch)
2017-04-02 22:56:54.986702: step 9510, loss = 2.30 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 22:56:58.921839: step 9520, loss = 2.30 (325.3 examples/sec; 0.394 sec/batch)
2017-04-02 22:57:02.860080: step 9530, loss = 2.30 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 22:57:06.817687: step 9540, loss = 2.30 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 22:57:10.782189: step 9550, loss = 2.30 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 22:57:15.439417: step 9560, loss = 2.31 (274.8 examples/sec; 0.466 sec/batch)
2017-04-02 22:57:19.370645: step 9570, loss = 2.30 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 22:57:23.311084: step 9580, loss = 2.30 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 22:57:27.248615: step 9590, loss = 2.30 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 22:57:31.259583: step 9600, loss = 2.30 (319.1 examples/sec; 0.401 sec/batch)
2017-04-02 22:57:35.196646: step 9610, loss = 2.30 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 22:57:39.147127: step 9620, loss = 2.30 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 22:57:43.089305: step 9630, loss = 2.30 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 22:57:47.035551: step 9640, loss = 2.30 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 22:57:50.998313: step 9650, loss = 2.30 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 22:57:54.954972: step 9660, loss = 2.30 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:57:58.902717: step 9670, loss = 2.30 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 22:58:02.849560: step 9680, loss = 2.30 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 22:58:06.778106: step 9690, loss = 2.31 (325.8 examples/sec; 0.393 sec/batch)
2017-04-02 22:58:10.770769: step 9700, loss = 2.30 (320.6 examples/sec; 0.399 sec/batch)
2017-04-02 22:58:14.735089: step 9710, loss = 2.30 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 22:58:18.695428: step 9720, loss = 2.30 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 22:58:22.626300: step 9730, loss = 2.31 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 22:58:26.570422: step 9740, loss = 2.31 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 22:58:30.500340: step 9750, loss = 2.30 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 22:58:34.433721: step 9760, loss = 2.31 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 22:58:38.377816: step 9770, loss = 2.30 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 22:58:42.288052: step 9780, loss = 2.31 (327.3 examples/sec; 0.391 sec/batch)
2017-04-02 22:58:46.228032: step 9790, loss = 2.30 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 22:58:50.230742: step 9800, loss = 2.31 (319.8 examples/sec; 0.400 sec/batch)
2017-04-02 22:58:54.174098: step 9810, loss = 2.30 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 22:58:58.119096: step 9820, loss = 2.30 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 22:59:02.062384: step 9830, loss = 2.30 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 22:59:05.981802: step 9840, loss = 2.30 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 22:59:09.929083: step 9850, loss = 2.31 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 22:59:13.879357: step 9860, loss = 2.31 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 22:59:17.825673: step 9870, loss = 2.30 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 22:59:21.750910: step 9880, loss = 2.30 (326.1 examples/sec; 0.393 sec/batch)
2017-04-02 22:59:25.685479: step 9890, loss = 2.30 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 22:59:29.702027: step 9900, loss = 2.31 (318.7 examples/sec; 0.402 sec/batch)
2017-04-02 22:59:33.641474: step 9910, loss = 2.30 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 22:59:37.566336: step 9920, loss = 2.31 (326.1 examples/sec; 0.392 sec/batch)
2017-04-02 22:59:41.477883: step 9930, loss = 2.30 (327.2 examples/sec; 0.391 sec/batch)
2017-04-02 22:59:45.434793: step 9940, loss = 2.30 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:59:49.355853: step 9950, loss = 2.31 (326.4 examples/sec; 0.392 sec/batch)
2017-04-02 22:59:53.274491: step 9960, loss = 2.30 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 22:59:57.214656: step 9970, loss = 2.30 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 23:00:01.159072: step 9980, loss = 2.31 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 23:00:05.099101: step 9990, loss = 2.30 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 23:00:09.107889: step 10000, loss = 2.30 (319.3 examples/sec; 0.401 sec/batch)
2017-04-02 23:00:13.047356: step 10010, loss = 2.31 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 23:00:16.987457: step 10020, loss = 2.30 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 23:00:20.905441: step 10030, loss = 2.30 (326.7 examples/sec; 0.392 sec/batch)
2017-04-02 23:00:24.811552: step 10040, loss = 2.30 (327.7 examples/sec; 0.391 sec/batch)
2017-04-02 23:00:28.737949: step 10050, loss = 2.31 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 23:00:32.643902: step 10060, loss = 2.31 (327.7 examples/sec; 0.391 sec/batch)
2017-04-02 23:00:36.583335: step 10070, loss = 2.31 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 23:00:40.554738: step 10080, loss = 2.30 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 23:01:27.002406: precision @ 1 = 0.100

2017-04-02 23:02:24.689488: step 10090, loss = 2.30 (12.3 examples/sec; 10.413 sec/batch)
2017-04-02 23:02:28.710748: step 10100, loss = 2.30 (318.3 examples/sec; 0.402 sec/batch)
2017-04-02 23:02:32.657197: step 10110, loss = 2.31 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 23:02:36.590079: step 10120, loss = 2.30 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 23:02:40.510197: step 10130, loss = 2.31 (326.5 examples/sec; 0.392 sec/batch)
2017-04-02 23:02:44.473881: step 10140, loss = 2.30 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 23:02:48.411811: step 10150, loss = 2.30 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 23:02:52.350277: step 10160, loss = 2.30 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 23:02:56.307925: step 10170, loss = 2.30 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 23:03:00.240751: step 10180, loss = 2.31 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 23:03:04.209276: step 10190, loss = 2.30 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 23:03:08.206212: step 10200, loss = 2.30 (320.2 examples/sec; 0.400 sec/batch)
2017-04-02 23:03:12.140826: step 10210, loss = 2.31 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 23:03:16.061939: step 10220, loss = 2.30 (326.4 examples/sec; 0.392 sec/batch)
2017-04-02 23:03:19.993317: step 10230, loss = 2.30 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 23:03:23.923588: step 10240, loss = 2.30 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 23:03:27.864331: step 10250, loss = 2.31 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 23:03:31.794154: step 10260, loss = 2.30 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 23:03:35.750328: step 10270, loss = 2.31 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 23:03:39.691275: step 10280, loss = 2.31 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 23:03:43.623672: step 10290, loss = 2.30 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 23:03:47.642464: step 10300, loss = 2.30 (318.5 examples/sec; 0.402 sec/batch)
2017-04-02 23:03:51.605445: step 10310, loss = 2.30 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 23:03:55.526789: step 10320, loss = 2.30 (326.4 examples/sec; 0.392 sec/batch)
2017-04-02 23:03:59.476041: step 10330, loss = 2.30 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 23:04:03.446565: step 10340, loss = 2.30 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 23:04:07.391670: step 10350, loss = 2.30 (324.5 examples/sec; 0.395 sec/batch)
2017-04-02 23:04:11.339781: step 10360, loss = 2.30 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 23:04:15.267704: step 10370, loss = 2.31 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 23:04:19.203658: step 10380, loss = 2.30 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 23:04:23.141728: step 10390, loss = 2.31 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 23:04:27.161830: step 10400, loss = 2.30 (318.4 examples/sec; 0.402 sec/batch)
2017-04-02 23:04:31.107968: step 10410, loss = 2.30 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 23:04:35.073418: step 10420, loss = 2.30 (322.8 examples/sec; 0.397 sec/batch)
2017-04-02 23:04:39.038592: step 10430, loss = 2.30 (322.8 examples/sec; 0.397 sec/batch)
2017-04-02 23:04:42.961395: step 10440, loss = 2.30 (326.3 examples/sec; 0.392 sec/batch)
2017-04-02 23:04:46.907538: step 10450, loss = 2.30 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 23:04:50.856194: step 10460, loss = 2.31 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 23:04:54.792020: step 10470, loss = 2.30 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 23:04:58.708887: step 10480, loss = 2.30 (326.8 examples/sec; 0.392 sec/batch)
2017-04-02 23:05:02.638230: step 10490, loss = 2.30 (325.8 examples/sec; 0.393 sec/batch)
2017-04-02 23:05:06.633230: step 10500, loss = 2.30 (320.4 examples/sec; 0.400 sec/batch)
2017-04-02 23:05:10.590009: step 10510, loss = 2.30 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 23:05:14.514399: step 10520, loss = 2.30 (326.2 examples/sec; 0.392 sec/batch)
2017-04-02 23:05:18.441454: step 10530, loss = 2.31 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 23:05:22.379123: step 10540, loss = 2.30 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 23:05:26.332675: step 10550, loss = 2.30 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 23:05:30.271042: step 10560, loss = 2.30 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 23:05:34.219017: step 10570, loss = 2.30 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 23:05:38.132918: step 10580, loss = 2.30 (327.0 examples/sec; 0.391 sec/batch)
2017-04-02 23:05:42.085616: step 10590, loss = 2.30 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 23:05:46.106916: step 10600, loss = 2.30 (318.3 examples/sec; 0.402 sec/batch)
2017-04-02 23:05:50.042719: step 10610, loss = 2.30 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 23:05:54.003604: step 10620, loss = 2.30 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 23:05:57.930409: step 10630, loss = 2.30 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 23:06:01.867093: step 10640, loss = 2.30 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 23:06:05.839361: step 10650, loss = 2.31 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 23:06:09.781514: step 10660, loss = 2.30 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 23:06:13.768864: step 10670, loss = 2.31 (321.0 examples/sec; 0.399 sec/batch)
2017-04-02 23:06:17.725660: step 10680, loss = 2.30 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 23:06:21.651734: step 10690, loss = 2.30 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 23:06:25.646664: step 10700, loss = 2.30 (320.4 examples/sec; 0.399 sec/batch)
2017-04-02 23:06:29.547503: step 10710, loss = 2.30 (328.1 examples/sec; 0.390 sec/batch)
2017-04-02 23:06:33.471818: step 10720, loss = 2.30 (326.2 examples/sec; 0.392 sec/batch)
2017-04-02 23:06:37.451436: step 10730, loss = 2.30 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 23:06:41.404997: step 10740, loss = 2.30 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 23:06:45.364390: step 10750, loss = 2.30 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 23:06:49.297760: step 10760, loss = 2.30 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 23:06:53.231680: step 10770, loss = 2.30 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 23:06:57.185326: step 10780, loss = 2.30 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 23:07:01.127405: step 10790, loss = 2.30 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 23:07:05.135835: step 10800, loss = 2.31 (319.3 examples/sec; 0.401 sec/batch)
2017-04-02 23:07:09.095362: step 10810, loss = 2.30 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 23:07:14.320124: step 10820, loss = 2.30 (245.0 examples/sec; 0.522 sec/batch)
2017-04-02 23:07:18.273612: step 10830, loss = 2.31 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 23:07:22.232849: step 10840, loss = 2.30 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 23:07:26.187115: step 10850, loss = 2.30 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 23:07:30.131247: step 10860, loss = 2.30 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 23:07:34.096487: step 10870, loss = 2.30 (322.8 examples/sec; 0.397 sec/batch)
2017-04-02 23:07:38.052632: step 10880, loss = 2.30 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 23:07:41.994153: step 10890, loss = 2.30 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 23:07:46.039049: step 10900, loss = 2.30 (316.4 examples/sec; 0.404 sec/batch)
2017-04-02 23:07:49.999649: step 10910, loss = 2.30 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 23:07:53.978152: step 10920, loss = 2.30 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 23:07:57.953855: step 10930, loss = 2.30 (322.0 examples/sec; 0.398 sec/batch)
2017-04-02 23:08:01.926818: step 10940, loss = 2.30 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 23:08:05.860423: step 10950, loss = 2.30 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 23:08:09.805420: step 10960, loss = 2.30 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 23:08:13.774795: step 10970, loss = 2.30 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 23:08:17.735322: step 10980, loss = 2.31 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 23:08:21.688075: step 10990, loss = 2.30 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 23:08:25.718261: step 11000, loss = 2.30 (317.6 examples/sec; 0.403 sec/batch)
2017-04-02 23:08:29.665436: step 11010, loss = 2.30 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 23:08:33.606925: step 11020, loss = 2.30 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 23:08:37.563526: step 11030, loss = 2.30 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 23:08:41.525479: step 11040, loss = 2.31 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 23:08:45.498201: step 11050, loss = 2.30 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 23:08:49.462780: step 11060, loss = 2.31 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 23:08:53.418554: step 11070, loss = 2.30 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 23:08:57.393976: step 11080, loss = 2.30 (322.0 examples/sec; 0.398 sec/batch)
2017-04-02 23:09:01.348106: step 11090, loss = 2.30 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 23:09:05.360398: step 11100, loss = 2.30 (319.0 examples/sec; 0.401 sec/batch)
2017-04-02 23:09:09.323639: step 11110, loss = 2.30 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 23:09:13.259324: step 11120, loss = 2.30 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 23:09:17.258188: step 11130, loss = 2.30 (320.1 examples/sec; 0.400 sec/batch)
2017-04-02 23:09:21.243961: step 11140, loss = 2.30 (321.1 examples/sec; 0.399 sec/batch)
2017-04-02 23:09:25.192812: step 11150, loss = 2.30 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 23:09:29.165125: step 11160, loss = 2.31 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 23:09:33.104330: step 11170, loss = 2.30 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 23:09:37.086168: step 11180, loss = 2.31 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 23:09:41.008722: step 11190, loss = 2.30 (326.3 examples/sec; 0.392 sec/batch)
2017-04-02 23:09:45.036046: step 11200, loss = 2.30 (317.8 examples/sec; 0.403 sec/batch)
2017-04-02 23:09:49.006154: step 11210, loss = 2.30 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 23:09:52.957737: step 11220, loss = 2.30 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 23:09:56.928360: step 11230, loss = 2.30 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 23:10:00.898847: step 11240, loss = 2.30 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 23:10:04.854022: step 11250, loss = 2.30 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 23:10:08.822451: step 11260, loss = 2.31 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 23:10:12.767437: step 11270, loss = 2.30 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 23:10:16.722817: step 11280, loss = 2.30 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 23:10:20.657053: step 11290, loss = 2.31 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 23:10:24.710254: step 11300, loss = 2.30 (315.8 examples/sec; 0.405 sec/batch)
2017-04-02 23:10:28.705927: step 11310, loss = 2.30 (320.3 examples/sec; 0.400 sec/batch)
2017-04-02 23:10:32.662899: step 11320, loss = 2.30 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 23:10:36.602831: step 11330, loss = 2.30 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 23:10:40.569075: step 11340, loss = 2.30 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 23:10:44.525865: step 11350, loss = 2.30 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 23:10:48.487275: step 11360, loss = 2.30 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 23:10:52.457400: step 11370, loss = 2.30 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 23:10:56.413071: step 11380, loss = 2.30 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 23:11:00.367646: step 11390, loss = 2.30 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 23:11:04.368935: step 11400, loss = 2.30 (319.9 examples/sec; 0.400 sec/batch)
2017-04-02 23:11:08.328936: step 11410, loss = 2.31 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 23:11:12.278128: step 11420, loss = 2.30 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 23:11:16.255516: step 11430, loss = 2.30 (321.8 examples/sec; 0.398 sec/batch)
2017-04-02 23:11:20.227696: step 11440, loss = 2.30 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 23:11:24.181542: step 11450, loss = 2.30 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 23:11:28.158603: step 11460, loss = 2.31 (321.8 examples/sec; 0.398 sec/batch)
2017-04-02 23:11:32.130092: step 11470, loss = 2.30 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 23:11:36.114314: step 11480, loss = 2.30 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 23:11:40.087051: step 11490, loss = 2.30 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 23:11:44.109654: step 11500, loss = 2.30 (318.2 examples/sec; 0.402 sec/batch)
2017-04-02 23:11:48.098438: step 11510, loss = 2.30 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 23:11:52.115784: step 11520, loss = 2.30 (318.6 examples/sec; 0.402 sec/batch)
2017-04-02 23:11:56.128099: step 11530, loss = 2.30 (319.0 examples/sec; 0.401 sec/batch)
2017-04-02 23:12:00.120179: step 11540, loss = 2.30 (320.6 examples/sec; 0.399 sec/batch)
2017-04-02 23:12:04.137737: step 11550, loss = 2.30 (318.6 examples/sec; 0.402 sec/batch)
2017-04-02 23:12:08.120219: step 11560, loss = 2.30 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 23:12:12.138589: step 11570, loss = 2.30 (318.5 examples/sec; 0.402 sec/batch)
2017-04-02 23:12:16.107339: step 11580, loss = 2.30 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 23:12:20.081701: step 11590, loss = 2.30 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 23:12:24.174396: step 11600, loss = 2.30 (312.8 examples/sec; 0.409 sec/batch)
2017-04-02 23:12:28.189903: step 11610, loss = 2.30 (318.8 examples/sec; 0.402 sec/batch)
2017-04-02 23:12:32.174771: step 11620, loss = 2.30 (321.2 examples/sec; 0.398 sec/batch)
2017-04-02 23:12:36.180562: step 11630, loss = 2.30 (319.5 examples/sec; 0.401 sec/batch)
2017-04-02 23:12:40.189216: step 11640, loss = 2.30 (319.3 examples/sec; 0.401 sec/batch)
2017-04-02 23:12:44.186920: step 11650, loss = 2.30 (320.2 examples/sec; 0.400 sec/batch)
2017-04-02 23:12:48.211133: step 11660, loss = 2.31 (318.1 examples/sec; 0.402 sec/batch)
2017-04-02 23:12:52.213766: step 11670, loss = 2.30 (319.8 examples/sec; 0.400 sec/batch)
2017-04-02 23:12:56.202427: step 11680, loss = 2.30 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 23:13:00.185019: step 11690, loss = 2.30 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 23:13:04.243433: step 11700, loss = 2.30 (315.4 examples/sec; 0.406 sec/batch)
2017-04-02 23:13:08.259241: step 11710, loss = 2.30 (318.7 examples/sec; 0.402 sec/batch)
2017-04-02 23:13:12.271948: step 11720, loss = 2.30 (319.0 examples/sec; 0.401 sec/batch)
2017-04-02 23:13:16.294662: step 11730, loss = 2.31 (318.2 examples/sec; 0.402 sec/batch)
2017-04-02 23:13:20.310050: step 11740, loss = 2.30 (318.8 examples/sec; 0.402 sec/batch)
2017-04-02 23:13:24.339909: step 11750, loss = 2.30 (317.6 examples/sec; 0.403 sec/batch)
2017-04-02 23:13:28.351404: step 11760, loss = 2.30 (319.1 examples/sec; 0.401 sec/batch)
2017-04-02 23:13:32.343383: step 11770, loss = 2.31 (320.6 examples/sec; 0.399 sec/batch)
2017-04-02 23:13:36.333224: step 11780, loss = 2.30 (320.8 examples/sec; 0.399 sec/batch)
2017-04-02 23:13:40.321950: step 11790, loss = 2.30 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 23:13:44.372341: step 11800, loss = 2.30 (316.0 examples/sec; 0.405 sec/batch)
2017-04-02 23:13:48.401911: step 11810, loss = 2.30 (317.7 examples/sec; 0.403 sec/batch)
2017-04-02 23:13:52.365065: step 11820, loss = 2.31 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 23:13:56.347573: step 11830, loss = 2.30 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 23:14:00.343326: step 11840, loss = 2.30 (320.3 examples/sec; 0.400 sec/batch)
2017-04-02 23:14:04.314930: step 11850, loss = 2.30 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 23:14:08.318986: step 11860, loss = 2.30 (319.7 examples/sec; 0.400 sec/batch)
2017-04-02 23:14:12.321985: step 11870, loss = 2.30 (319.8 examples/sec; 0.400 sec/batch)
2017-04-02 23:14:16.330768: step 11880, loss = 2.30 (319.3 examples/sec; 0.401 sec/batch)
2017-04-02 23:14:20.315977: step 11890, loss = 2.30 (321.2 examples/sec; 0.399 sec/batch)
2017-04-02 23:14:24.388800: step 11900, loss = 2.30 (314.3 examples/sec; 0.407 sec/batch)
2017-04-02 23:14:28.361527: step 11910, loss = 2.31 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 23:14:32.357666: step 11920, loss = 2.30 (320.3 examples/sec; 0.400 sec/batch)
2017-04-02 23:14:36.329566: step 11930, loss = 2.31 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 23:14:40.338513: step 11940, loss = 2.31 (319.3 examples/sec; 0.401 sec/batch)
2017-04-02 23:14:44.335174: step 11950, loss = 2.30 (320.3 examples/sec; 0.400 sec/batch)
2017-04-02 23:14:48.319593: step 11960, loss = 2.30 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 23:14:52.313225: step 11970, loss = 2.30 (320.5 examples/sec; 0.399 sec/batch)
2017-04-02 23:14:56.275762: step 11980, loss = 2.31 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 23:15:00.278651: step 11990, loss = 2.30 (319.8 examples/sec; 0.400 sec/batch)
2017-04-02 23:15:04.372881: step 12000, loss = 2.31 (312.6 examples/sec; 0.409 sec/batch)
2017-04-02 23:15:08.401232: step 12010, loss = 2.31 (317.7 examples/sec; 0.403 sec/batch)
2017-04-02 23:15:12.407104: step 12020, loss = 2.30 (319.5 examples/sec; 0.401 sec/batch)
2017-04-02 23:16:11.777855: precision @ 1 = 0.103

2017-04-02 23:16:47.651661: step 12030, loss = 2.30 (13.4 examples/sec; 9.524 sec/batch)
2017-04-02 23:16:51.655377: step 12040, loss = 2.31 (319.7 examples/sec; 0.400 sec/batch)
2017-04-02 23:16:55.615411: step 12050, loss = 2.30 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 23:16:59.610629: step 12060, loss = 2.30 (320.4 examples/sec; 0.400 sec/batch)
2017-04-02 23:17:03.582161: step 12070, loss = 2.30 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 23:17:07.548184: step 12080, loss = 2.30 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 23:17:11.491711: step 12090, loss = 2.30 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 23:17:16.114790: step 12100, loss = 2.30 (276.9 examples/sec; 0.462 sec/batch)
2017-04-02 23:17:20.053540: step 12110, loss = 2.30 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 23:17:24.032045: step 12120, loss = 2.30 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 23:17:27.954626: step 12130, loss = 2.30 (326.3 examples/sec; 0.392 sec/batch)
2017-04-02 23:17:31.887461: step 12140, loss = 2.30 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 23:17:35.851535: step 12150, loss = 2.31 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 23:17:39.822592: step 12160, loss = 2.30 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 23:17:43.807765: step 12170, loss = 2.31 (321.2 examples/sec; 0.399 sec/batch)
2017-04-02 23:17:47.765941: step 12180, loss = 2.30 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 23:17:51.706146: step 12190, loss = 2.30 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 23:17:55.746157: step 12200, loss = 2.30 (316.8 examples/sec; 0.404 sec/batch)
2017-04-02 23:17:59.702955: step 12210, loss = 2.30 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 23:18:03.660447: step 12220, loss = 2.30 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 23:18:07.605129: step 12230, loss = 2.30 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 23:18:11.566304: step 12240, loss = 2.30 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 23:18:15.503610: step 12250, loss = 2.30 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 23:18:19.472702: step 12260, loss = 2.30 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 23:18:23.436777: step 12270, loss = 2.30 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 23:18:27.375879: step 12280, loss = 2.30 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 23:18:31.306670: step 12290, loss = 2.30 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 23:18:35.336315: step 12300, loss = 2.30 (317.6 examples/sec; 0.403 sec/batch)
2017-04-02 23:18:39.341167: step 12310, loss = 2.31 (319.6 examples/sec; 0.400 sec/batch)
2017-04-02 23:18:43.315516: step 12320, loss = 2.30 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 23:18:47.287371: step 12330, loss = 2.30 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 23:18:51.266224: step 12340, loss = 2.30 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 23:18:55.239292: step 12350, loss = 2.30 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 23:18:59.184628: step 12360, loss = 2.30 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 23:19:03.168317: step 12370, loss = 2.30 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 23:19:07.157458: step 12380, loss = 2.30 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 23:19:11.141100: step 12390, loss = 2.31 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 23:19:15.170309: step 12400, loss = 2.30 (317.7 examples/sec; 0.403 sec/batch)
2017-04-02 23:19:19.148257: step 12410, loss = 2.30 (321.8 examples/sec; 0.398 sec/batch)
2017-04-02 23:19:23.132391: step 12420, loss = 2.30 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 23:19:27.066087: step 12430, loss = 2.30 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 23:19:31.031534: step 12440, loss = 2.30 (322.8 examples/sec; 0.397 sec/batch)
2017-04-02 23:19:34.993778: step 12450, loss = 2.30 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 23:19:38.938419: step 12460, loss = 2.30 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 23:19:42.907073: step 12470, loss = 2.30 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 23:19:46.833526: step 12480, loss = 2.30 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 23:19:50.814713: step 12490, loss = 2.30 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 23:19:54.854839: step 12500, loss = 2.30 (316.8 examples/sec; 0.404 sec/batch)
2017-04-02 23:19:58.838766: step 12510, loss = 2.30 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 23:20:02.802698: step 12520, loss = 2.30 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 23:20:06.771752: step 12530, loss = 2.30 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 23:20:10.693912: step 12540, loss = 2.30 (326.4 examples/sec; 0.392 sec/batch)
2017-04-02 23:20:14.656155: step 12550, loss = 2.30 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 23:20:18.606613: step 12560, loss = 2.30 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 23:20:22.575394: step 12570, loss = 2.30 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 23:20:26.527016: step 12580, loss = 2.31 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 23:20:30.450169: step 12590, loss = 2.30 (326.3 examples/sec; 0.392 sec/batch)
2017-04-02 23:20:34.468776: step 12600, loss = 2.30 (318.5 examples/sec; 0.402 sec/batch)
2017-04-02 23:20:38.432376: step 12610, loss = 2.30 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 23:20:42.408763: step 12620, loss = 2.30 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 23:20:46.383923: step 12630, loss = 2.30 (322.0 examples/sec; 0.398 sec/batch)
2017-04-02 23:20:50.341735: step 12640, loss = 2.30 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 23:20:54.299470: step 12650, loss = 2.30 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 23:20:58.282726: step 12660, loss = 2.30 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 23:21:02.239200: step 12670, loss = 2.30 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 23:21:06.234626: step 12680, loss = 2.30 (320.4 examples/sec; 0.400 sec/batch)
2017-04-02 23:21:10.207671: step 12690, loss = 2.30 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 23:21:14.245863: step 12700, loss = 2.30 (317.0 examples/sec; 0.404 sec/batch)
2017-04-02 23:21:18.195215: step 12710, loss = 2.31 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 23:21:22.182278: step 12720, loss = 2.30 (321.0 examples/sec; 0.399 sec/batch)
2017-04-02 23:21:26.123138: step 12730, loss = 2.30 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 23:21:30.074868: step 12740, loss = 2.31 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 23:21:34.057665: step 12750, loss = 2.30 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 23:21:38.041636: step 12760, loss = 2.30 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 23:21:42.021298: step 12770, loss = 2.31 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 23:21:45.966489: step 12780, loss = 2.31 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 23:21:50.084164: step 12790, loss = 2.31 (310.9 examples/sec; 0.412 sec/batch)
2017-04-02 23:21:54.103814: step 12800, loss = 2.30 (318.4 examples/sec; 0.402 sec/batch)
2017-04-02 23:21:58.081457: step 12810, loss = 2.30 (321.8 examples/sec; 0.398 sec/batch)
2017-04-02 23:22:02.043840: step 12820, loss = 2.30 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 23:22:06.001807: step 12830, loss = 2.30 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 23:22:09.948683: step 12840, loss = 2.30 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 23:22:13.904927: step 12850, loss = 2.30 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 23:22:17.897588: step 12860, loss = 2.30 (320.6 examples/sec; 0.399 sec/batch)
2017-04-02 23:22:21.849003: step 12870, loss = 2.30 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 23:22:25.841704: step 12880, loss = 2.30 (320.6 examples/sec; 0.399 sec/batch)
2017-04-02 23:22:29.821895: step 12890, loss = 2.30 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 23:22:33.809983: step 12900, loss = 2.30 (321.0 examples/sec; 0.399 sec/batch)
2017-04-02 23:22:37.770758: step 12910, loss = 2.30 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 23:22:41.754926: step 12920, loss = 2.30 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 23:22:45.728173: step 12930, loss = 2.30 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 23:22:49.694525: step 12940, loss = 2.30 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 23:22:53.666563: step 12950, loss = 2.30 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 23:22:57.632392: step 12960, loss = 2.30 (322.8 examples/sec; 0.397 sec/batch)
2017-04-02 23:23:01.602679: step 12970, loss = 2.30 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 23:23:05.566277: step 12980, loss = 2.30 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 23:23:09.506644: step 12990, loss = 2.30 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 23:23:13.539330: step 13000, loss = 2.30 (317.4 examples/sec; 0.403 sec/batch)
2017-04-02 23:23:17.512501: step 13010, loss = 2.30 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 23:23:21.474555: step 13020, loss = 2.30 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 23:23:25.431852: step 13030, loss = 2.30 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 23:23:29.393704: step 13040, loss = 2.30 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 23:23:33.354836: step 13050, loss = 2.30 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 23:23:37.328219: step 13060, loss = 2.30 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 23:23:41.298194: step 13070, loss = 2.30 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 23:23:45.267882: step 13080, loss = 2.30 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 23:23:49.215581: step 13090, loss = 2.30 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 23:23:53.287788: step 13100, loss = 2.30 (314.3 examples/sec; 0.407 sec/batch)
2017-04-02 23:23:57.258035: step 13110, loss = 2.30 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 23:24:01.251665: step 13120, loss = 2.31 (320.5 examples/sec; 0.399 sec/batch)
2017-04-02 23:24:05.231296: step 13130, loss = 2.31 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 23:24:09.183711: step 13140, loss = 2.30 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 23:24:13.148433: step 13150, loss = 2.30 (322.8 examples/sec; 0.396 sec/batch)
2017-04-02 23:24:17.079907: step 13160, loss = 2.30 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 23:24:21.058244: step 13170, loss = 2.30 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 23:24:25.028025: step 13180, loss = 2.30 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 23:24:29.026911: step 13190, loss = 2.30 (320.1 examples/sec; 0.400 sec/batch)
2017-04-02 23:24:33.101883: step 13200, loss = 2.30 (314.1 examples/sec; 0.407 sec/batch)
2017-04-02 23:24:37.100248: step 13210, loss = 2.30 (320.1 examples/sec; 0.400 sec/batch)
2017-04-02 23:24:41.071954: step 13220, loss = 2.30 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 23:24:45.070631: step 13230, loss = 2.30 (320.1 examples/sec; 0.400 sec/batch)
2017-04-02 23:24:49.062424: step 13240, loss = 2.31 (320.7 examples/sec; 0.399 sec/batch)
2017-04-02 23:24:53.041811: step 13250, loss = 2.30 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 23:24:57.071126: step 13260, loss = 2.30 (317.7 examples/sec; 0.403 sec/batch)
2017-04-02 23:25:01.133154: step 13270, loss = 2.30 (315.1 examples/sec; 0.406 sec/batch)
2017-04-02 23:25:05.123068: step 13280, loss = 2.30 (320.8 examples/sec; 0.399 sec/batch)
2017-04-02 23:25:09.100086: step 13290, loss = 2.30 (321.8 examples/sec; 0.398 sec/batch)
2017-04-02 23:25:13.158502: step 13300, loss = 2.30 (315.4 examples/sec; 0.406 sec/batch)
2017-04-02 23:25:17.124183: step 13310, loss = 2.30 (322.8 examples/sec; 0.397 sec/batch)
2017-04-02 23:25:21.130405: step 13320, loss = 2.30 (319.5 examples/sec; 0.401 sec/batch)
2017-04-02 23:25:25.137768: step 13330, loss = 2.31 (319.4 examples/sec; 0.401 sec/batch)
2017-04-02 23:25:29.103919: step 13340, loss = 2.30 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 23:25:33.098152: step 13350, loss = 2.30 (320.5 examples/sec; 0.399 sec/batch)
2017-04-02 23:25:37.086558: step 13360, loss = 2.30 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 23:25:41.078534: step 13370, loss = 2.31 (320.6 examples/sec; 0.399 sec/batch)
2017-04-02 23:25:45.076482: step 13380, loss = 2.30 (320.2 examples/sec; 0.400 sec/batch)
2017-04-02 23:25:49.060380: step 13390, loss = 2.30 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 23:25:53.126779: step 13400, loss = 2.30 (314.8 examples/sec; 0.407 sec/batch)
2017-04-02 23:25:57.153387: step 13410, loss = 2.30 (317.9 examples/sec; 0.403 sec/batch)
2017-04-02 23:26:01.124662: step 13420, loss = 2.30 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 23:26:05.111107: step 13430, loss = 2.30 (321.1 examples/sec; 0.399 sec/batch)
2017-04-02 23:26:09.106554: step 13440, loss = 2.30 (320.4 examples/sec; 0.400 sec/batch)
2017-04-02 23:26:13.085570: step 13450, loss = 2.30 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 23:26:17.047725: step 13460, loss = 2.30 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 23:26:21.020244: step 13470, loss = 2.31 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 23:26:25.053061: step 13480, loss = 2.30 (317.4 examples/sec; 0.403 sec/batch)
2017-04-02 23:26:29.039512: step 13490, loss = 2.30 (321.1 examples/sec; 0.399 sec/batch)
2017-04-02 23:26:33.093697: step 13500, loss = 2.30 (315.7 examples/sec; 0.405 sec/batch)
2017-04-02 23:26:37.092668: step 13510, loss = 2.30 (320.1 examples/sec; 0.400 sec/batch)
2017-04-02 23:26:41.079471: step 13520, loss = 2.30 (321.1 examples/sec; 0.399 sec/batch)
2017-04-02 23:26:45.075552: step 13530, loss = 2.30 (320.3 examples/sec; 0.400 sec/batch)
2017-04-02 23:26:49.031297: step 13540, loss = 2.31 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 23:26:52.988128: step 13550, loss = 2.30 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 23:26:56.994452: step 13560, loss = 2.30 (319.5 examples/sec; 0.401 sec/batch)
2017-04-02 23:27:01.006661: step 13570, loss = 2.30 (319.0 examples/sec; 0.401 sec/batch)
2017-04-02 23:27:04.999642: step 13580, loss = 2.30 (320.6 examples/sec; 0.399 sec/batch)
2017-04-02 23:27:08.969987: step 13590, loss = 2.30 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 23:27:13.761855: step 13600, loss = 2.31 (267.1 examples/sec; 0.479 sec/batch)
2017-04-02 23:27:17.725378: step 13610, loss = 2.30 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 23:27:21.705425: step 13620, loss = 2.31 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 23:27:25.686588: step 13630, loss = 2.30 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 23:27:29.693283: step 13640, loss = 2.30 (319.5 examples/sec; 0.401 sec/batch)
2017-04-02 23:27:33.663530: step 13650, loss = 2.30 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 23:27:37.635978: step 13660, loss = 2.30 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 23:27:41.614741: step 13670, loss = 2.30 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 23:27:45.581884: step 13680, loss = 2.30 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 23:27:49.542163: step 13690, loss = 2.30 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 23:27:53.549521: step 13700, loss = 2.31 (319.4 examples/sec; 0.401 sec/batch)
2017-04-02 23:27:57.505750: step 13710, loss = 2.30 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 23:28:01.463691: step 13720, loss = 2.30 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 23:28:05.464558: step 13730, loss = 2.30 (319.9 examples/sec; 0.400 sec/batch)
2017-04-02 23:28:09.420241: step 13740, loss = 2.30 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 23:28:13.396103: step 13750, loss = 2.30 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 23:28:17.368381: step 13760, loss = 2.30 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 23:28:21.352994: step 13770, loss = 2.31 (321.2 examples/sec; 0.398 sec/batch)
2017-04-02 23:28:25.327947: step 13780, loss = 2.30 (322.0 examples/sec; 0.397 sec/batch)
2017-04-02 23:28:29.304943: step 13790, loss = 2.30 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 23:28:33.347202: step 13800, loss = 2.30 (316.7 examples/sec; 0.404 sec/batch)
2017-04-02 23:28:37.304146: step 13810, loss = 2.30 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 23:28:41.287670: step 13820, loss = 2.30 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 23:28:45.268781: step 13830, loss = 2.30 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 23:28:49.254055: step 13840, loss = 2.31 (321.2 examples/sec; 0.399 sec/batch)
2017-04-02 23:28:53.234537: step 13850, loss = 2.30 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 23:28:57.207856: step 13860, loss = 2.30 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 23:29:01.146244: step 13870, loss = 2.30 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 23:29:05.129365: step 13880, loss = 2.30 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 23:29:09.097871: step 13890, loss = 2.30 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 23:29:13.126686: step 13900, loss = 2.31 (317.7 examples/sec; 0.403 sec/batch)
2017-04-02 23:29:17.122593: step 13910, loss = 2.30 (320.3 examples/sec; 0.400 sec/batch)
2017-04-02 23:29:21.103047: step 13920, loss = 2.30 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 23:29:25.109418: step 13930, loss = 2.31 (319.5 examples/sec; 0.401 sec/batch)
2017-04-02 23:29:29.082400: step 13940, loss = 2.30 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 23:29:33.054122: step 13950, loss = 2.31 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 23:29:37.072425: step 13960, loss = 2.30 (318.5 examples/sec; 0.402 sec/batch)
2017-04-02 23:29:41.072355: step 13970, loss = 2.30 (320.0 examples/sec; 0.400 sec/batch)
2017-04-02 23:29:45.041174: step 13980, loss = 2.30 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 23:29:49.038925: step 13990, loss = 2.30 (320.2 examples/sec; 0.400 sec/batch)
2017-04-02 23:29:53.142577: step 14000, loss = 2.30 (311.9 examples/sec; 0.410 sec/batch)
2017-04-02 23:29:57.109517: step 14010, loss = 2.30 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 23:30:01.122146: step 14020, loss = 2.30 (319.0 examples/sec; 0.401 sec/batch)
2017-04-02 23:30:05.087171: step 14030, loss = 2.31 (322.8 examples/sec; 0.397 sec/batch)
2017-04-02 23:30:09.050718: step 14040, loss = 2.30 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 23:30:13.065498: step 14050, loss = 2.30 (318.8 examples/sec; 0.401 sec/batch)
2017-04-02 23:30:17.056350: step 14060, loss = 2.30 (320.7 examples/sec; 0.399 sec/batch)
2017-04-02 23:30:21.035253: step 14070, loss = 2.30 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 23:30:25.019718: step 14080, loss = 2.30 (321.2 examples/sec; 0.398 sec/batch)
2017-04-02 23:30:29.006342: step 14090, loss = 2.30 (321.1 examples/sec; 0.399 sec/batch)
2017-04-02 23:30:33.036803: step 14100, loss = 2.30 (317.6 examples/sec; 0.403 sec/batch)
2017-04-02 23:31:29.307391: precision @ 1 = 0.096

2017-04-02 23:32:57.555418: step 14110, loss = 2.30 (8.9 examples/sec; 14.452 sec/batch)
2017-04-02 23:33:01.528066: step 14120, loss = 2.31 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 23:33:05.498481: step 14130, loss = 2.30 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 23:33:09.465067: step 14140, loss = 2.30 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 23:33:13.420995: step 14150, loss = 2.30 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 23:33:17.401428: step 14160, loss = 2.30 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 23:33:21.391130: step 14170, loss = 2.30 (320.8 examples/sec; 0.399 sec/batch)
2017-04-02 23:33:25.338312: step 14180, loss = 2.30 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 23:33:29.263972: step 14190, loss = 2.30 (326.1 examples/sec; 0.393 sec/batch)
2017-04-02 23:33:33.300380: step 14200, loss = 2.30 (317.1 examples/sec; 0.404 sec/batch)
2017-04-02 23:33:37.233638: step 14210, loss = 2.30 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 23:33:41.167639: step 14220, loss = 2.30 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 23:33:45.140322: step 14230, loss = 2.31 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 23:33:49.107330: step 14240, loss = 2.30 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 23:33:53.077750: step 14250, loss = 2.31 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 23:33:57.058062: step 14260, loss = 2.30 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 23:34:00.988966: step 14270, loss = 2.30 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 23:34:04.954576: step 14280, loss = 2.30 (322.8 examples/sec; 0.397 sec/batch)
2017-04-02 23:34:08.945952: step 14290, loss = 2.30 (320.7 examples/sec; 0.399 sec/batch)
2017-04-02 23:34:13.005271: step 14300, loss = 2.30 (315.3 examples/sec; 0.406 sec/batch)
2017-04-02 23:34:16.936116: step 14310, loss = 2.30 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 23:34:20.919516: step 14320, loss = 2.30 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 23:34:24.867993: step 14330, loss = 2.30 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 23:34:28.819939: step 14340, loss = 2.30 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 23:34:32.805425: step 14350, loss = 2.30 (321.2 examples/sec; 0.399 sec/batch)
2017-04-02 23:34:36.771584: step 14360, loss = 2.31 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 23:34:40.719518: step 14370, loss = 2.30 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 23:34:44.714401: step 14380, loss = 2.30 (320.4 examples/sec; 0.399 sec/batch)
2017-04-02 23:34:48.677314: step 14390, loss = 2.30 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 23:34:52.719622: step 14400, loss = 2.30 (316.7 examples/sec; 0.404 sec/batch)
2017-04-02 23:34:56.707147: step 14410, loss = 2.30 (321.0 examples/sec; 0.399 sec/batch)
2017-04-02 23:35:00.705993: step 14420, loss = 2.30 (320.1 examples/sec; 0.400 sec/batch)
2017-04-02 23:35:04.680449: step 14430, loss = 2.30 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 23:35:08.644096: step 14440, loss = 2.31 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 23:35:12.612931: step 14450, loss = 2.30 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 23:35:16.596382: step 14460, loss = 2.30 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 23:35:20.561388: step 14470, loss = 2.30 (322.8 examples/sec; 0.397 sec/batch)
2017-04-02 23:35:24.547630: step 14480, loss = 2.30 (321.1 examples/sec; 0.399 sec/batch)
2017-04-02 23:35:28.510480: step 14490, loss = 2.31 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 23:35:32.547810: step 14500, loss = 2.30 (317.0 examples/sec; 0.404 sec/batch)
2017-04-02 23:35:36.525828: step 14510, loss = 2.30 (321.8 examples/sec; 0.398 sec/batch)
2017-04-02 23:35:40.482459: step 14520, loss = 2.30 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 23:35:44.447642: step 14530, loss = 2.30 (322.8 examples/sec; 0.397 sec/batch)
2017-04-02 23:35:48.391781: step 14540, loss = 2.30 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 23:35:52.374067: step 14550, loss = 2.30 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 23:35:56.347506: step 14560, loss = 2.30 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 23:36:00.305936: step 14570, loss = 2.30 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 23:36:04.277169: step 14580, loss = 2.30 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 23:36:08.238042: step 14590, loss = 2.30 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 23:36:12.284524: step 14600, loss = 2.30 (316.3 examples/sec; 0.405 sec/batch)
2017-04-02 23:36:16.264897: step 14610, loss = 2.30 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 23:36:20.231815: step 14620, loss = 2.30 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 23:36:24.236901: step 14630, loss = 2.30 (319.6 examples/sec; 0.401 sec/batch)
2017-04-02 23:36:28.225437: step 14640, loss = 2.30 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 23:36:32.188086: step 14650, loss = 2.30 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 23:36:36.192459: step 14660, loss = 2.30 (319.7 examples/sec; 0.400 sec/batch)
2017-04-02 23:36:40.150972: step 14670, loss = 2.30 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 23:36:44.111893: step 14680, loss = 2.31 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 23:36:48.091171: step 14690, loss = 2.30 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 23:36:52.140490: step 14700, loss = 2.30 (316.1 examples/sec; 0.405 sec/batch)
2017-04-02 23:36:56.124733: step 14710, loss = 2.30 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 23:37:00.126158: step 14720, loss = 2.30 (319.9 examples/sec; 0.400 sec/batch)
2017-04-02 23:37:04.083568: step 14730, loss = 2.30 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 23:37:08.017418: step 14740, loss = 2.30 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 23:37:11.994743: step 14750, loss = 2.30 (321.8 examples/sec; 0.398 sec/batch)
2017-04-02 23:37:16.726788: step 14760, loss = 2.30 (270.5 examples/sec; 0.473 sec/batch)
2017-04-02 23:37:20.693567: step 14770, loss = 2.31 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 23:37:24.624271: step 14780, loss = 2.31 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 23:37:28.582690: step 14790, loss = 2.30 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 23:37:32.608646: step 14800, loss = 2.30 (317.9 examples/sec; 0.403 sec/batch)
2017-04-02 23:37:36.587778: step 14810, loss = 2.30 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 23:37:40.538345: step 14820, loss = 2.30 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 23:37:44.485388: step 14830, loss = 2.30 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 23:37:48.465685: step 14840, loss = 2.30 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 23:37:52.454172: step 14850, loss = 2.30 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 23:37:56.413068: step 14860, loss = 2.30 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 23:38:00.375799: step 14870, loss = 2.31 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 23:38:04.345117: step 14880, loss = 2.30 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 23:38:08.298445: step 14890, loss = 2.30 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 23:38:12.323484: step 14900, loss = 2.30 (318.0 examples/sec; 0.403 sec/batch)
2017-04-02 23:38:16.298798: step 14910, loss = 2.30 (322.0 examples/sec; 0.398 sec/batch)
2017-04-02 23:38:20.246764: step 14920, loss = 2.30 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 23:38:24.216875: step 14930, loss = 2.30 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 23:38:28.165167: step 14940, loss = 2.30 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 23:38:32.176440: step 14950, loss = 2.30 (319.1 examples/sec; 0.401 sec/batch)
2017-04-02 23:38:36.122823: step 14960, loss = 2.30 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 23:38:40.073203: step 14970, loss = 2.30 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 23:38:44.009030: step 14980, loss = 2.31 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 23:38:47.985205: step 14990, loss = 2.30 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 23:38:52.008699: step 15000, loss = 2.31 (318.1 examples/sec; 0.402 sec/batch)
2017-04-02 23:38:55.961370: step 15010, loss = 2.30 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 23:38:59.895983: step 15020, loss = 2.31 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 23:39:03.817404: step 15030, loss = 2.30 (326.4 examples/sec; 0.392 sec/batch)
2017-04-02 23:39:07.781287: step 15040, loss = 2.30 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 23:39:11.718084: step 15050, loss = 2.30 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 23:39:15.693721: step 15060, loss = 2.30 (322.0 examples/sec; 0.398 sec/batch)
2017-04-02 23:39:19.628056: step 15070, loss = 2.30 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 23:39:23.610461: step 15080, loss = 2.30 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 23:39:27.562785: step 15090, loss = 2.30 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 23:39:31.612470: step 15100, loss = 2.30 (316.1 examples/sec; 0.405 sec/batch)
2017-04-02 23:39:35.570221: step 15110, loss = 2.30 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 23:39:39.506796: step 15120, loss = 2.30 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 23:39:43.431740: step 15130, loss = 2.30 (326.1 examples/sec; 0.392 sec/batch)
2017-04-02 23:39:47.415284: step 15140, loss = 2.30 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 23:39:51.392200: step 15150, loss = 2.30 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 23:39:55.350928: step 15160, loss = 2.30 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 23:39:59.290470: step 15170, loss = 2.30 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 23:40:03.231632: step 15180, loss = 2.30 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 23:40:07.195134: step 15190, loss = 2.31 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 23:40:11.230269: step 15200, loss = 2.30 (317.2 examples/sec; 0.404 sec/batch)
2017-04-02 23:40:15.214789: step 15210, loss = 2.30 (321.2 examples/sec; 0.398 sec/batch)
2017-04-02 23:40:19.151408: step 15220, loss = 2.30 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 23:40:23.085125: step 15230, loss = 2.30 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 23:40:27.047513: step 15240, loss = 2.30 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 23:40:31.022055: step 15250, loss = 2.30 (322.0 examples/sec; 0.397 sec/batch)
2017-04-02 23:40:34.976414: step 15260, loss = 2.30 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 23:40:38.913692: step 15270, loss = 2.30 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 23:40:42.887477: step 15280, loss = 2.30 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 23:40:46.827146: step 15290, loss = 2.31 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 23:40:50.859509: step 15300, loss = 2.30 (317.4 examples/sec; 0.403 sec/batch)
2017-04-02 23:40:54.810147: step 15310, loss = 2.30 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 23:40:58.763891: step 15320, loss = 2.31 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 23:41:02.731873: step 15330, loss = 2.30 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 23:41:06.669035: step 15340, loss = 2.30 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 23:41:10.620496: step 15350, loss = 2.30 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 23:41:14.600042: step 15360, loss = 2.30 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 23:41:18.532084: step 15370, loss = 2.30 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 23:41:22.481559: step 15380, loss = 2.30 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 23:41:26.433609: step 15390, loss = 2.30 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 23:41:30.455744: step 15400, loss = 2.30 (318.2 examples/sec; 0.402 sec/batch)
2017-04-02 23:41:34.421112: step 15410, loss = 2.30 (322.8 examples/sec; 0.397 sec/batch)
2017-04-02 23:41:38.374285: step 15420, loss = 2.31 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 23:41:42.332882: step 15430, loss = 2.30 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 23:41:46.283232: step 15440, loss = 2.30 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 23:41:50.257395: step 15450, loss = 2.30 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 23:41:54.223495: step 15460, loss = 2.30 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 23:41:58.187007: step 15470, loss = 2.31 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 23:42:02.118179: step 15480, loss = 2.30 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 23:42:06.066366: step 15490, loss = 2.30 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 23:42:10.087525: step 15500, loss = 2.30 (318.3 examples/sec; 0.402 sec/batch)
2017-04-02 23:42:14.062015: step 15510, loss = 2.30 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 23:42:18.002146: step 15520, loss = 2.30 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 23:42:21.988341: step 15530, loss = 2.30 (321.1 examples/sec; 0.399 sec/batch)
2017-04-02 23:42:25.954812: step 15540, loss = 2.30 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 23:42:29.911068: step 15550, loss = 2.31 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 23:42:33.877390: step 15560, loss = 2.30 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 23:42:37.826625: step 15570, loss = 2.30 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 23:42:41.787961: step 15580, loss = 2.30 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 23:42:45.745553: step 15590, loss = 2.30 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 23:42:49.793902: step 15600, loss = 2.30 (316.2 examples/sec; 0.405 sec/batch)
2017-04-02 23:42:53.777111: step 15610, loss = 2.30 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 23:42:57.746999: step 15620, loss = 2.30 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 23:43:01.692134: step 15630, loss = 2.30 (324.5 examples/sec; 0.395 sec/batch)
2017-04-02 23:43:05.628536: step 15640, loss = 2.30 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 23:43:09.621547: step 15650, loss = 2.30 (320.6 examples/sec; 0.399 sec/batch)
2017-04-02 23:43:13.602471: step 15660, loss = 2.30 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 23:43:17.557862: step 15670, loss = 2.30 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 23:43:21.487770: step 15680, loss = 2.30 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 23:43:25.410881: step 15690, loss = 2.30 (326.3 examples/sec; 0.392 sec/batch)
2017-04-02 23:43:29.441717: step 15700, loss = 2.30 (317.6 examples/sec; 0.403 sec/batch)
2017-04-02 23:43:33.404274: step 15710, loss = 2.31 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 23:43:37.362562: step 15720, loss = 2.30 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 23:43:41.299778: step 15730, loss = 2.30 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 23:43:45.268139: step 15740, loss = 2.30 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 23:43:49.255597: step 15750, loss = 2.30 (321.0 examples/sec; 0.399 sec/batch)
2017-04-02 23:43:53.218329: step 15760, loss = 2.30 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 23:43:57.164491: step 15770, loss = 2.30 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 23:44:01.137871: step 15780, loss = 2.30 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 23:44:05.118035: step 15790, loss = 2.30 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 23:44:09.181040: step 15800, loss = 2.30 (315.0 examples/sec; 0.406 sec/batch)
2017-04-02 23:44:13.138982: step 15810, loss = 2.30 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 23:44:17.131465: step 15820, loss = 2.30 (320.6 examples/sec; 0.399 sec/batch)
2017-04-02 23:44:21.076979: step 15830, loss = 2.30 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 23:44:25.043292: step 15840, loss = 2.30 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 23:44:29.047839: step 15850, loss = 2.30 (319.6 examples/sec; 0.400 sec/batch)
2017-04-02 23:44:33.000505: step 15860, loss = 2.30 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 23:44:36.967603: step 15870, loss = 2.31 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 23:44:40.902029: step 15880, loss = 2.30 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 23:44:44.890337: step 15890, loss = 2.30 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 23:44:48.937750: step 15900, loss = 2.30 (316.3 examples/sec; 0.405 sec/batch)
2017-04-02 23:44:52.925357: step 15910, loss = 2.30 (321.0 examples/sec; 0.399 sec/batch)
2017-04-02 23:44:56.916565: step 15920, loss = 2.30 (320.7 examples/sec; 0.399 sec/batch)
2017-04-02 23:45:00.871695: step 15930, loss = 2.30 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 23:45:04.823023: step 15940, loss = 2.30 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 23:45:08.796059: step 15950, loss = 2.30 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 23:45:12.767908: step 15960, loss = 2.30 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 23:45:16.735928: step 15970, loss = 2.30 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 23:45:20.727426: step 15980, loss = 2.31 (320.7 examples/sec; 0.399 sec/batch)
2017-04-02 23:45:24.709912: step 15990, loss = 2.30 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 23:45:28.737520: step 16000, loss = 2.30 (317.8 examples/sec; 0.403 sec/batch)
2017-04-02 23:46:46.478478: precision @ 1 = 0.092

