2017-04-02 22:39:48.310116: step 0, loss = 4.67 (372.5 examples/sec; 0.344 sec/batch)
2017-04-02 22:39:52.293259: step 10, loss = 4.65 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 22:39:56.217217: step 20, loss = 4.64 (326.2 examples/sec; 0.392 sec/batch)
2017-04-02 22:40:00.160463: step 30, loss = 4.62 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 22:40:04.100813: step 40, loss = 4.60 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 22:40:08.070242: step 50, loss = 4.58 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 22:40:12.022621: step 60, loss = 4.56 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 22:40:15.992618: step 70, loss = 4.54 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 22:40:19.953324: step 80, loss = 4.53 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 22:40:23.905359: step 90, loss = 4.51 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 22:40:27.922809: step 100, loss = 4.49 (318.6 examples/sec; 0.402 sec/batch)
2017-04-02 22:40:31.853917: step 110, loss = 4.47 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 22:40:35.812491: step 120, loss = 4.46 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 22:40:39.751270: step 130, loss = 4.45 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 22:40:43.747234: step 140, loss = 4.43 (320.3 examples/sec; 0.400 sec/batch)
2017-04-02 22:40:47.711071: step 150, loss = 4.41 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 22:40:51.656976: step 160, loss = 4.39 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 22:40:55.626595: step 170, loss = 4.37 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 22:40:59.574311: step 180, loss = 4.36 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 22:41:03.534018: step 190, loss = 4.34 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 22:41:07.530780: step 200, loss = 4.32 (320.3 examples/sec; 0.400 sec/batch)
2017-04-02 22:41:11.454664: step 210, loss = 4.31 (326.2 examples/sec; 0.392 sec/batch)
2017-04-02 22:41:15.412444: step 220, loss = 4.29 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 22:41:19.331030: step 230, loss = 4.28 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 22:41:23.283377: step 240, loss = 4.26 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 22:41:27.230552: step 250, loss = 4.24 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 22:41:31.169455: step 260, loss = 4.23 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 22:41:35.098578: step 270, loss = 4.22 (325.8 examples/sec; 0.393 sec/batch)
2017-04-02 22:41:39.055888: step 280, loss = 4.20 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:41:43.374402: step 290, loss = 4.18 (296.4 examples/sec; 0.432 sec/batch)
2017-04-02 22:41:48.213732: step 300, loss = 4.17 (264.5 examples/sec; 0.484 sec/batch)
2017-04-02 22:41:52.169547: step 310, loss = 4.15 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 22:41:56.132848: step 320, loss = 4.14 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 22:42:00.099408: step 330, loss = 4.12 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 22:42:04.038206: step 340, loss = 4.11 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 22:42:08.025680: step 350, loss = 4.09 (321.0 examples/sec; 0.399 sec/batch)
2017-04-02 22:42:12.000482: step 360, loss = 4.08 (322.0 examples/sec; 0.397 sec/batch)
2017-04-02 22:42:15.987018: step 370, loss = 4.07 (321.1 examples/sec; 0.399 sec/batch)
2017-04-02 22:42:19.913169: step 380, loss = 4.05 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 22:42:23.880486: step 390, loss = 4.04 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 22:42:27.914526: step 400, loss = 4.02 (317.3 examples/sec; 0.403 sec/batch)
2017-04-02 22:42:31.857564: step 410, loss = 4.01 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 22:42:35.795860: step 420, loss = 4.00 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 22:42:39.756550: step 430, loss = 3.98 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 22:42:43.734659: step 440, loss = 3.98 (321.8 examples/sec; 0.398 sec/batch)
2017-04-02 22:42:47.682435: step 450, loss = 3.96 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 22:42:51.635992: step 460, loss = 3.94 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 22:42:55.604832: step 470, loss = 3.93 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 22:42:59.541788: step 480, loss = 3.92 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 22:43:03.492841: step 490, loss = 3.90 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 22:43:07.522819: step 500, loss = 3.89 (317.6 examples/sec; 0.403 sec/batch)
2017-04-02 22:43:11.479916: step 510, loss = 3.88 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:43:15.422823: step 520, loss = 3.87 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 22:43:19.362245: step 530, loss = 3.85 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 22:43:23.291761: step 540, loss = 3.84 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 22:43:27.226399: step 550, loss = 3.83 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 22:43:31.199344: step 560, loss = 3.82 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 22:43:35.130523: step 570, loss = 3.80 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 22:43:39.083704: step 580, loss = 3.79 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 22:43:43.041491: step 590, loss = 3.79 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 22:43:47.073816: step 600, loss = 3.77 (317.4 examples/sec; 0.403 sec/batch)
2017-04-02 22:43:51.036298: step 610, loss = 3.76 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 22:43:55.009075: step 620, loss = 3.75 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 22:43:58.963620: step 630, loss = 3.73 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 22:44:02.907994: step 640, loss = 3.72 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 22:44:06.861946: step 650, loss = 3.71 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 22:44:10.837307: step 660, loss = 3.70 (322.0 examples/sec; 0.398 sec/batch)
2017-04-02 22:44:14.752980: step 670, loss = 3.69 (326.9 examples/sec; 0.392 sec/batch)
2017-04-02 22:44:18.719410: step 680, loss = 3.68 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 22:44:22.647574: step 690, loss = 3.67 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 22:44:26.650483: step 700, loss = 3.66 (319.8 examples/sec; 0.400 sec/batch)
2017-04-02 22:44:30.635166: step 710, loss = 3.65 (321.2 examples/sec; 0.398 sec/batch)
2017-04-02 22:44:34.569632: step 720, loss = 3.63 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 22:44:38.539166: step 730, loss = 3.63 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 22:44:42.528989: step 740, loss = 3.62 (320.8 examples/sec; 0.399 sec/batch)
2017-04-02 22:44:46.464732: step 750, loss = 3.60 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 22:44:50.385808: step 760, loss = 3.59 (326.4 examples/sec; 0.392 sec/batch)
2017-04-02 22:44:54.333393: step 770, loss = 3.58 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 22:44:58.274631: step 780, loss = 3.57 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 22:45:02.298401: step 790, loss = 3.56 (318.1 examples/sec; 0.402 sec/batch)
2017-04-02 22:45:06.313672: step 800, loss = 3.55 (318.8 examples/sec; 0.402 sec/batch)
2017-04-02 22:45:10.263807: step 810, loss = 3.54 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 22:45:14.204942: step 820, loss = 3.53 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 22:45:18.158595: step 830, loss = 3.52 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 22:45:22.114207: step 840, loss = 3.52 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 22:45:26.063051: step 850, loss = 3.50 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 22:45:30.016486: step 860, loss = 3.49 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 22:45:33.975865: step 870, loss = 3.48 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 22:45:37.950169: step 880, loss = 3.48 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 22:45:41.908273: step 890, loss = 3.46 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 22:45:45.914547: step 900, loss = 3.46 (319.5 examples/sec; 0.401 sec/batch)
2017-04-02 22:45:49.893579: step 910, loss = 3.45 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 22:45:53.994256: step 920, loss = 3.44 (312.1 examples/sec; 0.410 sec/batch)
2017-04-02 22:45:58.857272: step 930, loss = 3.43 (263.2 examples/sec; 0.486 sec/batch)
2017-04-02 22:46:02.810792: step 940, loss = 3.42 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 22:46:06.745131: step 950, loss = 3.41 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 22:46:10.693710: step 960, loss = 3.40 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 22:46:14.637745: step 970, loss = 3.40 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 22:46:18.593251: step 980, loss = 3.39 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 22:46:22.558220: step 990, loss = 3.38 (322.8 examples/sec; 0.396 sec/batch)
2017-04-02 22:46:26.563209: step 1000, loss = 3.37 (319.6 examples/sec; 0.400 sec/batch)
2017-04-02 22:46:30.499995: step 1010, loss = 3.36 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 22:46:34.436428: step 1020, loss = 3.35 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 22:46:38.362684: step 1030, loss = 3.34 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 22:46:42.319745: step 1040, loss = 3.34 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:46:46.273846: step 1050, loss = 3.33 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 22:46:50.218430: step 1060, loss = 3.32 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 22:46:54.167051: step 1070, loss = 3.31 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 22:46:58.126262: step 1080, loss = 3.30 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 22:47:02.061002: step 1090, loss = 3.29 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 22:47:06.058441: step 1100, loss = 3.28 (320.2 examples/sec; 0.400 sec/batch)
2017-04-02 22:47:10.002336: step 1110, loss = 3.28 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 22:47:13.937571: step 1120, loss = 3.27 (325.3 examples/sec; 0.394 sec/batch)
2017-04-02 22:47:17.893915: step 1130, loss = 3.26 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:47:21.851849: step 1140, loss = 3.26 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 22:47:25.805029: step 1150, loss = 3.25 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 22:47:29.716399: step 1160, loss = 3.24 (327.3 examples/sec; 0.391 sec/batch)
2017-04-02 22:47:33.678017: step 1170, loss = 3.23 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 22:47:37.641837: step 1180, loss = 3.22 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 22:47:41.566570: step 1190, loss = 3.22 (326.1 examples/sec; 0.392 sec/batch)
2017-04-02 22:47:45.567954: step 1200, loss = 3.21 (319.9 examples/sec; 0.400 sec/batch)
2017-04-02 22:47:49.546747: step 1210, loss = 3.20 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 22:47:53.486029: step 1220, loss = 3.20 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 22:47:57.375556: step 1230, loss = 3.19 (329.1 examples/sec; 0.389 sec/batch)
2017-04-02 22:48:01.327545: step 1240, loss = 3.18 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 22:48:05.267987: step 1250, loss = 3.18 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 22:48:09.206005: step 1260, loss = 3.17 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 22:48:13.156575: step 1270, loss = 3.16 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 22:48:17.099136: step 1280, loss = 3.16 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 22:48:21.046593: step 1290, loss = 3.15 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 22:48:25.060903: step 1300, loss = 3.14 (318.9 examples/sec; 0.401 sec/batch)
2017-04-02 22:48:29.014276: step 1310, loss = 3.13 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 22:48:32.995827: step 1320, loss = 3.12 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 22:48:36.952157: step 1330, loss = 3.12 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:48:40.916753: step 1340, loss = 3.12 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 22:48:44.864683: step 1350, loss = 3.11 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 22:48:48.814602: step 1360, loss = 3.10 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 22:48:52.786435: step 1370, loss = 3.09 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 22:48:56.760604: step 1380, loss = 3.09 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 22:49:00.725087: step 1390, loss = 3.08 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 22:49:04.759055: step 1400, loss = 3.08 (317.3 examples/sec; 0.403 sec/batch)
2017-04-02 22:49:08.717735: step 1410, loss = 3.07 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 22:49:12.644470: step 1420, loss = 3.06 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 22:49:16.605655: step 1430, loss = 3.06 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 22:49:20.577522: step 1440, loss = 3.05 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 22:49:24.554847: step 1450, loss = 3.05 (321.8 examples/sec; 0.398 sec/batch)
2017-04-02 22:49:28.506252: step 1460, loss = 3.04 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 22:49:32.481358: step 1470, loss = 3.03 (322.0 examples/sec; 0.398 sec/batch)
2017-04-02 22:49:36.431460: step 1480, loss = 3.03 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 22:49:40.388148: step 1490, loss = 3.02 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:49:44.415932: step 1500, loss = 3.02 (317.8 examples/sec; 0.403 sec/batch)
2017-04-02 22:49:48.942244: step 1510, loss = 3.01 (282.8 examples/sec; 0.453 sec/batch)
2017-04-02 22:49:52.896905: step 1520, loss = 3.00 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 22:49:56.836560: step 1530, loss = 3.00 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 22:50:00.762789: step 1540, loss = 2.99 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 22:50:04.693301: step 1550, loss = 2.99 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 22:50:08.635574: step 1560, loss = 2.98 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 22:50:12.584293: step 1570, loss = 2.98 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 22:50:16.508655: step 1580, loss = 2.97 (326.2 examples/sec; 0.392 sec/batch)
2017-04-02 22:50:20.461579: step 1590, loss = 2.97 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 22:50:24.469225: step 1600, loss = 2.96 (319.4 examples/sec; 0.401 sec/batch)
2017-04-02 22:50:28.424317: step 1610, loss = 2.96 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 22:50:32.386079: step 1620, loss = 2.95 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 22:50:36.345763: step 1630, loss = 2.95 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 22:50:40.274234: step 1640, loss = 2.94 (325.8 examples/sec; 0.393 sec/batch)
2017-04-02 22:50:44.192925: step 1650, loss = 2.94 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 22:50:48.162648: step 1660, loss = 2.93 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 22:50:52.136231: step 1670, loss = 2.92 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 22:50:56.084510: step 1680, loss = 2.93 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 22:50:59.978177: step 1690, loss = 2.92 (328.7 examples/sec; 0.389 sec/batch)
2017-04-02 22:51:03.975133: step 1700, loss = 2.91 (320.2 examples/sec; 0.400 sec/batch)
2017-04-02 22:51:07.908769: step 1710, loss = 2.91 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 22:51:11.857036: step 1720, loss = 2.90 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 22:51:15.779952: step 1730, loss = 2.90 (326.3 examples/sec; 0.392 sec/batch)
2017-04-02 22:51:19.729072: step 1740, loss = 2.89 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 22:51:23.667051: step 1750, loss = 2.89 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 22:51:27.626086: step 1760, loss = 2.88 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 22:51:31.570504: step 1770, loss = 2.88 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 22:51:35.504436: step 1780, loss = 2.87 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 22:51:39.457244: step 1790, loss = 2.87 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 22:51:43.499215: step 1800, loss = 2.86 (316.7 examples/sec; 0.404 sec/batch)
2017-04-02 22:51:47.427616: step 1810, loss = 2.86 (325.8 examples/sec; 0.393 sec/batch)
2017-04-02 22:51:51.368926: step 1820, loss = 2.85 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 22:51:55.320462: step 1830, loss = 2.85 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 22:51:59.244587: step 1840, loss = 2.85 (326.2 examples/sec; 0.392 sec/batch)
2017-04-02 22:52:03.206633: step 1850, loss = 2.84 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 22:52:07.159614: step 1860, loss = 2.84 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 22:52:11.115950: step 1870, loss = 2.83 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:52:15.048279: step 1880, loss = 2.83 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 22:52:19.014262: step 1890, loss = 2.82 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 22:52:23.018930: step 1900, loss = 2.82 (319.6 examples/sec; 0.400 sec/batch)
2017-04-02 22:52:26.965612: step 1910, loss = 2.82 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 22:52:30.889277: step 1920, loss = 2.81 (326.2 examples/sec; 0.392 sec/batch)
2017-04-02 22:52:34.850664: step 1930, loss = 2.81 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 22:52:38.777374: step 1940, loss = 2.81 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 22:52:42.719465: step 1950, loss = 2.80 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 22:52:46.654848: step 1960, loss = 2.80 (325.3 examples/sec; 0.394 sec/batch)
2017-04-02 22:52:50.626954: step 1970, loss = 2.79 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 22:52:54.644383: step 1980, loss = 2.79 (318.6 examples/sec; 0.402 sec/batch)
2017-04-02 22:52:58.618932: step 1990, loss = 2.79 (322.0 examples/sec; 0.397 sec/batch)
2017-04-02 22:53:02.701891: step 2000, loss = 2.78 (313.5 examples/sec; 0.408 sec/batch)
2017-04-02 22:53:06.648104: step 2010, loss = 2.78 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 22:53:10.602837: step 2020, loss = 2.77 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 22:53:14.541352: step 2030, loss = 2.77 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 22:53:18.458336: step 2040, loss = 2.77 (326.8 examples/sec; 0.392 sec/batch)
2017-04-02 22:53:22.398699: step 2050, loss = 2.76 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 22:53:26.338506: step 2060, loss = 2.76 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 22:55:07.048696: step 2070, loss = 2.76 (12.7 examples/sec; 10.071 sec/batch)
2017-04-02 22:55:10.986521: step 2080, loss = 2.75 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 22:55:14.952836: step 2090, loss = 2.75 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 22:55:18.988886: step 2100, loss = 2.75 (317.1 examples/sec; 0.404 sec/batch)
2017-04-02 22:55:22.934590: step 2110, loss = 2.74 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 22:55:26.887831: step 2120, loss = 2.74 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 22:55:30.866028: step 2130, loss = 2.73 (321.8 examples/sec; 0.398 sec/batch)
2017-04-02 22:55:34.833779: step 2140, loss = 2.73 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 22:55:38.779246: step 2150, loss = 2.73 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 22:55:42.730603: step 2160, loss = 2.72 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 22:55:46.667715: step 2170, loss = 2.72 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 22:55:50.623920: step 2180, loss = 2.72 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:55:54.582329: step 2190, loss = 2.71 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 22:55:58.582979: step 2200, loss = 2.71 (319.9 examples/sec; 0.400 sec/batch)
2017-04-02 22:56:02.510866: step 2210, loss = 2.71 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 22:56:06.441214: step 2220, loss = 2.70 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 22:56:10.389575: step 2230, loss = 2.70 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 22:56:14.314836: step 2240, loss = 2.70 (326.1 examples/sec; 0.393 sec/batch)
2017-04-02 22:56:18.262600: step 2250, loss = 2.70 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 22:56:22.223357: step 2260, loss = 2.69 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 22:56:26.206493: step 2270, loss = 2.69 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 22:56:30.143245: step 2280, loss = 2.69 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 22:56:34.172378: step 2290, loss = 2.68 (317.7 examples/sec; 0.403 sec/batch)
2017-04-02 22:56:38.175124: step 2300, loss = 2.68 (319.8 examples/sec; 0.400 sec/batch)
2017-04-02 22:56:42.157400: step 2310, loss = 2.68 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 22:56:46.120894: step 2320, loss = 2.67 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 22:56:50.058247: step 2330, loss = 2.67 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 22:56:54.005753: step 2340, loss = 2.67 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 22:56:57.979932: step 2350, loss = 2.67 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 22:57:01.949920: step 2360, loss = 2.66 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 22:57:05.901257: step 2370, loss = 2.66 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 22:57:09.879736: step 2380, loss = 2.66 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 22:57:13.824836: step 2390, loss = 2.65 (324.5 examples/sec; 0.395 sec/batch)
2017-04-02 22:57:17.852956: step 2400, loss = 2.65 (317.8 examples/sec; 0.403 sec/batch)
2017-04-02 22:57:21.808819: step 2410, loss = 2.65 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 22:57:25.772145: step 2420, loss = 2.65 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 22:57:29.754285: step 2430, loss = 2.64 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 22:57:33.717438: step 2440, loss = 2.64 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 22:57:37.661821: step 2450, loss = 2.64 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 22:57:41.630895: step 2460, loss = 2.64 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 22:57:45.584460: step 2470, loss = 2.63 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 22:57:49.564603: step 2480, loss = 2.63 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 22:57:53.528928: step 2490, loss = 2.62 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 22:57:57.545758: step 2500, loss = 2.62 (318.7 examples/sec; 0.402 sec/batch)
2017-04-02 22:58:01.530658: step 2510, loss = 2.62 (321.2 examples/sec; 0.398 sec/batch)
2017-04-02 22:58:05.463063: step 2520, loss = 2.62 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 22:58:09.407953: step 2530, loss = 2.61 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 22:58:13.372639: step 2540, loss = 2.61 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 22:58:17.332037: step 2550, loss = 2.61 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 22:58:21.324872: step 2560, loss = 2.61 (320.6 examples/sec; 0.399 sec/batch)
2017-04-02 22:58:25.334246: step 2570, loss = 2.61 (319.3 examples/sec; 0.401 sec/batch)
2017-04-02 22:58:29.287351: step 2580, loss = 2.60 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 22:58:33.261845: step 2590, loss = 2.60 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 22:58:37.271865: step 2600, loss = 2.60 (319.2 examples/sec; 0.401 sec/batch)
2017-04-02 22:58:41.265222: step 2610, loss = 2.60 (320.5 examples/sec; 0.399 sec/batch)
2017-04-02 22:58:45.220279: step 2620, loss = 2.59 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 22:58:49.191176: step 2630, loss = 2.59 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 22:58:53.123694: step 2640, loss = 2.59 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 22:58:57.100951: step 2650, loss = 2.59 (321.8 examples/sec; 0.398 sec/batch)
2017-04-02 22:59:01.081909: step 2660, loss = 2.58 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 22:59:05.080674: step 2670, loss = 2.58 (320.1 examples/sec; 0.400 sec/batch)
2017-04-02 22:59:09.053121: step 2680, loss = 2.58 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 22:59:13.033698: step 2690, loss = 2.58 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 22:59:17.073525: step 2700, loss = 2.58 (316.8 examples/sec; 0.404 sec/batch)
2017-04-02 22:59:21.026871: step 2710, loss = 2.58 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 22:59:24.971859: step 2720, loss = 2.57 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 22:59:28.918861: step 2730, loss = 2.57 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 22:59:32.863268: step 2740, loss = 2.57 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 22:59:36.793081: step 2750, loss = 2.56 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 22:59:40.750999: step 2760, loss = 2.56 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 22:59:44.713440: step 2770, loss = 2.56 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 22:59:49.226790: step 2780, loss = 2.56 (283.6 examples/sec; 0.451 sec/batch)
2017-04-02 22:59:53.169725: step 2790, loss = 2.56 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 22:59:57.194329: step 2800, loss = 2.55 (318.0 examples/sec; 0.402 sec/batch)
2017-04-02 23:00:01.141079: step 2810, loss = 2.55 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 23:00:05.124847: step 2820, loss = 2.55 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 23:00:09.066256: step 2830, loss = 2.55 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 23:00:13.011763: step 2840, loss = 2.55 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 23:00:16.958161: step 2850, loss = 2.55 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 23:00:20.916368: step 2860, loss = 2.54 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 23:00:24.877128: step 2870, loss = 2.54 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 23:00:28.836826: step 2880, loss = 2.54 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 23:00:32.790072: step 2890, loss = 2.54 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 23:00:36.822219: step 2900, loss = 2.54 (317.4 examples/sec; 0.403 sec/batch)
2017-04-02 23:00:40.802555: step 2910, loss = 2.53 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 23:00:44.756747: step 2920, loss = 2.53 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 23:00:48.696973: step 2930, loss = 2.53 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 23:00:52.646187: step 2940, loss = 2.53 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 23:00:56.641405: step 2950, loss = 2.53 (320.4 examples/sec; 0.400 sec/batch)
2017-04-02 23:01:00.585631: step 2960, loss = 2.52 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 23:01:04.527760: step 2970, loss = 2.52 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 23:01:08.474568: step 2980, loss = 2.52 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 23:01:12.429910: step 2990, loss = 2.52 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 23:01:16.481954: step 3000, loss = 2.52 (315.9 examples/sec; 0.405 sec/batch)
2017-04-02 23:01:20.478298: step 3010, loss = 2.52 (320.3 examples/sec; 0.400 sec/batch)
2017-04-02 23:01:24.466258: step 3020, loss = 2.52 (321.0 examples/sec; 0.399 sec/batch)
2017-04-02 23:01:28.441489: step 3030, loss = 2.51 (322.0 examples/sec; 0.398 sec/batch)
2017-04-02 23:01:32.421606: step 3040, loss = 2.51 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 23:01:36.393100: step 3050, loss = 2.51 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 23:01:40.363193: step 3060, loss = 2.51 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 23:01:44.310934: step 3070, loss = 2.51 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 23:01:48.281264: step 3080, loss = 2.51 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 23:01:52.258506: step 3090, loss = 2.50 (321.8 examples/sec; 0.398 sec/batch)
2017-04-02 23:01:56.292718: step 3100, loss = 2.50 (317.3 examples/sec; 0.403 sec/batch)
2017-04-02 23:02:00.273060: step 3110, loss = 2.50 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 23:02:04.248559: step 3120, loss = 2.50 (322.0 examples/sec; 0.398 sec/batch)
2017-04-02 23:02:08.198044: step 3130, loss = 2.49 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 23:02:12.169427: step 3140, loss = 2.50 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 23:02:16.141592: step 3150, loss = 2.49 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 23:02:20.083926: step 3160, loss = 2.49 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 23:02:24.045520: step 3170, loss = 2.49 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 23:02:28.024015: step 3180, loss = 2.49 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 23:02:32.022642: step 3190, loss = 2.49 (320.1 examples/sec; 0.400 sec/batch)
2017-04-02 23:02:36.027668: step 3200, loss = 2.48 (319.6 examples/sec; 0.401 sec/batch)
2017-04-02 23:02:39.990065: step 3210, loss = 2.48 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 23:02:43.971579: step 3220, loss = 2.48 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 23:02:47.905746: step 3230, loss = 2.48 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 23:02:51.863730: step 3240, loss = 2.48 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 23:02:55.830118: step 3250, loss = 2.48 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 23:02:59.799344: step 3260, loss = 2.48 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 23:03:03.756082: step 3270, loss = 2.47 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 23:03:07.737369: step 3280, loss = 2.47 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 23:03:11.701353: step 3290, loss = 2.47 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 23:03:15.742611: step 3300, loss = 2.47 (316.7 examples/sec; 0.404 sec/batch)
2017-04-02 23:03:19.681354: step 3310, loss = 2.48 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 23:03:23.638796: step 3320, loss = 2.47 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 23:03:27.596470: step 3330, loss = 2.47 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 23:03:31.567238: step 3340, loss = 2.47 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 23:03:35.504404: step 3350, loss = 2.47 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 23:03:39.449100: step 3360, loss = 2.46 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 23:03:43.413124: step 3370, loss = 2.46 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 23:03:47.373661: step 3380, loss = 2.46 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 23:03:51.348319: step 3390, loss = 2.46 (322.0 examples/sec; 0.397 sec/batch)
2017-04-02 23:03:55.386921: step 3400, loss = 2.46 (316.9 examples/sec; 0.404 sec/batch)
2017-04-02 23:03:59.323051: step 3410, loss = 2.46 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 23:04:03.287745: step 3420, loss = 2.46 (322.8 examples/sec; 0.396 sec/batch)
2017-04-02 23:04:07.246575: step 3430, loss = 2.45 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 23:04:11.229435: step 3440, loss = 2.45 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 23:04:15.164301: step 3450, loss = 2.45 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 23:04:19.144955: step 3460, loss = 2.45 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 23:04:23.108350: step 3470, loss = 2.45 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 23:04:27.062120: step 3480, loss = 2.45 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 23:04:31.009038: step 3490, loss = 2.45 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 23:04:35.048153: step 3500, loss = 2.45 (316.9 examples/sec; 0.404 sec/batch)
2017-04-02 23:04:38.998892: step 3510, loss = 2.44 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 23:04:42.948631: step 3520, loss = 2.45 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 23:04:47.394234: step 3530, loss = 2.45 (287.9 examples/sec; 0.445 sec/batch)
2017-04-02 23:04:51.974627: step 3540, loss = 2.44 (279.5 examples/sec; 0.458 sec/batch)
2017-04-02 23:04:55.947062: step 3550, loss = 2.44 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 23:04:59.888167: step 3560, loss = 2.44 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 23:05:03.837491: step 3570, loss = 2.44 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 23:05:07.770278: step 3580, loss = 2.44 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 23:05:11.742667: step 3590, loss = 2.44 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 23:05:15.732891: step 3600, loss = 2.44 (320.8 examples/sec; 0.399 sec/batch)
2017-04-02 23:05:19.692310: step 3610, loss = 2.44 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 23:05:23.700294: step 3620, loss = 2.43 (319.4 examples/sec; 0.401 sec/batch)
2017-04-02 23:05:27.666426: step 3630, loss = 2.43 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 23:05:31.589622: step 3640, loss = 2.43 (326.3 examples/sec; 0.392 sec/batch)
2017-04-02 23:05:35.529913: step 3650, loss = 2.43 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 23:05:39.460941: step 3660, loss = 2.43 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 23:05:43.379604: step 3670, loss = 2.43 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 23:05:47.302759: step 3680, loss = 2.43 (326.3 examples/sec; 0.392 sec/batch)
2017-04-02 23:05:51.242490: step 3690, loss = 2.43 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 23:05:55.258306: step 3700, loss = 2.43 (318.7 examples/sec; 0.402 sec/batch)
2017-04-02 23:05:59.238900: step 3710, loss = 2.42 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 23:06:03.182316: step 3720, loss = 2.43 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 23:06:07.161563: step 3730, loss = 2.42 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 23:06:11.142381: step 3740, loss = 2.42 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 23:06:15.139238: step 3750, loss = 2.42 (320.3 examples/sec; 0.400 sec/batch)
2017-04-02 23:06:19.130136: step 3760, loss = 2.41 (320.7 examples/sec; 0.399 sec/batch)
2017-04-02 23:06:23.097065: step 3770, loss = 2.42 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 23:06:27.044004: step 3780, loss = 2.42 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 23:06:31.017299: step 3790, loss = 2.42 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 23:06:35.029877: step 3800, loss = 2.42 (319.0 examples/sec; 0.401 sec/batch)
2017-04-02 23:06:38.972104: step 3810, loss = 2.41 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 23:06:42.922747: step 3820, loss = 2.42 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 23:06:46.857607: step 3830, loss = 2.41 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 23:06:50.826710: step 3840, loss = 2.42 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 23:06:54.786298: step 3850, loss = 2.41 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 23:06:58.740469: step 3860, loss = 2.41 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 23:07:02.679334: step 3870, loss = 2.41 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 23:07:06.621171: step 3880, loss = 2.41 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 23:07:10.563878: step 3890, loss = 2.41 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 23:07:14.595328: step 3900, loss = 2.41 (317.5 examples/sec; 0.403 sec/batch)
2017-04-02 23:07:18.550043: step 3910, loss = 2.41 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 23:07:22.502622: step 3920, loss = 2.40 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 23:07:26.411317: step 3930, loss = 2.40 (327.5 examples/sec; 0.391 sec/batch)
2017-04-02 23:07:30.330156: step 3940, loss = 2.40 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 23:07:34.274818: step 3950, loss = 2.40 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 23:07:38.241486: step 3960, loss = 2.40 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 23:07:42.165730: step 3970, loss = 2.40 (326.2 examples/sec; 0.392 sec/batch)
2017-04-02 23:07:46.099476: step 3980, loss = 2.40 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 23:07:50.056539: step 3990, loss = 2.40 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 23:07:54.071489: step 4000, loss = 2.40 (318.8 examples/sec; 0.401 sec/batch)
2017-04-02 23:10:40.650881: step 4010, loss = 2.40 (7.7 examples/sec; 16.658 sec/batch)
2017-04-02 23:10:44.606544: step 4020, loss = 2.39 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 23:10:48.563420: step 4030, loss = 2.40 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 23:10:52.509453: step 4040, loss = 2.40 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 23:10:56.462095: step 4050, loss = 2.40 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 23:11:00.391550: step 4060, loss = 2.40 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 23:11:04.344816: step 4070, loss = 2.39 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 23:11:08.274982: step 4080, loss = 2.40 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 23:11:12.222013: step 4090, loss = 2.39 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 23:11:16.257665: step 4100, loss = 2.39 (317.2 examples/sec; 0.404 sec/batch)
2017-04-02 23:11:20.204926: step 4110, loss = 2.39 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 23:11:24.176110: step 4120, loss = 2.39 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 23:11:28.132826: step 4130, loss = 2.39 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 23:11:32.083914: step 4140, loss = 2.39 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 23:11:36.025689: step 4150, loss = 2.38 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 23:11:39.968586: step 4160, loss = 2.39 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 23:11:43.930055: step 4170, loss = 2.39 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 23:11:47.874251: step 4180, loss = 2.39 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 23:11:51.845131: step 4190, loss = 2.39 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 23:11:55.856611: step 4200, loss = 2.38 (319.1 examples/sec; 0.401 sec/batch)
2017-04-02 23:11:59.801155: step 4210, loss = 2.39 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 23:12:03.752322: step 4220, loss = 2.38 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 23:12:07.710905: step 4230, loss = 2.38 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 23:12:11.680224: step 4240, loss = 2.38 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 23:12:15.658490: step 4250, loss = 2.38 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 23:12:19.609609: step 4260, loss = 2.38 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 23:12:23.579751: step 4270, loss = 2.38 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 23:12:27.516786: step 4280, loss = 2.38 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 23:12:31.498947: step 4290, loss = 2.38 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 23:12:35.559950: step 4300, loss = 2.38 (315.2 examples/sec; 0.406 sec/batch)
2017-04-02 23:12:39.573140: step 4310, loss = 2.38 (318.9 examples/sec; 0.401 sec/batch)
2017-04-02 23:12:43.529048: step 4320, loss = 2.38 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 23:12:47.486266: step 4330, loss = 2.37 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 23:12:51.481586: step 4340, loss = 2.37 (320.4 examples/sec; 0.400 sec/batch)
2017-04-02 23:12:55.460944: step 4350, loss = 2.38 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 23:12:59.440072: step 4360, loss = 2.38 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 23:13:03.411084: step 4370, loss = 2.37 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 23:13:07.344276: step 4380, loss = 2.37 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 23:13:11.316557: step 4390, loss = 2.37 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 23:13:15.351627: step 4400, loss = 2.37 (317.2 examples/sec; 0.404 sec/batch)
2017-04-02 23:13:19.341191: step 4410, loss = 2.37 (320.8 examples/sec; 0.399 sec/batch)
2017-04-02 23:13:23.317414: step 4420, loss = 2.37 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 23:13:27.308299: step 4430, loss = 2.37 (320.7 examples/sec; 0.399 sec/batch)
2017-04-02 23:13:31.286504: step 4440, loss = 2.37 (321.8 examples/sec; 0.398 sec/batch)
2017-04-02 23:13:35.253651: step 4450, loss = 2.37 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 23:13:39.227429: step 4460, loss = 2.37 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 23:13:43.240333: step 4470, loss = 2.37 (319.0 examples/sec; 0.401 sec/batch)
2017-04-02 23:13:47.194837: step 4480, loss = 2.36 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 23:13:51.156998: step 4490, loss = 2.37 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 23:13:55.196943: step 4500, loss = 2.37 (316.8 examples/sec; 0.404 sec/batch)
2017-04-02 23:13:59.185250: step 4510, loss = 2.37 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 23:14:03.158266: step 4520, loss = 2.37 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 23:14:07.100505: step 4530, loss = 2.37 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 23:14:11.134993: step 4540, loss = 2.36 (317.3 examples/sec; 0.403 sec/batch)
2017-04-02 23:14:16.213138: step 4550, loss = 2.36 (252.1 examples/sec; 0.508 sec/batch)
2017-04-02 23:14:20.161162: step 4560, loss = 2.37 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 23:14:24.128779: step 4570, loss = 2.36 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 23:14:28.084531: step 4580, loss = 2.36 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 23:14:32.017124: step 4590, loss = 2.36 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 23:14:36.039208: step 4600, loss = 2.36 (318.2 examples/sec; 0.402 sec/batch)
2017-04-02 23:14:39.984763: step 4610, loss = 2.36 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 23:14:43.957720: step 4620, loss = 2.36 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 23:14:47.914388: step 4630, loss = 2.36 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 23:14:51.868926: step 4640, loss = 2.36 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 23:14:55.855778: step 4650, loss = 2.36 (321.1 examples/sec; 0.399 sec/batch)
2017-04-02 23:14:59.770266: step 4660, loss = 2.36 (327.0 examples/sec; 0.391 sec/batch)
2017-04-02 23:15:03.698851: step 4670, loss = 2.36 (325.8 examples/sec; 0.393 sec/batch)
2017-04-02 23:15:07.675340: step 4680, loss = 2.36 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 23:15:11.598924: step 4690, loss = 2.36 (326.2 examples/sec; 0.392 sec/batch)
2017-04-02 23:15:15.618036: step 4700, loss = 2.36 (318.5 examples/sec; 0.402 sec/batch)
2017-04-02 23:15:19.600546: step 4710, loss = 2.36 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 23:15:23.543816: step 4720, loss = 2.36 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 23:15:27.474409: step 4730, loss = 2.36 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 23:15:31.440893: step 4740, loss = 2.35 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 23:15:35.388787: step 4750, loss = 2.36 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 23:15:39.367836: step 4760, loss = 2.36 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 23:15:43.287936: step 4770, loss = 2.35 (326.5 examples/sec; 0.392 sec/batch)
2017-04-02 23:15:47.255786: step 4780, loss = 2.35 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 23:15:51.191523: step 4790, loss = 2.36 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 23:15:55.191840: step 4800, loss = 2.36 (320.0 examples/sec; 0.400 sec/batch)
2017-04-02 23:15:59.167207: step 4810, loss = 2.36 (322.0 examples/sec; 0.398 sec/batch)
2017-04-02 23:16:03.117247: step 4820, loss = 2.35 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 23:16:07.052167: step 4830, loss = 2.35 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 23:16:10.998471: step 4840, loss = 2.35 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 23:16:14.933812: step 4850, loss = 2.35 (325.3 examples/sec; 0.394 sec/batch)
2017-04-02 23:16:18.892869: step 4860, loss = 2.35 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 23:16:22.840181: step 4870, loss = 2.35 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 23:16:26.802552: step 4880, loss = 2.35 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 23:16:30.762779: step 4890, loss = 2.35 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 23:16:34.785660: step 4900, loss = 2.35 (318.2 examples/sec; 0.402 sec/batch)
2017-04-02 23:16:38.754711: step 4910, loss = 2.35 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 23:16:42.718505: step 4920, loss = 2.35 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 23:16:46.664414: step 4930, loss = 2.35 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 23:16:50.625724: step 4940, loss = 2.35 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 23:16:54.559590: step 4950, loss = 2.35 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 23:16:58.510908: step 4960, loss = 2.35 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 23:17:02.488992: step 4970, loss = 2.35 (321.8 examples/sec; 0.398 sec/batch)
2017-04-02 23:17:06.435676: step 4980, loss = 2.35 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 23:17:10.363941: step 4990, loss = 2.35 (325.8 examples/sec; 0.393 sec/batch)
2017-04-02 23:17:14.399628: step 5000, loss = 2.35 (317.2 examples/sec; 0.404 sec/batch)
2017-04-02 23:17:18.351682: step 5010, loss = 2.35 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 23:17:22.293890: step 5020, loss = 2.35 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 23:17:26.234282: step 5030, loss = 2.35 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 23:17:30.188567: step 5040, loss = 2.34 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 23:17:34.125650: step 5050, loss = 2.35 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 23:17:38.069313: step 5060, loss = 2.35 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 23:17:42.020862: step 5070, loss = 2.34 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 23:17:45.965925: step 5080, loss = 2.34 (324.5 examples/sec; 0.395 sec/batch)
2017-04-02 23:17:49.910993: step 5090, loss = 2.34 (324.5 examples/sec; 0.395 sec/batch)
2017-04-02 23:17:53.909456: step 5100, loss = 2.34 (320.1 examples/sec; 0.400 sec/batch)
2017-04-02 23:17:57.849863: step 5110, loss = 2.34 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 23:18:01.795974: step 5120, loss = 2.34 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 23:18:05.755175: step 5130, loss = 2.34 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 23:18:09.717701: step 5140, loss = 2.34 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 23:18:13.636535: step 5150, loss = 2.34 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 23:18:17.577002: step 5160, loss = 2.34 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 23:18:21.536434: step 5170, loss = 2.34 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 23:18:25.438125: step 5180, loss = 2.34 (328.1 examples/sec; 0.390 sec/batch)
2017-04-02 23:18:29.394326: step 5190, loss = 2.34 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 23:18:33.413228: step 5200, loss = 2.34 (318.5 examples/sec; 0.402 sec/batch)
2017-04-02 23:18:37.363155: step 5210, loss = 2.34 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 23:18:41.303043: step 5220, loss = 2.34 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 23:18:45.246010: step 5230, loss = 2.34 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 23:18:49.208099: step 5240, loss = 2.34 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 23:18:53.156833: step 5250, loss = 2.34 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 23:18:57.081581: step 5260, loss = 2.34 (326.1 examples/sec; 0.392 sec/batch)
2017-04-02 23:19:01.025944: step 5270, loss = 2.34 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 23:19:04.964088: step 5280, loss = 2.34 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 23:19:08.876213: step 5290, loss = 2.34 (327.2 examples/sec; 0.391 sec/batch)
2017-04-02 23:19:12.872766: step 5300, loss = 2.33 (320.3 examples/sec; 0.400 sec/batch)
2017-04-02 23:19:16.838861: step 5310, loss = 2.34 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 23:19:20.779798: step 5320, loss = 2.34 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 23:19:24.711689: step 5330, loss = 2.34 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 23:19:28.661128: step 5340, loss = 2.33 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 23:19:33.277592: step 5350, loss = 2.34 (277.3 examples/sec; 0.462 sec/batch)
2017-04-02 23:19:37.662504: step 5360, loss = 2.33 (291.9 examples/sec; 0.438 sec/batch)
2017-04-02 23:19:41.622718: step 5370, loss = 2.34 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 23:19:45.530868: step 5380, loss = 2.34 (327.5 examples/sec; 0.391 sec/batch)
2017-04-02 23:19:49.467973: step 5390, loss = 2.34 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 23:19:53.441382: step 5400, loss = 2.33 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 23:19:57.367922: step 5410, loss = 2.33 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 23:20:01.334735: step 5420, loss = 2.33 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 23:20:05.265215: step 5430, loss = 2.33 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 23:20:09.218691: step 5440, loss = 2.33 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 23:20:13.166127: step 5450, loss = 2.33 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 23:20:17.127888: step 5460, loss = 2.33 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 23:20:21.060799: step 5470, loss = 2.33 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 23:20:25.009526: step 5480, loss = 2.33 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 23:20:28.936155: step 5490, loss = 2.33 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 23:20:32.933542: step 5500, loss = 2.33 (320.2 examples/sec; 0.400 sec/batch)
2017-04-02 23:20:37.808551: step 5510, loss = 2.33 (262.6 examples/sec; 0.487 sec/batch)
2017-04-02 23:20:42.109172: step 5520, loss = 2.33 (297.6 examples/sec; 0.430 sec/batch)
2017-04-02 23:20:46.030301: step 5530, loss = 2.33 (326.4 examples/sec; 0.392 sec/batch)
2017-04-02 23:20:49.974548: step 5540, loss = 2.33 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 23:20:53.905854: step 5550, loss = 2.33 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 23:20:57.847091: step 5560, loss = 2.33 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 23:21:01.797115: step 5570, loss = 2.33 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 23:21:05.753282: step 5580, loss = 2.33 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 23:21:09.702858: step 5590, loss = 2.33 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 23:21:13.729039: step 5600, loss = 2.33 (317.9 examples/sec; 0.403 sec/batch)
2017-04-02 23:21:17.880823: step 5610, loss = 2.33 (308.3 examples/sec; 0.415 sec/batch)
2017-04-02 23:21:21.809597: step 5620, loss = 2.33 (325.8 examples/sec; 0.393 sec/batch)
2017-04-02 23:21:26.042555: step 5630, loss = 2.33 (302.4 examples/sec; 0.423 sec/batch)
2017-04-02 23:21:30.421500: step 5640, loss = 2.33 (292.3 examples/sec; 0.438 sec/batch)
2017-04-02 23:21:34.694754: step 5650, loss = 2.33 (299.5 examples/sec; 0.427 sec/batch)
2017-04-02 23:21:39.140783: step 5660, loss = 2.33 (287.9 examples/sec; 0.445 sec/batch)
2017-04-02 23:21:43.561764: step 5670, loss = 2.33 (289.5 examples/sec; 0.442 sec/batch)
2017-04-02 23:21:47.513808: step 5680, loss = 2.33 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 23:21:51.450212: step 5690, loss = 2.33 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 23:21:55.467623: step 5700, loss = 2.33 (318.6 examples/sec; 0.402 sec/batch)
2017-04-02 23:21:59.394199: step 5710, loss = 2.33 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 23:22:03.347691: step 5720, loss = 2.33 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 23:22:07.297476: step 5730, loss = 2.33 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 23:22:11.225159: step 5740, loss = 2.33 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 23:22:15.172877: step 5750, loss = 2.33 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 23:22:19.130379: step 5760, loss = 2.33 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 23:22:23.085783: step 5770, loss = 2.33 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 23:22:27.021161: step 5780, loss = 2.33 (325.3 examples/sec; 0.394 sec/batch)
2017-04-02 23:22:30.948569: step 5790, loss = 2.33 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 23:22:34.962626: step 5800, loss = 2.33 (318.9 examples/sec; 0.401 sec/batch)
2017-04-02 23:22:38.926869: step 5810, loss = 2.33 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 23:22:42.873022: step 5820, loss = 2.32 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 23:22:46.793123: step 5830, loss = 2.33 (326.5 examples/sec; 0.392 sec/batch)
2017-04-02 23:22:50.707541: step 5840, loss = 2.33 (327.0 examples/sec; 0.391 sec/batch)
2017-04-02 23:22:54.681205: step 5850, loss = 2.33 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 23:22:58.613411: step 5860, loss = 2.33 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 23:23:02.552522: step 5870, loss = 2.32 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 23:23:06.511070: step 5880, loss = 2.32 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 23:23:10.426820: step 5890, loss = 2.32 (326.9 examples/sec; 0.392 sec/batch)
2017-04-02 23:23:14.450199: step 5900, loss = 2.32 (318.1 examples/sec; 0.402 sec/batch)
2017-04-02 23:23:18.394634: step 5910, loss = 2.32 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 23:23:22.328523: step 5920, loss = 2.32 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 23:23:26.277580: step 5930, loss = 2.32 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 23:23:30.231875: step 5940, loss = 2.32 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 23:23:34.184733: step 5950, loss = 2.32 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 23:23:38.126779: step 5960, loss = 2.32 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 23:23:42.068523: step 5970, loss = 2.32 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 23:23:46.022142: step 5980, loss = 2.32 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 23:23:49.966058: step 5990, loss = 2.32 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 23:23:53.968889: step 6000, loss = 2.32 (319.8 examples/sec; 0.400 sec/batch)
2017-04-02 23:25:02.581481: step 6010, loss = 2.32 (18.7 examples/sec; 6.861 sec/batch)
2017-04-02 23:25:06.547752: step 6020, loss = 2.33 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 23:25:10.474738: step 6030, loss = 2.32 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 23:25:14.455154: step 6040, loss = 2.32 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 23:25:18.396491: step 6050, loss = 2.32 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 23:25:22.381765: step 6060, loss = 2.32 (321.2 examples/sec; 0.399 sec/batch)
2017-04-02 23:25:26.324238: step 6070, loss = 2.32 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 23:25:30.274710: step 6080, loss = 2.32 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 23:25:34.230735: step 6090, loss = 2.32 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 23:25:38.280902: step 6100, loss = 2.32 (316.0 examples/sec; 0.405 sec/batch)
2017-04-02 23:25:42.501578: step 6110, loss = 2.32 (303.3 examples/sec; 0.422 sec/batch)
2017-04-02 23:25:46.463955: step 6120, loss = 2.32 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 23:25:50.384241: step 6130, loss = 2.32 (326.5 examples/sec; 0.392 sec/batch)
2017-04-02 23:25:54.331417: step 6140, loss = 2.32 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 23:25:58.270377: step 6150, loss = 2.32 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 23:26:02.239924: step 6160, loss = 2.32 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 23:26:06.168683: step 6170, loss = 2.32 (325.8 examples/sec; 0.393 sec/batch)
2017-04-02 23:26:10.128507: step 6180, loss = 2.32 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 23:26:14.036314: step 6190, loss = 2.32 (327.5 examples/sec; 0.391 sec/batch)
2017-04-02 23:26:18.023613: step 6200, loss = 2.32 (321.0 examples/sec; 0.399 sec/batch)
2017-04-02 23:26:21.983187: step 6210, loss = 2.32 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 23:26:25.909401: step 6220, loss = 2.32 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 23:26:29.857277: step 6230, loss = 2.32 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 23:26:33.804579: step 6240, loss = 2.32 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 23:26:37.725720: step 6250, loss = 2.32 (326.4 examples/sec; 0.392 sec/batch)
2017-04-02 23:26:41.659582: step 6260, loss = 2.32 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 23:26:45.612874: step 6270, loss = 2.32 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 23:26:49.545462: step 6280, loss = 2.32 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 23:26:53.485691: step 6290, loss = 2.32 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 23:26:57.491260: step 6300, loss = 2.32 (319.6 examples/sec; 0.401 sec/batch)
2017-04-02 23:27:01.453522: step 6310, loss = 2.32 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 23:27:05.383115: step 6320, loss = 2.32 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 23:27:09.342268: step 6330, loss = 2.32 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 23:27:13.292362: step 6340, loss = 2.32 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 23:27:17.227863: step 6350, loss = 2.32 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 23:27:21.177721: step 6360, loss = 2.32 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 23:27:25.112059: step 6370, loss = 2.32 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 23:27:29.059845: step 6380, loss = 2.32 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 23:27:33.001212: step 6390, loss = 2.32 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 23:27:37.002435: step 6400, loss = 2.31 (319.9 examples/sec; 0.400 sec/batch)
2017-04-02 23:27:40.939000: step 6410, loss = 2.32 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 23:27:44.913882: step 6420, loss = 2.32 (322.0 examples/sec; 0.397 sec/batch)
2017-04-02 23:27:48.861507: step 6430, loss = 2.32 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 23:27:52.786631: step 6440, loss = 2.32 (326.1 examples/sec; 0.393 sec/batch)
2017-04-02 23:27:56.745749: step 6450, loss = 2.31 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 23:28:00.692172: step 6460, loss = 2.32 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 23:28:04.605987: step 6470, loss = 2.31 (327.0 examples/sec; 0.391 sec/batch)
2017-04-02 23:28:08.527844: step 6480, loss = 2.32 (326.4 examples/sec; 0.392 sec/batch)
2017-04-02 23:28:12.432356: step 6490, loss = 2.31 (327.8 examples/sec; 0.390 sec/batch)
2017-04-02 23:28:16.474124: step 6500, loss = 2.31 (316.7 examples/sec; 0.404 sec/batch)
2017-04-02 23:28:20.411494: step 6510, loss = 2.32 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 23:28:24.368664: step 6520, loss = 2.31 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 23:28:28.328998: step 6530, loss = 2.31 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 23:28:32.255072: step 6540, loss = 2.32 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 23:28:36.186610: step 6550, loss = 2.32 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 23:28:40.134953: step 6560, loss = 2.32 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 23:28:44.077733: step 6570, loss = 2.32 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 23:28:47.998393: step 6580, loss = 2.31 (326.5 examples/sec; 0.392 sec/batch)
2017-04-02 23:28:51.954131: step 6590, loss = 2.31 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 23:28:55.948355: step 6600, loss = 2.31 (320.5 examples/sec; 0.399 sec/batch)
2017-04-02 23:28:59.903928: step 6610, loss = 2.32 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 23:29:03.839367: step 6620, loss = 2.32 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 23:29:07.775443: step 6630, loss = 2.32 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 23:29:11.764253: step 6640, loss = 2.32 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 23:29:15.690534: step 6650, loss = 2.32 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 23:29:19.626285: step 6660, loss = 2.31 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 23:29:23.559531: step 6670, loss = 2.32 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 23:29:27.507106: step 6680, loss = 2.31 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 23:29:31.463173: step 6690, loss = 2.32 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 23:29:35.458477: step 6700, loss = 2.32 (320.4 examples/sec; 0.400 sec/batch)
2017-04-02 23:29:39.422971: step 6710, loss = 2.31 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 23:29:43.341527: step 6720, loss = 2.31 (326.7 examples/sec; 0.392 sec/batch)
2017-04-02 23:29:47.265372: step 6730, loss = 2.31 (326.2 examples/sec; 0.392 sec/batch)
2017-04-02 23:29:51.513979: step 6740, loss = 2.31 (301.3 examples/sec; 0.425 sec/batch)
2017-04-02 23:29:55.443899: step 6750, loss = 2.31 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 23:29:59.385384: step 6760, loss = 2.31 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 23:30:03.329522: step 6770, loss = 2.31 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 23:30:07.261069: step 6780, loss = 2.31 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 23:30:11.206502: step 6790, loss = 2.31 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 23:30:15.183177: step 6800, loss = 2.31 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 23:30:19.119122: step 6810, loss = 2.31 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 23:30:23.071866: step 6820, loss = 2.31 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 23:30:27.064020: step 6830, loss = 2.31 (320.6 examples/sec; 0.399 sec/batch)
2017-04-02 23:30:31.003210: step 6840, loss = 2.31 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 23:30:34.925403: step 6850, loss = 2.31 (326.3 examples/sec; 0.392 sec/batch)
2017-04-02 23:30:39.761161: step 6860, loss = 2.31 (264.7 examples/sec; 0.484 sec/batch)
2017-04-02 23:30:43.671272: step 6870, loss = 2.31 (327.4 examples/sec; 0.391 sec/batch)
2017-04-02 23:30:47.572593: step 6880, loss = 2.31 (328.1 examples/sec; 0.390 sec/batch)
2017-04-02 23:30:51.511530: step 6890, loss = 2.31 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 23:30:55.502482: step 6900, loss = 2.31 (320.7 examples/sec; 0.399 sec/batch)
2017-04-02 23:30:59.435849: step 6910, loss = 2.31 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 23:31:03.345608: step 6920, loss = 2.31 (327.4 examples/sec; 0.391 sec/batch)
2017-04-02 23:31:07.292405: step 6930, loss = 2.31 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 23:31:11.203490: step 6940, loss = 2.31 (327.3 examples/sec; 0.391 sec/batch)
2017-04-02 23:31:15.137165: step 6950, loss = 2.31 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 23:31:19.082375: step 6960, loss = 2.31 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 23:31:23.009108: step 6970, loss = 2.31 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 23:31:26.943444: step 6980, loss = 2.31 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 23:31:30.888323: step 6990, loss = 2.31 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 23:31:34.905271: step 7000, loss = 2.31 (318.6 examples/sec; 0.402 sec/batch)
2017-04-02 23:31:38.836391: step 7010, loss = 2.31 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 23:31:42.745483: step 7020, loss = 2.31 (327.4 examples/sec; 0.391 sec/batch)
2017-04-02 23:31:46.701133: step 7030, loss = 2.31 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 23:31:50.623192: step 7040, loss = 2.31 (326.4 examples/sec; 0.392 sec/batch)
2017-04-02 23:31:54.556663: step 7050, loss = 2.31 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 23:31:58.479870: step 7060, loss = 2.31 (326.3 examples/sec; 0.392 sec/batch)
2017-04-02 23:32:02.416443: step 7070, loss = 2.31 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 23:32:06.355886: step 7080, loss = 2.31 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 23:32:10.288612: step 7090, loss = 2.31 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 23:32:14.280947: step 7100, loss = 2.31 (320.6 examples/sec; 0.399 sec/batch)
2017-04-02 23:32:18.226516: step 7110, loss = 2.31 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 23:32:22.146031: step 7120, loss = 2.31 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 23:32:26.065600: step 7130, loss = 2.31 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 23:32:30.006616: step 7140, loss = 2.31 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 23:32:33.950625: step 7150, loss = 2.31 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 23:32:37.897382: step 7160, loss = 2.31 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 23:32:41.825508: step 7170, loss = 2.31 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 23:32:45.739792: step 7180, loss = 2.31 (327.0 examples/sec; 0.391 sec/batch)
2017-04-02 23:32:49.856072: step 7190, loss = 2.31 (311.0 examples/sec; 0.412 sec/batch)
2017-04-02 23:32:53.882927: step 7200, loss = 2.31 (317.9 examples/sec; 0.403 sec/batch)
2017-04-02 23:32:57.856175: step 7210, loss = 2.31 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 23:33:01.796748: step 7220, loss = 2.31 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 23:33:05.748712: step 7230, loss = 2.31 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 23:33:09.685518: step 7240, loss = 2.31 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 23:33:13.628834: step 7250, loss = 2.31 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 23:33:17.556782: step 7260, loss = 2.31 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 23:33:21.497550: step 7270, loss = 2.31 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 23:33:25.605133: step 7280, loss = 2.31 (311.6 examples/sec; 0.411 sec/batch)
2017-04-02 23:33:29.544655: step 7290, loss = 2.31 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 23:33:33.517131: step 7300, loss = 2.31 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 23:33:37.449134: step 7310, loss = 2.31 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 23:33:41.408561: step 7320, loss = 2.31 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 23:33:45.310346: step 7330, loss = 2.31 (328.1 examples/sec; 0.390 sec/batch)
2017-04-02 23:33:49.208415: step 7340, loss = 2.31 (328.4 examples/sec; 0.390 sec/batch)
2017-04-02 23:33:53.117057: step 7350, loss = 2.31 (327.5 examples/sec; 0.391 sec/batch)
2017-04-02 23:33:57.055028: step 7360, loss = 2.31 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 23:34:00.963598: step 7370, loss = 2.31 (327.5 examples/sec; 0.391 sec/batch)
2017-04-02 23:34:04.889379: step 7380, loss = 2.31 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 23:34:08.838448: step 7390, loss = 2.32 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 23:34:12.837453: step 7400, loss = 2.31 (320.1 examples/sec; 0.400 sec/batch)
2017-04-02 23:34:16.749760: step 7410, loss = 2.31 (327.2 examples/sec; 0.391 sec/batch)
2017-04-02 23:34:20.700333: step 7420, loss = 2.31 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 23:34:24.629199: step 7430, loss = 2.32 (325.8 examples/sec; 0.393 sec/batch)
2017-04-02 23:34:28.537067: step 7440, loss = 2.31 (327.5 examples/sec; 0.391 sec/batch)
2017-04-02 23:34:32.484465: step 7450, loss = 2.31 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 23:34:36.408556: step 7460, loss = 2.31 (326.2 examples/sec; 0.392 sec/batch)
2017-04-02 23:34:40.342763: step 7470, loss = 2.31 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 23:34:44.270885: step 7480, loss = 2.31 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 23:34:48.210639: step 7490, loss = 2.31 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 23:34:52.198274: step 7500, loss = 2.31 (321.0 examples/sec; 0.399 sec/batch)
2017-04-02 23:34:56.128655: step 7510, loss = 2.31 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 23:35:00.068038: step 7520, loss = 2.31 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 23:35:04.004765: step 7530, loss = 2.31 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 23:35:07.926725: step 7540, loss = 2.31 (326.4 examples/sec; 0.392 sec/batch)
2017-04-02 23:35:12.061356: step 7550, loss = 2.31 (309.6 examples/sec; 0.413 sec/batch)
2017-04-02 23:35:15.998966: step 7560, loss = 2.31 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 23:35:19.935239: step 7570, loss = 2.31 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 23:35:23.863097: step 7580, loss = 2.31 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 23:35:27.797871: step 7590, loss = 2.31 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 23:35:31.796598: step 7600, loss = 2.31 (320.1 examples/sec; 0.400 sec/batch)
2017-04-02 23:35:35.720516: step 7610, loss = 2.31 (326.2 examples/sec; 0.392 sec/batch)
2017-04-02 23:35:39.645423: step 7620, loss = 2.31 (326.1 examples/sec; 0.392 sec/batch)
2017-04-02 23:35:43.607121: step 7630, loss = 2.31 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 23:35:47.555384: step 7640, loss = 2.31 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 23:35:51.481123: step 7650, loss = 2.31 (326.1 examples/sec; 0.393 sec/batch)
2017-04-02 23:35:55.385671: step 7660, loss = 2.31 (327.8 examples/sec; 0.390 sec/batch)
2017-04-02 23:35:59.316850: step 7670, loss = 2.31 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 23:36:03.260369: step 7680, loss = 2.31 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 23:36:07.206980: step 7690, loss = 2.31 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 23:36:11.185245: step 7700, loss = 2.31 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 23:36:15.118348: step 7710, loss = 2.31 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 23:36:19.071731: step 7720, loss = 2.31 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 23:36:23.002024: step 7730, loss = 2.31 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 23:36:26.914624: step 7740, loss = 2.31 (327.1 examples/sec; 0.391 sec/batch)
2017-04-02 23:36:30.854488: step 7750, loss = 2.31 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 23:36:34.831198: step 7760, loss = 2.31 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 23:36:38.752215: step 7770, loss = 2.31 (326.4 examples/sec; 0.392 sec/batch)
2017-04-02 23:36:42.688149: step 7780, loss = 2.31 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 23:36:46.645601: step 7790, loss = 2.31 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 23:36:50.643748: step 7800, loss = 2.30 (320.1 examples/sec; 0.400 sec/batch)
2017-04-02 23:36:54.578364: step 7810, loss = 2.31 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 23:36:58.520257: step 7820, loss = 2.31 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 23:37:02.466877: step 7830, loss = 2.31 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 23:37:06.394301: step 7840, loss = 2.30 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 23:37:10.319123: step 7850, loss = 2.31 (326.1 examples/sec; 0.392 sec/batch)
2017-04-02 23:37:14.257132: step 7860, loss = 2.31 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 23:37:18.172723: step 7870, loss = 2.31 (326.9 examples/sec; 0.392 sec/batch)
2017-04-02 23:37:22.112005: step 7880, loss = 2.31 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 23:37:26.027330: step 7890, loss = 2.31 (326.9 examples/sec; 0.392 sec/batch)
2017-04-02 23:37:30.054388: step 7900, loss = 2.31 (317.9 examples/sec; 0.403 sec/batch)
2017-04-02 23:37:33.981437: step 7910, loss = 2.30 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 23:37:37.926629: step 7920, loss = 2.31 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 23:37:41.860080: step 7930, loss = 2.31 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 23:37:45.793733: step 7940, loss = 2.31 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 23:37:49.722756: step 7950, loss = 2.31 (325.8 examples/sec; 0.393 sec/batch)
2017-04-02 23:37:53.659408: step 7960, loss = 2.31 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 23:37:57.581850: step 7970, loss = 2.31 (326.3 examples/sec; 0.392 sec/batch)
2017-04-02 23:38:01.513202: step 7980, loss = 2.30 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 23:38:05.460497: step 7990, loss = 2.31 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 23:38:09.459482: step 8000, loss = 2.31 (320.1 examples/sec; 0.400 sec/batch)
2017-04-02 23:39:31.326072: step 8010, loss = 2.31 (15.6 examples/sec; 8.187 sec/batch)
2017-04-02 23:39:35.263419: step 8020, loss = 2.30 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 23:39:39.201205: step 8030, loss = 2.31 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 23:39:43.130368: step 8040, loss = 2.30 (325.8 examples/sec; 0.393 sec/batch)
2017-04-02 23:39:47.070076: step 8050, loss = 2.31 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 23:39:51.027804: step 8060, loss = 2.31 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 23:39:54.982244: step 8070, loss = 2.31 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 23:39:58.970588: step 8080, loss = 2.31 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 23:40:02.936557: step 8090, loss = 2.31 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 23:40:06.968014: step 8100, loss = 2.31 (317.5 examples/sec; 0.403 sec/batch)
2017-04-02 23:40:10.934670: step 8110, loss = 2.30 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 23:40:14.852432: step 8120, loss = 2.30 (326.7 examples/sec; 0.392 sec/batch)
2017-04-02 23:40:18.771476: step 8130, loss = 2.31 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 23:40:22.711087: step 8140, loss = 2.31 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 23:40:26.674455: step 8150, loss = 2.31 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 23:40:30.637547: step 8160, loss = 2.31 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 23:40:34.588774: step 8170, loss = 2.31 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 23:40:39.187438: step 8180, loss = 2.30 (278.3 examples/sec; 0.460 sec/batch)
2017-04-02 23:40:43.157773: step 8190, loss = 2.31 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 23:40:47.179603: step 8200, loss = 2.31 (318.3 examples/sec; 0.402 sec/batch)
2017-04-02 23:40:51.112860: step 8210, loss = 2.31 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 23:40:55.077680: step 8220, loss = 2.30 (322.8 examples/sec; 0.396 sec/batch)
2017-04-02 23:40:59.094537: step 8230, loss = 2.31 (318.7 examples/sec; 0.402 sec/batch)
2017-04-02 23:41:03.067835: step 8240, loss = 2.30 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 23:41:07.045087: step 8250, loss = 2.30 (321.8 examples/sec; 0.398 sec/batch)
2017-04-02 23:41:11.022176: step 8260, loss = 2.30 (321.8 examples/sec; 0.398 sec/batch)
2017-04-02 23:41:14.950643: step 8270, loss = 2.31 (325.8 examples/sec; 0.393 sec/batch)
2017-04-02 23:41:18.901254: step 8280, loss = 2.31 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 23:41:22.858196: step 8290, loss = 2.31 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 23:41:26.903627: step 8300, loss = 2.31 (316.4 examples/sec; 0.405 sec/batch)
2017-04-02 23:41:30.837939: step 8310, loss = 2.30 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 23:41:34.906065: step 8320, loss = 2.31 (314.6 examples/sec; 0.407 sec/batch)
2017-04-02 23:41:38.849838: step 8330, loss = 2.30 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 23:41:42.802029: step 8340, loss = 2.31 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 23:41:46.752440: step 8350, loss = 2.31 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 23:41:50.697801: step 8360, loss = 2.31 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 23:41:54.612800: step 8370, loss = 2.31 (326.9 examples/sec; 0.391 sec/batch)
2017-04-02 23:41:58.806590: step 8380, loss = 2.31 (305.2 examples/sec; 0.419 sec/batch)
2017-04-02 23:42:02.754389: step 8390, loss = 2.30 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 23:42:06.764966: step 8400, loss = 2.30 (319.2 examples/sec; 0.401 sec/batch)
2017-04-02 23:42:10.690346: step 8410, loss = 2.30 (326.1 examples/sec; 0.393 sec/batch)
2017-04-02 23:42:14.629129: step 8420, loss = 2.31 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 23:42:18.574137: step 8430, loss = 2.31 (324.5 examples/sec; 0.395 sec/batch)
2017-04-02 23:42:22.470026: step 8440, loss = 2.31 (328.6 examples/sec; 0.390 sec/batch)
2017-04-02 23:42:26.395962: step 8450, loss = 2.31 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 23:42:30.318778: step 8460, loss = 2.31 (326.3 examples/sec; 0.392 sec/batch)
2017-04-02 23:42:34.254002: step 8470, loss = 2.31 (325.3 examples/sec; 0.394 sec/batch)
2017-04-02 23:42:38.196411: step 8480, loss = 2.31 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 23:42:42.124685: step 8490, loss = 2.31 (325.8 examples/sec; 0.393 sec/batch)
2017-04-02 23:42:46.126734: step 8500, loss = 2.31 (319.8 examples/sec; 0.400 sec/batch)
2017-04-02 23:42:50.062645: step 8510, loss = 2.31 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 23:42:54.006035: step 8520, loss = 2.31 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 23:42:57.948186: step 8530, loss = 2.30 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 23:43:01.885706: step 8540, loss = 2.30 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 23:43:05.790847: step 8550, loss = 2.31 (327.8 examples/sec; 0.391 sec/batch)
2017-04-02 23:43:09.718561: step 8560, loss = 2.30 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 23:43:13.674106: step 8570, loss = 2.30 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 23:43:17.628160: step 8580, loss = 2.31 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 23:43:21.608535: step 8590, loss = 2.31 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 23:43:25.604808: step 8600, loss = 2.31 (320.3 examples/sec; 0.400 sec/batch)
2017-04-02 23:43:29.533093: step 8610, loss = 2.30 (325.8 examples/sec; 0.393 sec/batch)
2017-04-02 23:43:33.492564: step 8620, loss = 2.31 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 23:43:37.431822: step 8630, loss = 2.30 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 23:43:41.365396: step 8640, loss = 2.30 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 23:43:45.304217: step 8650, loss = 2.30 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 23:43:49.244465: step 8660, loss = 2.30 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 23:43:53.168913: step 8670, loss = 2.30 (326.2 examples/sec; 0.392 sec/batch)
2017-04-02 23:43:57.109845: step 8680, loss = 2.31 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 23:44:01.032121: step 8690, loss = 2.30 (326.3 examples/sec; 0.392 sec/batch)
2017-04-02 23:44:05.048033: step 8700, loss = 2.31 (318.7 examples/sec; 0.402 sec/batch)
2017-04-02 23:44:08.991882: step 8710, loss = 2.31 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 23:44:12.905053: step 8720, loss = 2.30 (327.1 examples/sec; 0.391 sec/batch)
2017-04-02 23:44:16.872800: step 8730, loss = 2.31 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 23:44:20.803086: step 8740, loss = 2.30 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 23:44:24.716579: step 8750, loss = 2.30 (327.1 examples/sec; 0.391 sec/batch)
2017-04-02 23:44:28.651076: step 8760, loss = 2.31 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 23:44:32.578847: step 8770, loss = 2.30 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 23:44:36.507494: step 8780, loss = 2.30 (325.8 examples/sec; 0.393 sec/batch)
2017-04-02 23:44:40.514552: step 8790, loss = 2.30 (319.4 examples/sec; 0.401 sec/batch)
2017-04-02 23:44:45.448481: step 8800, loss = 2.30 (259.4 examples/sec; 0.493 sec/batch)
2017-04-02 23:44:49.557795: step 8810, loss = 2.31 (311.5 examples/sec; 0.411 sec/batch)
2017-04-02 23:44:53.460073: step 8820, loss = 2.31 (328.0 examples/sec; 0.390 sec/batch)
2017-04-02 23:44:57.387277: step 8830, loss = 2.31 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 23:45:01.341051: step 8840, loss = 2.30 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 23:45:05.293100: step 8850, loss = 2.31 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 23:45:09.229391: step 8860, loss = 2.31 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 23:45:13.160922: step 8870, loss = 2.30 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 23:45:17.095737: step 8880, loss = 2.30 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 23:45:21.042589: step 8890, loss = 2.31 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 23:45:25.070451: step 8900, loss = 2.31 (317.8 examples/sec; 0.403 sec/batch)
2017-04-02 23:45:29.023799: step 8910, loss = 2.31 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 23:45:32.962765: step 8920, loss = 2.31 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 23:45:36.883859: step 8930, loss = 2.31 (326.4 examples/sec; 0.392 sec/batch)
2017-04-02 23:45:40.842371: step 8940, loss = 2.30 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 23:45:44.777014: step 8950, loss = 2.30 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 23:45:48.708526: step 8960, loss = 2.31 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 23:45:52.692086: step 8970, loss = 2.30 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 23:45:56.623892: step 8980, loss = 2.30 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 23:46:00.548208: step 8990, loss = 2.30 (326.2 examples/sec; 0.392 sec/batch)
2017-04-02 23:46:04.531658: step 9000, loss = 2.31 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 23:46:08.466280: step 9010, loss = 2.31 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 23:46:12.449449: step 9020, loss = 2.31 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 23:46:16.385309: step 9030, loss = 2.31 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 23:46:20.357867: step 9040, loss = 2.30 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 23:46:24.325412: step 9050, loss = 2.30 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 23:46:28.268630: step 9060, loss = 2.30 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 23:46:32.199726: step 9070, loss = 2.30 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 23:46:36.138982: step 9080, loss = 2.30 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 23:46:40.110503: step 9090, loss = 2.31 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 23:46:44.147521: step 9100, loss = 2.30 (317.1 examples/sec; 0.404 sec/batch)
2017-04-02 23:46:48.092750: step 9110, loss = 2.31 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 23:46:52.051728: step 9120, loss = 2.30 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 23:46:56.032038: step 9130, loss = 2.30 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 23:46:59.965812: step 9140, loss = 2.30 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 23:47:03.935719: step 9150, loss = 2.31 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 23:47:07.878266: step 9160, loss = 2.31 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 23:47:11.849496: step 9170, loss = 2.30 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 23:47:15.812826: step 9180, loss = 2.31 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 23:47:19.806865: step 9190, loss = 2.31 (320.5 examples/sec; 0.399 sec/batch)
2017-04-02 23:47:23.841572: step 9200, loss = 2.30 (317.2 examples/sec; 0.403 sec/batch)
2017-04-02 23:47:27.805965: step 9210, loss = 2.31 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 23:47:31.802736: step 9220, loss = 2.30 (320.3 examples/sec; 0.400 sec/batch)
2017-04-02 23:47:35.762131: step 9230, loss = 2.30 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 23:47:39.725705: step 9240, loss = 2.30 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 23:47:43.678483: step 9250, loss = 2.30 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 23:47:47.624609: step 9260, loss = 2.30 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 23:47:51.562220: step 9270, loss = 2.30 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 23:47:55.507416: step 9280, loss = 2.31 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 23:47:59.462298: step 9290, loss = 2.31 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 23:48:03.478228: step 9300, loss = 2.31 (318.7 examples/sec; 0.402 sec/batch)
2017-04-02 23:48:07.460213: step 9310, loss = 2.30 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 23:48:11.410149: step 9320, loss = 2.30 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 23:48:15.386106: step 9330, loss = 2.30 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 23:48:19.324159: step 9340, loss = 2.30 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 23:48:23.282018: step 9350, loss = 2.30 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 23:48:27.267149: step 9360, loss = 2.30 (321.2 examples/sec; 0.399 sec/batch)
2017-04-02 23:48:31.214337: step 9370, loss = 2.30 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 23:48:35.170829: step 9380, loss = 2.31 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 23:48:39.150987: step 9390, loss = 2.31 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 23:48:43.193897: step 9400, loss = 2.30 (316.6 examples/sec; 0.404 sec/batch)
2017-04-02 23:48:47.171472: step 9410, loss = 2.31 (321.8 examples/sec; 0.398 sec/batch)
2017-04-02 23:48:51.160527: step 9420, loss = 2.30 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 23:48:55.106425: step 9430, loss = 2.30 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 23:48:59.062178: step 9440, loss = 2.31 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 23:49:02.995822: step 9450, loss = 2.30 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 23:49:06.941951: step 9460, loss = 2.31 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 23:49:10.908890: step 9470, loss = 2.30 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 23:49:14.876014: step 9480, loss = 2.31 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 23:49:18.804436: step 9490, loss = 2.30 (325.8 examples/sec; 0.393 sec/batch)
2017-04-02 23:49:22.813439: step 9500, loss = 2.30 (319.3 examples/sec; 0.401 sec/batch)
2017-04-02 23:49:26.762508: step 9510, loss = 2.30 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 23:49:30.696341: step 9520, loss = 2.30 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 23:49:34.668054: step 9530, loss = 2.31 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 23:49:38.665636: step 9540, loss = 2.30 (320.2 examples/sec; 0.400 sec/batch)
2017-04-02 23:49:42.620704: step 9550, loss = 2.30 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 23:49:46.566611: step 9560, loss = 2.30 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 23:49:50.491276: step 9570, loss = 2.30 (326.1 examples/sec; 0.392 sec/batch)
2017-04-02 23:49:54.426197: step 9580, loss = 2.31 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 23:49:58.356078: step 9590, loss = 2.30 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 23:50:02.370019: step 9600, loss = 2.30 (318.9 examples/sec; 0.401 sec/batch)
2017-04-02 23:50:06.334126: step 9610, loss = 2.30 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 23:50:10.290606: step 9620, loss = 2.30 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 23:50:14.240631: step 9630, loss = 2.30 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 23:50:18.156609: step 9640, loss = 2.30 (326.9 examples/sec; 0.392 sec/batch)
2017-04-02 23:50:22.077450: step 9650, loss = 2.30 (326.5 examples/sec; 0.392 sec/batch)
2017-04-02 23:50:26.022651: step 9660, loss = 2.30 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 23:50:29.960719: step 9670, loss = 2.30 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 23:50:33.869587: step 9680, loss = 2.30 (327.5 examples/sec; 0.391 sec/batch)
2017-04-02 23:50:38.600614: step 9690, loss = 2.30 (270.6 examples/sec; 0.473 sec/batch)
2017-04-02 23:50:42.607982: step 9700, loss = 2.30 (319.4 examples/sec; 0.401 sec/batch)
2017-04-02 23:50:46.533293: step 9710, loss = 2.30 (326.1 examples/sec; 0.393 sec/batch)
2017-04-02 23:50:50.482131: step 9720, loss = 2.30 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 23:50:54.462084: step 9730, loss = 2.30 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 23:50:58.393361: step 9740, loss = 2.30 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 23:51:02.376588: step 9750, loss = 2.31 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 23:51:06.331421: step 9760, loss = 2.30 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 23:51:10.250342: step 9770, loss = 2.30 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 23:51:14.182717: step 9780, loss = 2.30 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 23:51:18.361781: step 9790, loss = 2.30 (306.3 examples/sec; 0.418 sec/batch)
2017-04-02 23:51:22.368333: step 9800, loss = 2.31 (319.5 examples/sec; 0.401 sec/batch)
2017-04-02 23:51:26.297966: step 9810, loss = 2.30 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 23:51:30.253571: step 9820, loss = 2.30 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 23:51:34.233103: step 9830, loss = 2.31 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 23:51:38.166071: step 9840, loss = 2.31 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 23:51:42.094143: step 9850, loss = 2.30 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 23:51:46.080847: step 9860, loss = 2.30 (321.1 examples/sec; 0.399 sec/batch)
2017-04-02 23:51:50.010622: step 9870, loss = 2.31 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 23:51:53.958754: step 9880, loss = 2.30 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 23:51:57.881695: step 9890, loss = 2.30 (326.3 examples/sec; 0.392 sec/batch)
2017-04-02 23:52:01.871635: step 9900, loss = 2.30 (320.8 examples/sec; 0.399 sec/batch)
2017-04-02 23:52:05.817433: step 9910, loss = 2.30 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 23:52:09.735550: step 9920, loss = 2.30 (326.7 examples/sec; 0.392 sec/batch)
2017-04-02 23:52:13.672441: step 9930, loss = 2.30 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 23:52:17.592831: step 9940, loss = 2.31 (326.5 examples/sec; 0.392 sec/batch)
2017-04-02 23:52:21.513438: step 9950, loss = 2.30 (326.5 examples/sec; 0.392 sec/batch)
2017-04-02 23:52:25.462277: step 9960, loss = 2.31 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 23:52:29.396808: step 9970, loss = 2.30 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 23:52:33.346108: step 9980, loss = 2.30 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 23:52:37.269464: step 9990, loss = 2.30 (326.3 examples/sec; 0.392 sec/batch)
2017-04-02 23:52:41.255711: step 10000, loss = 2.30 (321.1 examples/sec; 0.399 sec/batch)
2017-04-02 23:54:33.651137: step 10020, loss = 2.30 (11.8 examples/sec; 10.845 sec/batch)
2017-04-02 23:54:37.589463: step 10030, loss = 2.30 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 23:54:41.538659: step 10040, loss = 2.30 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 23:54:45.478323: step 10050, loss = 2.31 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 23:54:49.433333: step 10060, loss = 2.30 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 23:54:53.381697: step 10070, loss = 2.31 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 23:54:57.354943: step 10080, loss = 2.30 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 23:55:01.297820: step 10090, loss = 2.30 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 23:55:05.307530: step 10100, loss = 2.31 (319.2 examples/sec; 0.401 sec/batch)
2017-04-02 23:55:09.275049: step 10110, loss = 2.31 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 23:55:13.220406: step 10120, loss = 2.30 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 23:55:17.178277: step 10130, loss = 2.30 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 23:55:21.169914: step 10140, loss = 2.30 (320.7 examples/sec; 0.399 sec/batch)
2017-04-02 23:55:25.092637: step 10150, loss = 2.30 (326.3 examples/sec; 0.392 sec/batch)
2017-04-02 23:55:29.062692: step 10160, loss = 2.31 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 23:55:33.018576: step 10170, loss = 2.30 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 23:55:36.969733: step 10180, loss = 2.30 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 23:55:40.942419: step 10190, loss = 2.30 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 23:55:44.974148: step 10200, loss = 2.31 (317.5 examples/sec; 0.403 sec/batch)
2017-04-02 23:55:48.956008: step 10210, loss = 2.30 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 23:55:52.912879: step 10220, loss = 2.31 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 23:55:56.859269: step 10230, loss = 2.30 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 23:56:00.805311: step 10240, loss = 2.30 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 23:56:04.742238: step 10250, loss = 2.30 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 23:56:08.712178: step 10260, loss = 2.30 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 23:56:12.657429: step 10270, loss = 2.30 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 23:56:16.617865: step 10280, loss = 2.30 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 23:56:20.556575: step 10290, loss = 2.30 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 23:56:24.581635: step 10300, loss = 2.30 (318.0 examples/sec; 0.403 sec/batch)
2017-04-02 23:56:28.538607: step 10310, loss = 2.30 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 23:56:32.503751: step 10320, loss = 2.30 (322.8 examples/sec; 0.397 sec/batch)
2017-04-02 23:56:36.499309: step 10330, loss = 2.30 (320.4 examples/sec; 0.400 sec/batch)
2017-04-02 23:56:40.478206: step 10340, loss = 2.30 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 23:56:44.452935: step 10350, loss = 2.30 (322.0 examples/sec; 0.397 sec/batch)
2017-04-02 23:56:48.433772: step 10360, loss = 2.30 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 23:56:52.392349: step 10370, loss = 2.30 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 23:56:56.362400: step 10380, loss = 2.30 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 23:57:00.329755: step 10390, loss = 2.30 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 23:57:04.369826: step 10400, loss = 2.30 (316.8 examples/sec; 0.404 sec/batch)
2017-04-02 23:57:08.312362: step 10410, loss = 2.30 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 23:57:12.276177: step 10420, loss = 2.31 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 23:57:16.241890: step 10430, loss = 2.30 (322.8 examples/sec; 0.397 sec/batch)
2017-04-02 23:57:20.180614: step 10440, loss = 2.30 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 23:57:24.427047: step 10450, loss = 2.30 (301.4 examples/sec; 0.425 sec/batch)
2017-04-02 23:57:28.597851: step 10460, loss = 2.30 (306.9 examples/sec; 0.417 sec/batch)
2017-04-02 23:57:32.705698: step 10470, loss = 2.30 (311.6 examples/sec; 0.411 sec/batch)
2017-04-02 23:57:36.884588: step 10480, loss = 2.30 (306.3 examples/sec; 0.418 sec/batch)
2017-04-02 23:57:40.988345: step 10490, loss = 2.30 (311.9 examples/sec; 0.410 sec/batch)
2017-04-02 23:57:45.032431: step 10500, loss = 2.30 (316.5 examples/sec; 0.404 sec/batch)
2017-04-02 23:57:48.982846: step 10510, loss = 2.31 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 23:57:52.937805: step 10520, loss = 2.30 (323.6 examples/sec; 0.395 sec/batch)
2017-04-02 23:57:56.867663: step 10530, loss = 2.31 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 23:58:00.815592: step 10540, loss = 2.31 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 23:58:04.778860: step 10550, loss = 2.31 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 23:58:08.707680: step 10560, loss = 2.31 (325.8 examples/sec; 0.393 sec/batch)
2017-04-02 23:58:12.677323: step 10570, loss = 2.30 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 23:58:16.645450: step 10580, loss = 2.30 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 23:58:20.632368: step 10590, loss = 2.31 (321.0 examples/sec; 0.399 sec/batch)
2017-04-02 23:58:24.679694: step 10600, loss = 2.30 (316.3 examples/sec; 0.405 sec/batch)
2017-04-02 23:58:28.634061: step 10610, loss = 2.30 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 23:58:32.623117: step 10620, loss = 2.30 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 23:58:36.576151: step 10630, loss = 2.30 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 23:58:40.927691: step 10640, loss = 2.30 (294.1 examples/sec; 0.435 sec/batch)
2017-04-02 23:58:45.633198: step 10650, loss = 2.30 (272.0 examples/sec; 0.471 sec/batch)
2017-04-02 23:58:49.598119: step 10660, loss = 2.30 (322.8 examples/sec; 0.396 sec/batch)
2017-04-02 23:58:53.541534: step 10670, loss = 2.31 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 23:58:57.466245: step 10680, loss = 2.30 (326.1 examples/sec; 0.392 sec/batch)
2017-04-02 23:59:01.402888: step 10690, loss = 2.30 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 23:59:05.415598: step 10700, loss = 2.31 (319.0 examples/sec; 0.401 sec/batch)
2017-04-02 23:59:09.369117: step 10710, loss = 2.30 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 23:59:13.306142: step 10720, loss = 2.30 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 23:59:17.211972: step 10730, loss = 2.30 (327.7 examples/sec; 0.391 sec/batch)
2017-04-02 23:59:21.114068: step 10740, loss = 2.31 (328.0 examples/sec; 0.390 sec/batch)
2017-04-02 23:59:25.082210: step 10750, loss = 2.30 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 23:59:29.041744: step 10760, loss = 2.30 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 23:59:32.984607: step 10770, loss = 2.30 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 23:59:36.914162: step 10780, loss = 2.31 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 23:59:40.858968: step 10790, loss = 2.30 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 23:59:44.835734: step 10800, loss = 2.30 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 23:59:48.802816: step 10810, loss = 2.30 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 23:59:52.721705: step 10820, loss = 2.30 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 23:59:56.630821: step 10830, loss = 2.31 (327.4 examples/sec; 0.391 sec/batch)
2017-04-03 00:00:00.555950: step 10840, loss = 2.30 (326.1 examples/sec; 0.393 sec/batch)
2017-04-03 00:00:04.513034: step 10850, loss = 2.31 (323.5 examples/sec; 0.396 sec/batch)
2017-04-03 00:00:08.466818: step 10860, loss = 2.30 (323.7 examples/sec; 0.395 sec/batch)
2017-04-03 00:00:12.421782: step 10870, loss = 2.31 (323.6 examples/sec; 0.395 sec/batch)
2017-04-03 00:00:16.357418: step 10880, loss = 2.30 (325.2 examples/sec; 0.394 sec/batch)
2017-04-03 00:00:20.301680: step 10890, loss = 2.30 (324.5 examples/sec; 0.394 sec/batch)
2017-04-03 00:00:24.323947: step 10900, loss = 2.30 (318.2 examples/sec; 0.402 sec/batch)
2017-04-03 00:00:28.256312: step 10910, loss = 2.31 (325.5 examples/sec; 0.393 sec/batch)
2017-04-03 00:00:32.182936: step 10920, loss = 2.30 (326.0 examples/sec; 0.393 sec/batch)
2017-04-03 00:00:36.144877: step 10930, loss = 2.30 (323.1 examples/sec; 0.396 sec/batch)
2017-04-03 00:00:41.387812: step 10940, loss = 2.30 (244.1 examples/sec; 0.524 sec/batch)
2017-04-03 00:00:45.341452: step 10950, loss = 2.30 (323.8 examples/sec; 0.395 sec/batch)
2017-04-03 00:00:49.252493: step 10960, loss = 2.30 (327.3 examples/sec; 0.391 sec/batch)
2017-04-03 00:00:53.209021: step 10970, loss = 2.30 (323.5 examples/sec; 0.396 sec/batch)
2017-04-03 00:00:57.135941: step 10980, loss = 2.30 (326.0 examples/sec; 0.393 sec/batch)
2017-04-03 00:01:01.069710: step 10990, loss = 2.30 (325.4 examples/sec; 0.393 sec/batch)
2017-04-03 00:01:05.063643: step 11000, loss = 2.30 (320.5 examples/sec; 0.399 sec/batch)
2017-04-03 00:01:09.021204: step 11010, loss = 2.30 (323.4 examples/sec; 0.396 sec/batch)
2017-04-03 00:01:12.936796: step 11020, loss = 2.30 (326.9 examples/sec; 0.392 sec/batch)
2017-04-03 00:01:16.853408: step 11030, loss = 2.30 (326.8 examples/sec; 0.392 sec/batch)
2017-04-03 00:01:20.769818: step 11040, loss = 2.30 (326.8 examples/sec; 0.392 sec/batch)
2017-04-03 00:01:24.788643: step 11050, loss = 2.30 (318.5 examples/sec; 0.402 sec/batch)
2017-04-03 00:01:28.757078: step 11060, loss = 2.30 (322.5 examples/sec; 0.397 sec/batch)
2017-04-03 00:01:32.738247: step 11070, loss = 2.30 (321.5 examples/sec; 0.398 sec/batch)
2017-04-03 00:01:36.718663: step 11080, loss = 2.30 (321.6 examples/sec; 0.398 sec/batch)
2017-04-03 00:01:41.689851: step 11090, loss = 2.30 (257.5 examples/sec; 0.497 sec/batch)
2017-04-03 00:01:45.840057: step 11100, loss = 2.30 (308.4 examples/sec; 0.415 sec/batch)
2017-04-03 00:01:49.802322: step 11110, loss = 2.30 (323.0 examples/sec; 0.396 sec/batch)
2017-04-03 00:01:53.801963: step 11120, loss = 2.30 (320.0 examples/sec; 0.400 sec/batch)
2017-04-03 00:01:57.753227: step 11130, loss = 2.30 (323.9 examples/sec; 0.395 sec/batch)
2017-04-03 00:02:01.678860: step 11140, loss = 2.30 (326.1 examples/sec; 0.393 sec/batch)
2017-04-03 00:02:05.635531: step 11150, loss = 2.31 (323.5 examples/sec; 0.396 sec/batch)
2017-04-03 00:02:09.597754: step 11160, loss = 2.30 (323.1 examples/sec; 0.396 sec/batch)
2017-04-03 00:02:13.534024: step 11170, loss = 2.30 (325.2 examples/sec; 0.394 sec/batch)
2017-04-03 00:02:17.497989: step 11180, loss = 2.30 (322.9 examples/sec; 0.396 sec/batch)
2017-04-03 00:02:21.412454: step 11190, loss = 2.30 (327.0 examples/sec; 0.391 sec/batch)
2017-04-03 00:02:25.423460: step 11200, loss = 2.30 (319.1 examples/sec; 0.401 sec/batch)
2017-04-03 00:02:29.341853: step 11210, loss = 2.30 (326.7 examples/sec; 0.392 sec/batch)
2017-04-03 00:02:33.297025: step 11220, loss = 2.30 (323.6 examples/sec; 0.396 sec/batch)
2017-04-03 00:02:37.243536: step 11230, loss = 2.30 (324.3 examples/sec; 0.395 sec/batch)
2017-04-03 00:02:41.193627: step 11240, loss = 2.30 (324.0 examples/sec; 0.395 sec/batch)
2017-04-03 00:02:45.154943: step 11250, loss = 2.30 (323.1 examples/sec; 0.396 sec/batch)
2017-04-03 00:02:49.097628: step 11260, loss = 2.30 (324.7 examples/sec; 0.394 sec/batch)
2017-04-03 00:02:53.045027: step 11270, loss = 2.30 (324.3 examples/sec; 0.395 sec/batch)
2017-04-03 00:02:57.007938: step 11280, loss = 2.30 (323.0 examples/sec; 0.396 sec/batch)
2017-04-03 00:03:00.927250: step 11290, loss = 2.30 (326.6 examples/sec; 0.392 sec/batch)
2017-04-03 00:03:04.951005: step 11300, loss = 2.31 (318.1 examples/sec; 0.402 sec/batch)
2017-04-03 00:03:08.884539: step 11310, loss = 2.30 (325.4 examples/sec; 0.393 sec/batch)
2017-04-03 00:03:12.841519: step 11320, loss = 2.31 (323.5 examples/sec; 0.396 sec/batch)
2017-04-03 00:03:16.824887: step 11330, loss = 2.30 (321.3 examples/sec; 0.398 sec/batch)
2017-04-03 00:03:20.816824: step 11340, loss = 2.31 (320.6 examples/sec; 0.399 sec/batch)
2017-04-03 00:03:25.669017: step 11350, loss = 2.30 (263.8 examples/sec; 0.485 sec/batch)
2017-04-03 00:03:29.877268: step 11360, loss = 2.30 (304.2 examples/sec; 0.421 sec/batch)
2017-04-03 00:03:33.824193: step 11370, loss = 2.31 (324.3 examples/sec; 0.395 sec/batch)
2017-04-03 00:03:37.768342: step 11380, loss = 2.30 (324.5 examples/sec; 0.394 sec/batch)
2017-04-03 00:03:41.718348: step 11390, loss = 2.30 (324.1 examples/sec; 0.395 sec/batch)
2017-04-03 00:03:45.719365: step 11400, loss = 2.30 (319.9 examples/sec; 0.400 sec/batch)
2017-04-03 00:03:49.669432: step 11410, loss = 2.31 (324.0 examples/sec; 0.395 sec/batch)
2017-04-03 00:03:53.613552: step 11420, loss = 2.31 (324.5 examples/sec; 0.394 sec/batch)
2017-04-03 00:03:57.561150: step 11430, loss = 2.30 (324.2 examples/sec; 0.395 sec/batch)
2017-04-03 00:04:01.505878: step 11440, loss = 2.30 (324.5 examples/sec; 0.394 sec/batch)
2017-04-03 00:04:05.448049: step 11450, loss = 2.30 (324.7 examples/sec; 0.394 sec/batch)
2017-04-03 00:04:09.405551: step 11460, loss = 2.30 (323.4 examples/sec; 0.396 sec/batch)
2017-04-03 00:04:13.368987: step 11470, loss = 2.30 (323.0 examples/sec; 0.396 sec/batch)
2017-04-03 00:04:17.334992: step 11480, loss = 2.30 (322.7 examples/sec; 0.397 sec/batch)
2017-04-03 00:04:21.272388: step 11490, loss = 2.30 (325.1 examples/sec; 0.394 sec/batch)
2017-04-03 00:04:25.300052: step 11500, loss = 2.30 (317.8 examples/sec; 0.403 sec/batch)
2017-04-03 00:04:29.239991: step 11510, loss = 2.30 (324.9 examples/sec; 0.394 sec/batch)
2017-04-03 00:04:33.193017: step 11520, loss = 2.30 (323.8 examples/sec; 0.395 sec/batch)
2017-04-03 00:04:37.162043: step 11530, loss = 2.30 (322.5 examples/sec; 0.397 sec/batch)
2017-04-03 00:04:41.182043: step 11540, loss = 2.30 (318.4 examples/sec; 0.402 sec/batch)
2017-04-03 00:04:45.667606: step 11550, loss = 2.30 (285.4 examples/sec; 0.449 sec/batch)
2017-04-03 00:04:50.172005: step 11560, loss = 2.30 (284.2 examples/sec; 0.450 sec/batch)
2017-04-03 00:04:54.117237: step 11570, loss = 2.31 (324.4 examples/sec; 0.395 sec/batch)
2017-04-03 00:04:58.039777: step 11580, loss = 2.30 (326.3 examples/sec; 0.392 sec/batch)
2017-04-03 00:05:01.992826: step 11590, loss = 2.30 (323.8 examples/sec; 0.395 sec/batch)
2017-04-03 00:05:06.013241: step 11600, loss = 2.30 (318.4 examples/sec; 0.402 sec/batch)
2017-04-03 00:05:09.951623: step 11610, loss = 2.30 (325.0 examples/sec; 0.394 sec/batch)
2017-04-03 00:05:13.904398: step 11620, loss = 2.30 (323.8 examples/sec; 0.395 sec/batch)
2017-04-03 00:05:17.852989: step 11630, loss = 2.30 (324.2 examples/sec; 0.395 sec/batch)
2017-04-03 00:05:21.811108: step 11640, loss = 2.30 (323.4 examples/sec; 0.396 sec/batch)
2017-04-03 00:05:25.755908: step 11650, loss = 2.30 (324.5 examples/sec; 0.394 sec/batch)
2017-04-03 00:05:29.732405: step 11660, loss = 2.30 (321.9 examples/sec; 0.398 sec/batch)
2017-04-03 00:05:33.668087: step 11670, loss = 2.30 (325.2 examples/sec; 0.394 sec/batch)
2017-04-03 00:05:37.646558: step 11680, loss = 2.30 (321.7 examples/sec; 0.398 sec/batch)
2017-04-03 00:05:41.622847: step 11690, loss = 2.30 (321.9 examples/sec; 0.398 sec/batch)
2017-04-03 00:05:45.634689: step 11700, loss = 2.30 (319.1 examples/sec; 0.401 sec/batch)
2017-04-03 00:05:49.568396: step 11710, loss = 2.30 (325.4 examples/sec; 0.393 sec/batch)
2017-04-03 00:05:53.479505: step 11720, loss = 2.30 (327.3 examples/sec; 0.391 sec/batch)
2017-04-03 00:05:57.419095: step 11730, loss = 2.30 (324.9 examples/sec; 0.394 sec/batch)
2017-04-03 00:06:01.405676: step 11740, loss = 2.30 (321.1 examples/sec; 0.399 sec/batch)
2017-04-03 00:06:05.363416: step 11750, loss = 2.30 (323.4 examples/sec; 0.396 sec/batch)
2017-04-03 00:06:09.292166: step 11760, loss = 2.30 (325.8 examples/sec; 0.393 sec/batch)
2017-04-03 00:06:13.247666: step 11770, loss = 2.30 (323.6 examples/sec; 0.396 sec/batch)
2017-04-03 00:06:17.217848: step 11780, loss = 2.30 (322.4 examples/sec; 0.397 sec/batch)
2017-04-03 00:06:21.179451: step 11790, loss = 2.30 (323.1 examples/sec; 0.396 sec/batch)
2017-04-03 00:06:25.180503: step 11800, loss = 2.30 (319.9 examples/sec; 0.400 sec/batch)
2017-04-03 00:06:29.147678: step 11810, loss = 2.30 (322.6 examples/sec; 0.397 sec/batch)
2017-04-03 00:06:33.102168: step 11820, loss = 2.30 (323.7 examples/sec; 0.395 sec/batch)
2017-04-03 00:06:37.077452: step 11830, loss = 2.30 (322.0 examples/sec; 0.398 sec/batch)
2017-04-03 00:06:41.023067: step 11840, loss = 2.30 (324.4 examples/sec; 0.395 sec/batch)
2017-04-03 00:06:44.978574: step 11850, loss = 2.30 (323.6 examples/sec; 0.396 sec/batch)
2017-04-03 00:06:48.932080: step 11860, loss = 2.30 (323.8 examples/sec; 0.395 sec/batch)
2017-04-03 00:06:52.912029: step 11870, loss = 2.30 (321.6 examples/sec; 0.398 sec/batch)
2017-04-03 00:06:56.862068: step 11880, loss = 2.30 (324.0 examples/sec; 0.395 sec/batch)
2017-04-03 00:07:00.800471: step 11890, loss = 2.31 (325.0 examples/sec; 0.394 sec/batch)
2017-04-03 00:07:04.830263: step 11900, loss = 2.31 (317.6 examples/sec; 0.403 sec/batch)
2017-04-03 00:07:08.790842: step 11910, loss = 2.30 (323.2 examples/sec; 0.396 sec/batch)
2017-04-03 00:07:12.738109: step 11920, loss = 2.30 (324.3 examples/sec; 0.395 sec/batch)
2017-04-03 00:07:16.699329: step 11930, loss = 2.30 (323.1 examples/sec; 0.396 sec/batch)
2017-04-03 00:07:20.665056: step 11940, loss = 2.30 (322.8 examples/sec; 0.397 sec/batch)
2017-04-03 00:07:24.618486: step 11950, loss = 2.30 (323.8 examples/sec; 0.395 sec/batch)
2017-04-03 00:07:28.617226: step 11960, loss = 2.30 (320.1 examples/sec; 0.400 sec/batch)
2017-04-03 00:07:32.560831: step 11970, loss = 2.30 (324.6 examples/sec; 0.394 sec/batch)
2017-04-03 00:07:36.525265: step 11980, loss = 2.31 (322.9 examples/sec; 0.396 sec/batch)
2017-04-03 00:07:40.491358: step 11990, loss = 2.30 (322.7 examples/sec; 0.397 sec/batch)
2017-04-03 00:07:44.553751: step 12000, loss = 2.30 (315.1 examples/sec; 0.406 sec/batch)
2017-04-03 00:07:48.501477: step 12010, loss = 2.30 (324.2 examples/sec; 0.395 sec/batch)
2017-04-03 00:07:52.464769: step 12020, loss = 2.31 (323.0 examples/sec; 0.396 sec/batch)
2017-04-03 00:07:56.402861: step 12030, loss = 2.31 (325.0 examples/sec; 0.394 sec/batch)
2017-04-03 00:08:00.364834: step 12040, loss = 2.30 (323.1 examples/sec; 0.396 sec/batch)
2017-04-03 00:08:04.340796: step 12050, loss = 2.30 (321.9 examples/sec; 0.398 sec/batch)
2017-04-03 00:08:08.284681: step 12060, loss = 2.30 (324.6 examples/sec; 0.394 sec/batch)
2017-04-03 00:08:12.230843: step 12070, loss = 2.30 (324.4 examples/sec; 0.395 sec/batch)
2017-04-03 00:08:16.175539: step 12080, loss = 2.30 (324.5 examples/sec; 0.394 sec/batch)
2017-04-03 00:08:20.122292: step 12090, loss = 2.30 (324.3 examples/sec; 0.395 sec/batch)
2017-04-03 00:08:24.139914: step 12100, loss = 2.30 (318.6 examples/sec; 0.402 sec/batch)
2017-04-03 00:08:28.116867: step 12110, loss = 2.30 (321.9 examples/sec; 0.398 sec/batch)
2017-04-03 00:08:32.068094: step 12120, loss = 2.30 (324.0 examples/sec; 0.395 sec/batch)
2017-04-03 00:08:36.018183: step 12130, loss = 2.30 (324.0 examples/sec; 0.395 sec/batch)
2017-04-03 00:08:39.972016: step 12140, loss = 2.30 (323.7 examples/sec; 0.395 sec/batch)
2017-04-03 00:08:43.918041: step 12150, loss = 2.30 (324.4 examples/sec; 0.395 sec/batch)
2017-04-03 00:08:47.877406: step 12160, loss = 2.30 (323.3 examples/sec; 0.396 sec/batch)
2017-04-03 00:08:51.830781: step 12170, loss = 2.30 (323.8 examples/sec; 0.395 sec/batch)
2017-04-03 00:08:55.776306: step 12180, loss = 2.30 (324.4 examples/sec; 0.395 sec/batch)
2017-04-03 00:10:31.549115: step 12190, loss = 2.31 (13.4 examples/sec; 9.577 sec/batch)
2017-04-03 00:10:35.535445: step 12200, loss = 2.30 (321.1 examples/sec; 0.399 sec/batch)
2017-04-03 00:10:40.318097: step 12210, loss = 2.30 (267.6 examples/sec; 0.478 sec/batch)
2017-04-03 00:10:44.288066: step 12220, loss = 2.31 (322.4 examples/sec; 0.397 sec/batch)
2017-04-03 00:10:48.234340: step 12230, loss = 2.30 (324.4 examples/sec; 0.395 sec/batch)
2017-04-03 00:10:52.203036: step 12240, loss = 2.30 (322.5 examples/sec; 0.397 sec/batch)
2017-04-03 00:10:56.148857: step 12250, loss = 2.30 (324.4 examples/sec; 0.395 sec/batch)
2017-04-03 00:11:00.084781: step 12260, loss = 2.31 (325.2 examples/sec; 0.394 sec/batch)
2017-04-03 00:11:04.019698: step 12270, loss = 2.30 (325.3 examples/sec; 0.393 sec/batch)
2017-04-03 00:11:07.985182: step 12280, loss = 2.30 (322.8 examples/sec; 0.397 sec/batch)
2017-04-03 00:11:11.969682: step 12290, loss = 2.30 (321.2 examples/sec; 0.398 sec/batch)
2017-04-03 00:11:15.958970: step 12300, loss = 2.30 (320.9 examples/sec; 0.399 sec/batch)
2017-04-03 00:11:19.919586: step 12310, loss = 2.30 (323.2 examples/sec; 0.396 sec/batch)
2017-04-03 00:11:23.813057: step 12320, loss = 2.30 (328.8 examples/sec; 0.389 sec/batch)
2017-04-03 00:11:27.762551: step 12330, loss = 2.30 (324.1 examples/sec; 0.395 sec/batch)
2017-04-03 00:11:31.708393: step 12340, loss = 2.30 (324.4 examples/sec; 0.395 sec/batch)
2017-04-03 00:11:35.644282: step 12350, loss = 2.30 (325.2 examples/sec; 0.394 sec/batch)
2017-04-03 00:11:39.585195: step 12360, loss = 2.30 (324.8 examples/sec; 0.394 sec/batch)
2017-04-03 00:11:43.541191: step 12370, loss = 2.30 (323.6 examples/sec; 0.396 sec/batch)
2017-04-03 00:11:47.490856: step 12380, loss = 2.31 (324.1 examples/sec; 0.395 sec/batch)
2017-04-03 00:11:51.417102: step 12390, loss = 2.30 (326.0 examples/sec; 0.393 sec/batch)
2017-04-03 00:11:55.418252: step 12400, loss = 2.30 (319.9 examples/sec; 0.400 sec/batch)
2017-04-03 00:11:59.333147: step 12410, loss = 2.30 (327.0 examples/sec; 0.391 sec/batch)
2017-04-03 00:12:03.253224: step 12420, loss = 2.30 (326.5 examples/sec; 0.392 sec/batch)
2017-04-03 00:12:07.198725: step 12430, loss = 2.30 (324.4 examples/sec; 0.395 sec/batch)
2017-04-03 00:12:11.149157: step 12440, loss = 2.30 (324.0 examples/sec; 0.395 sec/batch)
2017-04-03 00:12:15.114887: step 12450, loss = 2.30 (322.8 examples/sec; 0.397 sec/batch)
2017-04-03 00:12:19.055214: step 12460, loss = 2.30 (324.8 examples/sec; 0.394 sec/batch)
2017-04-03 00:12:23.016509: step 12470, loss = 2.30 (323.1 examples/sec; 0.396 sec/batch)
2017-04-03 00:12:26.966385: step 12480, loss = 2.31 (324.1 examples/sec; 0.395 sec/batch)
2017-04-03 00:12:30.930013: step 12490, loss = 2.31 (322.9 examples/sec; 0.396 sec/batch)
2017-04-03 00:12:34.930616: step 12500, loss = 2.31 (320.0 examples/sec; 0.400 sec/batch)
2017-04-03 00:12:38.905766: step 12510, loss = 2.30 (322.0 examples/sec; 0.398 sec/batch)
2017-04-03 00:12:42.832719: step 12520, loss = 2.30 (326.0 examples/sec; 0.393 sec/batch)
2017-04-03 00:12:46.782599: step 12530, loss = 2.30 (324.1 examples/sec; 0.395 sec/batch)
2017-04-03 00:12:50.745644: step 12540, loss = 2.30 (323.0 examples/sec; 0.396 sec/batch)
2017-04-03 00:12:54.721335: step 12550, loss = 2.30 (322.0 examples/sec; 0.398 sec/batch)
2017-04-03 00:12:58.718555: step 12560, loss = 2.30 (320.2 examples/sec; 0.400 sec/batch)
2017-04-03 00:13:02.651644: step 12570, loss = 2.31 (325.4 examples/sec; 0.393 sec/batch)
2017-04-03 00:13:06.624649: step 12580, loss = 2.30 (322.2 examples/sec; 0.397 sec/batch)
2017-04-03 00:13:10.574395: step 12590, loss = 2.30 (324.1 examples/sec; 0.395 sec/batch)
2017-04-03 00:13:14.626651: step 12600, loss = 2.31 (315.9 examples/sec; 0.405 sec/batch)
2017-04-03 00:13:18.578146: step 12610, loss = 2.30 (323.9 examples/sec; 0.395 sec/batch)
2017-04-03 00:13:22.537642: step 12620, loss = 2.30 (323.3 examples/sec; 0.396 sec/batch)
2017-04-03 00:13:26.458270: step 12630, loss = 2.30 (326.5 examples/sec; 0.392 sec/batch)
2017-04-03 00:13:30.428565: step 12640, loss = 2.30 (322.4 examples/sec; 0.397 sec/batch)
2017-04-03 00:13:34.385078: step 12650, loss = 2.30 (323.5 examples/sec; 0.396 sec/batch)
2017-04-03 00:13:38.369157: step 12660, loss = 2.30 (321.3 examples/sec; 0.398 sec/batch)
2017-04-03 00:13:42.331703: step 12670, loss = 2.30 (323.0 examples/sec; 0.396 sec/batch)
2017-04-03 00:13:46.289340: step 12680, loss = 2.30 (323.4 examples/sec; 0.396 sec/batch)
2017-04-03 00:13:50.213906: step 12690, loss = 2.30 (326.2 examples/sec; 0.392 sec/batch)
2017-04-03 00:13:54.217541: step 12700, loss = 2.30 (319.7 examples/sec; 0.400 sec/batch)
2017-04-03 00:13:58.174617: step 12710, loss = 2.30 (323.5 examples/sec; 0.396 sec/batch)
2017-04-03 00:14:02.121216: step 12720, loss = 2.30 (324.3 examples/sec; 0.395 sec/batch)
2017-04-03 00:14:06.068283: step 12730, loss = 2.30 (324.3 examples/sec; 0.395 sec/batch)
2017-04-03 00:14:09.994398: step 12740, loss = 2.30 (326.0 examples/sec; 0.393 sec/batch)
2017-04-03 00:14:13.959114: step 12750, loss = 2.30 (322.8 examples/sec; 0.396 sec/batch)
2017-04-03 00:14:17.895558: step 12760, loss = 2.30 (325.2 examples/sec; 0.394 sec/batch)
2017-04-03 00:14:21.824346: step 12770, loss = 2.30 (325.8 examples/sec; 0.393 sec/batch)
2017-04-03 00:14:25.763310: step 12780, loss = 2.30 (325.0 examples/sec; 0.394 sec/batch)
2017-04-03 00:14:29.720396: step 12790, loss = 2.30 (323.5 examples/sec; 0.396 sec/batch)
2017-04-03 00:14:33.734636: step 12800, loss = 2.30 (318.9 examples/sec; 0.401 sec/batch)
2017-04-03 00:14:37.685049: step 12810, loss = 2.30 (324.0 examples/sec; 0.395 sec/batch)
2017-04-03 00:14:41.633022: step 12820, loss = 2.31 (324.2 examples/sec; 0.395 sec/batch)
2017-04-03 00:14:45.577340: step 12830, loss = 2.30 (324.5 examples/sec; 0.394 sec/batch)
2017-04-03 00:14:49.527782: step 12840, loss = 2.30 (324.0 examples/sec; 0.395 sec/batch)
2017-04-03 00:14:53.474833: step 12850, loss = 2.31 (324.3 examples/sec; 0.395 sec/batch)
2017-04-03 00:14:57.447704: step 12860, loss = 2.30 (322.2 examples/sec; 0.397 sec/batch)
2017-04-03 00:15:01.400770: step 12870, loss = 2.30 (323.8 examples/sec; 0.395 sec/batch)
2017-04-03 00:15:05.364187: step 12880, loss = 2.30 (323.0 examples/sec; 0.396 sec/batch)
2017-04-03 00:15:09.296027: step 12890, loss = 2.30 (325.5 examples/sec; 0.393 sec/batch)
2017-04-03 00:15:13.317509: step 12900, loss = 2.31 (318.3 examples/sec; 0.402 sec/batch)
2017-04-03 00:15:17.298098: step 12910, loss = 2.30 (321.6 examples/sec; 0.398 sec/batch)
2017-04-03 00:15:21.277547: step 12920, loss = 2.30 (321.7 examples/sec; 0.398 sec/batch)
2017-04-03 00:15:25.219870: step 12930, loss = 2.30 (324.7 examples/sec; 0.394 sec/batch)
2017-04-03 00:15:29.159977: step 12940, loss = 2.30 (324.9 examples/sec; 0.394 sec/batch)
2017-04-03 00:15:33.121539: step 12950, loss = 2.30 (323.1 examples/sec; 0.396 sec/batch)
2017-04-03 00:15:37.091464: step 12960, loss = 2.31 (322.4 examples/sec; 0.397 sec/batch)
2017-04-03 00:15:41.077192: step 12970, loss = 2.31 (321.1 examples/sec; 0.399 sec/batch)
2017-04-03 00:15:45.079047: step 12980, loss = 2.30 (319.9 examples/sec; 0.400 sec/batch)
2017-04-03 00:15:49.022351: step 12990, loss = 2.30 (324.6 examples/sec; 0.394 sec/batch)
2017-04-03 00:15:53.008876: step 13000, loss = 2.30 (321.1 examples/sec; 0.399 sec/batch)
2017-04-03 00:15:56.957317: step 13010, loss = 2.31 (324.2 examples/sec; 0.395 sec/batch)
2017-04-03 00:16:00.883519: step 13020, loss = 2.30 (326.0 examples/sec; 0.393 sec/batch)
2017-04-03 00:16:04.851975: step 13030, loss = 2.30 (322.5 examples/sec; 0.397 sec/batch)
2017-04-03 00:16:08.837856: step 13040, loss = 2.30 (321.1 examples/sec; 0.399 sec/batch)
2017-04-03 00:16:12.811370: step 13050, loss = 2.30 (322.1 examples/sec; 0.397 sec/batch)
2017-04-03 00:16:16.795258: step 13060, loss = 2.30 (321.3 examples/sec; 0.398 sec/batch)
2017-04-03 00:16:20.769212: step 13070, loss = 2.30 (322.1 examples/sec; 0.397 sec/batch)
2017-04-03 00:16:24.737400: step 13080, loss = 2.30 (322.6 examples/sec; 0.397 sec/batch)
2017-04-03 00:16:28.702128: step 13090, loss = 2.30 (322.8 examples/sec; 0.396 sec/batch)
2017-04-03 00:16:32.717515: step 13100, loss = 2.30 (318.8 examples/sec; 0.402 sec/batch)
2017-04-03 00:16:36.690413: step 13110, loss = 2.31 (322.2 examples/sec; 0.397 sec/batch)
2017-04-03 00:16:40.660315: step 13120, loss = 2.30 (322.4 examples/sec; 0.397 sec/batch)
2017-04-03 00:16:44.621292: step 13130, loss = 2.30 (323.2 examples/sec; 0.396 sec/batch)
2017-04-03 00:16:48.589088: step 13140, loss = 2.30 (322.6 examples/sec; 0.397 sec/batch)
2017-04-03 00:16:52.556991: step 13150, loss = 2.31 (322.6 examples/sec; 0.397 sec/batch)
2017-04-03 00:16:56.508920: step 13160, loss = 2.30 (323.9 examples/sec; 0.395 sec/batch)
2017-04-03 00:17:00.482775: step 13170, loss = 2.30 (322.1 examples/sec; 0.397 sec/batch)
2017-04-03 00:17:04.461990: step 13180, loss = 2.31 (321.7 examples/sec; 0.398 sec/batch)
2017-04-03 00:17:08.433558: step 13190, loss = 2.30 (322.3 examples/sec; 0.397 sec/batch)
2017-04-03 00:17:12.461145: step 13200, loss = 2.30 (317.8 examples/sec; 0.403 sec/batch)
2017-04-03 00:17:16.388214: step 13210, loss = 2.30 (325.9 examples/sec; 0.393 sec/batch)
2017-04-03 00:17:20.341964: step 13220, loss = 2.30 (323.7 examples/sec; 0.395 sec/batch)
2017-04-03 00:17:24.332069: step 13230, loss = 2.30 (320.8 examples/sec; 0.399 sec/batch)
2017-04-03 00:17:28.283445: step 13240, loss = 2.30 (323.9 examples/sec; 0.395 sec/batch)
2017-04-03 00:17:32.220227: step 13250, loss = 2.30 (325.1 examples/sec; 0.394 sec/batch)
2017-04-03 00:17:36.162638: step 13260, loss = 2.30 (324.7 examples/sec; 0.394 sec/batch)
2017-04-03 00:17:40.091543: step 13270, loss = 2.30 (325.8 examples/sec; 0.393 sec/batch)
2017-04-03 00:17:44.070138: step 13280, loss = 2.30 (321.7 examples/sec; 0.398 sec/batch)
2017-04-03 00:17:48.016848: step 13290, loss = 2.30 (324.3 examples/sec; 0.395 sec/batch)
2017-04-03 00:17:52.041401: step 13300, loss = 2.30 (318.0 examples/sec; 0.402 sec/batch)
2017-04-03 00:17:55.989939: step 13310, loss = 2.31 (324.2 examples/sec; 0.395 sec/batch)
2017-04-03 00:17:59.927055: step 13320, loss = 2.30 (325.1 examples/sec; 0.394 sec/batch)
2017-04-03 00:18:03.884849: step 13330, loss = 2.30 (323.4 examples/sec; 0.396 sec/batch)
2017-04-03 00:18:07.857439: step 13340, loss = 2.30 (322.2 examples/sec; 0.397 sec/batch)
2017-04-03 00:18:11.804681: step 13350, loss = 2.30 (324.3 examples/sec; 0.395 sec/batch)
2017-04-03 00:18:15.776239: step 13360, loss = 2.30 (322.3 examples/sec; 0.397 sec/batch)
2017-04-03 00:18:19.750246: step 13370, loss = 2.30 (322.1 examples/sec; 0.397 sec/batch)
2017-04-03 00:18:23.678667: step 13380, loss = 2.30 (325.8 examples/sec; 0.393 sec/batch)
2017-04-03 00:18:27.598404: step 13390, loss = 2.30 (326.6 examples/sec; 0.392 sec/batch)
2017-04-03 00:18:31.633302: step 13400, loss = 2.31 (317.2 examples/sec; 0.403 sec/batch)
2017-04-03 00:18:35.574883: step 13410, loss = 2.30 (324.7 examples/sec; 0.394 sec/batch)
2017-04-03 00:18:39.543381: step 13420, loss = 2.30 (322.5 examples/sec; 0.397 sec/batch)
2017-04-03 00:18:43.498289: step 13430, loss = 2.30 (323.6 examples/sec; 0.395 sec/batch)
2017-04-03 00:18:47.494437: step 13440, loss = 2.30 (320.3 examples/sec; 0.400 sec/batch)
2017-04-03 00:18:51.437719: step 13450, loss = 2.31 (324.6 examples/sec; 0.394 sec/batch)
2017-04-03 00:18:55.389773: step 13460, loss = 2.30 (323.9 examples/sec; 0.395 sec/batch)
2017-04-03 00:18:59.317212: step 13470, loss = 2.30 (325.9 examples/sec; 0.393 sec/batch)
2017-04-03 00:19:03.270950: step 13480, loss = 2.30 (323.7 examples/sec; 0.395 sec/batch)
2017-04-03 00:19:07.232250: step 13490, loss = 2.30 (323.1 examples/sec; 0.396 sec/batch)
2017-04-03 00:19:11.263328: step 13500, loss = 2.30 (317.5 examples/sec; 0.403 sec/batch)
2017-04-03 00:19:15.234424: step 13510, loss = 2.30 (322.3 examples/sec; 0.397 sec/batch)
2017-04-03 00:19:19.217339: step 13520, loss = 2.30 (321.4 examples/sec; 0.398 sec/batch)
2017-04-03 00:19:23.181443: step 13530, loss = 2.30 (322.9 examples/sec; 0.396 sec/batch)
2017-04-03 00:19:27.134334: step 13540, loss = 2.30 (323.8 examples/sec; 0.395 sec/batch)
2017-04-03 00:19:31.100888: step 13550, loss = 2.30 (322.7 examples/sec; 0.397 sec/batch)
2017-04-03 00:19:35.024179: step 13560, loss = 2.31 (326.3 examples/sec; 0.392 sec/batch)
2017-04-03 00:19:38.990302: step 13570, loss = 2.30 (322.7 examples/sec; 0.397 sec/batch)
2017-04-03 00:19:42.970342: step 13580, loss = 2.30 (321.6 examples/sec; 0.398 sec/batch)
2017-04-03 00:19:46.888780: step 13590, loss = 2.30 (326.7 examples/sec; 0.392 sec/batch)
2017-04-03 00:19:50.922841: step 13600, loss = 2.30 (317.3 examples/sec; 0.403 sec/batch)
2017-04-03 00:19:54.870834: step 13610, loss = 2.31 (324.2 examples/sec; 0.395 sec/batch)
2017-04-03 00:19:58.793124: step 13620, loss = 2.31 (326.3 examples/sec; 0.392 sec/batch)
2017-04-03 00:20:02.727814: step 13630, loss = 2.30 (325.3 examples/sec; 0.393 sec/batch)
2017-04-03 00:20:06.645772: step 13640, loss = 2.30 (326.7 examples/sec; 0.392 sec/batch)
2017-04-03 00:20:10.561094: step 13650, loss = 2.30 (326.9 examples/sec; 0.392 sec/batch)
2017-04-03 00:20:14.479017: step 13660, loss = 2.30 (326.7 examples/sec; 0.392 sec/batch)
2017-04-03 00:20:18.430092: step 13670, loss = 2.30 (324.0 examples/sec; 0.395 sec/batch)
2017-04-03 00:20:22.361733: step 13680, loss = 2.30 (325.6 examples/sec; 0.393 sec/batch)
2017-04-03 00:20:26.289167: step 13690, loss = 2.30 (325.9 examples/sec; 0.393 sec/batch)
2017-04-03 00:20:30.273268: step 13700, loss = 2.30 (321.3 examples/sec; 0.398 sec/batch)
2017-04-03 00:20:34.205069: step 13710, loss = 2.31 (325.6 examples/sec; 0.393 sec/batch)
2017-04-03 00:20:38.789792: step 13720, loss = 2.30 (279.2 examples/sec; 0.458 sec/batch)
2017-04-03 00:20:42.746640: step 13730, loss = 2.30 (323.5 examples/sec; 0.396 sec/batch)
2017-04-03 00:20:46.693412: step 13740, loss = 2.30 (324.3 examples/sec; 0.395 sec/batch)
2017-04-03 00:20:50.632651: step 13750, loss = 2.30 (324.9 examples/sec; 0.394 sec/batch)
2017-04-03 00:20:54.594260: step 13760, loss = 2.30 (323.1 examples/sec; 0.396 sec/batch)
2017-04-03 00:20:58.572805: step 13770, loss = 2.30 (321.7 examples/sec; 0.398 sec/batch)
2017-04-03 00:21:02.510594: step 13780, loss = 2.30 (325.1 examples/sec; 0.394 sec/batch)
2017-04-03 00:21:06.455213: step 13790, loss = 2.30 (324.5 examples/sec; 0.394 sec/batch)
2017-04-03 00:21:10.455438: step 13800, loss = 2.31 (320.0 examples/sec; 0.400 sec/batch)
2017-04-03 00:21:14.421105: step 13810, loss = 2.30 (322.8 examples/sec; 0.397 sec/batch)
2017-04-03 00:21:18.379611: step 13820, loss = 2.30 (323.4 examples/sec; 0.396 sec/batch)
2017-04-03 00:21:22.372767: step 13830, loss = 2.30 (320.5 examples/sec; 0.399 sec/batch)
2017-04-03 00:21:26.361426: step 13840, loss = 2.31 (320.9 examples/sec; 0.399 sec/batch)
2017-04-03 00:21:30.343749: step 13850, loss = 2.30 (321.4 examples/sec; 0.398 sec/batch)
2017-04-03 00:21:34.317654: step 13860, loss = 2.31 (322.1 examples/sec; 0.397 sec/batch)
2017-04-03 00:21:38.275342: step 13870, loss = 2.30 (323.4 examples/sec; 0.396 sec/batch)
2017-04-03 00:21:42.244202: step 13880, loss = 2.30 (322.5 examples/sec; 0.397 sec/batch)
2017-04-03 00:21:46.265536: step 13890, loss = 2.30 (318.3 examples/sec; 0.402 sec/batch)
2017-04-03 00:21:50.346765: step 13900, loss = 2.30 (313.6 examples/sec; 0.408 sec/batch)
2017-04-03 00:21:54.311180: step 13910, loss = 2.30 (322.9 examples/sec; 0.396 sec/batch)
2017-04-03 00:21:58.281707: step 13920, loss = 2.30 (322.4 examples/sec; 0.397 sec/batch)
2017-04-03 00:22:02.245102: step 13930, loss = 2.30 (323.0 examples/sec; 0.396 sec/batch)
2017-04-03 00:22:06.205326: step 13940, loss = 2.31 (323.2 examples/sec; 0.396 sec/batch)
2017-04-03 00:22:10.145307: step 13950, loss = 2.30 (324.9 examples/sec; 0.394 sec/batch)
2017-04-03 00:22:14.132879: step 13960, loss = 2.30 (321.0 examples/sec; 0.399 sec/batch)
2017-04-03 00:22:18.093640: step 13970, loss = 2.30 (323.2 examples/sec; 0.396 sec/batch)
2017-04-03 00:22:22.031587: step 13980, loss = 2.30 (325.0 examples/sec; 0.394 sec/batch)
2017-04-03 00:22:27.047321: step 13990, loss = 2.30 (255.2 examples/sec; 0.502 sec/batch)
2017-04-03 00:22:31.202923: step 14000, loss = 2.30 (308.0 examples/sec; 0.416 sec/batch)
2017-04-03 00:24:06.487019: step 14010, loss = 2.31 (13.4 examples/sec; 9.528 sec/batch)
2017-04-03 00:24:10.444128: step 14020, loss = 2.31 (323.5 examples/sec; 0.396 sec/batch)
2017-04-03 00:24:14.390227: step 14030, loss = 2.30 (324.4 examples/sec; 0.395 sec/batch)
2017-04-03 00:24:18.322124: step 14040, loss = 2.30 (325.5 examples/sec; 0.393 sec/batch)
2017-04-03 00:24:22.289957: step 14050, loss = 2.30 (322.6 examples/sec; 0.397 sec/batch)
2017-04-03 00:24:26.363614: step 14060, loss = 2.30 (314.2 examples/sec; 0.407 sec/batch)
2017-04-03 00:24:31.493918: step 14070, loss = 2.30 (249.5 examples/sec; 0.513 sec/batch)
2017-04-03 00:24:35.457551: step 14080, loss = 2.30 (322.9 examples/sec; 0.396 sec/batch)
2017-04-03 00:24:39.452668: step 14090, loss = 2.30 (320.4 examples/sec; 0.400 sec/batch)
2017-04-03 00:24:43.481280: step 14100, loss = 2.30 (317.7 examples/sec; 0.403 sec/batch)
2017-04-03 00:24:47.444359: step 14110, loss = 2.30 (323.0 examples/sec; 0.396 sec/batch)
2017-04-03 00:24:51.402246: step 14120, loss = 2.30 (323.4 examples/sec; 0.396 sec/batch)
2017-04-03 00:24:55.382281: step 14130, loss = 2.30 (321.6 examples/sec; 0.398 sec/batch)
2017-04-03 00:24:59.357897: step 14140, loss = 2.30 (322.0 examples/sec; 0.398 sec/batch)
2017-04-03 00:25:03.374482: step 14150, loss = 2.30 (318.7 examples/sec; 0.402 sec/batch)
2017-04-03 00:25:07.472366: step 14160, loss = 2.30 (312.4 examples/sec; 0.410 sec/batch)
2017-04-03 00:25:11.456985: step 14170, loss = 2.30 (321.2 examples/sec; 0.398 sec/batch)
2017-04-03 00:25:15.448799: step 14180, loss = 2.30 (320.7 examples/sec; 0.399 sec/batch)
2017-04-03 00:25:19.397516: step 14190, loss = 2.30 (324.2 examples/sec; 0.395 sec/batch)
2017-04-03 00:25:23.448936: step 14200, loss = 2.30 (315.9 examples/sec; 0.405 sec/batch)
2017-04-03 00:25:27.425541: step 14210, loss = 2.30 (321.9 examples/sec; 0.398 sec/batch)
2017-04-03 00:25:31.378054: step 14220, loss = 2.30 (323.8 examples/sec; 0.395 sec/batch)
2017-04-03 00:25:35.345982: step 14230, loss = 2.30 (322.6 examples/sec; 0.397 sec/batch)
2017-04-03 00:25:39.395324: step 14240, loss = 2.30 (316.1 examples/sec; 0.405 sec/batch)
2017-04-03 00:25:43.572479: step 14250, loss = 2.30 (306.4 examples/sec; 0.418 sec/batch)
2017-04-03 00:25:47.559147: step 14260, loss = 2.30 (321.1 examples/sec; 0.399 sec/batch)
2017-04-03 00:25:51.524878: step 14270, loss = 2.30 (322.8 examples/sec; 0.397 sec/batch)
2017-04-03 00:25:55.482336: step 14280, loss = 2.30 (323.4 examples/sec; 0.396 sec/batch)
2017-04-03 00:25:59.462328: step 14290, loss = 2.30 (321.6 examples/sec; 0.398 sec/batch)
2017-04-03 00:26:03.519497: step 14300, loss = 2.30 (315.5 examples/sec; 0.406 sec/batch)
2017-04-03 00:26:07.497916: step 14310, loss = 2.30 (321.7 examples/sec; 0.398 sec/batch)
2017-04-03 00:26:11.465805: step 14320, loss = 2.30 (322.6 examples/sec; 0.397 sec/batch)
2017-04-03 00:26:15.406686: step 14330, loss = 2.30 (324.8 examples/sec; 0.394 sec/batch)
2017-04-03 00:26:19.365137: step 14340, loss = 2.30 (323.4 examples/sec; 0.396 sec/batch)
2017-04-03 00:26:23.311200: step 14350, loss = 2.30 (324.4 examples/sec; 0.395 sec/batch)
2017-04-03 00:26:27.273328: step 14360, loss = 2.30 (323.1 examples/sec; 0.396 sec/batch)
2017-04-03 00:26:31.212260: step 14370, loss = 2.30 (325.0 examples/sec; 0.394 sec/batch)
2017-04-03 00:26:35.190264: step 14380, loss = 2.30 (321.8 examples/sec; 0.398 sec/batch)
2017-04-03 00:26:39.149723: step 14390, loss = 2.30 (323.3 examples/sec; 0.396 sec/batch)
2017-04-03 00:26:43.178804: step 14400, loss = 2.30 (317.7 examples/sec; 0.403 sec/batch)
2017-04-03 00:26:47.122156: step 14410, loss = 2.30 (324.6 examples/sec; 0.394 sec/batch)
2017-04-03 00:26:51.112604: step 14420, loss = 2.30 (320.8 examples/sec; 0.399 sec/batch)
2017-04-03 00:26:55.143724: step 14430, loss = 2.30 (317.5 examples/sec; 0.403 sec/batch)
2017-04-03 00:26:59.131947: step 14440, loss = 2.30 (320.9 examples/sec; 0.399 sec/batch)
2017-04-03 00:27:03.124247: step 14450, loss = 2.31 (320.6 examples/sec; 0.399 sec/batch)
2017-04-03 00:27:07.093701: step 14460, loss = 2.30 (322.5 examples/sec; 0.397 sec/batch)
2017-04-03 00:27:11.021513: step 14470, loss = 2.30 (325.9 examples/sec; 0.393 sec/batch)
2017-04-03 00:27:14.975184: step 14480, loss = 2.30 (323.7 examples/sec; 0.395 sec/batch)
2017-04-03 00:27:18.936870: step 14490, loss = 2.30 (323.1 examples/sec; 0.396 sec/batch)
2017-04-03 00:27:22.975675: step 14500, loss = 2.30 (316.9 examples/sec; 0.404 sec/batch)
2017-04-03 00:27:26.943620: step 14510, loss = 2.30 (322.6 examples/sec; 0.397 sec/batch)
2017-04-03 00:27:30.885874: step 14520, loss = 2.30 (324.7 examples/sec; 0.394 sec/batch)
2017-04-03 00:27:34.845919: step 14530, loss = 2.30 (323.2 examples/sec; 0.396 sec/batch)
2017-04-03 00:27:38.813606: step 14540, loss = 2.30 (322.6 examples/sec; 0.397 sec/batch)
2017-04-03 00:27:42.758776: step 14550, loss = 2.30 (324.4 examples/sec; 0.395 sec/batch)
2017-04-03 00:27:46.763064: step 14560, loss = 2.30 (319.7 examples/sec; 0.400 sec/batch)
2017-04-03 00:27:50.738647: step 14570, loss = 2.30 (322.0 examples/sec; 0.398 sec/batch)
2017-04-03 00:27:54.702170: step 14580, loss = 2.30 (322.9 examples/sec; 0.396 sec/batch)
2017-04-03 00:27:58.662102: step 14590, loss = 2.30 (323.2 examples/sec; 0.396 sec/batch)
2017-04-03 00:28:02.668691: step 14600, loss = 2.30 (319.5 examples/sec; 0.401 sec/batch)
2017-04-03 00:28:06.621768: step 14610, loss = 2.30 (323.8 examples/sec; 0.395 sec/batch)
2017-04-03 00:28:10.580592: step 14620, loss = 2.31 (323.3 examples/sec; 0.396 sec/batch)
2017-04-03 00:28:14.529022: step 14630, loss = 2.31 (324.2 examples/sec; 0.395 sec/batch)
2017-04-03 00:28:18.502555: step 14640, loss = 2.30 (322.1 examples/sec; 0.397 sec/batch)
2017-04-03 00:28:22.506448: step 14650, loss = 2.30 (319.7 examples/sec; 0.400 sec/batch)
2017-04-03 00:28:26.499889: step 14660, loss = 2.30 (320.5 examples/sec; 0.399 sec/batch)
2017-04-03 00:28:31.397087: step 14670, loss = 2.30 (261.4 examples/sec; 0.490 sec/batch)
2017-04-03 00:28:35.580863: step 14680, loss = 2.30 (305.9 examples/sec; 0.418 sec/batch)
2017-04-03 00:28:39.538945: step 14690, loss = 2.30 (323.4 examples/sec; 0.396 sec/batch)
2017-04-03 00:28:43.554675: step 14700, loss = 2.30 (318.7 examples/sec; 0.402 sec/batch)
2017-04-03 00:28:47.497156: step 14710, loss = 2.30 (324.7 examples/sec; 0.394 sec/batch)
2017-04-03 00:28:51.453547: step 14720, loss = 2.30 (323.5 examples/sec; 0.396 sec/batch)
2017-04-03 00:28:55.401731: step 14730, loss = 2.30 (324.2 examples/sec; 0.395 sec/batch)
2017-04-03 00:28:59.355103: step 14740, loss = 2.30 (323.8 examples/sec; 0.395 sec/batch)
2017-04-03 00:29:03.729024: step 14750, loss = 2.30 (292.6 examples/sec; 0.437 sec/batch)
2017-04-03 00:29:08.453647: step 14760, loss = 2.30 (270.9 examples/sec; 0.472 sec/batch)
2017-04-03 00:29:12.408873: step 14770, loss = 2.31 (323.6 examples/sec; 0.396 sec/batch)
2017-04-03 00:29:16.363092: step 14780, loss = 2.30 (323.7 examples/sec; 0.395 sec/batch)
2017-04-03 00:29:20.313610: step 14790, loss = 2.30 (324.0 examples/sec; 0.395 sec/batch)
2017-04-03 00:29:24.356247: step 14800, loss = 2.30 (316.6 examples/sec; 0.404 sec/batch)
2017-04-03 00:29:28.352266: step 14810, loss = 2.30 (320.3 examples/sec; 0.400 sec/batch)
2017-04-03 00:29:32.314167: step 14820, loss = 2.31 (323.1 examples/sec; 0.396 sec/batch)
2017-04-03 00:29:36.237234: step 14830, loss = 2.30 (326.3 examples/sec; 0.392 sec/batch)
2017-04-03 00:29:40.180820: step 14840, loss = 2.30 (324.6 examples/sec; 0.394 sec/batch)
2017-04-03 00:29:44.145979: step 14850, loss = 2.30 (322.8 examples/sec; 0.397 sec/batch)
2017-04-03 00:29:48.097749: step 14860, loss = 2.30 (323.9 examples/sec; 0.395 sec/batch)
2017-04-03 00:29:52.064751: step 14870, loss = 2.30 (322.7 examples/sec; 0.397 sec/batch)
2017-04-03 00:29:56.022055: step 14880, loss = 2.30 (323.5 examples/sec; 0.396 sec/batch)
2017-04-03 00:29:59.979501: step 14890, loss = 2.30 (323.4 examples/sec; 0.396 sec/batch)
2017-04-03 00:30:04.026015: step 14900, loss = 2.30 (316.3 examples/sec; 0.405 sec/batch)
2017-04-03 00:30:07.976219: step 14910, loss = 2.30 (324.0 examples/sec; 0.395 sec/batch)
2017-04-03 00:30:11.947246: step 14920, loss = 2.30 (322.3 examples/sec; 0.397 sec/batch)
2017-04-03 00:30:15.869094: step 14930, loss = 2.30 (326.4 examples/sec; 0.392 sec/batch)
2017-04-03 00:30:19.865855: step 14940, loss = 2.30 (320.3 examples/sec; 0.400 sec/batch)
2017-04-03 00:30:23.859897: step 14950, loss = 2.30 (320.5 examples/sec; 0.399 sec/batch)
2017-04-03 00:30:27.848423: step 14960, loss = 2.30 (320.9 examples/sec; 0.399 sec/batch)
2017-04-03 00:30:31.821044: step 14970, loss = 2.31 (322.2 examples/sec; 0.397 sec/batch)
2017-04-03 00:30:35.804605: step 14980, loss = 2.30 (321.3 examples/sec; 0.398 sec/batch)
2017-04-03 00:30:40.436970: step 14990, loss = 2.30 (276.3 examples/sec; 0.463 sec/batch)
2017-04-03 00:30:44.466337: step 15000, loss = 2.30 (317.7 examples/sec; 0.403 sec/batch)
2017-04-03 00:30:48.437034: step 15010, loss = 2.31 (322.4 examples/sec; 0.397 sec/batch)
2017-04-03 00:30:52.402048: step 15020, loss = 2.30 (322.8 examples/sec; 0.397 sec/batch)
2017-04-03 00:30:56.380385: step 15030, loss = 2.30 (321.7 examples/sec; 0.398 sec/batch)
2017-04-03 00:31:00.369249: step 15040, loss = 2.30 (320.9 examples/sec; 0.399 sec/batch)
2017-04-03 00:31:04.346912: step 15050, loss = 2.30 (321.8 examples/sec; 0.398 sec/batch)
2017-04-03 00:31:08.338441: step 15060, loss = 2.30 (320.7 examples/sec; 0.399 sec/batch)
2017-04-03 00:31:12.295751: step 15070, loss = 2.30 (323.5 examples/sec; 0.396 sec/batch)
2017-04-03 00:31:16.266485: step 15080, loss = 2.30 (322.4 examples/sec; 0.397 sec/batch)
2017-04-03 00:31:20.244760: step 15090, loss = 2.31 (321.7 examples/sec; 0.398 sec/batch)
2017-04-03 00:31:24.324438: step 15100, loss = 2.30 (313.8 examples/sec; 0.408 sec/batch)
2017-04-03 00:31:28.292799: step 15110, loss = 2.30 (322.6 examples/sec; 0.397 sec/batch)
2017-04-03 00:31:32.286611: step 15120, loss = 2.30 (320.5 examples/sec; 0.399 sec/batch)
2017-04-03 00:31:36.251538: step 15130, loss = 2.30 (322.8 examples/sec; 0.396 sec/batch)
2017-04-03 00:31:40.220711: step 15140, loss = 2.30 (322.5 examples/sec; 0.397 sec/batch)
2017-04-03 00:31:44.182045: step 15150, loss = 2.30 (323.1 examples/sec; 0.396 sec/batch)
2017-04-03 00:31:48.167072: step 15160, loss = 2.30 (321.2 examples/sec; 0.399 sec/batch)
2017-04-03 00:31:52.163954: step 15170, loss = 2.30 (320.2 examples/sec; 0.400 sec/batch)
2017-04-03 00:31:56.147496: step 15180, loss = 2.30 (321.3 examples/sec; 0.398 sec/batch)
2017-04-03 00:32:00.149036: step 15190, loss = 2.30 (319.9 examples/sec; 0.400 sec/batch)
2017-04-03 00:32:04.163216: step 15200, loss = 2.30 (318.9 examples/sec; 0.401 sec/batch)
2017-04-03 00:32:08.134073: step 15210, loss = 2.30 (322.3 examples/sec; 0.397 sec/batch)
2017-04-03 00:32:12.111757: step 15220, loss = 2.31 (321.8 examples/sec; 0.398 sec/batch)
2017-04-03 00:32:16.101695: step 15230, loss = 2.30 (320.8 examples/sec; 0.399 sec/batch)
2017-04-03 00:32:20.057329: step 15240, loss = 2.30 (323.6 examples/sec; 0.396 sec/batch)
2017-04-03 00:32:24.033115: step 15250, loss = 2.30 (321.9 examples/sec; 0.398 sec/batch)
2017-04-03 00:32:27.985890: step 15260, loss = 2.30 (323.8 examples/sec; 0.395 sec/batch)
2017-04-03 00:32:31.957743: step 15270, loss = 2.30 (322.3 examples/sec; 0.397 sec/batch)
2017-04-03 00:32:35.936930: step 15280, loss = 2.30 (321.7 examples/sec; 0.398 sec/batch)
2017-04-03 00:32:39.920644: step 15290, loss = 2.30 (321.3 examples/sec; 0.398 sec/batch)
2017-04-03 00:32:43.953730: step 15300, loss = 2.30 (317.4 examples/sec; 0.403 sec/batch)
2017-04-03 00:32:47.945208: step 15310, loss = 2.30 (320.7 examples/sec; 0.399 sec/batch)
2017-04-03 00:32:51.918096: step 15320, loss = 2.30 (322.2 examples/sec; 0.397 sec/batch)
2017-04-03 00:32:55.882551: step 15330, loss = 2.30 (322.9 examples/sec; 0.396 sec/batch)
2017-04-03 00:32:59.861111: step 15340, loss = 2.30 (321.7 examples/sec; 0.398 sec/batch)
2017-04-03 00:33:03.878708: step 15350, loss = 2.31 (318.6 examples/sec; 0.402 sec/batch)
2017-04-03 00:33:07.857319: step 15360, loss = 2.30 (321.7 examples/sec; 0.398 sec/batch)
2017-04-03 00:33:11.854945: step 15370, loss = 2.30 (320.2 examples/sec; 0.400 sec/batch)
2017-04-03 00:33:16.041937: step 15380, loss = 2.30 (305.7 examples/sec; 0.419 sec/batch)
2017-04-03 00:33:20.010873: step 15390, loss = 2.30 (322.5 examples/sec; 0.397 sec/batch)
2017-04-03 00:33:24.016388: step 15400, loss = 2.30 (319.6 examples/sec; 0.401 sec/batch)
2017-04-03 00:33:28.013583: step 15410, loss = 2.30 (320.2 examples/sec; 0.400 sec/batch)
2017-04-03 00:33:32.028917: step 15420, loss = 2.30 (318.8 examples/sec; 0.402 sec/batch)
2017-04-03 00:33:36.030112: step 15430, loss = 2.30 (319.9 examples/sec; 0.400 sec/batch)
2017-04-03 00:33:40.000271: step 15440, loss = 2.30 (322.4 examples/sec; 0.397 sec/batch)
2017-04-03 00:33:43.975810: step 15450, loss = 2.30 (322.0 examples/sec; 0.398 sec/batch)
2017-04-03 00:33:47.915854: step 15460, loss = 2.31 (324.9 examples/sec; 0.394 sec/batch)
2017-04-03 00:33:51.911703: step 15470, loss = 2.30 (320.3 examples/sec; 0.400 sec/batch)
2017-04-03 00:33:55.856929: step 15480, loss = 2.30 (324.4 examples/sec; 0.395 sec/batch)
2017-04-03 00:33:59.793375: step 15490, loss = 2.30 (325.2 examples/sec; 0.394 sec/batch)
2017-04-03 00:34:03.813114: step 15500, loss = 2.30 (318.4 examples/sec; 0.402 sec/batch)
2017-04-03 00:34:07.787074: step 15510, loss = 2.30 (322.1 examples/sec; 0.397 sec/batch)
2017-04-03 00:34:11.765162: step 15520, loss = 2.30 (321.8 examples/sec; 0.398 sec/batch)
2017-04-03 00:34:15.776207: step 15530, loss = 2.30 (319.1 examples/sec; 0.401 sec/batch)
2017-04-03 00:34:19.744435: step 15540, loss = 2.30 (322.6 examples/sec; 0.397 sec/batch)
2017-04-03 00:34:23.648617: step 15550, loss = 2.30 (327.9 examples/sec; 0.390 sec/batch)
2017-04-03 00:34:27.602133: step 15560, loss = 2.30 (323.8 examples/sec; 0.395 sec/batch)
2017-04-03 00:34:31.598759: step 15570, loss = 2.30 (320.3 examples/sec; 0.400 sec/batch)
2017-04-03 00:34:35.556918: step 15580, loss = 2.30 (323.4 examples/sec; 0.396 sec/batch)
2017-04-03 00:34:39.516177: step 15590, loss = 2.31 (323.3 examples/sec; 0.396 sec/batch)
2017-04-03 00:34:43.530166: step 15600, loss = 2.30 (318.9 examples/sec; 0.401 sec/batch)
2017-04-03 00:34:47.493985: step 15610, loss = 2.30 (322.9 examples/sec; 0.396 sec/batch)
2017-04-03 00:34:51.460956: step 15620, loss = 2.30 (322.7 examples/sec; 0.397 sec/batch)
2017-04-03 00:34:55.421623: step 15630, loss = 2.30 (323.2 examples/sec; 0.396 sec/batch)
2017-04-03 00:34:59.365133: step 15640, loss = 2.30 (324.6 examples/sec; 0.394 sec/batch)
2017-04-03 00:35:03.297120: step 15650, loss = 2.30 (325.5 examples/sec; 0.393 sec/batch)
2017-04-03 00:35:07.251646: step 15660, loss = 2.30 (323.7 examples/sec; 0.395 sec/batch)
2017-04-03 00:35:11.212212: step 15670, loss = 2.30 (323.2 examples/sec; 0.396 sec/batch)
2017-04-03 00:35:15.196324: step 15680, loss = 2.30 (321.3 examples/sec; 0.398 sec/batch)
2017-04-03 00:35:19.133021: step 15690, loss = 2.30 (325.1 examples/sec; 0.394 sec/batch)
2017-04-03 00:35:23.144411: step 15700, loss = 2.30 (319.1 examples/sec; 0.401 sec/batch)
2017-04-03 00:35:27.100873: step 15710, loss = 2.31 (323.5 examples/sec; 0.396 sec/batch)
2017-04-03 00:35:31.044840: step 15720, loss = 2.30 (324.5 examples/sec; 0.394 sec/batch)
2017-04-03 00:35:34.992661: step 15730, loss = 2.30 (324.2 examples/sec; 0.395 sec/batch)
2017-04-03 00:35:38.940902: step 15740, loss = 2.30 (324.2 examples/sec; 0.395 sec/batch)
2017-04-03 00:35:42.892709: step 15750, loss = 2.30 (323.9 examples/sec; 0.395 sec/batch)
2017-04-03 00:35:46.865758: step 15760, loss = 2.30 (322.2 examples/sec; 0.397 sec/batch)
2017-04-03 00:35:50.806982: step 15770, loss = 2.30 (324.8 examples/sec; 0.394 sec/batch)
2017-04-03 00:35:54.761843: step 15780, loss = 2.30 (323.7 examples/sec; 0.395 sec/batch)
2017-04-03 00:35:58.758093: step 15790, loss = 2.30 (320.3 examples/sec; 0.400 sec/batch)
2017-04-03 00:36:02.776841: step 15800, loss = 2.30 (318.5 examples/sec; 0.402 sec/batch)
2017-04-03 00:36:06.765980: step 15810, loss = 2.31 (320.9 examples/sec; 0.399 sec/batch)
2017-04-03 00:36:10.709150: step 15820, loss = 2.30 (324.6 examples/sec; 0.394 sec/batch)
2017-04-03 00:36:14.684829: step 15830, loss = 2.31 (322.0 examples/sec; 0.398 sec/batch)
2017-04-03 00:36:18.636143: step 15840, loss = 2.30 (323.9 examples/sec; 0.395 sec/batch)
2017-04-03 00:36:22.581134: step 15850, loss = 2.30 (324.5 examples/sec; 0.394 sec/batch)
2017-04-03 00:36:26.541103: step 15860, loss = 2.30 (323.2 examples/sec; 0.396 sec/batch)
2017-04-03 00:36:30.478690: step 15870, loss = 2.30 (325.1 examples/sec; 0.394 sec/batch)
2017-04-03 00:36:34.427858: step 15880, loss = 2.30 (324.1 examples/sec; 0.395 sec/batch)
2017-04-03 00:36:38.371275: step 15890, loss = 2.30 (324.6 examples/sec; 0.394 sec/batch)
2017-04-03 00:36:42.395926: step 15900, loss = 2.30 (318.0 examples/sec; 0.402 sec/batch)
2017-04-03 00:36:46.318012: step 15910, loss = 2.30 (326.4 examples/sec; 0.392 sec/batch)
2017-04-03 00:36:50.246803: step 15920, loss = 2.30 (325.8 examples/sec; 0.393 sec/batch)
2017-04-03 00:36:54.214683: step 15930, loss = 2.30 (322.6 examples/sec; 0.397 sec/batch)
2017-04-03 00:36:58.173953: step 15940, loss = 2.30 (323.3 examples/sec; 0.396 sec/batch)
2017-04-03 00:37:02.129632: step 15950, loss = 2.30 (323.6 examples/sec; 0.396 sec/batch)
2017-04-03 00:37:06.070988: step 15960, loss = 2.30 (324.8 examples/sec; 0.394 sec/batch)
2017-04-03 00:37:10.029180: step 15970, loss = 2.30 (323.4 examples/sec; 0.396 sec/batch)
2017-04-03 00:37:13.965597: step 15980, loss = 2.30 (325.2 examples/sec; 0.394 sec/batch)
2017-04-03 00:37:17.919534: step 15990, loss = 2.30 (323.7 examples/sec; 0.395 sec/batch)
2017-04-03 00:37:21.958088: step 16000, loss = 2.30 (316.9 examples/sec; 0.404 sec/batch)
