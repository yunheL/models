2017-04-02 21:14:10.814302: step 0, loss = 4.68 (137.5 examples/sec; 0.931 sec/batch)
2017-04-02 21:14:20.756416: step 10, loss = 4.64 (128.7 examples/sec; 0.994 sec/batch)
2017-04-02 21:14:30.795077: step 20, loss = 4.50 (127.5 examples/sec; 1.004 sec/batch)
2017-04-02 21:14:40.664508: step 30, loss = 4.43 (129.7 examples/sec; 0.987 sec/batch)
2017-04-02 21:14:50.527526: step 40, loss = 4.57 (129.8 examples/sec; 0.986 sec/batch)
2017-04-02 21:15:00.472852: step 50, loss = 4.33 (128.7 examples/sec; 0.995 sec/batch)
2017-04-02 21:15:10.466292: step 60, loss = 4.37 (128.1 examples/sec; 0.999 sec/batch)
2017-04-02 21:15:20.213361: step 70, loss = 4.21 (131.3 examples/sec; 0.975 sec/batch)
2017-04-02 21:15:30.055031: step 80, loss = 4.16 (130.1 examples/sec; 0.984 sec/batch)
2017-04-02 21:15:40.080213: step 90, loss = 4.01 (127.7 examples/sec; 1.003 sec/batch)
2017-04-02 21:15:50.135366: step 100, loss = 4.36 (127.3 examples/sec; 1.006 sec/batch)
2017-04-02 21:16:00.041759: step 110, loss = 4.54 (129.2 examples/sec; 0.991 sec/batch)
2017-04-02 21:16:09.969304: step 120, loss = 4.12 (128.9 examples/sec; 0.993 sec/batch)
2017-04-02 21:16:20.000438: step 130, loss = 3.97 (127.6 examples/sec; 1.003 sec/batch)
2017-04-02 21:16:30.039405: step 140, loss = 4.14 (127.5 examples/sec; 1.004 sec/batch)
2017-04-02 21:16:40.128518: step 150, loss = 4.04 (126.9 examples/sec; 1.009 sec/batch)
2017-04-02 21:16:50.163774: step 160, loss = 3.95 (127.6 examples/sec; 1.004 sec/batch)
2017-04-02 21:17:00.320924: step 170, loss = 3.91 (126.0 examples/sec; 1.016 sec/batch)
2017-04-02 21:17:10.180011: step 180, loss = 3.82 (129.8 examples/sec; 0.986 sec/batch)
2017-04-02 21:17:20.191483: step 190, loss = 3.99 (127.9 examples/sec; 1.001 sec/batch)
2017-04-02 21:17:30.389420: step 200, loss = 3.76 (125.5 examples/sec; 1.020 sec/batch)
2017-04-02 21:17:40.221845: step 210, loss = 3.80 (130.2 examples/sec; 0.983 sec/batch)
2017-04-02 21:17:50.145769: step 220, loss = 3.72 (129.0 examples/sec; 0.992 sec/batch)
2017-04-02 21:18:00.290378: step 230, loss = 4.01 (126.2 examples/sec; 1.014 sec/batch)
2017-04-02 21:18:10.323599: step 240, loss = 3.69 (127.6 examples/sec; 1.003 sec/batch)
2017-04-02 21:18:20.470832: step 250, loss = 3.80 (126.1 examples/sec; 1.015 sec/batch)
2017-04-02 21:18:30.477285: step 260, loss = 3.80 (127.9 examples/sec; 1.001 sec/batch)
2017-04-02 21:18:40.391629: step 270, loss = 3.60 (129.1 examples/sec; 0.991 sec/batch)
2017-04-02 21:18:50.263667: step 280, loss = 3.63 (129.7 examples/sec; 0.987 sec/batch)
2017-04-02 21:19:00.348935: step 290, loss = 3.69 (126.9 examples/sec; 1.009 sec/batch)
2017-04-02 21:19:10.560964: step 300, loss = 3.52 (125.3 examples/sec; 1.021 sec/batch)
2017-04-02 21:19:20.662581: step 310, loss = 3.54 (126.7 examples/sec; 1.010 sec/batch)
2017-04-02 21:19:30.631699: step 320, loss = 3.53 (128.4 examples/sec; 0.997 sec/batch)
2017-04-02 21:19:40.567700: step 330, loss = 3.55 (128.8 examples/sec; 0.994 sec/batch)
2017-04-02 21:19:50.718832: step 340, loss = 3.58 (126.1 examples/sec; 1.015 sec/batch)
2017-04-02 21:20:00.818731: step 350, loss = 3.40 (126.7 examples/sec; 1.010 sec/batch)
2017-04-02 21:20:10.720752: step 360, loss = 3.38 (129.3 examples/sec; 0.990 sec/batch)
2017-04-02 21:20:20.816063: step 370, loss = 3.58 (126.8 examples/sec; 1.010 sec/batch)
2017-04-02 21:20:30.880259: step 380, loss = 3.31 (127.2 examples/sec; 1.006 sec/batch)
2017-04-02 21:20:40.825164: step 390, loss = 3.52 (128.7 examples/sec; 0.994 sec/batch)
2017-04-02 21:20:50.934854: step 400, loss = 3.29 (126.6 examples/sec; 1.011 sec/batch)
2017-04-02 21:21:01.012085: step 410, loss = 3.50 (127.0 examples/sec; 1.008 sec/batch)
2017-04-02 21:21:11.131767: step 420, loss = 3.46 (126.5 examples/sec; 1.012 sec/batch)
2017-04-02 21:21:21.070604: step 430, loss = 3.37 (128.8 examples/sec; 0.994 sec/batch)
2017-04-02 21:21:31.075605: step 440, loss = 3.41 (127.9 examples/sec; 1.001 sec/batch)
2017-04-02 21:21:41.150152: step 450, loss = 3.36 (127.1 examples/sec; 1.007 sec/batch)
2017-04-02 21:21:51.571038: step 460, loss = 3.23 (122.8 examples/sec; 1.042 sec/batch)
2017-04-02 21:22:01.444076: step 470, loss = 3.12 (129.6 examples/sec; 0.987 sec/batch)
2017-04-02 21:22:11.330586: step 480, loss = 3.64 (129.5 examples/sec; 0.989 sec/batch)
2017-04-02 21:22:21.143878: step 490, loss = 3.32 (130.4 examples/sec; 0.981 sec/batch)
2017-04-02 21:22:31.267775: step 500, loss = 3.22 (126.4 examples/sec; 1.012 sec/batch)
2017-04-02 21:22:41.258734: step 510, loss = 3.07 (128.1 examples/sec; 0.999 sec/batch)
2017-04-02 21:22:51.284962: step 520, loss = 3.20 (127.7 examples/sec; 1.003 sec/batch)
2017-04-02 21:23:01.235966: step 530, loss = 3.09 (128.6 examples/sec; 0.995 sec/batch)
2017-04-02 21:23:11.370721: step 540, loss = 2.95 (126.3 examples/sec; 1.013 sec/batch)
2017-04-02 21:23:21.508581: step 550, loss = 3.12 (126.3 examples/sec; 1.014 sec/batch)
2017-04-02 21:23:31.418391: step 560, loss = 3.05 (129.2 examples/sec; 0.991 sec/batch)
2017-04-02 21:23:41.305227: step 570, loss = 3.29 (129.5 examples/sec; 0.989 sec/batch)
2017-04-02 21:23:51.197368: step 580, loss = 2.85 (129.4 examples/sec; 0.989 sec/batch)
2017-04-02 21:24:01.275679: step 590, loss = 2.97 (127.0 examples/sec; 1.008 sec/batch)
2017-04-02 21:24:12.390922: step 600, loss = 3.03 (115.2 examples/sec; 1.112 sec/batch)
2017-04-02 21:24:22.290079: step 610, loss = 2.98 (129.3 examples/sec; 0.990 sec/batch)
2017-04-02 21:24:32.236752: step 620, loss = 2.85 (128.7 examples/sec; 0.995 sec/batch)
2017-04-02 21:24:42.035226: step 630, loss = 2.95 (130.6 examples/sec; 0.980 sec/batch)
2017-04-02 21:24:51.906441: step 640, loss = 2.98 (129.7 examples/sec; 0.987 sec/batch)
2017-04-02 21:25:02.050756: step 650, loss = 3.03 (126.2 examples/sec; 1.014 sec/batch)
2017-04-02 21:25:12.145770: step 660, loss = 2.79 (126.8 examples/sec; 1.010 sec/batch)
2017-04-02 21:25:22.313534: step 670, loss = 2.78 (125.9 examples/sec; 1.017 sec/batch)
2017-04-02 21:25:32.468090: step 680, loss = 3.05 (126.1 examples/sec; 1.015 sec/batch)
2017-04-02 21:25:42.508087: step 690, loss = 2.90 (127.5 examples/sec; 1.004 sec/batch)
2017-04-02 21:25:52.307184: step 700, loss = 3.23 (130.6 examples/sec; 0.980 sec/batch)
2017-04-02 21:26:02.193063: step 710, loss = 2.81 (129.5 examples/sec; 0.989 sec/batch)
2017-04-02 21:26:12.424407: step 720, loss = 2.71 (125.1 examples/sec; 1.023 sec/batch)
2017-04-02 21:26:22.504740: step 730, loss = 2.87 (127.0 examples/sec; 1.008 sec/batch)
2017-04-02 21:26:32.330580: step 740, loss = 2.70 (130.3 examples/sec; 0.983 sec/batch)
2017-04-02 21:26:42.509436: step 750, loss = 2.76 (125.8 examples/sec; 1.018 sec/batch)
2017-04-02 21:26:52.541985: step 760, loss = 2.88 (127.6 examples/sec; 1.003 sec/batch)
2017-04-02 21:27:02.391260: step 770, loss = 2.75 (130.0 examples/sec; 0.985 sec/batch)
2017-04-02 21:27:12.221866: step 780, loss = 2.94 (130.2 examples/sec; 0.983 sec/batch)
2017-04-02 21:27:22.179275: step 790, loss = 2.72 (128.5 examples/sec; 0.996 sec/batch)
2017-04-02 21:27:32.274463: step 800, loss = 2.61 (126.8 examples/sec; 1.010 sec/batch)
2017-04-02 21:27:42.197375: step 810, loss = 2.61 (129.0 examples/sec; 0.992 sec/batch)
2017-04-02 21:27:52.245934: step 820, loss = 2.87 (127.4 examples/sec; 1.005 sec/batch)
2017-04-02 21:28:02.151745: step 830, loss = 2.69 (129.2 examples/sec; 0.991 sec/batch)
2017-04-02 21:28:11.983290: step 840, loss = 2.52 (130.2 examples/sec; 0.983 sec/batch)
2017-04-02 21:28:21.967966: step 850, loss = 2.52 (128.2 examples/sec; 0.998 sec/batch)
2017-04-02 21:28:31.871487: step 860, loss = 2.54 (129.2 examples/sec; 0.990 sec/batch)
2017-04-02 21:28:41.875932: step 870, loss = 2.71 (127.9 examples/sec; 1.000 sec/batch)
2017-04-02 21:28:51.658718: step 880, loss = 2.57 (130.8 examples/sec; 0.978 sec/batch)
2017-04-02 21:29:01.831152: step 890, loss = 2.45 (125.8 examples/sec; 1.017 sec/batch)
2017-04-02 21:29:11.920890: step 900, loss = 3.06 (126.9 examples/sec; 1.009 sec/batch)
2017-04-02 21:29:21.757306: step 910, loss = 2.58 (130.1 examples/sec; 0.984 sec/batch)
2017-04-02 21:29:31.648692: step 920, loss = 2.54 (129.4 examples/sec; 0.989 sec/batch)
2017-04-02 21:29:41.339639: step 930, loss = 2.43 (132.1 examples/sec; 0.969 sec/batch)
2017-04-02 21:29:51.148318: step 940, loss = 2.55 (130.5 examples/sec; 0.981 sec/batch)
2017-04-02 21:30:01.097705: step 950, loss = 2.49 (128.7 examples/sec; 0.995 sec/batch)
2017-04-02 21:30:11.042624: step 960, loss = 2.41 (128.7 examples/sec; 0.994 sec/batch)
2017-04-02 21:30:20.994432: step 970, loss = 2.38 (128.6 examples/sec; 0.995 sec/batch)
2017-04-02 21:30:30.743982: step 980, loss = 2.42 (131.3 examples/sec; 0.975 sec/batch)
2017-04-02 21:30:40.795564: step 990, loss = 2.40 (127.3 examples/sec; 1.005 sec/batch)
2017-04-02 21:30:50.990657: step 1000, loss = 2.38 (125.6 examples/sec; 1.020 sec/batch)
2017-04-02 21:31:00.956778: step 1010, loss = 2.53 (128.4 examples/sec; 0.997 sec/batch)
2017-04-02 21:31:10.778588: step 1020, loss = 2.46 (130.3 examples/sec; 0.982 sec/batch)
2017-04-02 21:31:20.492739: step 1030, loss = 2.24 (131.8 examples/sec; 0.971 sec/batch)
2017-04-02 21:31:30.456983: step 1040, loss = 2.37 (128.5 examples/sec; 0.996 sec/batch)
2017-04-02 21:31:40.106724: step 1050, loss = 2.37 (132.6 examples/sec; 0.965 sec/batch)
2017-04-02 21:31:50.123141: step 1060, loss = 2.45 (127.8 examples/sec; 1.002 sec/batch)
2017-04-02 21:32:00.020183: step 1070, loss = 2.32 (129.3 examples/sec; 0.990 sec/batch)
^Z
[1]  + 25511 suspended  python cifar10_train.py
(tensorflow) ➜  src git:(master) ✗ hostname
rockhopper-06
(tensorflow) ➜  src git:(master) ✗ fg %1
[1]  + 25511 continued  python cifar10_train.py
2017-04-02 21:32:29.342063: step 1080, loss = 2.39 (43.7 examples/sec; 2.932 sec/batch)
2017-04-02 21:32:39.225772: step 1090, loss = 2.35 (129.5 examples/sec; 0.988 sec/batch)
2017-04-02 21:32:49.254189: step 1100, loss = 2.21 (127.6 examples/sec; 1.003 sec/batch)
2017-04-02 21:32:59.334022: step 1110, loss = 2.14 (127.0 examples/sec; 1.008 sec/batch)
2017-04-02 21:33:09.158597: step 1120, loss = 2.29 (130.3 examples/sec; 0.982 sec/batch)
2017-04-02 21:33:19.004365: step 1130, loss = 2.38 (130.0 examples/sec; 0.985 sec/batch)
2017-04-02 21:33:28.826728: step 1140, loss = 2.28 (130.3 examples/sec; 0.982 sec/batch)
2017-04-02 21:33:38.948756: step 1150, loss = 2.12 (126.5 examples/sec; 1.012 sec/batch)
2017-04-02 21:33:48.831039: step 1160, loss = 2.28 (129.5 examples/sec; 0.988 sec/batch)
2017-04-02 21:33:58.659257: step 1170, loss = 2.30 (130.2 examples/sec; 0.983 sec/batch)
2017-04-02 21:34:08.545346: step 1180, loss = 2.07 (129.5 examples/sec; 0.989 sec/batch)
2017-04-02 21:34:19.141447: step 1190, loss = 2.27 (120.8 examples/sec; 1.060 sec/batch)
2017-04-02 21:34:28.994819: step 1200, loss = 2.29 (129.9 examples/sec; 0.985 sec/batch)
2017-04-02 21:34:38.970934: step 1210, loss = 2.21 (128.3 examples/sec; 0.998 sec/batch)
2017-04-02 21:34:48.881024: step 1220, loss = 2.06 (129.2 examples/sec; 0.991 sec/batch)
2017-04-02 21:34:58.656461: step 1230, loss = 2.26 (130.9 examples/sec; 0.978 sec/batch)
2017-04-02 21:35:08.550728: step 1240, loss = 2.24 (129.4 examples/sec; 0.989 sec/batch)
2017-04-02 21:35:18.447732: step 1250, loss = 2.07 (129.3 examples/sec; 0.990 sec/batch)
2017-04-02 21:35:28.450732: step 1260, loss = 2.34 (128.0 examples/sec; 1.000 sec/batch)
2017-04-02 21:35:38.287141: step 1270, loss = 1.97 (130.1 examples/sec; 0.984 sec/batch)
2017-04-02 21:35:48.057724: step 1280, loss = 2.22 (131.0 examples/sec; 0.977 sec/batch)
2017-04-02 21:35:57.982631: step 1290, loss = 1.99 (129.0 examples/sec; 0.992 sec/batch)
2017-04-02 21:36:07.931121: step 1300, loss = 1.93 (128.7 examples/sec; 0.995 sec/batch)
2017-04-02 21:36:17.771578: step 1310, loss = 2.08 (130.1 examples/sec; 0.984 sec/batch)
2017-04-02 21:36:27.556240: step 1320, loss = 2.30 (130.8 examples/sec; 0.978 sec/batch)
2017-04-02 21:36:37.639775: step 1330, loss = 2.06 (126.9 examples/sec; 1.008 sec/batch)
2017-04-02 21:36:47.519424: step 1340, loss = 2.28 (129.6 examples/sec; 0.988 sec/batch)
2017-04-02 21:36:57.388408: step 1350, loss = 2.16 (129.7 examples/sec; 0.987 sec/batch)
2017-04-02 21:37:07.267893: step 1360, loss = 2.27 (129.6 examples/sec; 0.988 sec/batch)
2017-04-02 21:37:17.175733: step 1370, loss = 2.20 (129.2 examples/sec; 0.991 sec/batch)
2017-04-02 21:37:27.052998: step 1380, loss = 2.02 (129.6 examples/sec; 0.988 sec/batch)
2017-04-02 21:37:37.090385: step 1390, loss = 2.16 (127.5 examples/sec; 1.004 sec/batch)
2017-04-02 21:37:46.908178: step 1400, loss = 2.35 (130.4 examples/sec; 0.982 sec/batch)
2017-04-02 21:37:56.919869: step 1410, loss = 1.96 (127.9 examples/sec; 1.001 sec/batch)
2017-04-02 21:38:06.781769: step 1420, loss = 1.96 (129.8 examples/sec; 0.986 sec/batch)
2017-04-02 21:38:16.721593: step 1430, loss = 2.10 (128.8 examples/sec; 0.994 sec/batch)
2017-04-02 21:38:26.518621: step 1440, loss = 2.04 (130.7 examples/sec; 0.980 sec/batch)
2017-04-02 21:38:36.361096: step 1450, loss = 1.87 (130.0 examples/sec; 0.984 sec/batch)
2017-04-02 21:38:46.182029: step 1460, loss = 1.86 (130.3 examples/sec; 0.982 sec/batch)
2017-04-02 21:38:56.021534: step 1470, loss = 1.95 (130.1 examples/sec; 0.984 sec/batch)
2017-04-02 21:39:06.028273: step 1480, loss = 2.09 (127.9 examples/sec; 1.001 sec/batch)
2017-04-02 21:39:15.899589: step 1490, loss = 2.07 (129.7 examples/sec; 0.987 sec/batch)
2017-04-02 21:39:25.970134: step 1500, loss = 2.02 (127.1 examples/sec; 1.007 sec/batch)
2017-04-02 21:39:35.719796: step 1510, loss = 1.88 (131.3 examples/sec; 0.975 sec/batch)
2017-04-02 21:39:45.547515: step 1520, loss = 1.79 (130.2 examples/sec; 0.983 sec/batch)
2017-04-02 21:39:55.360231: step 1530, loss = 1.80 (130.4 examples/sec; 0.981 sec/batch)
2017-04-02 21:40:05.220488: step 1540, loss = 1.90 (129.8 examples/sec; 0.986 sec/batch)
2017-04-02 21:40:15.298634: step 1550, loss = 1.91 (127.0 examples/sec; 1.008 sec/batch)
2017-04-02 21:40:25.186731: step 1560, loss = 1.82 (129.4 examples/sec; 0.989 sec/batch)
2017-04-02 21:40:34.953564: step 1570, loss = 1.91 (131.1 examples/sec; 0.977 sec/batch)
2017-04-02 21:40:44.901637: step 1580, loss = 2.04 (128.7 examples/sec; 0.995 sec/batch)
2017-04-02 21:40:54.786109: step 1590, loss = 1.90 (129.5 examples/sec; 0.988 sec/batch)
2017-04-02 21:41:04.726638: step 1600, loss = 1.92 (128.8 examples/sec; 0.994 sec/batch)
2017-04-02 21:41:14.511023: step 1610, loss = 1.88 (130.8 examples/sec; 0.978 sec/batch)
2017-04-02 21:41:24.246970: step 1620, loss = 1.88 (131.5 examples/sec; 0.974 sec/batch)
2017-04-02 21:41:34.068998: step 1630, loss = 1.78 (130.3 examples/sec; 0.982 sec/batch)
2017-04-02 21:41:43.991010: step 1640, loss = 1.85 (129.0 examples/sec; 0.992 sec/batch)
2017-04-02 21:41:54.054154: step 1650, loss = 1.85 (127.2 examples/sec; 1.006 sec/batch)
2017-04-02 21:42:03.921144: step 1660, loss = 1.80 (129.7 examples/sec; 0.987 sec/batch)
2017-04-02 21:42:13.623844: step 1670, loss = 1.80 (131.9 examples/sec; 0.970 sec/batch)
2017-04-02 21:42:23.433067: step 1680, loss = 1.69 (130.5 examples/sec; 0.981 sec/batch)
2017-04-02 21:42:33.342030: step 1690, loss = 1.96 (129.2 examples/sec; 0.991 sec/batch)
2017-04-02 21:42:43.232633: step 1700, loss = 1.77 (129.4 examples/sec; 0.989 sec/batch)
2017-04-02 21:42:53.429460: step 1710, loss = 2.08 (125.5 examples/sec; 1.020 sec/batch)
2017-04-02 21:43:03.316541: step 1720, loss = 1.74 (129.5 examples/sec; 0.989 sec/batch)
2017-04-02 21:43:13.270925: step 1730, loss = 1.75 (128.6 examples/sec; 0.995 sec/batch)
2017-04-02 21:43:23.290717: step 1740, loss = 1.76 (127.7 examples/sec; 1.002 sec/batch)
2017-04-02 21:43:33.112496: step 1750, loss = 1.90 (130.3 examples/sec; 0.982 sec/batch)
2017-04-02 21:43:42.870781: step 1760, loss = 1.69 (131.2 examples/sec; 0.976 sec/batch)
2017-04-02 21:43:52.674392: step 1770, loss = 1.80 (130.6 examples/sec; 0.980 sec/batch)
2017-04-02 21:44:02.727496: step 1780, loss = 1.70 (127.3 examples/sec; 1.005 sec/batch)
2017-04-02 21:44:14.310033: step 1790, loss = 1.75 (110.5 examples/sec; 1.158 sec/batch)
2017-04-02 21:44:24.412196: step 1800, loss = 1.68 (126.7 examples/sec; 1.010 sec/batch)
2017-04-02 21:44:34.268895: step 1810, loss = 1.78 (129.9 examples/sec; 0.986 sec/batch)
2017-04-02 21:44:44.235138: step 1820, loss = 1.72 (128.4 examples/sec; 0.997 sec/batch)
2017-04-02 21:44:54.214204: step 1830, loss = 1.86 (128.3 examples/sec; 0.998 sec/batch)
2017-04-02 21:45:04.155005: step 1840, loss = 1.62 (128.8 examples/sec; 0.994 sec/batch)
2017-04-02 21:45:14.218727: step 1850, loss = 1.67 (127.2 examples/sec; 1.006 sec/batch)
2017-04-02 21:45:24.138579: step 1860, loss = 1.66 (129.0 examples/sec; 0.992 sec/batch)
2017-04-02 21:45:33.970665: step 1870, loss = 1.76 (130.2 examples/sec; 0.983 sec/batch)
2017-04-02 21:45:44.101284: step 1880, loss = 1.84 (126.3 examples/sec; 1.013 sec/batch)
2017-04-02 21:45:53.901506: step 1890, loss = 1.71 (130.6 examples/sec; 0.980 sec/batch)
2017-04-02 21:46:03.703662: step 1900, loss = 1.52 (130.6 examples/sec; 0.980 sec/batch)
2017-04-02 21:46:13.613632: step 1910, loss = 1.67 (129.2 examples/sec; 0.991 sec/batch)
2017-04-02 21:46:23.499187: step 1920, loss = 1.69 (129.5 examples/sec; 0.989 sec/batch)
2017-04-02 21:46:33.425102: step 1930, loss = 1.71 (129.0 examples/sec; 0.993 sec/batch)
2017-04-02 21:46:43.175333: step 1940, loss = 1.82 (131.3 examples/sec; 0.975 sec/batch)
2017-04-02 21:46:52.962581: step 1950, loss = 1.57 (130.8 examples/sec; 0.979 sec/batch)
2017-04-02 21:47:03.023789: step 1960, loss = 1.49 (127.2 examples/sec; 1.006 sec/batch)
2017-04-02 21:47:13.213645: step 1970, loss = 1.61 (125.6 examples/sec; 1.019 sec/batch)
2017-04-02 21:47:23.219693: step 1980, loss = 1.69 (127.9 examples/sec; 1.001 sec/batch)
2017-04-02 21:47:33.040830: step 1990, loss = 1.56 (130.3 examples/sec; 0.982 sec/batch)
2017-04-02 21:47:42.912135: step 2000, loss = 1.55 (129.7 examples/sec; 0.987 sec/batch)
2017-04-02 21:49:01.481274: precision @ 1 = 0.100

2017-04-02 21:49:37.113801: step 2010, loss = 1.69 (11.2 examples/sec; 11.420 sec/batch)
2017-04-02 21:49:46.932447: step 2020, loss = 1.65 (130.4 examples/sec; 0.982 sec/batch)
2017-04-02 21:49:57.094910: step 2030, loss = 1.69 (126.0 examples/sec; 1.016 sec/batch)
2017-04-02 21:50:07.110758: step 2040, loss = 1.66 (127.8 examples/sec; 1.002 sec/batch)
2017-04-02 21:50:16.852888: step 2050, loss = 1.92 (131.4 examples/sec; 0.974 sec/batch)
2017-04-02 21:50:26.831540: step 2060, loss = 1.70 (128.3 examples/sec; 0.998 sec/batch)
2017-04-02 21:50:36.610943: step 2070, loss = 1.54 (130.9 examples/sec; 0.978 sec/batch)
2017-04-02 21:50:46.587060: step 2080, loss = 1.79 (128.3 examples/sec; 0.998 sec/batch)
2017-04-02 21:50:56.403445: step 2090, loss = 1.87 (130.4 examples/sec; 0.982 sec/batch)
2017-04-02 21:51:06.518668: step 2100, loss = 1.79 (126.5 examples/sec; 1.012 sec/batch)
2017-04-02 21:51:16.420906: step 2110, loss = 1.58 (129.3 examples/sec; 0.990 sec/batch)
2017-04-02 21:51:26.226194: step 2120, loss = 1.39 (130.5 examples/sec; 0.981 sec/batch)
2017-04-02 21:51:35.987160: step 2130, loss = 1.50 (131.1 examples/sec; 0.976 sec/batch)
2017-04-02 21:51:45.676724: step 2140, loss = 1.46 (132.1 examples/sec; 0.969 sec/batch)
2017-04-02 21:51:56.946578: step 2150, loss = 1.51 (113.6 examples/sec; 1.127 sec/batch)
2017-04-02 21:52:06.711651: step 2160, loss = 1.57 (131.1 examples/sec; 0.977 sec/batch)
2017-04-02 21:52:16.297524: step 2170, loss = 1.39 (133.5 examples/sec; 0.959 sec/batch)
2017-04-02 21:52:26.110249: step 2180, loss = 1.85 (130.4 examples/sec; 0.981 sec/batch)
2017-04-02 21:52:35.950731: step 2190, loss = 1.60 (130.1 examples/sec; 0.984 sec/batch)
2017-04-02 21:52:46.006296: step 2200, loss = 1.51 (127.3 examples/sec; 1.006 sec/batch)
2017-04-02 21:52:55.912241: step 2210, loss = 1.63 (129.2 examples/sec; 0.991 sec/batch)
2017-04-02 21:53:05.766505: step 2220, loss = 1.53 (129.9 examples/sec; 0.985 sec/batch)
2017-04-02 21:53:15.767862: step 2230, loss = 1.36 (128.0 examples/sec; 1.000 sec/batch)
2017-04-02 21:53:25.695016: step 2240, loss = 1.55 (128.9 examples/sec; 0.993 sec/batch)
2017-04-02 21:53:35.509560: step 2250, loss = 1.59 (130.4 examples/sec; 0.981 sec/batch)
2017-04-02 21:53:45.311144: step 2260, loss = 1.47 (130.6 examples/sec; 0.980 sec/batch)
2017-04-02 21:53:55.055923: step 2270, loss = 1.26 (131.4 examples/sec; 0.974 sec/batch)
2017-04-02 21:54:04.870355: step 2280, loss = 1.37 (130.4 examples/sec; 0.981 sec/batch)
2017-04-02 21:54:15.534193: step 2290, loss = 1.43 (120.0 examples/sec; 1.066 sec/batch)
2017-04-02 21:54:25.483539: step 2300, loss = 1.35 (128.7 examples/sec; 0.995 sec/batch)
2017-04-02 21:54:35.211184: step 2310, loss = 1.37 (131.6 examples/sec; 0.973 sec/batch)
2017-04-02 21:54:44.928906: step 2320, loss = 1.67 (131.7 examples/sec; 0.972 sec/batch)
2017-04-02 21:54:54.615761: step 2330, loss = 1.42 (132.1 examples/sec; 0.969 sec/batch)
2017-04-02 21:55:04.513947: step 2340, loss = 1.56 (129.3 examples/sec; 0.990 sec/batch)
2017-04-02 21:55:14.331458: step 2350, loss = 1.56 (130.4 examples/sec; 0.982 sec/batch)
2017-04-02 21:55:24.140174: step 2360, loss = 1.59 (130.5 examples/sec; 0.981 sec/batch)
2017-04-02 21:55:34.165948: step 2370, loss = 1.49 (127.7 examples/sec; 1.003 sec/batch)
2017-04-02 21:55:44.008584: step 2380, loss = 1.49 (130.0 examples/sec; 0.984 sec/batch)
2017-04-02 21:55:53.740047: step 2390, loss = 1.48 (131.5 examples/sec; 0.973 sec/batch)
2017-04-02 21:56:03.715130: step 2400, loss = 1.49 (128.3 examples/sec; 0.998 sec/batch)
2017-04-02 21:56:13.664960: step 2410, loss = 1.42 (128.6 examples/sec; 0.995 sec/batch)
2017-04-02 21:56:23.380850: step 2420, loss = 1.31 (131.7 examples/sec; 0.972 sec/batch)
2017-04-02 21:56:33.111572: step 2430, loss = 1.59 (131.5 examples/sec; 0.973 sec/batch)
2017-04-02 21:56:43.049163: step 2440, loss = 1.39 (128.8 examples/sec; 0.994 sec/batch)
2017-04-02 21:56:52.907778: step 2450, loss = 1.34 (129.8 examples/sec; 0.986 sec/batch)
2017-04-02 21:57:03.043158: step 2460, loss = 1.61 (126.3 examples/sec; 1.014 sec/batch)
2017-04-02 21:57:12.995404: step 2470, loss = 1.29 (128.6 examples/sec; 0.995 sec/batch)
2017-04-02 21:57:22.784306: step 2480, loss = 1.41 (130.8 examples/sec; 0.979 sec/batch)
2017-04-02 21:57:32.644209: step 2490, loss = 1.37 (129.8 examples/sec; 0.986 sec/batch)
2017-04-02 21:57:42.820545: step 2500, loss = 1.47 (125.8 examples/sec; 1.018 sec/batch)
2017-04-02 21:57:52.683757: step 2510, loss = 1.42 (129.8 examples/sec; 0.986 sec/batch)
2017-04-02 21:58:02.615815: step 2520, loss = 1.51 (128.9 examples/sec; 0.993 sec/batch)
2017-04-02 21:58:12.495901: step 2530, loss = 1.47 (129.6 examples/sec; 0.988 sec/batch)
2017-04-02 21:58:22.202890: step 2540, loss = 1.23 (131.9 examples/sec; 0.971 sec/batch)
2017-04-02 21:58:32.019074: step 2550, loss = 1.54 (130.4 examples/sec; 0.982 sec/batch)
2017-04-02 21:58:42.017398: step 2560, loss = 1.59 (128.0 examples/sec; 1.000 sec/batch)
2017-04-02 21:58:51.763448: step 2570, loss = 1.54 (131.3 examples/sec; 0.975 sec/batch)
2017-04-02 21:59:01.679421: step 2580, loss = 1.38 (129.1 examples/sec; 0.992 sec/batch)
2017-04-02 21:59:11.732858: step 2590, loss = 1.12 (127.3 examples/sec; 1.005 sec/batch)
2017-04-02 21:59:21.737224: step 2600, loss = 1.50 (127.9 examples/sec; 1.000 sec/batch)
2017-04-02 21:59:31.646871: step 2610, loss = 1.47 (129.2 examples/sec; 0.991 sec/batch)
2017-04-02 21:59:41.538265: step 2620, loss = 1.43 (129.4 examples/sec; 0.989 sec/batch)
2017-04-02 21:59:51.238739: step 2630, loss = 1.46 (132.0 examples/sec; 0.970 sec/batch)
2017-04-02 22:00:01.138470: step 2640, loss = 1.38 (129.3 examples/sec; 0.990 sec/batch)
2017-04-02 22:00:11.033791: step 2650, loss = 1.41 (129.4 examples/sec; 0.990 sec/batch)
2017-04-02 22:00:20.887682: step 2660, loss = 1.31 (129.9 examples/sec; 0.985 sec/batch)
2017-04-02 22:00:30.767778: step 2670, loss = 1.51 (129.6 examples/sec; 0.988 sec/batch)
2017-04-02 22:00:40.603069: step 2680, loss = 1.45 (130.1 examples/sec; 0.984 sec/batch)
2017-04-02 22:00:50.425334: step 2690, loss = 1.25 (130.3 examples/sec; 0.982 sec/batch)
2017-04-02 22:01:00.386982: step 2700, loss = 1.26 (128.5 examples/sec; 0.996 sec/batch)
2017-04-02 22:01:10.257862: step 2710, loss = 1.58 (129.7 examples/sec; 0.987 sec/batch)
2017-04-02 22:01:20.017818: step 2720, loss = 1.48 (131.1 examples/sec; 0.976 sec/batch)
2017-04-02 22:01:29.911399: step 2730, loss = 1.26 (129.4 examples/sec; 0.989 sec/batch)
2017-04-02 22:01:39.822709: step 2740, loss = 1.41 (129.1 examples/sec; 0.991 sec/batch)
2017-04-02 22:01:49.559775: step 2750, loss = 1.49 (131.5 examples/sec; 0.974 sec/batch)
2017-04-02 22:01:59.433369: step 2760, loss = 1.33 (129.6 examples/sec; 0.987 sec/batch)
2017-04-02 22:02:09.315367: step 2770, loss = 1.30 (129.5 examples/sec; 0.988 sec/batch)
2017-04-02 22:02:19.200536: step 2780, loss = 1.41 (129.5 examples/sec; 0.989 sec/batch)
2017-04-02 22:02:29.122533: step 2790, loss = 1.47 (129.0 examples/sec; 0.992 sec/batch)
2017-04-02 22:02:39.223717: step 2800, loss = 1.21 (126.7 examples/sec; 1.010 sec/batch)
2017-04-02 22:02:49.207383: step 2810, loss = 1.31 (128.2 examples/sec; 0.998 sec/batch)
2017-04-02 22:02:58.983568: step 2820, loss = 1.28 (130.9 examples/sec; 0.978 sec/batch)
2017-04-02 22:03:08.793666: step 2830, loss = 1.37 (130.5 examples/sec; 0.981 sec/batch)
2017-04-02 22:03:18.515273: step 2840, loss = 1.23 (131.7 examples/sec; 0.972 sec/batch)
2017-04-02 22:03:28.274886: step 2850, loss = 1.25 (131.2 examples/sec; 0.976 sec/batch)
2017-04-02 22:03:38.012348: step 2860, loss = 1.35 (131.5 examples/sec; 0.974 sec/batch)
2017-04-02 22:03:47.874288: step 2870, loss = 1.22 (129.8 examples/sec; 0.986 sec/batch)
2017-04-02 22:03:57.778209: step 2880, loss = 1.21 (129.2 examples/sec; 0.990 sec/batch)
2017-04-02 22:04:07.794729: step 2890, loss = 1.30 (127.8 examples/sec; 1.002 sec/batch)
2017-04-02 22:04:19.024388: step 2900, loss = 1.24 (114.0 examples/sec; 1.123 sec/batch)
2017-04-02 22:04:28.936720: step 2910, loss = 1.40 (129.1 examples/sec; 0.991 sec/batch)
2017-04-02 22:04:38.726416: step 2920, loss = 1.31 (130.7 examples/sec; 0.979 sec/batch)
2017-04-02 22:04:48.685169: step 2930, loss = 1.33 (128.5 examples/sec; 0.996 sec/batch)
2017-04-02 22:04:58.621113: step 2940, loss = 1.38 (128.8 examples/sec; 0.994 sec/batch)
2017-04-02 22:05:08.456734: step 2950, loss = 1.23 (130.1 examples/sec; 0.984 sec/batch)
2017-04-02 22:05:18.335748: step 2960, loss = 1.08 (129.6 examples/sec; 0.988 sec/batch)
2017-04-02 22:05:28.172564: step 2970, loss = 1.44 (130.1 examples/sec; 0.984 sec/batch)
2017-04-02 22:05:38.116350: step 2980, loss = 1.57 (128.7 examples/sec; 0.994 sec/batch)
2017-04-02 22:05:47.828927: step 2990, loss = 1.32 (131.8 examples/sec; 0.971 sec/batch)
2017-04-02 22:05:57.737541: step 3000, loss = 1.32 (129.2 examples/sec; 0.991 sec/batch)
2017-04-02 22:06:07.774317: step 3010, loss = 1.39 (127.5 examples/sec; 1.004 sec/batch)
2017-04-02 22:06:17.548138: step 3020, loss = 1.50 (131.0 examples/sec; 0.977 sec/batch)
2017-04-02 22:06:27.343045: step 3030, loss = 1.33 (130.7 examples/sec; 0.979 sec/batch)
2017-04-02 22:06:37.244840: step 3040, loss = 1.23 (129.3 examples/sec; 0.990 sec/batch)
2017-04-02 22:06:47.037080: step 3050, loss = 1.17 (130.7 examples/sec; 0.979 sec/batch)
2017-04-02 22:06:57.030630: step 3060, loss = 1.22 (128.1 examples/sec; 0.999 sec/batch)
2017-04-02 22:07:06.837623: step 3070, loss = 1.22 (130.5 examples/sec; 0.981 sec/batch)
2017-04-02 22:07:16.664399: step 3080, loss = 1.12 (130.3 examples/sec; 0.983 sec/batch)
2017-04-02 22:07:26.508343: step 3090, loss = 1.33 (130.0 examples/sec; 0.984 sec/batch)
2017-04-02 22:07:36.238481: step 3100, loss = 0.96 (131.6 examples/sec; 0.973 sec/batch)
2017-04-02 22:07:46.108676: step 3110, loss = 1.36 (129.7 examples/sec; 0.987 sec/batch)
2017-04-02 22:07:55.973804: step 3120, loss = 1.46 (129.7 examples/sec; 0.987 sec/batch)
2017-04-02 22:08:05.754683: step 3130, loss = 1.13 (130.9 examples/sec; 0.978 sec/batch)
2017-04-02 22:08:15.599004: step 3140, loss = 1.22 (130.0 examples/sec; 0.984 sec/batch)
2017-04-02 22:08:25.541350: step 3150, loss = 1.14 (128.7 examples/sec; 0.994 sec/batch)
2017-04-02 22:08:35.379702: step 3160, loss = 1.31 (130.1 examples/sec; 0.984 sec/batch)
2017-04-02 22:08:45.366549: step 3170, loss = 1.38 (128.2 examples/sec; 0.999 sec/batch)
2017-04-02 22:08:55.218290: step 3180, loss = 1.37 (129.9 examples/sec; 0.985 sec/batch)
2017-04-02 22:09:05.099560: step 3190, loss = 1.27 (129.5 examples/sec; 0.988 sec/batch)
2017-04-02 22:09:14.925054: step 3200, loss = 1.30 (130.3 examples/sec; 0.983 sec/batch)
2017-04-02 22:09:24.905344: step 3210, loss = 1.39 (128.3 examples/sec; 0.998 sec/batch)
2017-04-02 22:09:34.702077: step 3220, loss = 1.37 (130.7 examples/sec; 0.980 sec/batch)
2017-04-02 22:09:44.489542: step 3230, loss = 1.37 (130.8 examples/sec; 0.979 sec/batch)
2017-04-02 22:09:54.494540: step 3240, loss = 1.19 (127.9 examples/sec; 1.000 sec/batch)
2017-04-02 22:10:04.368237: step 3250, loss = 1.15 (129.6 examples/sec; 0.987 sec/batch)
2017-04-02 22:10:14.344153: step 3260, loss = 1.34 (128.3 examples/sec; 0.998 sec/batch)
2017-04-02 22:10:24.438477: step 3270, loss = 1.15 (126.8 examples/sec; 1.009 sec/batch)
2017-04-02 22:10:34.195354: step 3280, loss = 1.13 (131.2 examples/sec; 0.976 sec/batch)
2017-04-02 22:10:44.133241: step 3290, loss = 1.36 (128.8 examples/sec; 0.994 sec/batch)
2017-04-02 22:10:53.995758: step 3300, loss = 1.26 (129.8 examples/sec; 0.986 sec/batch)
2017-04-02 22:11:03.837040: step 3310, loss = 1.31 (130.1 examples/sec; 0.984 sec/batch)
2017-04-02 22:11:13.676340: step 3320, loss = 1.02 (130.1 examples/sec; 0.984 sec/batch)
2017-04-02 22:11:23.559367: step 3330, loss = 1.21 (129.5 examples/sec; 0.988 sec/batch)
2017-04-02 22:11:33.478050: step 3340, loss = 1.30 (129.0 examples/sec; 0.992 sec/batch)
2017-04-02 22:11:43.419984: step 3350, loss = 1.17 (128.7 examples/sec; 0.994 sec/batch)
2017-04-02 22:11:53.252148: step 3360, loss = 1.04 (130.2 examples/sec; 0.983 sec/batch)
2017-04-02 22:12:02.968289: step 3370, loss = 1.02 (131.7 examples/sec; 0.972 sec/batch)
2017-04-02 22:12:12.844565: step 3380, loss = 1.35 (129.6 examples/sec; 0.988 sec/batch)
2017-04-02 22:12:22.599729: step 3390, loss = 1.25 (131.2 examples/sec; 0.976 sec/batch)
2017-04-02 22:12:32.579387: step 3400, loss = 1.41 (128.3 examples/sec; 0.998 sec/batch)
2017-04-02 22:12:42.382821: step 3410, loss = 1.21 (130.6 examples/sec; 0.980 sec/batch)
2017-04-02 22:12:52.305329: step 3420, loss = 1.12 (129.0 examples/sec; 0.992 sec/batch)
2017-04-02 22:13:02.205686: step 3430, loss = 1.17 (129.3 examples/sec; 0.990 sec/batch)
2017-04-02 22:13:11.953256: step 3440, loss = 1.25 (131.3 examples/sec; 0.975 sec/batch)
2017-04-02 22:13:21.806253: step 3450, loss = 1.13 (129.9 examples/sec; 0.985 sec/batch)
2017-04-02 22:13:31.671221: step 3460, loss = 1.09 (129.8 examples/sec; 0.986 sec/batch)
2017-04-02 22:13:41.415636: step 3470, loss = 1.11 (131.4 examples/sec; 0.974 sec/batch)
2017-04-02 22:13:51.323488: step 3480, loss = 1.22 (129.2 examples/sec; 0.991 sec/batch)
2017-04-02 22:14:01.171262: step 3490, loss = 1.18 (130.0 examples/sec; 0.985 sec/batch)
2017-04-02 22:14:11.302128: step 3500, loss = 1.30 (126.3 examples/sec; 1.013 sec/batch)
2017-04-02 22:14:22.681464: step 3510, loss = 1.26 (112.5 examples/sec; 1.138 sec/batch)
2017-04-02 22:14:32.326852: step 3520, loss = 1.16 (132.7 examples/sec; 0.965 sec/batch)
2017-04-02 22:14:42.084647: step 3530, loss = 1.18 (131.2 examples/sec; 0.976 sec/batch)
2017-04-02 22:14:51.820380: step 3540, loss = 1.21 (131.5 examples/sec; 0.974 sec/batch)
2017-04-02 22:15:01.614804: step 3550, loss = 1.16 (130.7 examples/sec; 0.979 sec/batch)
2017-04-02 22:15:11.578907: step 3560, loss = 1.14 (128.5 examples/sec; 0.996 sec/batch)
2017-04-02 22:15:21.587432: step 3570, loss = 1.07 (127.9 examples/sec; 1.001 sec/batch)
2017-04-02 22:15:31.591593: step 3580, loss = 1.04 (127.9 examples/sec; 1.000 sec/batch)
2017-04-02 22:15:41.402725: step 3590, loss = 1.06 (130.5 examples/sec; 0.981 sec/batch)
2017-04-02 22:15:51.455358: step 3600, loss = 1.11 (127.3 examples/sec; 1.005 sec/batch)
2017-04-02 22:16:01.272750: step 3610, loss = 1.06 (130.4 examples/sec; 0.982 sec/batch)
2017-04-02 22:16:11.228698: step 3620, loss = 1.04 (128.6 examples/sec; 0.996 sec/batch)
2017-04-02 22:16:20.931494: step 3630, loss = 1.00 (131.9 examples/sec; 0.970 sec/batch)
2017-04-02 22:16:30.684444: step 3640, loss = 1.39 (131.2 examples/sec; 0.975 sec/batch)
2017-04-02 22:16:40.539463: step 3650, loss = 1.12 (129.9 examples/sec; 0.986 sec/batch)
2017-04-02 22:16:50.228350: step 3660, loss = 1.15 (132.1 examples/sec; 0.969 sec/batch)
2017-04-02 22:17:00.201517: step 3670, loss = 1.23 (128.3 examples/sec; 0.997 sec/batch)
2017-04-02 22:17:10.225383: step 3680, loss = 1.23 (127.7 examples/sec; 1.002 sec/batch)
2017-04-02 22:17:19.784655: step 3690, loss = 1.20 (133.9 examples/sec; 0.956 sec/batch)
2017-04-02 22:17:29.867256: step 3700, loss = 1.18 (127.0 examples/sec; 1.008 sec/batch)
2017-04-02 22:17:39.653054: step 3710, loss = 1.07 (130.8 examples/sec; 0.979 sec/batch)
2017-04-02 22:17:49.624583: step 3720, loss = 1.17 (128.4 examples/sec; 0.997 sec/batch)
2017-04-02 22:17:59.391042: step 3730, loss = 1.11 (131.1 examples/sec; 0.977 sec/batch)
2017-04-02 22:18:09.323671: step 3740, loss = 1.20 (128.9 examples/sec; 0.993 sec/batch)
2017-04-02 22:18:19.298028: step 3750, loss = 1.14 (128.3 examples/sec; 0.997 sec/batch)
2017-04-02 22:18:29.056684: step 3760, loss = 1.11 (131.2 examples/sec; 0.976 sec/batch)
2017-04-02 22:18:38.856240: step 3770, loss = 1.29 (130.6 examples/sec; 0.980 sec/batch)
2017-04-02 22:18:48.709181: step 3780, loss = 1.02 (129.9 examples/sec; 0.985 sec/batch)
2017-04-02 22:18:58.673482: step 3790, loss = 1.17 (128.5 examples/sec; 0.996 sec/batch)
2017-04-02 22:19:08.633981: step 3800, loss = 1.11 (128.5 examples/sec; 0.996 sec/batch)
2017-04-02 22:19:18.690720: step 3810, loss = 1.09 (127.3 examples/sec; 1.006 sec/batch)
2017-04-02 22:19:28.581730: step 3820, loss = 1.24 (129.4 examples/sec; 0.989 sec/batch)
2017-04-02 22:19:38.243789: step 3830, loss = 1.06 (132.5 examples/sec; 0.966 sec/batch)
2017-04-02 22:19:48.045204: step 3840, loss = 1.00 (130.6 examples/sec; 0.980 sec/batch)
2017-04-02 22:19:57.828514: step 3850, loss = 1.07 (130.8 examples/sec; 0.978 sec/batch)
2017-04-02 22:20:07.681513: step 3860, loss = 1.21 (129.9 examples/sec; 0.985 sec/batch)
2017-04-02 22:20:17.360057: step 3870, loss = 1.15 (132.3 examples/sec; 0.968 sec/batch)
2017-04-02 22:20:27.120383: step 3880, loss = 1.05 (131.1 examples/sec; 0.976 sec/batch)
2017-04-02 22:20:37.031452: step 3890, loss = 1.19 (129.1 examples/sec; 0.991 sec/batch)
2017-04-02 22:20:46.928300: step 3900, loss = 1.15 (129.3 examples/sec; 0.990 sec/batch)
2017-04-02 22:20:56.790235: step 3910, loss = 1.33 (129.8 examples/sec; 0.986 sec/batch)
2017-04-02 22:21:06.712260: step 3920, loss = 1.30 (129.0 examples/sec; 0.992 sec/batch)
2017-04-02 22:21:16.581122: step 3930, loss = 0.99 (129.7 examples/sec; 0.987 sec/batch)
2017-04-02 22:21:26.336922: step 3940, loss = 1.19 (131.2 examples/sec; 0.976 sec/batch)
2017-04-02 22:21:36.164076: step 3950, loss = 1.26 (130.3 examples/sec; 0.983 sec/batch)
2017-04-02 22:21:46.083935: step 3960, loss = 1.21 (129.0 examples/sec; 0.992 sec/batch)
2017-04-02 22:21:56.022472: step 3970, loss = 1.12 (128.8 examples/sec; 0.994 sec/batch)
2017-04-02 22:22:05.837475: step 3980, loss = 1.20 (130.4 examples/sec; 0.982 sec/batch)
2017-04-02 22:22:15.778940: step 3990, loss = 1.03 (128.8 examples/sec; 0.994 sec/batch)
2017-04-02 22:22:25.738366: step 4000, loss = 1.03 (128.5 examples/sec; 0.996 sec/batch)
2017-04-02 22:23:51.366268: precision @ 1 = 0.104

