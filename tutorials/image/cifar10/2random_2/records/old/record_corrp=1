2017-04-02 17:26:11.067377: step 0, loss = 4.68 (454.8 examples/sec; 0.281 sec/batch)
2017-04-02 17:26:15.192867: step 10, loss = 4.65 (310.3 examples/sec; 0.413 sec/batch)
2017-04-02 17:26:19.244874: step 20, loss = 4.64 (315.9 examples/sec; 0.405 sec/batch)
2017-04-02 17:26:23.314772: step 30, loss = 4.63 (314.5 examples/sec; 0.407 sec/batch)
2017-04-02 17:26:27.430721: step 40, loss = 4.61 (311.0 examples/sec; 0.412 sec/batch)
2017-04-02 17:26:31.569667: step 50, loss = 4.58 (309.3 examples/sec; 0.414 sec/batch)
2017-04-02 17:26:35.664597: step 60, loss = 4.57 (312.6 examples/sec; 0.409 sec/batch)
2017-04-02 17:26:39.807419: step 70, loss = 4.54 (309.0 examples/sec; 0.414 sec/batch)
2017-04-02 17:26:43.897299: step 80, loss = 4.54 (313.0 examples/sec; 0.409 sec/batch)
2017-04-02 17:26:48.014739: step 90, loss = 4.52 (310.9 examples/sec; 0.412 sec/batch)
2017-04-02 17:26:52.133698: step 100, loss = 4.50 (310.8 examples/sec; 0.412 sec/batch)
2017-04-02 17:26:56.215398: step 110, loss = 4.48 (313.6 examples/sec; 0.408 sec/batch)
2017-04-02 17:27:00.349728: step 120, loss = 4.46 (309.6 examples/sec; 0.413 sec/batch)
2017-04-02 17:27:04.475187: step 130, loss = 4.44 (310.3 examples/sec; 0.413 sec/batch)
2017-04-02 17:27:08.543643: step 140, loss = 4.42 (314.6 examples/sec; 0.407 sec/batch)
2017-04-02 17:27:12.616647: step 150, loss = 4.41 (314.3 examples/sec; 0.407 sec/batch)
2017-04-02 17:27:16.715547: step 160, loss = 4.39 (312.3 examples/sec; 0.410 sec/batch)
2017-04-02 17:27:20.825153: step 170, loss = 4.37 (311.5 examples/sec; 0.411 sec/batch)
2017-04-02 17:27:24.922422: step 180, loss = 4.36 (312.4 examples/sec; 0.410 sec/batch)
2017-04-02 17:27:29.002081: step 190, loss = 4.34 (313.8 examples/sec; 0.408 sec/batch)
2017-04-02 17:27:33.073127: step 200, loss = 4.33 (314.4 examples/sec; 0.407 sec/batch)
2017-04-02 17:27:37.125793: step 210, loss = 4.31 (315.8 examples/sec; 0.405 sec/batch)
2017-04-02 17:27:41.179212: step 220, loss = 4.29 (315.8 examples/sec; 0.405 sec/batch)
2017-04-02 17:27:45.229819: step 230, loss = 4.28 (316.0 examples/sec; 0.405 sec/batch)
2017-04-02 17:27:49.299646: step 240, loss = 4.26 (314.5 examples/sec; 0.407 sec/batch)
2017-04-02 17:27:53.394995: step 250, loss = 4.25 (312.5 examples/sec; 0.410 sec/batch)
2017-04-02 17:27:57.515360: step 260, loss = 4.23 (310.7 examples/sec; 0.412 sec/batch)
2017-04-02 17:28:01.739400: step 270, loss = 4.22 (303.0 examples/sec; 0.422 sec/batch)
2017-04-02 17:28:05.824496: step 280, loss = 4.20 (313.3 examples/sec; 0.409 sec/batch)
2017-04-02 17:28:09.913229: step 290, loss = 4.18 (313.1 examples/sec; 0.409 sec/batch)
2017-04-02 17:28:14.072116: step 300, loss = 4.17 (307.8 examples/sec; 0.416 sec/batch)
2017-04-02 17:28:18.191105: step 310, loss = 4.16 (310.8 examples/sec; 0.412 sec/batch)
2017-04-02 17:28:22.367538: step 320, loss = 4.14 (306.5 examples/sec; 0.418 sec/batch)
2017-04-02 17:28:26.502589: step 330, loss = 4.13 (309.5 examples/sec; 0.414 sec/batch)

