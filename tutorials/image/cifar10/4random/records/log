2017-04-02 22:04:24.766535: step 0, loss = 4.68 (52.4 examples/sec; 2.445 sec/batch)
2017-04-02 22:04:28.747820: step 10, loss = 4.63 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 22:04:32.750014: step 20, loss = 4.50 (319.8 examples/sec; 0.400 sec/batch)
2017-04-02 22:04:36.709690: step 30, loss = 4.34 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 22:04:40.644493: step 40, loss = 4.35 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 22:04:44.563950: step 50, loss = 4.31 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 22:04:48.529626: step 60, loss = 4.20 (322.8 examples/sec; 0.397 sec/batch)
2017-04-02 22:04:52.506526: step 70, loss = 4.37 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 22:04:56.480608: step 80, loss = 4.08 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 22:05:00.411162: step 90, loss = 4.12 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 22:05:04.401121: step 100, loss = 4.02 (320.8 examples/sec; 0.399 sec/batch)
2017-04-02 22:05:08.377094: step 110, loss = 4.28 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 22:05:12.337100: step 120, loss = 4.13 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 22:05:16.285188: step 130, loss = 4.08 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 22:05:20.225043: step 140, loss = 4.02 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 22:05:24.181366: step 150, loss = 3.94 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:05:28.108992: step 160, loss = 4.04 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 22:05:32.036166: step 170, loss = 3.95 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 22:05:36.011279: step 180, loss = 3.85 (322.0 examples/sec; 0.398 sec/batch)
2017-04-02 22:05:40.023160: step 190, loss = 3.82 (319.1 examples/sec; 0.401 sec/batch)
2017-04-02 22:05:44.055850: step 200, loss = 3.64 (317.4 examples/sec; 0.403 sec/batch)
2017-04-02 22:05:47.983711: step 210, loss = 3.92 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 22:05:51.917393: step 220, loss = 3.69 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 22:05:55.846885: step 230, loss = 3.69 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 22:05:59.792131: step 240, loss = 3.86 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 22:06:03.711845: step 250, loss = 3.59 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 22:06:07.645173: step 260, loss = 3.74 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 22:06:11.602320: step 270, loss = 3.78 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:06:15.520987: step 280, loss = 3.70 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 22:06:19.451602: step 290, loss = 3.69 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 22:06:23.436783: step 300, loss = 3.71 (321.2 examples/sec; 0.399 sec/batch)
2017-04-02 22:06:27.436857: step 310, loss = 3.60 (320.0 examples/sec; 0.400 sec/batch)
2017-04-02 22:06:31.402123: step 320, loss = 3.58 (322.8 examples/sec; 0.397 sec/batch)
2017-04-02 22:06:35.614290: step 330, loss = 3.47 (303.9 examples/sec; 0.421 sec/batch)
2017-04-02 22:06:39.533968: step 340, loss = 3.38 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 22:06:43.471977: step 350, loss = 3.56 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 22:06:47.421800: step 360, loss = 3.52 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 22:06:51.421314: step 370, loss = 3.51 (320.0 examples/sec; 0.400 sec/batch)
2017-04-02 22:06:55.352069: step 380, loss = 3.51 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 22:06:59.276994: step 390, loss = 3.52 (326.1 examples/sec; 0.392 sec/batch)
2017-04-02 22:07:03.276538: step 400, loss = 3.33 (320.0 examples/sec; 0.400 sec/batch)
2017-04-02 22:07:07.201228: step 410, loss = 3.34 (326.1 examples/sec; 0.392 sec/batch)
2017-04-02 22:07:11.165974: step 420, loss = 3.29 (322.8 examples/sec; 0.396 sec/batch)
2017-04-02 22:07:15.102316: step 430, loss = 3.28 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 22:07:19.011771: step 440, loss = 3.22 (327.4 examples/sec; 0.391 sec/batch)
2017-04-02 22:07:22.923359: step 450, loss = 3.33 (327.2 examples/sec; 0.391 sec/batch)
2017-04-02 22:07:26.850065: step 460, loss = 3.13 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 22:07:30.796454: step 470, loss = 3.25 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 22:07:34.763175: step 480, loss = 3.33 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 22:07:38.913938: step 490, loss = 3.11 (308.4 examples/sec; 0.415 sec/batch)
2017-04-02 22:07:42.895154: step 500, loss = 3.02 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 22:07:46.836940: step 510, loss = 3.21 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 22:07:50.764433: step 520, loss = 3.11 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 22:07:54.684384: step 530, loss = 3.28 (326.5 examples/sec; 0.392 sec/batch)
2017-04-02 22:07:58.613142: step 540, loss = 3.09 (325.8 examples/sec; 0.393 sec/batch)
2017-04-02 22:08:02.524792: step 550, loss = 2.88 (327.2 examples/sec; 0.391 sec/batch)
2017-04-02 22:08:06.458521: step 560, loss = 3.20 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 22:08:10.403513: step 570, loss = 3.52 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 22:08:14.390480: step 580, loss = 3.02 (321.0 examples/sec; 0.399 sec/batch)
2017-04-02 22:08:18.356612: step 590, loss = 2.87 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 22:08:22.363921: step 600, loss = 3.02 (319.4 examples/sec; 0.401 sec/batch)
2017-04-02 22:08:26.329518: step 610, loss = 2.96 (322.8 examples/sec; 0.397 sec/batch)
2017-04-02 22:08:30.286030: step 620, loss = 2.95 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:08:34.236806: step 630, loss = 2.80 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 22:08:38.160748: step 640, loss = 3.25 (326.2 examples/sec; 0.392 sec/batch)
2017-04-02 22:08:42.075414: step 650, loss = 2.84 (327.0 examples/sec; 0.391 sec/batch)
2017-04-02 22:08:45.995894: step 660, loss = 2.95 (326.5 examples/sec; 0.392 sec/batch)
2017-04-02 22:08:49.901413: step 670, loss = 3.08 (327.7 examples/sec; 0.391 sec/batch)
2017-04-02 22:08:53.816311: step 680, loss = 2.97 (327.0 examples/sec; 0.391 sec/batch)
2017-04-02 22:08:57.733896: step 690, loss = 2.91 (326.7 examples/sec; 0.392 sec/batch)
2017-04-02 22:09:01.701424: step 700, loss = 2.77 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 22:09:05.680072: step 710, loss = 2.89 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 22:09:09.613156: step 720, loss = 2.69 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 22:09:13.566835: step 730, loss = 2.74 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 22:09:17.464731: step 740, loss = 2.62 (328.4 examples/sec; 0.390 sec/batch)
2017-04-02 22:09:21.359693: step 750, loss = 2.77 (328.6 examples/sec; 0.389 sec/batch)
2017-04-02 22:09:25.311588: step 760, loss = 2.57 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 22:09:29.227017: step 770, loss = 2.58 (326.9 examples/sec; 0.392 sec/batch)
2017-04-02 22:09:33.124776: step 780, loss = 2.72 (328.4 examples/sec; 0.390 sec/batch)
2017-04-02 22:09:37.054933: step 790, loss = 2.82 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 22:09:41.045378: step 800, loss = 2.76 (320.8 examples/sec; 0.399 sec/batch)
2017-04-02 22:09:44.966201: step 810, loss = 2.67 (326.5 examples/sec; 0.392 sec/batch)
2017-04-02 22:09:48.881987: step 820, loss = 2.49 (326.9 examples/sec; 0.392 sec/batch)
2017-04-02 22:09:52.812483: step 830, loss = 2.71 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 22:09:56.765107: step 840, loss = 2.46 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 22:10:00.723603: step 850, loss = 2.64 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 22:10:04.653773: step 860, loss = 2.61 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 22:10:08.562375: step 870, loss = 2.54 (327.5 examples/sec; 0.391 sec/batch)
2017-04-02 22:10:12.463297: step 880, loss = 2.69 (328.1 examples/sec; 0.390 sec/batch)
2017-04-02 22:10:16.384082: step 890, loss = 2.55 (326.5 examples/sec; 0.392 sec/batch)
2017-04-02 22:10:20.372126: step 900, loss = 2.79 (321.0 examples/sec; 0.399 sec/batch)
2017-04-02 22:10:24.285252: step 910, loss = 2.76 (327.1 examples/sec; 0.391 sec/batch)
2017-04-02 22:10:28.163428: step 920, loss = 2.56 (330.1 examples/sec; 0.388 sec/batch)
2017-04-02 22:10:32.082611: step 930, loss = 2.80 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 22:10:35.990218: step 940, loss = 2.46 (327.6 examples/sec; 0.391 sec/batch)
2017-04-02 22:10:39.889323: step 950, loss = 2.60 (328.3 examples/sec; 0.390 sec/batch)
2017-04-02 22:10:43.778100: step 960, loss = 2.54 (329.2 examples/sec; 0.389 sec/batch)
2017-04-02 22:10:47.681647: step 970, loss = 2.58 (327.9 examples/sec; 0.390 sec/batch)
2017-04-02 22:10:51.589430: step 980, loss = 2.43 (327.6 examples/sec; 0.391 sec/batch)
2017-04-02 22:10:55.512671: step 990, loss = 2.35 (326.3 examples/sec; 0.392 sec/batch)
2017-04-02 22:10:59.484771: step 1000, loss = 2.77 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 22:11:03.377927: step 1010, loss = 2.40 (328.8 examples/sec; 0.389 sec/batch)
2017-04-02 22:11:07.309609: step 1020, loss = 2.24 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 22:11:11.209631: step 1030, loss = 2.43 (328.2 examples/sec; 0.390 sec/batch)
2017-04-02 22:11:15.117039: step 1040, loss = 2.44 (327.6 examples/sec; 0.391 sec/batch)
2017-04-02 22:11:19.025201: step 1050, loss = 2.33 (327.5 examples/sec; 0.391 sec/batch)
2017-04-02 22:11:22.948717: step 1060, loss = 2.36 (326.2 examples/sec; 0.392 sec/batch)
2017-04-02 22:11:26.848576: step 1070, loss = 2.58 (328.2 examples/sec; 0.390 sec/batch)
2017-04-02 22:11:30.779036: step 1080, loss = 2.28 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 22:11:34.708617: step 1090, loss = 2.38 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 22:11:38.678915: step 1100, loss = 2.24 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 22:11:42.632496: step 1110, loss = 2.35 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 22:11:46.560777: step 1120, loss = 2.19 (325.8 examples/sec; 0.393 sec/batch)
2017-04-02 22:11:50.458596: step 1130, loss = 2.37 (328.4 examples/sec; 0.390 sec/batch)
2017-04-02 22:11:54.352788: step 1140, loss = 2.26 (328.7 examples/sec; 0.389 sec/batch)
2017-04-02 22:11:58.252537: step 1150, loss = 2.12 (328.2 examples/sec; 0.390 sec/batch)
2017-04-02 22:12:02.147191: step 1160, loss = 2.18 (328.7 examples/sec; 0.389 sec/batch)
2017-04-02 22:12:06.037514: step 1170, loss = 2.09 (329.0 examples/sec; 0.389 sec/batch)
2017-04-02 22:12:09.947515: step 1180, loss = 2.26 (327.4 examples/sec; 0.391 sec/batch)
2017-04-02 22:12:13.875784: step 1190, loss = 2.21 (325.8 examples/sec; 0.393 sec/batch)
2017-04-02 22:12:17.842928: step 1200, loss = 2.27 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 22:12:21.773410: step 1210, loss = 2.30 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 22:12:25.690897: step 1220, loss = 2.27 (326.7 examples/sec; 0.392 sec/batch)
2017-04-02 22:12:29.605752: step 1230, loss = 2.36 (327.0 examples/sec; 0.391 sec/batch)
2017-04-02 22:12:33.488157: step 1240, loss = 2.28 (329.7 examples/sec; 0.388 sec/batch)
2017-04-02 22:12:37.402789: step 1250, loss = 2.08 (327.0 examples/sec; 0.391 sec/batch)
2017-04-02 22:12:41.305844: step 1260, loss = 2.12 (327.9 examples/sec; 0.390 sec/batch)
2017-04-02 22:12:45.233035: step 1270, loss = 2.36 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 22:12:49.123181: step 1280, loss = 2.01 (329.0 examples/sec; 0.389 sec/batch)
2017-04-02 22:12:53.043154: step 1290, loss = 2.13 (326.5 examples/sec; 0.392 sec/batch)
2017-04-02 22:12:56.998435: step 1300, loss = 2.06 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 22:13:00.924634: step 1310, loss = 2.20 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 22:13:04.818540: step 1320, loss = 2.07 (328.7 examples/sec; 0.389 sec/batch)
2017-04-02 22:13:08.731296: step 1330, loss = 2.07 (327.1 examples/sec; 0.391 sec/batch)
2017-04-02 22:13:12.611782: step 1340, loss = 2.02 (329.9 examples/sec; 0.388 sec/batch)
2017-04-02 22:13:16.540121: step 1350, loss = 1.97 (325.8 examples/sec; 0.393 sec/batch)
2017-04-02 22:13:20.456847: step 1360, loss = 2.19 (326.8 examples/sec; 0.392 sec/batch)
2017-04-02 22:13:24.366621: step 1370, loss = 1.92 (327.4 examples/sec; 0.391 sec/batch)
2017-04-02 22:13:28.284929: step 1380, loss = 1.97 (326.7 examples/sec; 0.392 sec/batch)
2017-04-02 22:13:32.218067: step 1390, loss = 2.14 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 22:13:36.148813: step 1400, loss = 2.09 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 22:13:40.094010: step 1410, loss = 1.96 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 22:13:44.019626: step 1420, loss = 2.09 (326.1 examples/sec; 0.393 sec/batch)
2017-04-02 22:13:47.991972: step 1430, loss = 2.32 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 22:13:51.952588: step 1440, loss = 2.09 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 22:13:55.883745: step 1450, loss = 1.87 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 22:13:59.827742: step 1460, loss = 1.90 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 22:14:03.730408: step 1470, loss = 2.19 (328.0 examples/sec; 0.390 sec/batch)
2017-04-02 22:14:07.653266: step 1480, loss = 1.94 (326.3 examples/sec; 0.392 sec/batch)
2017-04-02 22:14:11.525171: step 1490, loss = 1.94 (330.6 examples/sec; 0.387 sec/batch)
2017-04-02 22:14:15.484531: step 1500, loss = 2.25 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 22:14:27.444143: step 1510, loss = 2.03 (107.0 examples/sec; 1.196 sec/batch)
2017-04-02 22:14:31.353591: step 1520, loss = 2.05 (327.4 examples/sec; 0.391 sec/batch)
2017-04-02 22:14:35.261670: step 1530, loss = 1.96 (327.5 examples/sec; 0.391 sec/batch)
2017-04-02 22:14:39.137524: step 1540, loss = 1.98 (330.2 examples/sec; 0.388 sec/batch)
2017-04-02 22:14:43.032293: step 1550, loss = 2.01 (328.6 examples/sec; 0.389 sec/batch)
2017-04-02 22:14:46.947194: step 1560, loss = 1.80 (327.0 examples/sec; 0.391 sec/batch)
2017-04-02 22:14:50.880638: step 1570, loss = 1.93 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 22:14:54.844439: step 1580, loss = 1.88 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 22:14:59.065623: step 1590, loss = 1.86 (303.2 examples/sec; 0.422 sec/batch)
2017-04-02 22:15:03.054667: step 1600, loss = 1.82 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 22:15:06.963993: step 1610, loss = 1.95 (327.4 examples/sec; 0.391 sec/batch)
2017-04-02 22:15:10.849816: step 1620, loss = 2.24 (329.4 examples/sec; 0.389 sec/batch)
2017-04-02 22:15:14.785317: step 1630, loss = 1.68 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 22:15:18.685515: step 1640, loss = 1.95 (328.2 examples/sec; 0.390 sec/batch)
2017-04-02 22:15:22.568194: step 1650, loss = 1.79 (329.7 examples/sec; 0.388 sec/batch)
2017-04-02 22:15:26.462052: step 1660, loss = 1.92 (328.7 examples/sec; 0.389 sec/batch)
2017-04-02 22:15:30.360539: step 1670, loss = 1.91 (328.3 examples/sec; 0.390 sec/batch)
2017-04-02 22:15:34.233261: step 1680, loss = 1.81 (330.5 examples/sec; 0.387 sec/batch)
2017-04-02 22:15:38.290174: step 1690, loss = 1.97 (315.5 examples/sec; 0.406 sec/batch)
2017-04-02 22:15:43.170915: step 1700, loss = 1.84 (262.3 examples/sec; 0.488 sec/batch)
2017-04-02 22:15:47.928905: step 1710, loss = 1.88 (269.0 examples/sec; 0.476 sec/batch)
2017-04-02 22:15:52.002567: step 1720, loss = 1.62 (314.2 examples/sec; 0.407 sec/batch)
2017-04-02 22:15:55.935188: step 1730, loss = 1.72 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 22:15:59.833567: step 1740, loss = 1.81 (328.3 examples/sec; 0.390 sec/batch)
2017-04-02 22:16:03.709955: step 1750, loss = 1.80 (330.2 examples/sec; 0.388 sec/batch)
2017-04-02 22:16:07.628373: step 1760, loss = 1.74 (326.7 examples/sec; 0.392 sec/batch)
2017-04-02 22:16:11.536404: step 1770, loss = 1.83 (327.5 examples/sec; 0.391 sec/batch)
2017-04-02 22:16:15.473972: step 1780, loss = 1.87 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 22:16:19.340751: step 1790, loss = 1.82 (331.0 examples/sec; 0.387 sec/batch)
2017-04-02 22:16:23.297412: step 1800, loss = 1.69 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:16:27.184948: step 1810, loss = 1.65 (329.3 examples/sec; 0.389 sec/batch)
2017-04-02 22:16:31.078561: step 1820, loss = 1.75 (328.7 examples/sec; 0.389 sec/batch)
2017-04-02 22:16:34.971778: step 1830, loss = 1.76 (328.8 examples/sec; 0.389 sec/batch)
2017-04-02 22:16:38.880352: step 1840, loss = 1.69 (327.5 examples/sec; 0.391 sec/batch)
2017-04-02 22:16:42.788323: step 1850, loss = 1.77 (327.5 examples/sec; 0.391 sec/batch)
2017-04-02 22:16:46.679625: step 1860, loss = 1.76 (328.9 examples/sec; 0.389 sec/batch)
2017-04-02 22:16:50.558463: step 1870, loss = 1.88 (330.0 examples/sec; 0.388 sec/batch)
2017-04-02 22:16:54.475458: step 1880, loss = 1.52 (326.8 examples/sec; 0.392 sec/batch)
2017-04-02 22:16:58.376955: step 1890, loss = 1.67 (328.1 examples/sec; 0.390 sec/batch)
2017-04-02 22:17:02.329213: step 1900, loss = 1.71 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 22:17:06.227654: step 1910, loss = 1.60 (328.3 examples/sec; 0.390 sec/batch)
2017-04-02 22:17:10.117992: step 1920, loss = 1.73 (329.0 examples/sec; 0.389 sec/batch)
2017-04-02 22:17:14.027904: step 1930, loss = 1.72 (327.4 examples/sec; 0.391 sec/batch)
2017-04-02 22:17:17.928664: step 1940, loss = 1.67 (328.1 examples/sec; 0.390 sec/batch)
2017-04-02 22:17:21.832548: step 1950, loss = 1.61 (327.9 examples/sec; 0.390 sec/batch)
2017-04-02 22:17:25.724666: step 1960, loss = 1.57 (328.9 examples/sec; 0.389 sec/batch)
2017-04-02 22:17:29.630098: step 1970, loss = 1.61 (327.7 examples/sec; 0.391 sec/batch)
2017-04-02 22:17:33.561580: step 1980, loss = 1.77 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 22:17:37.456328: step 1990, loss = 1.63 (328.6 examples/sec; 0.389 sec/batch)
2017-04-02 22:17:41.422379: step 2000, loss = 1.58 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 22:17:45.295091: step 2010, loss = 1.89 (330.5 examples/sec; 0.387 sec/batch)
2017-04-02 22:17:49.165467: step 2020, loss = 1.62 (330.7 examples/sec; 0.387 sec/batch)
2017-04-02 22:17:53.043143: step 2030, loss = 1.62 (330.1 examples/sec; 0.388 sec/batch)
2017-04-02 22:20:16.480817: precision @ 1 = 0.663

2017-04-02 22:20:53.623029: step 2040, loss = 1.62 (7.1 examples/sec; 18.058 sec/batch)
2017-04-02 22:20:57.503379: step 2050, loss = 1.94 (329.9 examples/sec; 0.388 sec/batch)
2017-04-02 22:21:01.413426: step 2060, loss = 1.69 (327.4 examples/sec; 0.391 sec/batch)
2017-04-02 22:21:05.308822: step 2070, loss = 1.63 (328.6 examples/sec; 0.390 sec/batch)
2017-04-02 22:21:09.174892: step 2080, loss = 1.59 (331.1 examples/sec; 0.387 sec/batch)
2017-04-02 22:21:13.059287: step 2090, loss = 1.54 (329.5 examples/sec; 0.388 sec/batch)
2017-04-02 22:21:17.099031: step 2100, loss = 1.63 (316.9 examples/sec; 0.404 sec/batch)
2017-04-02 22:21:20.969417: step 2110, loss = 1.78 (330.7 examples/sec; 0.387 sec/batch)
2017-04-02 22:21:24.822816: step 2120, loss = 1.54 (332.2 examples/sec; 0.385 sec/batch)
2017-04-02 22:21:28.720735: step 2130, loss = 1.42 (328.4 examples/sec; 0.390 sec/batch)
2017-04-02 22:21:32.607371: step 2140, loss = 1.70 (329.3 examples/sec; 0.389 sec/batch)
2017-04-02 22:21:36.505813: step 2150, loss = 1.49 (328.3 examples/sec; 0.390 sec/batch)
2017-04-02 22:21:40.396796: step 2160, loss = 1.67 (329.0 examples/sec; 0.389 sec/batch)
2017-04-02 22:21:44.240987: step 2170, loss = 1.72 (333.0 examples/sec; 0.384 sec/batch)
2017-04-02 22:21:48.122525: step 2180, loss = 1.76 (329.8 examples/sec; 0.388 sec/batch)
2017-04-02 22:21:52.005470: step 2190, loss = 1.49 (329.6 examples/sec; 0.388 sec/batch)
2017-04-02 22:21:55.944385: step 2200, loss = 1.63 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 22:21:59.806392: step 2210, loss = 1.45 (331.4 examples/sec; 0.386 sec/batch)
2017-04-02 22:22:03.695802: step 2220, loss = 1.55 (329.1 examples/sec; 0.389 sec/batch)
2017-04-02 22:22:07.588030: step 2230, loss = 1.59 (328.9 examples/sec; 0.389 sec/batch)
2017-04-02 22:22:11.463397: step 2240, loss = 1.41 (330.3 examples/sec; 0.388 sec/batch)
2017-04-02 22:22:15.364424: step 2250, loss = 1.73 (328.1 examples/sec; 0.390 sec/batch)
2017-04-02 22:22:19.251706: step 2260, loss = 1.54 (329.3 examples/sec; 0.389 sec/batch)
2017-04-02 22:22:23.136667: step 2270, loss = 1.55 (329.5 examples/sec; 0.388 sec/batch)
2017-04-02 22:22:27.067345: step 2280, loss = 1.51 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 22:22:30.963084: step 2290, loss = 1.54 (328.6 examples/sec; 0.390 sec/batch)
2017-04-02 22:22:34.883243: step 2300, loss = 1.39 (326.5 examples/sec; 0.392 sec/batch)
2017-04-02 22:22:38.752741: step 2310, loss = 1.50 (330.8 examples/sec; 0.387 sec/batch)
2017-04-02 22:22:42.616455: step 2320, loss = 1.47 (331.3 examples/sec; 0.386 sec/batch)
2017-04-02 22:22:46.510988: step 2330, loss = 1.52 (328.7 examples/sec; 0.389 sec/batch)
2017-04-02 22:22:50.408949: step 2340, loss = 1.31 (328.4 examples/sec; 0.390 sec/batch)
2017-04-02 22:22:54.290063: step 2350, loss = 1.53 (329.8 examples/sec; 0.388 sec/batch)
2017-04-02 22:22:58.195939: step 2360, loss = 1.38 (327.7 examples/sec; 0.391 sec/batch)
2017-04-02 22:23:02.082027: step 2370, loss = 1.57 (329.4 examples/sec; 0.389 sec/batch)
2017-04-02 22:23:05.974715: step 2380, loss = 1.42 (328.8 examples/sec; 0.389 sec/batch)
2017-04-02 22:23:09.868757: step 2390, loss = 1.47 (328.7 examples/sec; 0.389 sec/batch)
2017-04-02 22:23:13.823737: step 2400, loss = 1.44 (323.6 examples/sec; 0.395 sec/batch)
2017-04-02 22:23:17.733635: step 2410, loss = 1.50 (327.4 examples/sec; 0.391 sec/batch)
2017-04-02 22:23:21.622069: step 2420, loss = 1.44 (329.2 examples/sec; 0.389 sec/batch)
2017-04-02 22:23:25.500750: step 2430, loss = 1.48 (330.0 examples/sec; 0.388 sec/batch)
2017-04-02 22:23:29.388929: step 2440, loss = 1.41 (329.2 examples/sec; 0.389 sec/batch)
2017-04-02 22:23:33.233254: step 2450, loss = 1.29 (333.0 examples/sec; 0.384 sec/batch)
2017-04-02 22:23:37.107979: step 2460, loss = 1.36 (330.3 examples/sec; 0.387 sec/batch)
2017-04-02 22:23:41.007105: step 2470, loss = 1.32 (328.3 examples/sec; 0.390 sec/batch)
2017-04-02 22:23:44.897687: step 2480, loss = 1.40 (329.0 examples/sec; 0.389 sec/batch)
2017-04-02 22:23:48.764684: step 2490, loss = 1.47 (331.0 examples/sec; 0.387 sec/batch)
2017-04-02 22:23:52.682806: step 2500, loss = 1.39 (326.7 examples/sec; 0.392 sec/batch)
2017-04-02 22:23:56.583893: step 2510, loss = 1.44 (328.1 examples/sec; 0.390 sec/batch)
2017-04-02 22:24:00.471365: step 2520, loss = 1.34 (329.3 examples/sec; 0.389 sec/batch)
2017-04-02 22:24:04.360725: step 2530, loss = 1.46 (329.1 examples/sec; 0.389 sec/batch)
2017-04-02 22:24:08.258562: step 2540, loss = 1.55 (328.4 examples/sec; 0.390 sec/batch)
2017-04-02 22:24:12.123202: step 2550, loss = 1.60 (331.2 examples/sec; 0.386 sec/batch)
2017-04-02 22:24:16.028877: step 2560, loss = 1.48 (327.7 examples/sec; 0.391 sec/batch)
2017-04-02 22:24:27.817807: step 2570, loss = 1.48 (108.6 examples/sec; 1.179 sec/batch)
2017-04-02 22:24:31.709039: step 2580, loss = 1.34 (328.9 examples/sec; 0.389 sec/batch)
2017-04-02 22:24:35.601551: step 2590, loss = 1.71 (328.8 examples/sec; 0.389 sec/batch)
2017-04-02 22:24:39.563753: step 2600, loss = 1.61 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 22:24:43.423967: step 2610, loss = 1.43 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 22:24:47.322653: step 2620, loss = 1.54 (328.3 examples/sec; 0.390 sec/batch)
2017-04-02 22:24:51.208626: step 2630, loss = 1.51 (329.4 examples/sec; 0.389 sec/batch)
2017-04-02 22:24:55.095485: step 2640, loss = 1.56 (329.3 examples/sec; 0.389 sec/batch)
2017-04-02 22:24:58.989845: step 2650, loss = 1.31 (328.7 examples/sec; 0.389 sec/batch)
2017-04-02 22:25:02.864526: step 2660, loss = 1.31 (330.3 examples/sec; 0.387 sec/batch)
2017-04-02 22:25:06.731990: step 2670, loss = 1.38 (331.0 examples/sec; 0.387 sec/batch)
2017-04-02 22:25:10.619819: step 2680, loss = 1.22 (329.2 examples/sec; 0.389 sec/batch)
2017-04-02 22:25:14.504644: step 2690, loss = 1.20 (329.5 examples/sec; 0.388 sec/batch)
2017-04-02 22:25:18.441065: step 2700, loss = 1.61 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 22:25:22.322378: step 2710, loss = 1.32 (329.8 examples/sec; 0.388 sec/batch)
2017-04-02 22:25:26.241264: step 2720, loss = 1.40 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 22:25:30.117283: step 2730, loss = 1.37 (330.2 examples/sec; 0.388 sec/batch)
2017-04-02 22:25:34.006172: step 2740, loss = 1.23 (329.1 examples/sec; 0.389 sec/batch)
2017-04-02 22:25:37.864500: step 2750, loss = 1.41 (331.8 examples/sec; 0.386 sec/batch)
2017-04-02 22:25:41.763080: step 2760, loss = 1.44 (328.3 examples/sec; 0.390 sec/batch)
2017-04-02 22:25:45.621457: step 2770, loss = 1.22 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 22:25:49.500272: step 2780, loss = 1.26 (330.0 examples/sec; 0.388 sec/batch)
2017-04-02 22:25:53.336433: step 2790, loss = 1.29 (333.7 examples/sec; 0.384 sec/batch)
2017-04-02 22:25:57.257367: step 2800, loss = 1.21 (326.5 examples/sec; 0.392 sec/batch)
2017-04-02 22:26:01.143720: step 2810, loss = 1.32 (329.4 examples/sec; 0.389 sec/batch)
2017-04-02 22:26:05.024865: step 2820, loss = 1.36 (329.8 examples/sec; 0.388 sec/batch)
2017-04-02 22:26:08.909343: step 2830, loss = 1.29 (329.5 examples/sec; 0.388 sec/batch)
2017-04-02 22:26:12.818330: step 2840, loss = 1.33 (327.5 examples/sec; 0.391 sec/batch)
2017-04-02 22:26:16.755033: step 2850, loss = 1.27 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 22:26:20.646233: step 2860, loss = 1.39 (328.9 examples/sec; 0.389 sec/batch)
2017-04-02 22:26:24.517785: step 2870, loss = 1.35 (330.6 examples/sec; 0.387 sec/batch)
2017-04-02 22:26:28.444371: step 2880, loss = 1.23 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 22:26:32.378573: step 2890, loss = 1.18 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 22:26:36.487300: step 2900, loss = 1.37 (311.5 examples/sec; 0.411 sec/batch)
2017-04-02 22:26:40.496457: step 2910, loss = 1.32 (319.3 examples/sec; 0.401 sec/batch)
2017-04-02 22:26:44.438439: step 2920, loss = 1.31 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 22:26:48.462848: step 2930, loss = 1.32 (318.1 examples/sec; 0.402 sec/batch)
2017-04-02 22:26:52.552481: step 2940, loss = 1.30 (313.0 examples/sec; 0.409 sec/batch)
2017-04-02 22:26:56.463920: step 2950, loss = 1.37 (327.2 examples/sec; 0.391 sec/batch)
2017-04-02 22:27:00.365899: step 2960, loss = 1.20 (328.0 examples/sec; 0.390 sec/batch)
2017-04-02 22:27:04.265251: step 2970, loss = 1.15 (328.3 examples/sec; 0.390 sec/batch)
2017-04-02 22:27:08.160725: step 2980, loss = 1.27 (328.6 examples/sec; 0.390 sec/batch)
2017-04-02 22:27:12.076553: step 2990, loss = 1.20 (326.9 examples/sec; 0.392 sec/batch)
2017-04-02 22:27:16.024713: step 3000, loss = 1.17 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 22:27:19.905934: step 3010, loss = 1.33 (329.8 examples/sec; 0.388 sec/batch)
2017-04-02 22:27:23.795547: step 3020, loss = 1.30 (329.1 examples/sec; 0.389 sec/batch)
2017-04-02 22:27:27.683666: step 3030, loss = 1.34 (329.2 examples/sec; 0.389 sec/batch)
2017-04-02 22:27:31.588138: step 3040, loss = 1.51 (327.8 examples/sec; 0.390 sec/batch)
2017-04-02 22:27:35.504595: step 3050, loss = 1.30 (326.8 examples/sec; 0.392 sec/batch)
2017-04-02 22:27:39.411976: step 3060, loss = 1.17 (327.6 examples/sec; 0.391 sec/batch)
2017-04-02 22:27:43.306194: step 3070, loss = 1.26 (328.7 examples/sec; 0.389 sec/batch)
2017-04-02 22:27:47.232176: step 3080, loss = 1.19 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 22:27:51.142264: step 3090, loss = 1.07 (327.4 examples/sec; 0.391 sec/batch)
2017-04-02 22:27:55.102755: step 3100, loss = 1.37 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 22:27:58.973430: step 3110, loss = 1.47 (330.7 examples/sec; 0.387 sec/batch)
2017-04-02 22:28:02.841971: step 3120, loss = 1.11 (330.9 examples/sec; 0.387 sec/batch)
2017-04-02 22:28:06.756289: step 3130, loss = 1.28 (327.0 examples/sec; 0.391 sec/batch)
2017-04-02 22:28:10.642630: step 3140, loss = 1.25 (329.4 examples/sec; 0.389 sec/batch)
2017-04-02 22:28:14.515026: step 3150, loss = 1.19 (330.5 examples/sec; 0.387 sec/batch)
2017-04-02 22:28:18.384857: step 3160, loss = 1.37 (330.8 examples/sec; 0.387 sec/batch)
2017-04-02 22:28:22.281165: step 3170, loss = 1.17 (328.5 examples/sec; 0.390 sec/batch)
2017-04-02 22:28:26.193230: step 3180, loss = 1.43 (327.2 examples/sec; 0.391 sec/batch)
2017-04-02 22:28:30.087605: step 3190, loss = 1.19 (328.7 examples/sec; 0.389 sec/batch)
2017-04-02 22:28:34.076148: step 3200, loss = 1.26 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 22:28:38.031363: step 3210, loss = 1.47 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 22:28:41.902542: step 3220, loss = 1.07 (330.6 examples/sec; 0.387 sec/batch)
2017-04-02 22:28:45.783801: step 3230, loss = 1.20 (329.8 examples/sec; 0.388 sec/batch)
2017-04-02 22:28:49.683615: step 3240, loss = 1.14 (328.2 examples/sec; 0.390 sec/batch)
2017-04-02 22:28:53.557399: step 3250, loss = 1.05 (330.4 examples/sec; 0.387 sec/batch)
2017-04-02 22:28:57.447926: step 3260, loss = 1.16 (329.0 examples/sec; 0.389 sec/batch)
2017-04-02 22:29:01.349515: step 3270, loss = 1.32 (328.1 examples/sec; 0.390 sec/batch)
2017-04-02 22:29:05.234787: step 3280, loss = 1.10 (329.4 examples/sec; 0.389 sec/batch)
2017-04-02 22:29:09.162401: step 3290, loss = 1.14 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 22:29:13.141871: step 3300, loss = 1.29 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 22:29:17.049506: step 3310, loss = 1.31 (327.6 examples/sec; 0.391 sec/batch)
2017-04-02 22:29:20.932629: step 3320, loss = 1.00 (329.6 examples/sec; 0.388 sec/batch)
2017-04-02 22:29:24.839767: step 3330, loss = 1.24 (327.6 examples/sec; 0.391 sec/batch)
2017-04-02 22:29:28.730283: step 3340, loss = 1.44 (329.0 examples/sec; 0.389 sec/batch)
2017-04-02 22:29:32.621525: step 3350, loss = 1.13 (328.9 examples/sec; 0.389 sec/batch)
2017-04-02 22:29:36.533774: step 3360, loss = 1.21 (327.2 examples/sec; 0.391 sec/batch)
2017-04-02 22:29:40.472625: step 3370, loss = 1.33 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 22:29:44.386548: step 3380, loss = 1.07 (327.0 examples/sec; 0.391 sec/batch)
2017-04-02 22:29:48.282053: step 3390, loss = 1.29 (328.6 examples/sec; 0.390 sec/batch)
2017-04-02 22:29:52.257672: step 3400, loss = 1.14 (322.0 examples/sec; 0.398 sec/batch)
2017-04-02 22:29:56.188303: step 3410, loss = 1.08 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 22:30:00.150041: step 3420, loss = 1.32 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 22:30:04.084330: step 3430, loss = 1.37 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 22:30:07.976875: step 3440, loss = 1.09 (328.8 examples/sec; 0.389 sec/batch)
2017-04-02 22:30:11.897150: step 3450, loss = 1.17 (326.5 examples/sec; 0.392 sec/batch)
2017-04-02 22:30:15.795912: step 3460, loss = 1.15 (328.3 examples/sec; 0.390 sec/batch)
2017-04-02 22:30:19.673063: step 3470, loss = 1.25 (330.1 examples/sec; 0.388 sec/batch)
2017-04-02 22:30:23.609630: step 3480, loss = 1.32 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 22:30:27.493778: step 3490, loss = 1.21 (329.5 examples/sec; 0.388 sec/batch)
2017-04-02 22:30:31.498513: step 3500, loss = 1.22 (319.6 examples/sec; 0.400 sec/batch)
2017-04-02 22:30:35.415495: step 3510, loss = 1.24 (326.8 examples/sec; 0.392 sec/batch)
2017-04-02 22:30:39.340229: step 3520, loss = 1.24 (326.1 examples/sec; 0.392 sec/batch)
2017-04-02 22:30:43.252820: step 3530, loss = 1.25 (327.1 examples/sec; 0.391 sec/batch)
2017-04-02 22:30:47.141007: step 3540, loss = 1.35 (329.2 examples/sec; 0.389 sec/batch)
2017-04-02 22:30:51.053372: step 3550, loss = 1.36 (327.2 examples/sec; 0.391 sec/batch)
2017-04-02 22:30:54.961306: step 3560, loss = 1.39 (327.5 examples/sec; 0.391 sec/batch)
2017-04-02 22:30:58.857514: step 3570, loss = 1.05 (328.5 examples/sec; 0.390 sec/batch)
2017-04-02 22:31:02.748546: step 3580, loss = 1.17 (329.0 examples/sec; 0.389 sec/batch)
2017-04-02 22:31:06.646332: step 3590, loss = 1.15 (328.4 examples/sec; 0.390 sec/batch)
2017-04-02 22:31:10.596141: step 3600, loss = 1.26 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 22:31:14.517947: step 3610, loss = 1.20 (326.4 examples/sec; 0.392 sec/batch)
2017-04-02 22:31:18.423535: step 3620, loss = 1.17 (327.7 examples/sec; 0.391 sec/batch)
2017-04-02 22:31:22.306840: step 3630, loss = 1.16 (329.6 examples/sec; 0.388 sec/batch)
2017-04-02 22:31:26.182209: step 3640, loss = 1.10 (330.3 examples/sec; 0.388 sec/batch)
2017-04-02 22:31:30.068699: step 3650, loss = 1.20 (329.3 examples/sec; 0.389 sec/batch)
2017-04-02 22:31:34.004728: step 3660, loss = 1.20 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 22:31:37.999835: step 3670, loss = 1.24 (320.4 examples/sec; 0.400 sec/batch)
2017-04-02 22:31:42.087641: step 3680, loss = 1.19 (313.1 examples/sec; 0.409 sec/batch)
2017-04-02 22:31:45.999118: step 3690, loss = 1.25 (327.2 examples/sec; 0.391 sec/batch)
2017-04-02 22:31:50.074033: step 3700, loss = 1.20 (314.1 examples/sec; 0.407 sec/batch)
2017-04-02 22:31:54.028362: step 3710, loss = 1.11 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 22:31:58.168450: step 3720, loss = 1.03 (309.2 examples/sec; 0.414 sec/batch)
2017-04-02 22:32:02.080137: step 3730, loss = 1.46 (327.2 examples/sec; 0.391 sec/batch)
2017-04-02 22:32:05.945305: step 3740, loss = 1.37 (331.2 examples/sec; 0.387 sec/batch)
2017-04-02 22:32:09.871575: step 3750, loss = 1.30 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 22:32:13.828094: step 3760, loss = 1.46 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:32:17.780103: step 3770, loss = 1.09 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 22:32:21.694789: step 3780, loss = 1.14 (327.0 examples/sec; 0.391 sec/batch)
2017-04-02 22:32:25.607101: step 3790, loss = 1.17 (327.2 examples/sec; 0.391 sec/batch)
2017-04-02 22:32:29.586245: step 3800, loss = 1.09 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 22:32:33.528396: step 3810, loss = 1.18 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 22:32:37.446659: step 3820, loss = 1.16 (326.7 examples/sec; 0.392 sec/batch)
2017-04-02 22:32:41.351224: step 3830, loss = 1.14 (327.8 examples/sec; 0.390 sec/batch)
2017-04-02 22:32:45.335647: step 3840, loss = 0.98 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 22:32:49.287222: step 3850, loss = 0.96 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 22:32:53.213546: step 3860, loss = 1.08 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 22:32:57.139889: step 3870, loss = 0.93 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 22:33:01.070261: step 3880, loss = 1.13 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 22:33:05.007139: step 3890, loss = 1.06 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 22:33:09.042017: step 3900, loss = 1.04 (317.2 examples/sec; 0.403 sec/batch)
2017-04-02 22:33:12.998515: step 3910, loss = 1.12 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:33:16.931059: step 3920, loss = 1.18 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 22:33:20.888780: step 3930, loss = 1.22 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 22:33:24.753918: step 3940, loss = 1.12 (331.2 examples/sec; 0.387 sec/batch)
2017-04-02 22:33:28.613601: step 3950, loss = 1.10 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 22:33:32.535791: step 3960, loss = 1.31 (326.3 examples/sec; 0.392 sec/batch)
2017-04-02 22:33:36.414617: step 3970, loss = 0.90 (330.0 examples/sec; 0.388 sec/batch)
2017-04-02 22:33:40.291742: step 3980, loss = 1.25 (330.1 examples/sec; 0.388 sec/batch)
2017-04-02 22:33:44.194909: step 3990, loss = 1.11 (327.9 examples/sec; 0.390 sec/batch)
2017-04-02 22:33:48.150280: step 4000, loss = 1.11 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 22:33:52.036995: step 4010, loss = 1.09 (329.3 examples/sec; 0.389 sec/batch)
2017-04-02 22:33:55.889593: step 4020, loss = 1.20 (332.2 examples/sec; 0.385 sec/batch)
2017-04-02 22:33:59.730152: step 4030, loss = 1.25 (333.3 examples/sec; 0.384 sec/batch)
2017-04-02 22:34:03.629288: step 4040, loss = 1.09 (328.3 examples/sec; 0.390 sec/batch)
2017-04-02 22:34:07.532090: step 4050, loss = 1.20 (328.0 examples/sec; 0.390 sec/batch)
2017-04-02 22:35:29.287700: precision @ 1 = 0.727

2017-04-02 22:36:06.697373: step 4060, loss = 1.13 (10.7 examples/sec; 11.917 sec/batch)
2017-04-02 22:36:10.646164: step 4070, loss = 1.19 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 22:36:14.592966: step 4080, loss = 1.05 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 22:36:18.517941: step 4090, loss = 1.13 (326.1 examples/sec; 0.392 sec/batch)
2017-04-02 22:36:22.510191: step 4100, loss = 1.15 (320.6 examples/sec; 0.399 sec/batch)
2017-04-02 22:36:26.433413: step 4110, loss = 1.17 (326.3 examples/sec; 0.392 sec/batch)
2017-04-02 22:36:30.369427: step 4120, loss = 1.29 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 22:36:34.333778: step 4130, loss = 0.96 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 22:36:38.213847: step 4140, loss = 1.05 (329.9 examples/sec; 0.388 sec/batch)
2017-04-02 22:36:42.065234: step 4150, loss = 1.06 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 22:36:46.008916: step 4160, loss = 0.98 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 22:36:49.830922: step 4170, loss = 0.98 (334.9 examples/sec; 0.382 sec/batch)
2017-04-02 22:36:53.685270: step 4180, loss = 1.43 (332.1 examples/sec; 0.385 sec/batch)
2017-04-02 22:36:57.539315: step 4190, loss = 1.13 (332.1 examples/sec; 0.385 sec/batch)
2017-04-02 22:37:01.453799: step 4200, loss = 1.23 (327.0 examples/sec; 0.391 sec/batch)
2017-04-02 22:37:05.351381: step 4210, loss = 1.19 (328.4 examples/sec; 0.390 sec/batch)
2017-04-02 22:37:09.230254: step 4220, loss = 1.10 (330.0 examples/sec; 0.388 sec/batch)
2017-04-02 22:37:13.088197: step 4230, loss = 1.17 (331.8 examples/sec; 0.386 sec/batch)
2017-04-02 22:37:16.970211: step 4240, loss = 0.95 (329.7 examples/sec; 0.388 sec/batch)
2017-04-02 22:37:20.817491: step 4250, loss = 1.15 (332.7 examples/sec; 0.385 sec/batch)
2017-04-02 22:37:24.675261: step 4260, loss = 1.35 (331.8 examples/sec; 0.386 sec/batch)
2017-04-02 22:37:28.546956: step 4270, loss = 1.15 (330.6 examples/sec; 0.387 sec/batch)
2017-04-02 22:37:32.426451: step 4280, loss = 1.03 (329.9 examples/sec; 0.388 sec/batch)
2017-04-02 22:37:36.287712: step 4290, loss = 1.10 (331.5 examples/sec; 0.386 sec/batch)
2017-04-02 22:37:40.244467: step 4300, loss = 1.05 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:37:44.102377: step 4310, loss = 1.19 (331.8 examples/sec; 0.386 sec/batch)
2017-04-02 22:37:47.954355: step 4320, loss = 1.11 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 22:37:51.812418: step 4330, loss = 1.20 (331.8 examples/sec; 0.386 sec/batch)
2017-04-02 22:37:55.676947: step 4340, loss = 1.01 (331.2 examples/sec; 0.386 sec/batch)
2017-04-02 22:37:59.624964: step 4350, loss = 1.11 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 22:38:03.823371: step 4360, loss = 0.98 (304.9 examples/sec; 0.420 sec/batch)
2017-04-02 22:38:08.031288: step 4370, loss = 1.11 (304.2 examples/sec; 0.421 sec/batch)
2017-04-02 22:38:12.355066: step 4380, loss = 1.11 (296.0 examples/sec; 0.432 sec/batch)
2017-04-02 22:38:16.718380: step 4390, loss = 1.16 (293.4 examples/sec; 0.436 sec/batch)
2017-04-02 22:38:21.238873: step 4400, loss = 0.96 (283.2 examples/sec; 0.452 sec/batch)
2017-04-02 22:38:25.730454: step 4410, loss = 1.23 (285.0 examples/sec; 0.449 sec/batch)
2017-04-02 22:38:30.185603: step 4420, loss = 0.97 (287.3 examples/sec; 0.446 sec/batch)
2017-04-02 22:38:34.478677: step 4430, loss = 1.03 (298.2 examples/sec; 0.429 sec/batch)
2017-04-02 22:38:38.713471: step 4440, loss = 0.95 (302.3 examples/sec; 0.423 sec/batch)
2017-04-02 22:38:43.049644: step 4450, loss = 1.34 (295.2 examples/sec; 0.434 sec/batch)
2017-04-02 22:38:47.032160: step 4460, loss = 1.01 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 22:38:50.908975: step 4470, loss = 1.18 (330.2 examples/sec; 0.388 sec/batch)
2017-04-02 22:38:54.761348: step 4480, loss = 1.06 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 22:38:58.634762: step 4490, loss = 0.96 (330.5 examples/sec; 0.387 sec/batch)
2017-04-02 22:39:02.583404: step 4500, loss = 1.18 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 22:39:06.485556: step 4510, loss = 1.14 (328.0 examples/sec; 0.390 sec/batch)
2017-04-02 22:39:10.376644: step 4520, loss = 1.09 (329.0 examples/sec; 0.389 sec/batch)
2017-04-02 22:39:14.220440: step 4530, loss = 1.02 (333.0 examples/sec; 0.384 sec/batch)
2017-04-02 22:39:19.922006: step 4540, loss = 0.96 (224.5 examples/sec; 0.570 sec/batch)
2017-04-02 22:39:28.294111: step 4550, loss = 1.00 (152.9 examples/sec; 0.837 sec/batch)
2017-04-02 22:39:35.177820: step 4560, loss = 1.03 (185.9 examples/sec; 0.688 sec/batch)
2017-04-02 22:39:39.845642: step 4570, loss = 1.09 (274.2 examples/sec; 0.467 sec/batch)
2017-04-02 22:39:43.756630: step 4580, loss = 0.91 (327.3 examples/sec; 0.391 sec/batch)
2017-04-02 22:39:47.649904: step 4590, loss = 0.96 (328.8 examples/sec; 0.389 sec/batch)
2017-04-02 22:39:51.608925: step 4600, loss = 0.99 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 22:39:55.520805: step 4610, loss = 0.89 (327.2 examples/sec; 0.391 sec/batch)
2017-04-02 22:39:59.428558: step 4620, loss = 1.00 (327.6 examples/sec; 0.391 sec/batch)
2017-04-02 22:40:03.345291: step 4630, loss = 0.98 (326.8 examples/sec; 0.392 sec/batch)
2017-04-02 22:40:07.236843: step 4640, loss = 1.04 (328.9 examples/sec; 0.389 sec/batch)
2017-04-02 22:40:11.145525: step 4650, loss = 1.15 (327.5 examples/sec; 0.391 sec/batch)
2017-04-02 22:40:15.030637: step 4660, loss = 1.14 (329.5 examples/sec; 0.389 sec/batch)
2017-04-02 22:40:18.938395: step 4670, loss = 1.01 (327.6 examples/sec; 0.391 sec/batch)
2017-04-02 22:40:22.802371: step 4680, loss = 1.04 (331.3 examples/sec; 0.386 sec/batch)
2017-04-02 22:40:26.668103: step 4690, loss = 0.99 (331.1 examples/sec; 0.387 sec/batch)
2017-04-02 22:40:30.586697: step 4700, loss = 1.22 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 22:40:34.851769: step 4710, loss = 1.05 (300.1 examples/sec; 0.427 sec/batch)
2017-04-02 22:40:38.786310: step 4720, loss = 1.05 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 22:40:42.650383: step 4730, loss = 0.98 (331.3 examples/sec; 0.386 sec/batch)
2017-04-02 22:40:46.830092: step 4740, loss = 1.09 (306.2 examples/sec; 0.418 sec/batch)
2017-04-02 22:40:51.281784: step 4750, loss = 1.22 (287.5 examples/sec; 0.445 sec/batch)
2017-04-02 22:40:55.684433: step 4760, loss = 0.90 (290.7 examples/sec; 0.440 sec/batch)
2017-04-02 22:41:00.170414: step 4770, loss = 1.15 (285.3 examples/sec; 0.449 sec/batch)
2017-04-02 22:41:04.644661: step 4780, loss = 0.95 (286.1 examples/sec; 0.447 sec/batch)
2017-04-02 22:41:08.973886: step 4790, loss = 0.79 (295.7 examples/sec; 0.433 sec/batch)
2017-04-02 22:41:13.492233: step 4800, loss = 1.07 (283.3 examples/sec; 0.452 sec/batch)
2017-04-02 22:41:17.887808: step 4810, loss = 1.01 (291.2 examples/sec; 0.440 sec/batch)
2017-04-02 22:41:22.317531: step 4820, loss = 0.92 (289.0 examples/sec; 0.443 sec/batch)
2017-04-02 22:41:26.891498: step 4830, loss = 0.88 (279.8 examples/sec; 0.457 sec/batch)
2017-04-02 22:41:31.280981: step 4840, loss = 0.85 (291.6 examples/sec; 0.439 sec/batch)
2017-04-02 22:41:35.687851: step 4850, loss = 1.02 (290.5 examples/sec; 0.441 sec/batch)
2017-04-02 22:41:40.057333: step 4860, loss = 1.02 (292.9 examples/sec; 0.437 sec/batch)
2017-04-02 22:41:44.254117: step 4870, loss = 0.91 (305.0 examples/sec; 0.420 sec/batch)
2017-04-02 22:41:48.491495: step 4880, loss = 1.11 (302.1 examples/sec; 0.424 sec/batch)
2017-04-02 22:41:52.957249: step 4890, loss = 1.17 (286.6 examples/sec; 0.447 sec/batch)
2017-04-02 22:41:56.873885: step 4900, loss = 0.99 (326.8 examples/sec; 0.392 sec/batch)
2017-04-02 22:42:00.788224: step 4910, loss = 1.26 (327.0 examples/sec; 0.391 sec/batch)
2017-04-02 22:42:04.729506: step 4920, loss = 0.94 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 22:42:08.668030: step 4930, loss = 0.89 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 22:42:12.556235: step 4940, loss = 0.98 (329.2 examples/sec; 0.389 sec/batch)
2017-04-02 22:42:16.422049: step 4950, loss = 1.04 (331.1 examples/sec; 0.387 sec/batch)
2017-04-02 22:42:20.518456: step 4960, loss = 1.26 (312.5 examples/sec; 0.410 sec/batch)
2017-04-02 22:42:24.515300: step 4970, loss = 0.98 (320.3 examples/sec; 0.400 sec/batch)
2017-04-02 22:42:28.528148: step 4980, loss = 1.07 (319.0 examples/sec; 0.401 sec/batch)
2017-04-02 22:42:32.530897: step 4990, loss = 1.20 (319.8 examples/sec; 0.400 sec/batch)
2017-04-02 22:42:36.596183: step 5000, loss = 0.91 (314.9 examples/sec; 0.407 sec/batch)
2017-04-02 22:42:40.648917: step 5010, loss = 1.10 (315.8 examples/sec; 0.405 sec/batch)
2017-04-02 22:42:44.611947: step 5020, loss = 0.86 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 22:42:48.646074: step 5030, loss = 1.09 (317.3 examples/sec; 0.403 sec/batch)
2017-04-02 22:42:52.661831: step 5040, loss = 1.03 (318.7 examples/sec; 0.402 sec/batch)
2017-04-02 22:42:56.731576: step 5050, loss = 0.99 (314.5 examples/sec; 0.407 sec/batch)
2017-04-02 22:43:00.762249: step 5060, loss = 0.96 (317.6 examples/sec; 0.403 sec/batch)
2017-04-02 22:43:04.805586: step 5070, loss = 0.89 (316.6 examples/sec; 0.404 sec/batch)
2017-04-02 22:43:08.917068: step 5080, loss = 0.85 (311.3 examples/sec; 0.411 sec/batch)
2017-04-02 22:43:13.003303: step 5090, loss = 0.95 (313.2 examples/sec; 0.409 sec/batch)
2017-04-02 22:43:17.155490: step 5100, loss = 0.97 (308.3 examples/sec; 0.415 sec/batch)
2017-04-02 22:43:21.184639: step 5110, loss = 1.07 (317.7 examples/sec; 0.403 sec/batch)
2017-04-02 22:43:25.278283: step 5120, loss = 0.99 (312.7 examples/sec; 0.409 sec/batch)
2017-04-02 22:43:29.414522: step 5130, loss = 1.04 (309.5 examples/sec; 0.414 sec/batch)
2017-04-02 22:43:33.546126: step 5140, loss = 0.95 (309.8 examples/sec; 0.413 sec/batch)
2017-04-02 22:43:37.642281: step 5150, loss = 1.06 (312.5 examples/sec; 0.410 sec/batch)
2017-04-02 22:43:41.719383: step 5160, loss = 1.03 (313.9 examples/sec; 0.408 sec/batch)
2017-04-02 22:43:45.755759: step 5170, loss = 0.92 (317.1 examples/sec; 0.404 sec/batch)
2017-04-02 22:43:49.848864: step 5180, loss = 0.96 (312.7 examples/sec; 0.409 sec/batch)
2017-04-02 22:43:53.960501: step 5190, loss = 1.28 (311.3 examples/sec; 0.411 sec/batch)
2017-04-02 22:43:58.099510: step 5200, loss = 1.00 (309.3 examples/sec; 0.414 sec/batch)
2017-04-02 22:44:01.988489: step 5210, loss = 1.09 (329.1 examples/sec; 0.389 sec/batch)
2017-04-02 22:44:05.895042: step 5220, loss = 0.95 (327.7 examples/sec; 0.391 sec/batch)
2017-04-02 22:44:09.811284: step 5230, loss = 1.05 (326.8 examples/sec; 0.392 sec/batch)
2017-04-02 22:44:13.705327: step 5240, loss = 1.08 (328.7 examples/sec; 0.389 sec/batch)
2017-04-02 22:44:17.628608: step 5250, loss = 1.03 (326.3 examples/sec; 0.392 sec/batch)
2017-04-02 22:44:21.540711: step 5260, loss = 1.20 (327.2 examples/sec; 0.391 sec/batch)
2017-04-02 22:44:25.574244: step 5270, loss = 1.12 (317.3 examples/sec; 0.403 sec/batch)
2017-04-02 22:44:29.475674: step 5280, loss = 0.95 (328.1 examples/sec; 0.390 sec/batch)
2017-04-02 22:44:33.395739: step 5290, loss = 1.01 (326.5 examples/sec; 0.392 sec/batch)
2017-04-02 22:44:37.331104: step 5300, loss = 1.14 (325.3 examples/sec; 0.394 sec/batch)
2017-04-02 22:44:41.314001: step 5310, loss = 1.17 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 22:44:45.164279: step 5320, loss = 1.17 (332.4 examples/sec; 0.385 sec/batch)
2017-04-02 22:44:49.011377: step 5330, loss = 1.09 (332.7 examples/sec; 0.385 sec/batch)
2017-04-02 22:44:52.857022: step 5340, loss = 0.88 (332.8 examples/sec; 0.385 sec/batch)
2017-04-02 22:44:56.721806: step 5350, loss = 1.00 (331.2 examples/sec; 0.386 sec/batch)
2017-04-02 22:45:00.610694: step 5360, loss = 1.07 (329.1 examples/sec; 0.389 sec/batch)
2017-04-02 22:45:04.443486: step 5370, loss = 0.97 (334.0 examples/sec; 0.383 sec/batch)
2017-04-02 22:45:08.284356: step 5380, loss = 1.02 (333.3 examples/sec; 0.384 sec/batch)
2017-04-02 22:45:12.148808: step 5390, loss = 1.16 (331.2 examples/sec; 0.386 sec/batch)
2017-04-02 22:45:16.033593: step 5400, loss = 1.16 (329.5 examples/sec; 0.388 sec/batch)
2017-04-02 22:45:19.914355: step 5410, loss = 0.88 (329.8 examples/sec; 0.388 sec/batch)
2017-04-02 22:45:23.765349: step 5420, loss = 1.01 (332.4 examples/sec; 0.385 sec/batch)
2017-04-02 22:45:27.605467: step 5430, loss = 1.13 (333.3 examples/sec; 0.384 sec/batch)
2017-04-02 22:45:31.453065: step 5440, loss = 1.14 (332.7 examples/sec; 0.385 sec/batch)
2017-04-02 22:45:35.302009: step 5450, loss = 0.96 (332.6 examples/sec; 0.385 sec/batch)
2017-04-02 22:45:39.158023: step 5460, loss = 1.04 (331.9 examples/sec; 0.386 sec/batch)
2017-04-02 22:45:42.998485: step 5470, loss = 1.13 (333.3 examples/sec; 0.384 sec/batch)
2017-04-02 22:45:46.858509: step 5480, loss = 0.95 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 22:45:50.698976: step 5490, loss = 1.01 (333.3 examples/sec; 0.384 sec/batch)
2017-04-02 22:45:54.574911: step 5500, loss = 1.13 (330.2 examples/sec; 0.388 sec/batch)
2017-04-02 22:45:58.437780: step 5510, loss = 0.94 (331.4 examples/sec; 0.386 sec/batch)
2017-04-02 22:46:10.508216: step 5520, loss = 1.12 (106.0 examples/sec; 1.207 sec/batch)
2017-04-02 22:46:14.454808: step 5530, loss = 1.11 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 22:46:18.522155: step 5540, loss = 1.01 (314.7 examples/sec; 0.407 sec/batch)
2017-04-02 22:46:22.404452: step 5550, loss = 1.18 (329.7 examples/sec; 0.388 sec/batch)
2017-04-02 22:46:26.426094: step 5560, loss = 1.06 (318.3 examples/sec; 0.402 sec/batch)
2017-04-02 22:46:30.372078: step 5570, loss = 1.08 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 22:46:34.493032: step 5580, loss = 0.82 (310.6 examples/sec; 0.412 sec/batch)
2017-04-02 22:46:38.468723: step 5590, loss = 0.99 (322.0 examples/sec; 0.398 sec/batch)
2017-04-02 22:46:42.360581: step 5600, loss = 0.92 (328.9 examples/sec; 0.389 sec/batch)
2017-04-02 22:46:46.231842: step 5610, loss = 1.02 (330.6 examples/sec; 0.387 sec/batch)
2017-04-02 22:46:50.078799: step 5620, loss = 0.80 (332.7 examples/sec; 0.385 sec/batch)
2017-04-02 22:46:53.939060: step 5630, loss = 0.92 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 22:46:57.814966: step 5640, loss = 1.00 (330.2 examples/sec; 0.388 sec/batch)
2017-04-02 22:47:01.677845: step 5650, loss = 1.02 (331.4 examples/sec; 0.386 sec/batch)
2017-04-02 22:47:05.514522: step 5660, loss = 0.99 (333.6 examples/sec; 0.384 sec/batch)
2017-04-02 22:47:09.364334: step 5670, loss = 0.90 (332.5 examples/sec; 0.385 sec/batch)
2017-04-02 22:47:13.213699: step 5680, loss = 0.92 (332.5 examples/sec; 0.385 sec/batch)
2017-04-02 22:47:17.062106: step 5690, loss = 1.04 (332.6 examples/sec; 0.385 sec/batch)
2017-04-02 22:47:20.973207: step 5700, loss = 1.00 (327.3 examples/sec; 0.391 sec/batch)
2017-04-02 22:47:24.840521: step 5710, loss = 0.99 (331.0 examples/sec; 0.387 sec/batch)
2017-04-02 22:47:28.697112: step 5720, loss = 0.93 (331.9 examples/sec; 0.386 sec/batch)
2017-04-02 22:47:32.566088: step 5730, loss = 0.88 (330.8 examples/sec; 0.387 sec/batch)
2017-04-02 22:47:36.460545: step 5740, loss = 1.01 (328.7 examples/sec; 0.389 sec/batch)
2017-04-02 22:47:40.323399: step 5750, loss = 0.90 (331.4 examples/sec; 0.386 sec/batch)
2017-04-02 22:47:44.176524: step 5760, loss = 0.97 (332.2 examples/sec; 0.385 sec/batch)
2017-04-02 22:47:48.068050: step 5770, loss = 1.04 (328.9 examples/sec; 0.389 sec/batch)
2017-04-02 22:47:51.933287: step 5780, loss = 0.95 (331.2 examples/sec; 0.387 sec/batch)
2017-04-02 22:47:55.801570: step 5790, loss = 0.99 (330.9 examples/sec; 0.387 sec/batch)
2017-04-02 22:47:59.732426: step 5800, loss = 0.93 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 22:48:03.581602: step 5810, loss = 1.07 (332.5 examples/sec; 0.385 sec/batch)
2017-04-02 22:48:07.441990: step 5820, loss = 0.91 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 22:48:11.291370: step 5830, loss = 0.96 (332.5 examples/sec; 0.385 sec/batch)
2017-04-02 22:48:15.121644: step 5840, loss = 1.03 (334.2 examples/sec; 0.383 sec/batch)
2017-04-02 22:48:18.973026: step 5850, loss = 1.00 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 22:48:22.784144: step 5860, loss = 0.92 (335.9 examples/sec; 0.381 sec/batch)
2017-04-02 22:48:26.628466: step 5870, loss = 0.92 (333.0 examples/sec; 0.384 sec/batch)
2017-04-02 22:48:30.470747: step 5880, loss = 1.04 (333.1 examples/sec; 0.384 sec/batch)
2017-04-02 22:48:34.324226: step 5890, loss = 0.98 (332.2 examples/sec; 0.385 sec/batch)
2017-04-02 22:48:38.225764: step 5900, loss = 0.88 (328.1 examples/sec; 0.390 sec/batch)
2017-04-02 22:48:42.061609: step 5910, loss = 1.08 (333.7 examples/sec; 0.384 sec/batch)
2017-04-02 22:48:45.928460: step 5920, loss = 0.85 (331.0 examples/sec; 0.387 sec/batch)
2017-04-02 22:48:49.756169: step 5930, loss = 0.95 (334.4 examples/sec; 0.383 sec/batch)
2017-04-02 22:48:53.600348: step 5940, loss = 0.96 (333.0 examples/sec; 0.384 sec/batch)
2017-04-02 22:48:57.460065: step 5950, loss = 0.81 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 22:49:01.303531: step 5960, loss = 0.93 (333.0 examples/sec; 0.384 sec/batch)
2017-04-02 22:49:05.160029: step 5970, loss = 0.92 (331.9 examples/sec; 0.386 sec/batch)
2017-04-02 22:49:09.001764: step 5980, loss = 0.83 (333.2 examples/sec; 0.384 sec/batch)
2017-04-02 22:49:12.878699: step 5990, loss = 0.99 (330.2 examples/sec; 0.388 sec/batch)
2017-04-02 22:49:16.910888: step 6000, loss = 0.94 (317.4 examples/sec; 0.403 sec/batch)
2017-04-02 22:49:21.073574: step 6010, loss = 0.97 (307.5 examples/sec; 0.416 sec/batch)
2017-04-02 22:51:00.428163: precision @ 1 = 0.789

2017-04-02 22:51:37.609433: step 6020, loss = 0.91 (9.4 examples/sec; 13.654 sec/batch)
2017-04-02 22:51:41.654674: step 6030, loss = 1.14 (316.4 examples/sec; 0.405 sec/batch)
2017-04-02 22:51:45.537039: step 6040, loss = 1.04 (329.7 examples/sec; 0.388 sec/batch)
2017-04-02 22:51:49.489677: step 6050, loss = 1.15 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 22:51:53.703213: step 6060, loss = 0.93 (303.8 examples/sec; 0.421 sec/batch)
2017-04-02 22:51:57.650724: step 6070, loss = 0.90 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 22:52:01.667044: step 6080, loss = 1.12 (318.7 examples/sec; 0.402 sec/batch)
2017-04-02 22:52:05.655331: step 6090, loss = 0.91 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 22:52:10.002226: step 6100, loss = 0.95 (294.5 examples/sec; 0.435 sec/batch)
2017-04-02 22:52:14.097024: step 6110, loss = 0.79 (312.6 examples/sec; 0.409 sec/batch)
2017-04-02 22:52:18.144790: step 6120, loss = 0.87 (316.2 examples/sec; 0.405 sec/batch)
2017-04-02 22:52:22.085835: step 6130, loss = 0.94 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 22:52:26.238650: step 6140, loss = 0.96 (308.2 examples/sec; 0.415 sec/batch)
2017-04-02 22:52:30.324110: step 6150, loss = 0.90 (313.3 examples/sec; 0.409 sec/batch)
2017-04-02 22:52:34.926876: step 6160, loss = 1.02 (278.1 examples/sec; 0.460 sec/batch)
2017-04-02 22:52:39.391307: step 6170, loss = 1.01 (286.7 examples/sec; 0.446 sec/batch)
2017-04-02 22:52:43.532461: step 6180, loss = 1.04 (309.1 examples/sec; 0.414 sec/batch)
2017-04-02 22:52:47.554859: step 6190, loss = 0.84 (318.2 examples/sec; 0.402 sec/batch)
2017-04-02 22:52:51.583985: step 6200, loss = 1.07 (317.7 examples/sec; 0.403 sec/batch)
2017-04-02 22:52:55.508935: step 6210, loss = 1.01 (326.1 examples/sec; 0.392 sec/batch)
2017-04-02 22:52:59.485937: step 6220, loss = 1.26 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 22:53:03.428584: step 6230, loss = 0.90 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 22:53:07.421401: step 6240, loss = 0.99 (320.6 examples/sec; 0.399 sec/batch)
2017-04-02 22:53:11.433018: step 6250, loss = 0.97 (319.1 examples/sec; 0.401 sec/batch)
2017-04-02 22:53:15.449258: step 6260, loss = 0.85 (318.7 examples/sec; 0.402 sec/batch)
2017-04-02 22:53:19.553163: step 6270, loss = 1.05 (311.9 examples/sec; 0.410 sec/batch)
2017-04-02 22:53:23.589191: step 6280, loss = 0.87 (317.1 examples/sec; 0.404 sec/batch)
2017-04-02 22:53:27.667187: step 6290, loss = 1.08 (313.9 examples/sec; 0.408 sec/batch)
2017-04-02 22:53:31.858257: step 6300, loss = 1.11 (305.4 examples/sec; 0.419 sec/batch)
2017-04-02 22:53:35.991224: step 6310, loss = 0.73 (309.7 examples/sec; 0.413 sec/batch)
2017-04-02 22:53:39.961178: step 6320, loss = 0.79 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 22:53:43.932522: step 6330, loss = 1.12 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 22:53:48.005526: step 6340, loss = 0.97 (314.3 examples/sec; 0.407 sec/batch)
2017-04-02 22:53:51.864465: step 6350, loss = 0.93 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 22:53:55.783899: step 6360, loss = 1.08 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 22:53:59.637606: step 6370, loss = 0.88 (332.1 examples/sec; 0.385 sec/batch)
2017-04-02 22:54:03.488180: step 6380, loss = 0.82 (332.4 examples/sec; 0.385 sec/batch)
2017-04-02 22:54:07.356040: step 6390, loss = 0.87 (330.9 examples/sec; 0.387 sec/batch)
2017-04-02 22:54:11.272751: step 6400, loss = 0.95 (326.8 examples/sec; 0.392 sec/batch)
2017-04-02 22:54:15.155585: step 6410, loss = 1.00 (329.7 examples/sec; 0.388 sec/batch)
2017-04-02 22:54:19.340708: step 6420, loss = 1.20 (305.8 examples/sec; 0.419 sec/batch)
2017-04-02 22:54:23.249719: step 6430, loss = 0.95 (327.4 examples/sec; 0.391 sec/batch)
2017-04-02 22:54:27.103062: step 6440, loss = 0.90 (332.2 examples/sec; 0.385 sec/batch)
2017-04-02 22:54:30.976842: step 6450, loss = 0.78 (330.4 examples/sec; 0.387 sec/batch)
2017-04-02 22:54:34.853670: step 6460, loss = 1.00 (330.2 examples/sec; 0.388 sec/batch)
2017-04-02 22:54:38.729636: step 6470, loss = 1.16 (330.2 examples/sec; 0.388 sec/batch)
2017-04-02 22:54:42.614672: step 6480, loss = 1.02 (329.5 examples/sec; 0.389 sec/batch)
2017-04-02 22:54:46.491498: step 6490, loss = 1.12 (330.2 examples/sec; 0.388 sec/batch)
2017-04-02 22:54:50.421301: step 6500, loss = 0.93 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 22:54:54.261339: step 6510, loss = 1.02 (333.3 examples/sec; 0.384 sec/batch)
2017-04-02 22:54:58.137537: step 6520, loss = 1.01 (330.2 examples/sec; 0.388 sec/batch)
2017-04-02 22:55:01.989485: step 6530, loss = 0.88 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 22:55:05.814682: step 6540, loss = 1.01 (334.6 examples/sec; 0.383 sec/batch)
2017-04-02 22:55:09.664077: step 6550, loss = 0.85 (332.5 examples/sec; 0.385 sec/batch)
2017-04-02 22:55:13.523607: step 6560, loss = 0.84 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 22:55:17.377284: step 6570, loss = 1.01 (332.2 examples/sec; 0.385 sec/batch)
2017-04-02 22:55:21.226039: step 6580, loss = 0.94 (332.6 examples/sec; 0.385 sec/batch)
2017-04-02 22:55:25.062933: step 6590, loss = 0.86 (333.6 examples/sec; 0.384 sec/batch)
2017-04-02 22:55:28.989399: step 6600, loss = 1.02 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 22:55:32.882204: step 6610, loss = 1.12 (328.8 examples/sec; 0.389 sec/batch)
2017-04-02 22:55:36.730397: step 6620, loss = 0.97 (332.6 examples/sec; 0.385 sec/batch)
2017-04-02 22:55:40.598188: step 6630, loss = 0.96 (330.9 examples/sec; 0.387 sec/batch)
2017-04-02 22:55:44.449948: step 6640, loss = 0.98 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 22:55:48.321671: step 6650, loss = 0.88 (330.6 examples/sec; 0.387 sec/batch)
2017-04-02 22:55:52.189525: step 6660, loss = 0.99 (330.9 examples/sec; 0.387 sec/batch)
2017-04-02 22:55:56.084496: step 6670, loss = 0.83 (328.6 examples/sec; 0.389 sec/batch)
2017-04-02 22:56:09.507286: step 6680, loss = 0.94 (95.4 examples/sec; 1.342 sec/batch)
2017-04-02 22:56:13.405298: step 6690, loss = 0.81 (328.4 examples/sec; 0.390 sec/batch)
2017-04-02 22:56:17.322899: step 6700, loss = 0.92 (326.7 examples/sec; 0.392 sec/batch)
2017-04-02 22:56:21.212694: step 6710, loss = 0.99 (329.1 examples/sec; 0.389 sec/batch)
2017-04-02 22:56:25.098072: step 6720, loss = 0.85 (329.4 examples/sec; 0.389 sec/batch)
2017-04-02 22:56:28.964505: step 6730, loss = 0.83 (331.1 examples/sec; 0.387 sec/batch)
2017-04-02 22:56:32.863207: step 6740, loss = 1.01 (328.3 examples/sec; 0.390 sec/batch)
2017-04-02 22:56:36.738585: step 6750, loss = 1.09 (330.3 examples/sec; 0.388 sec/batch)
2017-04-02 22:56:40.621408: step 6760, loss = 1.10 (329.7 examples/sec; 0.388 sec/batch)
2017-04-02 22:56:44.502068: step 6770, loss = 0.97 (329.8 examples/sec; 0.388 sec/batch)
2017-04-02 22:56:48.387579: step 6780, loss = 0.86 (329.4 examples/sec; 0.389 sec/batch)
2017-04-02 22:56:52.229528: step 6790, loss = 1.12 (333.2 examples/sec; 0.384 sec/batch)
2017-04-02 22:56:56.172201: step 6800, loss = 0.81 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 22:57:00.048510: step 6810, loss = 1.00 (330.2 examples/sec; 0.388 sec/batch)
2017-04-02 22:57:03.967183: step 6820, loss = 0.97 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 22:57:07.861203: step 6830, loss = 0.93 (328.7 examples/sec; 0.389 sec/batch)
2017-04-02 22:57:11.696404: step 6840, loss = 0.99 (333.8 examples/sec; 0.384 sec/batch)
2017-04-02 22:57:15.584212: step 6850, loss = 0.99 (329.2 examples/sec; 0.389 sec/batch)
2017-04-02 22:57:19.489602: step 6860, loss = 0.99 (327.8 examples/sec; 0.391 sec/batch)
2017-04-02 22:57:23.367600: step 6870, loss = 0.90 (330.1 examples/sec; 0.388 sec/batch)
2017-04-02 22:57:27.274478: step 6880, loss = 0.74 (327.6 examples/sec; 0.391 sec/batch)
2017-04-02 22:57:31.143834: step 6890, loss = 1.01 (330.8 examples/sec; 0.387 sec/batch)
2017-04-02 22:57:35.094691: step 6900, loss = 0.89 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 22:57:38.992629: step 6910, loss = 0.96 (328.4 examples/sec; 0.390 sec/batch)
2017-04-02 22:57:42.845228: step 6920, loss = 1.00 (332.2 examples/sec; 0.385 sec/batch)
2017-04-02 22:57:46.738358: step 6930, loss = 0.98 (328.8 examples/sec; 0.389 sec/batch)
2017-04-02 22:57:50.609768: step 6940, loss = 0.84 (330.6 examples/sec; 0.387 sec/batch)
2017-04-02 22:57:54.508309: step 6950, loss = 0.87 (328.3 examples/sec; 0.390 sec/batch)
2017-04-02 22:57:58.345469: step 6960, loss = 0.89 (333.6 examples/sec; 0.384 sec/batch)
2017-04-02 22:58:02.204050: step 6970, loss = 0.95 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 22:58:06.073469: step 6980, loss = 1.07 (330.8 examples/sec; 0.387 sec/batch)
2017-04-02 22:58:09.929450: step 6990, loss = 0.90 (332.0 examples/sec; 0.386 sec/batch)
2017-04-02 22:58:13.870305: step 7000, loss = 0.94 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 22:58:17.747717: step 7010, loss = 0.94 (330.1 examples/sec; 0.388 sec/batch)
2017-04-02 22:58:21.615792: step 7020, loss = 0.90 (330.9 examples/sec; 0.387 sec/batch)
2017-04-02 22:58:25.421655: step 7030, loss = 0.96 (336.3 examples/sec; 0.381 sec/batch)
2017-04-02 22:58:29.278510: step 7040, loss = 0.92 (331.9 examples/sec; 0.386 sec/batch)
2017-04-02 22:58:33.173285: step 7050, loss = 0.80 (328.6 examples/sec; 0.389 sec/batch)
2017-04-02 22:58:37.073476: step 7060, loss = 0.91 (328.2 examples/sec; 0.390 sec/batch)
2017-04-02 22:58:40.992909: step 7070, loss = 0.94 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 22:58:44.910695: step 7080, loss = 0.78 (326.7 examples/sec; 0.392 sec/batch)
2017-04-02 22:58:48.806437: step 7090, loss = 0.96 (328.6 examples/sec; 0.390 sec/batch)
2017-04-02 22:58:52.744880: step 7100, loss = 0.96 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 22:58:56.674475: step 7110, loss = 0.84 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 22:59:00.562385: step 7120, loss = 1.04 (329.2 examples/sec; 0.389 sec/batch)
2017-04-02 22:59:04.454718: step 7130, loss = 0.81 (328.9 examples/sec; 0.389 sec/batch)
2017-04-02 22:59:08.337875: step 7140, loss = 1.19 (329.6 examples/sec; 0.388 sec/batch)
2017-04-02 22:59:12.205868: step 7150, loss = 0.97 (330.9 examples/sec; 0.387 sec/batch)
2017-04-02 22:59:16.072953: step 7160, loss = 0.82 (331.0 examples/sec; 0.387 sec/batch)
2017-04-02 22:59:19.973256: step 7170, loss = 1.12 (328.2 examples/sec; 0.390 sec/batch)
2017-04-02 22:59:23.847175: step 7180, loss = 0.84 (330.4 examples/sec; 0.387 sec/batch)
2017-04-02 22:59:27.726406: step 7190, loss = 0.86 (330.0 examples/sec; 0.388 sec/batch)
2017-04-02 22:59:31.639599: step 7200, loss = 1.03 (327.1 examples/sec; 0.391 sec/batch)
2017-04-02 22:59:35.515601: step 7210, loss = 0.94 (330.2 examples/sec; 0.388 sec/batch)
2017-04-02 22:59:39.380359: step 7220, loss = 1.13 (331.2 examples/sec; 0.386 sec/batch)
2017-04-02 22:59:43.273351: step 7230, loss = 0.97 (328.8 examples/sec; 0.389 sec/batch)
2017-04-02 22:59:47.155950: step 7240, loss = 1.08 (329.7 examples/sec; 0.388 sec/batch)
2017-04-02 22:59:51.027633: step 7250, loss = 0.93 (330.6 examples/sec; 0.387 sec/batch)
2017-04-02 22:59:54.876629: step 7260, loss = 0.99 (332.6 examples/sec; 0.385 sec/batch)
2017-04-02 22:59:58.777684: step 7270, loss = 0.93 (328.1 examples/sec; 0.390 sec/batch)
2017-04-02 23:00:02.666882: step 7280, loss = 1.04 (329.1 examples/sec; 0.389 sec/batch)
2017-04-02 23:00:06.529469: step 7290, loss = 1.14 (331.4 examples/sec; 0.386 sec/batch)
2017-04-02 23:00:10.436861: step 7300, loss = 0.91 (327.6 examples/sec; 0.391 sec/batch)
2017-04-02 23:00:14.318189: step 7310, loss = 1.12 (329.8 examples/sec; 0.388 sec/batch)
2017-04-02 23:00:18.215895: step 7320, loss = 0.85 (328.4 examples/sec; 0.390 sec/batch)
2017-04-02 23:00:22.075402: step 7330, loss = 0.75 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 23:00:25.970978: step 7340, loss = 1.10 (328.6 examples/sec; 0.390 sec/batch)
2017-04-02 23:00:29.881044: step 7350, loss = 0.95 (327.4 examples/sec; 0.391 sec/batch)
2017-04-02 23:00:33.737500: step 7360, loss = 0.88 (331.9 examples/sec; 0.386 sec/batch)
2017-04-02 23:00:37.553276: step 7370, loss = 0.94 (335.4 examples/sec; 0.382 sec/batch)
2017-04-02 23:00:41.369976: step 7380, loss = 0.95 (335.4 examples/sec; 0.382 sec/batch)
2017-04-02 23:00:45.240318: step 7390, loss = 0.93 (330.7 examples/sec; 0.387 sec/batch)
2017-04-02 23:00:49.129997: step 7400, loss = 1.06 (329.1 examples/sec; 0.389 sec/batch)
2017-04-02 23:00:52.965536: step 7410, loss = 1.11 (333.7 examples/sec; 0.384 sec/batch)
2017-04-02 23:00:56.857660: step 7420, loss = 0.75 (328.9 examples/sec; 0.389 sec/batch)
2017-04-02 23:01:00.735453: step 7430, loss = 1.05 (330.1 examples/sec; 0.388 sec/batch)
2017-04-02 23:01:04.558460: step 7440, loss = 0.97 (334.8 examples/sec; 0.382 sec/batch)
2017-04-02 23:01:08.438581: step 7450, loss = 0.86 (329.9 examples/sec; 0.388 sec/batch)
2017-04-02 23:01:12.301420: step 7460, loss = 1.02 (331.4 examples/sec; 0.386 sec/batch)
2017-04-02 23:01:16.176569: step 7470, loss = 0.80 (330.3 examples/sec; 0.388 sec/batch)
2017-04-02 23:01:20.049041: step 7480, loss = 0.86 (330.5 examples/sec; 0.387 sec/batch)
2017-04-02 23:01:23.920174: step 7490, loss = 0.92 (330.7 examples/sec; 0.387 sec/batch)
2017-04-02 23:01:27.816337: step 7500, loss = 0.96 (328.5 examples/sec; 0.390 sec/batch)
2017-04-02 23:01:31.694275: step 7510, loss = 0.84 (330.1 examples/sec; 0.388 sec/batch)
2017-04-02 23:01:35.557680: step 7520, loss = 0.85 (331.3 examples/sec; 0.386 sec/batch)
2017-04-02 23:01:39.390539: step 7530, loss = 0.86 (334.0 examples/sec; 0.383 sec/batch)
2017-04-02 23:01:43.236758: step 7540, loss = 0.72 (332.8 examples/sec; 0.385 sec/batch)
2017-04-02 23:01:47.142226: step 7550, loss = 0.95 (327.7 examples/sec; 0.391 sec/batch)
2017-04-02 23:01:51.006097: step 7560, loss = 1.02 (331.3 examples/sec; 0.386 sec/batch)
2017-04-02 23:01:54.878825: step 7570, loss = 0.98 (330.5 examples/sec; 0.387 sec/batch)
2017-04-02 23:01:58.742712: step 7580, loss = 1.00 (331.3 examples/sec; 0.386 sec/batch)
2017-04-02 23:02:02.616483: step 7590, loss = 0.99 (330.4 examples/sec; 0.387 sec/batch)
2017-04-02 23:02:06.545013: step 7600, loss = 0.99 (325.8 examples/sec; 0.393 sec/batch)
2017-04-02 23:02:10.403492: step 7610, loss = 0.91 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 23:02:14.239986: step 7620, loss = 1.10 (333.6 examples/sec; 0.384 sec/batch)
2017-04-02 23:02:18.077910: step 7630, loss = 0.88 (333.5 examples/sec; 0.384 sec/batch)
2017-04-02 23:02:21.942331: step 7640, loss = 0.91 (331.2 examples/sec; 0.386 sec/batch)
2017-04-02 23:02:25.786066: step 7650, loss = 0.92 (333.0 examples/sec; 0.384 sec/batch)
2017-04-02 23:02:29.643559: step 7660, loss = 0.92 (331.8 examples/sec; 0.386 sec/batch)
2017-04-02 23:02:33.535300: step 7670, loss = 0.78 (328.9 examples/sec; 0.389 sec/batch)
2017-04-02 23:02:37.411307: step 7680, loss = 1.04 (330.2 examples/sec; 0.388 sec/batch)
2017-04-02 23:02:41.289633: step 7690, loss = 0.87 (330.0 examples/sec; 0.388 sec/batch)
2017-04-02 23:02:45.217603: step 7700, loss = 0.85 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 23:02:49.129428: step 7710, loss = 0.92 (327.2 examples/sec; 0.391 sec/batch)
2017-04-02 23:02:52.996965: step 7720, loss = 0.93 (331.0 examples/sec; 0.387 sec/batch)
2017-04-02 23:02:56.886617: step 7730, loss = 0.97 (329.1 examples/sec; 0.389 sec/batch)
2017-04-02 23:03:00.784498: step 7740, loss = 0.78 (328.4 examples/sec; 0.390 sec/batch)
2017-04-02 23:03:04.671169: step 7750, loss = 0.77 (329.3 examples/sec; 0.389 sec/batch)
2017-04-02 23:03:08.567748: step 7760, loss = 0.85 (328.5 examples/sec; 0.390 sec/batch)
2017-04-02 23:03:12.474598: step 7770, loss = 0.88 (327.6 examples/sec; 0.391 sec/batch)
2017-04-02 23:03:16.360887: step 7780, loss = 0.87 (329.4 examples/sec; 0.389 sec/batch)
2017-04-02 23:03:20.213489: step 7790, loss = 0.77 (332.2 examples/sec; 0.385 sec/batch)
2017-04-02 23:03:24.135436: step 7800, loss = 1.01 (326.4 examples/sec; 0.392 sec/batch)
2017-04-02 23:03:28.032608: step 7810, loss = 1.25 (328.4 examples/sec; 0.390 sec/batch)
2017-04-02 23:03:31.904935: step 7820, loss = 0.83 (330.6 examples/sec; 0.387 sec/batch)
2017-04-02 23:03:35.840094: step 7830, loss = 0.92 (325.3 examples/sec; 0.394 sec/batch)
2017-04-02 23:03:39.742898: step 7840, loss = 0.92 (328.0 examples/sec; 0.390 sec/batch)
2017-04-02 23:03:43.636784: step 7850, loss = 0.82 (328.7 examples/sec; 0.389 sec/batch)
2017-04-02 23:03:47.507611: step 7860, loss = 0.89 (330.7 examples/sec; 0.387 sec/batch)
2017-04-02 23:03:51.395302: step 7870, loss = 0.98 (329.2 examples/sec; 0.389 sec/batch)
2017-04-02 23:03:55.277145: step 7880, loss = 0.92 (329.7 examples/sec; 0.388 sec/batch)
2017-04-02 23:03:59.186140: step 7890, loss = 1.07 (327.4 examples/sec; 0.391 sec/batch)
2017-04-02 23:04:03.161116: step 7900, loss = 0.82 (322.0 examples/sec; 0.397 sec/batch)
2017-04-02 23:04:07.094327: step 7910, loss = 1.00 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 23:04:11.001182: step 7920, loss = 0.88 (327.6 examples/sec; 0.391 sec/batch)
2017-04-02 23:04:14.887285: step 7930, loss = 0.79 (329.4 examples/sec; 0.389 sec/batch)
2017-04-02 23:04:18.774388: step 7940, loss = 0.93 (329.3 examples/sec; 0.389 sec/batch)
2017-04-02 23:04:22.618893: step 7950, loss = 0.93 (332.9 examples/sec; 0.384 sec/batch)
2017-04-02 23:04:26.496297: step 7960, loss = 0.88 (330.1 examples/sec; 0.388 sec/batch)
2017-04-02 23:04:30.373426: step 7970, loss = 1.05 (330.1 examples/sec; 0.388 sec/batch)
2017-04-02 23:04:34.250719: step 7980, loss = 1.25 (330.1 examples/sec; 0.388 sec/batch)
2017-04-02 23:04:38.114854: step 7990, loss = 1.04 (331.3 examples/sec; 0.386 sec/batch)
2017-04-02 23:04:42.037694: step 8000, loss = 1.07 (326.3 examples/sec; 0.392 sec/batch)
2017-04-02 23:05:28.419416: precision @ 1 = 0.798

2017-04-02 23:06:10.493160: step 8010, loss = 0.79 (14.5 examples/sec; 8.846 sec/batch)
2017-04-02 23:06:14.333371: step 8020, loss = 0.95 (333.3 examples/sec; 0.384 sec/batch)
2017-04-02 23:06:18.148842: step 8030, loss = 0.96 (335.5 examples/sec; 0.382 sec/batch)
2017-04-02 23:06:21.982423: step 8040, loss = 0.76 (333.9 examples/sec; 0.383 sec/batch)
2017-04-02 23:06:25.807296: step 8050, loss = 0.84 (334.7 examples/sec; 0.382 sec/batch)
2017-04-02 23:06:29.678876: step 8060, loss = 1.14 (330.6 examples/sec; 0.387 sec/batch)
2017-04-02 23:06:33.509445: step 8070, loss = 0.93 (334.2 examples/sec; 0.383 sec/batch)
2017-04-02 23:06:37.353143: step 8080, loss = 0.85 (333.0 examples/sec; 0.384 sec/batch)
2017-04-02 23:06:41.210271: step 8090, loss = 0.90 (331.9 examples/sec; 0.386 sec/batch)
2017-04-02 23:06:45.106598: step 8100, loss = 0.86 (328.5 examples/sec; 0.390 sec/batch)
2017-04-02 23:06:48.933272: step 8110, loss = 1.00 (334.5 examples/sec; 0.383 sec/batch)
2017-04-02 23:06:52.792842: step 8120, loss = 0.78 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 23:06:56.646800: step 8130, loss = 1.00 (332.1 examples/sec; 0.385 sec/batch)
2017-04-02 23:07:00.508299: step 8140, loss = 0.96 (331.5 examples/sec; 0.386 sec/batch)
2017-04-02 23:07:04.361323: step 8150, loss = 1.06 (332.2 examples/sec; 0.385 sec/batch)
2017-04-02 23:07:08.187501: step 8160, loss = 1.12 (334.5 examples/sec; 0.383 sec/batch)
2017-04-02 23:07:12.044618: step 8170, loss = 1.03 (331.9 examples/sec; 0.386 sec/batch)
2017-04-02 23:07:15.888975: step 8180, loss = 0.90 (333.0 examples/sec; 0.384 sec/batch)
2017-04-02 23:07:19.741486: step 8190, loss = 0.88 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 23:07:23.650538: step 8200, loss = 0.76 (327.4 examples/sec; 0.391 sec/batch)
2017-04-02 23:07:27.471194: step 8210, loss = 1.26 (335.0 examples/sec; 0.382 sec/batch)
2017-04-02 23:07:31.293044: step 8220, loss = 0.79 (334.9 examples/sec; 0.382 sec/batch)
2017-04-02 23:07:35.142198: step 8230, loss = 0.96 (332.5 examples/sec; 0.385 sec/batch)
2017-04-02 23:07:38.956471: step 8240, loss = 0.87 (335.6 examples/sec; 0.381 sec/batch)
2017-04-02 23:07:42.832982: step 8250, loss = 1.00 (330.2 examples/sec; 0.388 sec/batch)
2017-04-02 23:07:46.673086: step 8260, loss = 0.78 (333.3 examples/sec; 0.384 sec/batch)
2017-04-02 23:07:50.518259: step 8270, loss = 0.82 (332.9 examples/sec; 0.385 sec/batch)
2017-04-02 23:07:54.549540: step 8280, loss = 1.06 (317.5 examples/sec; 0.403 sec/batch)
2017-04-02 23:07:58.401167: step 8290, loss = 0.94 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 23:08:02.336712: step 8300, loss = 0.82 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 23:08:06.203812: step 8310, loss = 1.07 (331.0 examples/sec; 0.387 sec/batch)
2017-04-02 23:08:10.122013: step 8320, loss = 0.90 (326.7 examples/sec; 0.392 sec/batch)
2017-04-02 23:08:14.032943: step 8330, loss = 0.89 (327.3 examples/sec; 0.391 sec/batch)
2017-04-02 23:08:17.870508: step 8340, loss = 0.95 (333.5 examples/sec; 0.384 sec/batch)
2017-04-02 23:08:21.721246: step 8350, loss = 1.01 (332.4 examples/sec; 0.385 sec/batch)
2017-04-02 23:08:25.579186: step 8360, loss = 0.94 (331.8 examples/sec; 0.386 sec/batch)
2017-04-02 23:08:29.435404: step 8370, loss = 0.64 (331.9 examples/sec; 0.386 sec/batch)
2017-04-02 23:08:33.278819: step 8380, loss = 0.86 (333.0 examples/sec; 0.384 sec/batch)
2017-04-02 23:08:37.174256: step 8390, loss = 0.76 (328.6 examples/sec; 0.390 sec/batch)
2017-04-02 23:08:41.093504: step 8400, loss = 1.03 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 23:08:44.976952: step 8410, loss = 0.80 (329.6 examples/sec; 0.388 sec/batch)
2017-04-02 23:08:48.816803: step 8420, loss = 0.89 (333.3 examples/sec; 0.384 sec/batch)
2017-04-02 23:08:52.650280: step 8430, loss = 0.89 (333.9 examples/sec; 0.383 sec/batch)
2017-04-02 23:08:56.534691: step 8440, loss = 0.80 (329.5 examples/sec; 0.388 sec/batch)
2017-04-02 23:09:00.400338: step 8450, loss = 1.10 (331.1 examples/sec; 0.387 sec/batch)
2017-04-02 23:09:04.272912: step 8460, loss = 1.10 (330.5 examples/sec; 0.387 sec/batch)
2017-04-02 23:09:08.164361: step 8470, loss = 1.03 (328.9 examples/sec; 0.389 sec/batch)
2017-04-02 23:09:12.027912: step 8480, loss = 0.89 (331.3 examples/sec; 0.386 sec/batch)
2017-04-02 23:09:15.946043: step 8490, loss = 0.81 (326.7 examples/sec; 0.392 sec/batch)
2017-04-02 23:09:19.882946: step 8500, loss = 0.99 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 23:09:23.788306: step 8510, loss = 0.89 (327.8 examples/sec; 0.391 sec/batch)
2017-04-02 23:09:27.603188: step 8520, loss = 1.05 (335.5 examples/sec; 0.381 sec/batch)
2017-04-02 23:09:31.447145: step 8530, loss = 1.07 (333.0 examples/sec; 0.384 sec/batch)
2017-04-02 23:09:35.305147: step 8540, loss = 0.92 (331.8 examples/sec; 0.386 sec/batch)
2017-04-02 23:09:39.166642: step 8550, loss = 0.91 (331.5 examples/sec; 0.386 sec/batch)
2017-04-02 23:09:43.050375: step 8560, loss = 1.11 (329.6 examples/sec; 0.388 sec/batch)
2017-04-02 23:09:46.898509: step 8570, loss = 0.87 (332.6 examples/sec; 0.385 sec/batch)
2017-04-02 23:09:50.776576: step 8580, loss = 0.99 (330.1 examples/sec; 0.388 sec/batch)
2017-04-02 23:09:54.662377: step 8590, loss = 0.83 (329.4 examples/sec; 0.389 sec/batch)
2017-04-02 23:09:58.577800: step 8600, loss = 0.83 (326.9 examples/sec; 0.392 sec/batch)
2017-04-02 23:10:02.478072: step 8610, loss = 1.00 (328.2 examples/sec; 0.390 sec/batch)
2017-04-02 23:10:06.325593: step 8620, loss = 0.82 (332.7 examples/sec; 0.385 sec/batch)
2017-04-02 23:10:10.184139: step 8630, loss = 0.90 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 23:10:14.035136: step 8640, loss = 0.85 (332.4 examples/sec; 0.385 sec/batch)
2017-04-02 23:10:17.925589: step 8650, loss = 0.78 (329.0 examples/sec; 0.389 sec/batch)
2017-04-02 23:10:21.769989: step 8660, loss = 0.87 (333.0 examples/sec; 0.384 sec/batch)
2017-04-02 23:10:25.613266: step 8670, loss = 0.96 (333.0 examples/sec; 0.384 sec/batch)
2017-04-02 23:10:29.503895: step 8680, loss = 0.96 (329.0 examples/sec; 0.389 sec/batch)
2017-04-02 23:10:33.336776: step 8690, loss = 0.85 (334.0 examples/sec; 0.383 sec/batch)
2017-04-02 23:10:37.234192: step 8700, loss = 0.95 (328.4 examples/sec; 0.390 sec/batch)
2017-04-02 23:10:41.080882: step 8710, loss = 0.92 (332.8 examples/sec; 0.385 sec/batch)
2017-04-02 23:10:44.963448: step 8720, loss = 0.95 (329.7 examples/sec; 0.388 sec/batch)
2017-04-02 23:10:48.819208: step 8730, loss = 0.86 (332.0 examples/sec; 0.386 sec/batch)
2017-04-02 23:10:52.702143: step 8740, loss = 0.96 (329.6 examples/sec; 0.388 sec/batch)
2017-04-02 23:10:56.594958: step 8750, loss = 0.83 (328.8 examples/sec; 0.389 sec/batch)
2017-04-02 23:11:00.461361: step 8760, loss = 0.97 (331.1 examples/sec; 0.387 sec/batch)
2017-04-02 23:11:04.317023: step 8770, loss = 0.79 (332.0 examples/sec; 0.386 sec/batch)
2017-04-02 23:11:08.184779: step 8780, loss = 0.85 (330.9 examples/sec; 0.387 sec/batch)
2017-04-02 23:11:12.073762: step 8790, loss = 1.08 (329.1 examples/sec; 0.389 sec/batch)
2017-04-02 23:11:16.013600: step 8800, loss = 0.75 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 23:11:19.871378: step 8810, loss = 0.96 (331.8 examples/sec; 0.386 sec/batch)
2017-04-02 23:11:23.741914: step 8820, loss = 0.94 (330.7 examples/sec; 0.387 sec/batch)
2017-04-02 23:11:27.628670: step 8830, loss = 0.85 (329.3 examples/sec; 0.389 sec/batch)
2017-04-02 23:11:31.465701: step 8840, loss = 0.76 (333.6 examples/sec; 0.384 sec/batch)
2017-04-02 23:11:35.343740: step 8850, loss = 0.97 (330.1 examples/sec; 0.388 sec/batch)
2017-04-02 23:11:39.199258: step 8860, loss = 0.80 (332.0 examples/sec; 0.386 sec/batch)
2017-04-02 23:11:43.103494: step 8870, loss = 0.83 (327.8 examples/sec; 0.390 sec/batch)
2017-04-02 23:11:46.956860: step 8880, loss = 0.89 (332.2 examples/sec; 0.385 sec/batch)
2017-04-02 23:11:50.804133: step 8890, loss = 0.94 (332.7 examples/sec; 0.385 sec/batch)
2017-04-02 23:11:54.751564: step 8900, loss = 0.95 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 23:11:58.610682: step 8910, loss = 0.94 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 23:12:02.466032: step 8920, loss = 0.75 (332.0 examples/sec; 0.386 sec/batch)
2017-04-02 23:12:06.341021: step 8930, loss = 0.95 (330.3 examples/sec; 0.387 sec/batch)
2017-04-02 23:12:10.199937: step 8940, loss = 0.92 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 23:12:14.083462: step 8950, loss = 0.89 (329.6 examples/sec; 0.388 sec/batch)
2017-04-02 23:12:17.970301: step 8960, loss = 0.79 (329.3 examples/sec; 0.389 sec/batch)
2017-04-02 23:12:21.842580: step 8970, loss = 0.82 (330.6 examples/sec; 0.387 sec/batch)
2017-04-02 23:12:25.730774: step 8980, loss = 0.89 (329.2 examples/sec; 0.389 sec/batch)
2017-04-02 23:12:29.608889: step 8990, loss = 1.00 (330.1 examples/sec; 0.388 sec/batch)
2017-04-02 23:12:33.522173: step 9000, loss = 0.84 (327.1 examples/sec; 0.391 sec/batch)
2017-04-02 23:12:37.396803: step 9010, loss = 0.95 (330.4 examples/sec; 0.387 sec/batch)
2017-04-02 23:12:41.268570: step 9020, loss = 1.02 (330.6 examples/sec; 0.387 sec/batch)
2017-04-02 23:12:45.119783: step 9030, loss = 0.79 (332.4 examples/sec; 0.385 sec/batch)
2017-04-02 23:12:49.009657: step 9040, loss = 0.92 (329.1 examples/sec; 0.389 sec/batch)
2017-04-02 23:12:52.909915: step 9050, loss = 0.91 (328.2 examples/sec; 0.390 sec/batch)
2017-04-02 23:12:56.781683: step 9060, loss = 0.88 (330.6 examples/sec; 0.387 sec/batch)
2017-04-02 23:13:00.621958: step 9070, loss = 0.93 (333.3 examples/sec; 0.384 sec/batch)
2017-04-02 23:13:04.467822: step 9080, loss = 0.89 (332.8 examples/sec; 0.385 sec/batch)
2017-04-02 23:13:08.297545: step 9090, loss = 0.89 (334.2 examples/sec; 0.383 sec/batch)
2017-04-02 23:13:12.192031: step 9100, loss = 0.81 (328.7 examples/sec; 0.389 sec/batch)
2017-04-02 23:13:16.035036: step 9110, loss = 0.76 (333.1 examples/sec; 0.384 sec/batch)
2017-04-02 23:13:19.917788: step 9120, loss = 1.04 (329.7 examples/sec; 0.388 sec/batch)
2017-04-02 23:13:23.793240: step 9130, loss = 0.80 (330.3 examples/sec; 0.388 sec/batch)
2017-04-02 23:13:27.662325: step 9140, loss = 0.96 (330.8 examples/sec; 0.387 sec/batch)
2017-04-02 23:13:31.517454: step 9150, loss = 0.98 (332.0 examples/sec; 0.386 sec/batch)
2017-04-02 23:13:35.386120: step 9160, loss = 0.93 (330.9 examples/sec; 0.387 sec/batch)
2017-04-02 23:13:39.257130: step 9170, loss = 0.91 (330.7 examples/sec; 0.387 sec/batch)
2017-04-02 23:13:43.136435: step 9180, loss = 0.85 (330.0 examples/sec; 0.388 sec/batch)
2017-04-02 23:13:47.016882: step 9190, loss = 0.86 (329.9 examples/sec; 0.388 sec/batch)
2017-04-02 23:13:50.951120: step 9200, loss = 0.85 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 23:13:54.844747: step 9210, loss = 0.83 (328.7 examples/sec; 0.389 sec/batch)
2017-04-02 23:13:58.698273: step 9220, loss = 1.00 (332.2 examples/sec; 0.385 sec/batch)
2017-04-02 23:14:02.534376: step 9230, loss = 0.88 (333.7 examples/sec; 0.384 sec/batch)
2017-04-02 23:14:06.435666: step 9240, loss = 0.81 (328.1 examples/sec; 0.390 sec/batch)
2017-04-02 23:14:10.310027: step 9250, loss = 0.80 (330.4 examples/sec; 0.387 sec/batch)
2017-04-02 23:14:14.184181: step 9260, loss = 0.90 (330.4 examples/sec; 0.387 sec/batch)
2017-04-02 23:14:18.077922: step 9270, loss = 0.91 (328.7 examples/sec; 0.389 sec/batch)
2017-04-02 23:14:21.960138: step 9280, loss = 1.00 (329.7 examples/sec; 0.388 sec/batch)
2017-04-02 23:14:25.872440: step 9290, loss = 0.90 (327.2 examples/sec; 0.391 sec/batch)
2017-04-02 23:14:29.793958: step 9300, loss = 0.76 (326.4 examples/sec; 0.392 sec/batch)
2017-04-02 23:14:33.666609: step 9310, loss = 0.91 (330.5 examples/sec; 0.387 sec/batch)
2017-04-02 23:14:37.547892: step 9320, loss = 0.79 (329.8 examples/sec; 0.388 sec/batch)
2017-04-02 23:14:41.413767: step 9330, loss = 0.82 (331.1 examples/sec; 0.387 sec/batch)
2017-04-02 23:14:45.263465: step 9340, loss = 0.89 (332.5 examples/sec; 0.385 sec/batch)
2017-04-02 23:14:49.145435: step 9350, loss = 0.90 (329.7 examples/sec; 0.388 sec/batch)
2017-04-02 23:14:53.006526: step 9360, loss = 0.81 (331.5 examples/sec; 0.386 sec/batch)
2017-04-02 23:14:56.861852: step 9370, loss = 0.79 (332.0 examples/sec; 0.386 sec/batch)
2017-04-02 23:15:00.749776: step 9380, loss = 1.03 (329.2 examples/sec; 0.389 sec/batch)
2017-04-02 23:15:04.625468: step 9390, loss = 1.02 (330.3 examples/sec; 0.388 sec/batch)
2017-04-02 23:15:08.560964: step 9400, loss = 0.98 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 23:15:12.440897: step 9410, loss = 0.77 (329.9 examples/sec; 0.388 sec/batch)
2017-04-02 23:15:16.324347: step 9420, loss = 0.89 (329.6 examples/sec; 0.388 sec/batch)
2017-04-02 23:15:20.227334: step 9430, loss = 0.81 (328.0 examples/sec; 0.390 sec/batch)
2017-04-02 23:15:24.102646: step 9440, loss = 0.94 (330.3 examples/sec; 0.388 sec/batch)
2017-04-02 23:15:27.999820: step 9450, loss = 0.82 (328.4 examples/sec; 0.390 sec/batch)
2017-04-02 23:15:31.858301: step 9460, loss = 0.76 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 23:15:35.741437: step 9470, loss = 0.72 (329.6 examples/sec; 0.388 sec/batch)
2017-04-02 23:15:39.601505: step 9480, loss = 0.92 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 23:15:43.449594: step 9490, loss = 0.92 (332.6 examples/sec; 0.385 sec/batch)
2017-04-02 23:15:47.381548: step 9500, loss = 0.87 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 23:15:51.265170: step 9510, loss = 1.23 (329.6 examples/sec; 0.388 sec/batch)
2017-04-02 23:15:55.116330: step 9520, loss = 1.00 (332.4 examples/sec; 0.385 sec/batch)
2017-04-02 23:15:59.267917: step 9530, loss = 0.82 (308.3 examples/sec; 0.415 sec/batch)
2017-04-02 23:16:11.398630: step 9540, loss = 0.89 (105.5 examples/sec; 1.213 sec/batch)
2017-04-02 23:16:15.259995: step 9550, loss = 1.06 (331.5 examples/sec; 0.386 sec/batch)
2017-04-02 23:16:19.112282: step 9560, loss = 0.84 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 23:16:22.971696: step 9570, loss = 0.97 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 23:16:26.813373: step 9580, loss = 0.84 (333.2 examples/sec; 0.384 sec/batch)
2017-04-02 23:16:30.691188: step 9590, loss = 0.86 (330.1 examples/sec; 0.388 sec/batch)
2017-04-02 23:16:34.596074: step 9600, loss = 0.66 (327.8 examples/sec; 0.390 sec/batch)
2017-04-02 23:16:38.466609: step 9610, loss = 0.91 (330.7 examples/sec; 0.387 sec/batch)
2017-04-02 23:16:42.294903: step 9620, loss = 0.79 (334.4 examples/sec; 0.383 sec/batch)
2017-04-02 23:16:46.136044: step 9630, loss = 0.99 (333.2 examples/sec; 0.384 sec/batch)
2017-04-02 23:16:50.007438: step 9640, loss = 0.82 (330.6 examples/sec; 0.387 sec/batch)
2017-04-02 23:16:53.874533: step 9650, loss = 0.89 (331.0 examples/sec; 0.387 sec/batch)
2017-04-02 23:16:57.734124: step 9660, loss = 0.87 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 23:17:01.600628: step 9670, loss = 0.82 (331.0 examples/sec; 0.387 sec/batch)
2017-04-02 23:17:05.439556: step 9680, loss = 0.87 (333.4 examples/sec; 0.384 sec/batch)
2017-04-02 23:17:09.330738: step 9690, loss = 0.89 (328.9 examples/sec; 0.389 sec/batch)
2017-04-02 23:17:13.246887: step 9700, loss = 0.99 (326.9 examples/sec; 0.392 sec/batch)
2017-04-02 23:17:17.116711: step 9710, loss = 0.92 (330.8 examples/sec; 0.387 sec/batch)
2017-04-02 23:17:21.011159: step 9720, loss = 0.87 (328.7 examples/sec; 0.389 sec/batch)
2017-04-02 23:17:24.850432: step 9730, loss = 0.82 (333.4 examples/sec; 0.384 sec/batch)
2017-04-02 23:17:28.696657: step 9740, loss = 0.93 (332.8 examples/sec; 0.385 sec/batch)
2017-04-02 23:17:32.521799: step 9750, loss = 0.82 (334.6 examples/sec; 0.383 sec/batch)
2017-04-02 23:17:36.370225: step 9760, loss = 0.95 (332.6 examples/sec; 0.385 sec/batch)
2017-04-02 23:17:40.194877: step 9770, loss = 0.82 (334.7 examples/sec; 0.382 sec/batch)
2017-04-02 23:17:44.021156: step 9780, loss = 0.96 (334.5 examples/sec; 0.383 sec/batch)
2017-04-02 23:17:47.869718: step 9790, loss = 0.84 (332.6 examples/sec; 0.385 sec/batch)
2017-04-02 23:17:51.773175: step 9800, loss = 1.04 (327.9 examples/sec; 0.390 sec/batch)
2017-04-02 23:17:55.613669: step 9810, loss = 0.93 (333.3 examples/sec; 0.384 sec/batch)
2017-04-02 23:17:59.469070: step 9820, loss = 0.85 (332.0 examples/sec; 0.386 sec/batch)
2017-04-02 23:18:03.329521: step 9830, loss = 0.89 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 23:18:07.210066: step 9840, loss = 1.09 (329.9 examples/sec; 0.388 sec/batch)
2017-04-02 23:18:11.065819: step 9850, loss = 0.96 (332.0 examples/sec; 0.386 sec/batch)
2017-04-02 23:18:14.892834: step 9860, loss = 0.92 (334.5 examples/sec; 0.383 sec/batch)
2017-04-02 23:18:18.741874: step 9870, loss = 0.89 (332.6 examples/sec; 0.385 sec/batch)
2017-04-02 23:18:22.581560: step 9880, loss = 0.72 (333.4 examples/sec; 0.384 sec/batch)
2017-04-02 23:18:26.454704: step 9890, loss = 0.94 (330.5 examples/sec; 0.387 sec/batch)
2017-04-02 23:18:30.364933: step 9900, loss = 0.88 (327.3 examples/sec; 0.391 sec/batch)
2017-04-02 23:18:34.235080: step 9910, loss = 0.80 (330.7 examples/sec; 0.387 sec/batch)
2017-04-02 23:18:38.114102: step 9920, loss = 0.86 (330.0 examples/sec; 0.388 sec/batch)
2017-04-02 23:18:41.965227: step 9930, loss = 0.92 (332.4 examples/sec; 0.385 sec/batch)
2017-04-02 23:18:45.798553: step 9940, loss = 0.79 (333.9 examples/sec; 0.383 sec/batch)
2017-04-02 23:18:49.634340: step 9950, loss = 0.91 (333.7 examples/sec; 0.384 sec/batch)
2017-04-02 23:18:53.509676: step 9960, loss = 0.94 (330.3 examples/sec; 0.388 sec/batch)
2017-04-02 23:18:57.697870: step 9970, loss = 0.86 (305.6 examples/sec; 0.419 sec/batch)
2017-04-02 23:19:01.597814: step 9980, loss = 0.89 (328.2 examples/sec; 0.390 sec/batch)
2017-04-02 23:19:05.485879: step 9990, loss = 1.17 (329.2 examples/sec; 0.389 sec/batch)
2017-04-02 23:19:09.433916: step 10000, loss = 0.80 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 23:19:13.291933: step 10010, loss = 0.77 (331.8 examples/sec; 0.386 sec/batch)
2017-04-02 23:19:17.177104: step 10020, loss = 1.01 (329.5 examples/sec; 0.389 sec/batch)
2017-04-02 23:20:22.245042: precision @ 1 = 0.105

2017-04-02 23:20:49.490325: step 10030, loss = 0.95 (13.9 examples/sec; 9.231 sec/batch)
2017-04-02 23:20:53.342185: step 10040, loss = 0.97 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 23:20:57.193449: step 10050, loss = 0.87 (332.4 examples/sec; 0.385 sec/batch)
2017-04-02 23:21:01.056333: step 10060, loss = 0.80 (331.4 examples/sec; 0.386 sec/batch)
2017-04-02 23:21:04.928214: step 10070, loss = 0.81 (330.6 examples/sec; 0.387 sec/batch)
2017-04-02 23:21:08.787785: step 10080, loss = 0.77 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 23:21:12.647106: step 10090, loss = 1.03 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 23:21:16.529416: step 10100, loss = 1.03 (329.7 examples/sec; 0.388 sec/batch)
2017-04-02 23:21:20.368260: step 10110, loss = 1.04 (333.4 examples/sec; 0.384 sec/batch)
2017-04-02 23:21:24.209218: step 10120, loss = 0.83 (333.3 examples/sec; 0.384 sec/batch)
2017-04-02 23:21:28.086837: step 10130, loss = 0.75 (330.1 examples/sec; 0.388 sec/batch)
2017-04-02 23:21:31.943101: step 10140, loss = 0.96 (331.9 examples/sec; 0.386 sec/batch)
2017-04-02 23:21:35.809971: step 10150, loss = 0.81 (331.0 examples/sec; 0.387 sec/batch)
2017-04-02 23:21:39.664912: step 10160, loss = 0.76 (332.0 examples/sec; 0.385 sec/batch)
2017-04-02 23:21:43.518850: step 10170, loss = 0.91 (332.1 examples/sec; 0.385 sec/batch)
2017-04-02 23:21:47.380378: step 10180, loss = 1.02 (331.5 examples/sec; 0.386 sec/batch)
2017-04-02 23:21:51.235307: step 10190, loss = 0.88 (332.0 examples/sec; 0.385 sec/batch)
2017-04-02 23:21:55.155795: step 10200, loss = 0.84 (326.5 examples/sec; 0.392 sec/batch)
2017-04-02 23:21:59.001570: step 10210, loss = 0.92 (332.8 examples/sec; 0.385 sec/batch)
2017-04-02 23:22:02.847524: step 10220, loss = 0.73 (332.8 examples/sec; 0.385 sec/batch)
2017-04-02 23:22:06.701356: step 10230, loss = 0.94 (332.1 examples/sec; 0.385 sec/batch)
2017-04-02 23:22:10.561504: step 10240, loss = 0.78 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 23:22:14.440065: step 10250, loss = 0.99 (330.0 examples/sec; 0.388 sec/batch)
2017-04-02 23:22:18.329937: step 10260, loss = 0.79 (329.1 examples/sec; 0.389 sec/batch)
2017-04-02 23:22:22.176134: step 10270, loss = 0.85 (332.8 examples/sec; 0.385 sec/batch)
2017-04-02 23:22:26.018551: step 10280, loss = 0.99 (333.1 examples/sec; 0.384 sec/batch)
2017-04-02 23:22:29.854684: step 10290, loss = 0.95 (333.7 examples/sec; 0.384 sec/batch)
2017-04-02 23:22:33.790302: step 10300, loss = 0.93 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 23:22:37.646923: step 10310, loss = 1.00 (331.9 examples/sec; 0.386 sec/batch)
2017-04-02 23:22:41.485450: step 10320, loss = 0.74 (333.5 examples/sec; 0.384 sec/batch)
2017-04-02 23:22:45.351446: step 10330, loss = 0.94 (331.1 examples/sec; 0.387 sec/batch)
2017-04-02 23:22:49.193998: step 10340, loss = 0.85 (333.1 examples/sec; 0.384 sec/batch)
2017-04-02 23:22:53.032805: step 10350, loss = 0.91 (333.4 examples/sec; 0.384 sec/batch)
2017-04-02 23:22:56.871470: step 10360, loss = 1.05 (333.4 examples/sec; 0.384 sec/batch)
2017-04-02 23:23:00.729621: step 10370, loss = 0.85 (331.8 examples/sec; 0.386 sec/batch)
2017-04-02 23:23:04.572874: step 10380, loss = 0.81 (333.1 examples/sec; 0.384 sec/batch)
2017-04-02 23:23:08.428654: step 10390, loss = 0.90 (332.0 examples/sec; 0.386 sec/batch)
2017-04-02 23:23:12.349030: step 10400, loss = 0.73 (326.5 examples/sec; 0.392 sec/batch)
2017-04-02 23:23:16.240864: step 10410, loss = 0.77 (328.9 examples/sec; 0.389 sec/batch)
2017-04-02 23:23:20.102280: step 10420, loss = 0.92 (331.5 examples/sec; 0.386 sec/batch)
2017-04-02 23:23:23.974984: step 10430, loss = 0.87 (330.5 examples/sec; 0.387 sec/batch)
2017-04-02 23:23:27.830039: step 10440, loss = 0.83 (332.0 examples/sec; 0.386 sec/batch)
2017-04-02 23:23:31.679331: step 10450, loss = 1.06 (332.5 examples/sec; 0.385 sec/batch)
2017-04-02 23:23:35.546512: step 10460, loss = 0.84 (331.0 examples/sec; 0.387 sec/batch)
2017-04-02 23:23:39.396318: step 10470, loss = 0.73 (332.5 examples/sec; 0.385 sec/batch)
2017-04-02 23:23:43.288526: step 10480, loss = 0.88 (328.9 examples/sec; 0.389 sec/batch)
2017-04-02 23:23:47.154651: step 10490, loss = 1.08 (331.1 examples/sec; 0.387 sec/batch)
2017-04-02 23:23:51.059038: step 10500, loss = 0.70 (327.8 examples/sec; 0.390 sec/batch)
2017-04-02 23:23:54.931147: step 10510, loss = 0.90 (330.6 examples/sec; 0.387 sec/batch)
2017-04-02 23:23:58.770804: step 10520, loss = 0.88 (333.4 examples/sec; 0.384 sec/batch)
2017-04-02 23:24:02.606118: step 10530, loss = 0.82 (333.7 examples/sec; 0.384 sec/batch)
2017-04-02 23:24:06.486657: step 10540, loss = 1.10 (329.9 examples/sec; 0.388 sec/batch)
2017-04-02 23:24:10.348149: step 10550, loss = 0.65 (331.5 examples/sec; 0.386 sec/batch)
2017-04-02 23:24:14.207908: step 10560, loss = 0.92 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 23:24:18.032129: step 10570, loss = 0.79 (334.7 examples/sec; 0.382 sec/batch)
2017-04-02 23:24:21.902041: step 10580, loss = 0.73 (330.8 examples/sec; 0.387 sec/batch)
2017-04-02 23:24:25.765715: step 10590, loss = 0.86 (331.3 examples/sec; 0.386 sec/batch)
2017-04-02 23:24:29.674151: step 10600, loss = 0.77 (327.5 examples/sec; 0.391 sec/batch)
2017-04-02 23:24:33.522489: step 10610, loss = 0.95 (332.6 examples/sec; 0.385 sec/batch)
2017-04-02 23:24:37.370560: step 10620, loss = 0.92 (332.6 examples/sec; 0.385 sec/batch)
2017-04-02 23:24:41.207717: step 10630, loss = 0.96 (333.6 examples/sec; 0.384 sec/batch)
2017-04-02 23:24:45.047704: step 10640, loss = 0.93 (333.3 examples/sec; 0.384 sec/batch)
2017-04-02 23:24:48.920325: step 10650, loss = 0.99 (330.5 examples/sec; 0.387 sec/batch)
2017-04-02 23:24:52.776082: step 10660, loss = 0.85 (332.0 examples/sec; 0.386 sec/batch)
2017-04-02 23:24:56.644625: step 10670, loss = 0.88 (330.9 examples/sec; 0.387 sec/batch)
2017-04-02 23:25:00.504029: step 10680, loss = 0.90 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 23:25:04.365851: step 10690, loss = 0.95 (331.4 examples/sec; 0.386 sec/batch)
2017-04-02 23:25:08.284329: step 10700, loss = 1.08 (326.7 examples/sec; 0.392 sec/batch)
2017-04-02 23:25:12.112338: step 10710, loss = 0.81 (334.4 examples/sec; 0.383 sec/batch)
2017-04-02 23:25:15.977043: step 10720, loss = 0.93 (331.2 examples/sec; 0.386 sec/batch)
2017-04-02 23:25:19.836115: step 10730, loss = 0.87 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 23:25:23.699264: step 10740, loss = 0.89 (331.3 examples/sec; 0.386 sec/batch)
2017-04-02 23:25:27.543754: step 10750, loss = 0.81 (332.9 examples/sec; 0.384 sec/batch)
2017-04-02 23:25:31.397438: step 10760, loss = 0.88 (332.1 examples/sec; 0.385 sec/batch)
2017-04-02 23:25:35.259600: step 10770, loss = 0.81 (331.4 examples/sec; 0.386 sec/batch)
2017-04-02 23:25:39.119133: step 10780, loss = 0.86 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 23:25:42.988777: step 10790, loss = 0.98 (330.8 examples/sec; 0.387 sec/batch)
2017-04-02 23:25:46.932293: step 10800, loss = 0.75 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 23:25:50.780485: step 10810, loss = 0.94 (332.6 examples/sec; 0.385 sec/batch)
2017-04-02 23:25:54.620654: step 10820, loss = 0.87 (333.3 examples/sec; 0.384 sec/batch)
2017-04-02 23:25:58.450498: step 10830, loss = 0.73 (334.2 examples/sec; 0.383 sec/batch)
2017-04-02 23:26:11.947255: step 10840, loss = 0.94 (94.8 examples/sec; 1.350 sec/batch)
2017-04-02 23:26:15.858554: step 10850, loss = 0.82 (327.3 examples/sec; 0.391 sec/batch)
2017-04-02 23:26:19.738172: step 10860, loss = 0.89 (329.9 examples/sec; 0.388 sec/batch)
2017-04-02 23:26:23.618869: step 10870, loss = 0.91 (329.8 examples/sec; 0.388 sec/batch)
2017-04-02 23:26:27.460041: step 10880, loss = 0.91 (333.2 examples/sec; 0.384 sec/batch)
2017-04-02 23:26:31.314284: step 10890, loss = 0.80 (332.1 examples/sec; 0.385 sec/batch)
2017-04-02 23:26:35.221582: step 10900, loss = 0.90 (327.6 examples/sec; 0.391 sec/batch)
2017-04-02 23:26:39.076461: step 10910, loss = 0.71 (332.0 examples/sec; 0.385 sec/batch)
2017-04-02 23:26:42.929069: step 10920, loss = 0.77 (332.2 examples/sec; 0.385 sec/batch)
2017-04-02 23:26:46.765283: step 10930, loss = 0.77 (333.7 examples/sec; 0.384 sec/batch)
2017-04-02 23:26:50.621002: step 10940, loss = 0.83 (332.0 examples/sec; 0.386 sec/batch)
2017-04-02 23:26:54.474049: step 10950, loss = 0.99 (332.2 examples/sec; 0.385 sec/batch)
2017-04-02 23:26:58.334001: step 10960, loss = 0.86 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 23:27:02.172058: step 10970, loss = 0.90 (333.5 examples/sec; 0.384 sec/batch)
2017-04-02 23:27:06.038354: step 10980, loss = 0.92 (331.1 examples/sec; 0.387 sec/batch)
2017-04-02 23:27:09.889942: step 10990, loss = 0.70 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 23:27:13.832417: step 11000, loss = 0.79 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 23:27:17.703718: step 11010, loss = 0.91 (330.6 examples/sec; 0.387 sec/batch)
2017-04-02 23:27:21.536444: step 11020, loss = 0.92 (334.0 examples/sec; 0.383 sec/batch)
2017-04-02 23:27:25.392848: step 11030, loss = 0.86 (331.9 examples/sec; 0.386 sec/batch)
2017-04-02 23:27:29.280535: step 11040, loss = 1.06 (329.2 examples/sec; 0.389 sec/batch)
2017-04-02 23:27:33.116260: step 11050, loss = 0.85 (333.7 examples/sec; 0.384 sec/batch)
2017-04-02 23:27:36.982419: step 11060, loss = 0.78 (331.1 examples/sec; 0.387 sec/batch)
2017-04-02 23:27:40.841133: step 11070, loss = 0.98 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 23:27:44.693128: step 11080, loss = 1.02 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 23:27:48.556471: step 11090, loss = 0.91 (331.3 examples/sec; 0.386 sec/batch)
2017-04-02 23:27:52.493171: step 11100, loss = 1.02 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 23:27:56.364262: step 11110, loss = 0.74 (330.7 examples/sec; 0.387 sec/batch)
2017-04-02 23:28:00.196484: step 11120, loss = 0.95 (334.0 examples/sec; 0.383 sec/batch)
2017-04-02 23:28:04.067234: step 11130, loss = 0.84 (330.7 examples/sec; 0.387 sec/batch)
2017-04-02 23:28:07.904847: step 11140, loss = 0.91 (333.5 examples/sec; 0.384 sec/batch)
2017-04-02 23:28:11.749340: step 11150, loss = 0.90 (332.9 examples/sec; 0.384 sec/batch)
2017-04-02 23:28:15.620550: step 11160, loss = 0.84 (330.6 examples/sec; 0.387 sec/batch)
2017-04-02 23:28:19.453646: step 11170, loss = 0.82 (333.9 examples/sec; 0.383 sec/batch)
2017-04-02 23:28:23.344461: step 11180, loss = 0.81 (329.0 examples/sec; 0.389 sec/batch)
2017-04-02 23:28:27.189190: step 11190, loss = 0.86 (332.9 examples/sec; 0.384 sec/batch)
2017-04-02 23:28:31.089116: step 11200, loss = 0.79 (328.2 examples/sec; 0.390 sec/batch)
2017-04-02 23:28:34.954906: step 11210, loss = 0.75 (331.1 examples/sec; 0.387 sec/batch)
2017-04-02 23:28:38.830321: step 11220, loss = 1.02 (330.3 examples/sec; 0.388 sec/batch)
2017-04-02 23:28:42.644482: step 11230, loss = 0.83 (335.6 examples/sec; 0.381 sec/batch)
2017-04-02 23:28:46.516592: step 11240, loss = 1.03 (330.6 examples/sec; 0.387 sec/batch)
2017-04-02 23:28:50.398342: step 11250, loss = 0.70 (329.7 examples/sec; 0.388 sec/batch)
2017-04-02 23:28:54.245959: step 11260, loss = 0.74 (332.7 examples/sec; 0.385 sec/batch)
2017-04-02 23:28:58.104135: step 11270, loss = 0.74 (331.8 examples/sec; 0.386 sec/batch)
2017-04-02 23:29:01.959303: step 11280, loss = 0.85 (332.0 examples/sec; 0.386 sec/batch)
2017-04-02 23:29:05.795408: step 11290, loss = 0.98 (333.7 examples/sec; 0.384 sec/batch)
2017-04-02 23:29:09.691433: step 11300, loss = 0.79 (328.5 examples/sec; 0.390 sec/batch)
2017-04-02 23:29:13.536640: step 11310, loss = 0.81 (332.9 examples/sec; 0.385 sec/batch)
2017-04-02 23:29:17.402127: step 11320, loss = 0.83 (331.1 examples/sec; 0.387 sec/batch)
2017-04-02 23:29:21.267037: step 11330, loss = 0.86 (331.2 examples/sec; 0.386 sec/batch)
2017-04-02 23:29:25.119510: step 11340, loss = 0.75 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 23:29:28.980664: step 11350, loss = 0.91 (331.5 examples/sec; 0.386 sec/batch)
2017-04-02 23:29:32.824136: step 11360, loss = 1.00 (333.0 examples/sec; 0.384 sec/batch)
2017-04-02 23:29:36.685540: step 11370, loss = 0.87 (331.5 examples/sec; 0.386 sec/batch)
2017-04-02 23:29:40.520262: step 11380, loss = 0.89 (333.8 examples/sec; 0.383 sec/batch)
2017-04-02 23:29:44.371972: step 11390, loss = 0.96 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 23:29:48.314891: step 11400, loss = 0.85 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 23:29:52.184059: step 11410, loss = 0.78 (330.8 examples/sec; 0.387 sec/batch)
2017-04-02 23:29:56.052727: step 11420, loss = 0.77 (330.9 examples/sec; 0.387 sec/batch)
2017-04-02 23:30:33.411287: step 11430, loss = 1.07 (34.3 examples/sec; 3.736 sec/batch)
2017-04-02 23:30:37.237480: step 11440, loss = 0.90 (334.5 examples/sec; 0.383 sec/batch)
2017-04-02 23:30:41.074352: step 11450, loss = 0.82 (333.6 examples/sec; 0.384 sec/batch)
2017-04-02 23:30:44.945290: step 11460, loss = 0.78 (330.7 examples/sec; 0.387 sec/batch)
2017-04-02 23:30:48.792691: step 11470, loss = 0.80 (332.7 examples/sec; 0.385 sec/batch)
2017-04-02 23:30:52.626009: step 11480, loss = 0.93 (333.9 examples/sec; 0.383 sec/batch)
2017-04-02 23:30:56.497329: step 11490, loss = 0.91 (330.6 examples/sec; 0.387 sec/batch)
2017-04-02 23:31:00.415758: step 11500, loss = 0.78 (326.7 examples/sec; 0.392 sec/batch)
2017-04-02 23:31:04.322041: step 11510, loss = 0.89 (327.7 examples/sec; 0.391 sec/batch)
2017-04-02 23:31:08.177585: step 11520, loss = 0.94 (332.0 examples/sec; 0.386 sec/batch)
2017-04-02 23:31:12.089457: step 11530, loss = 1.00 (327.2 examples/sec; 0.391 sec/batch)
2017-04-02 23:31:15.957472: step 11540, loss = 0.83 (330.9 examples/sec; 0.387 sec/batch)
2017-04-02 23:31:19.852119: step 11550, loss = 0.92 (328.7 examples/sec; 0.389 sec/batch)
2017-04-02 23:31:23.753489: step 11560, loss = 0.80 (328.1 examples/sec; 0.390 sec/batch)
2017-04-02 23:31:27.617975: step 11570, loss = 0.83 (331.2 examples/sec; 0.386 sec/batch)
2017-04-02 23:31:31.466644: step 11580, loss = 0.92 (332.6 examples/sec; 0.385 sec/batch)
2017-04-02 23:31:35.318254: step 11590, loss = 0.84 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 23:31:39.275455: step 11600, loss = 1.03 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 23:31:43.117580: step 11610, loss = 0.81 (333.1 examples/sec; 0.384 sec/batch)
2017-04-02 23:31:47.002425: step 11620, loss = 0.89 (329.5 examples/sec; 0.388 sec/batch)
2017-04-02 23:31:50.917638: step 11630, loss = 0.92 (326.9 examples/sec; 0.392 sec/batch)
2017-04-02 23:31:54.794576: step 11640, loss = 0.87 (330.2 examples/sec; 0.388 sec/batch)
2017-04-02 23:31:58.672060: step 11650, loss = 0.87 (330.1 examples/sec; 0.388 sec/batch)
2017-04-02 23:32:02.513445: step 11660, loss = 0.83 (333.2 examples/sec; 0.384 sec/batch)
2017-04-02 23:32:06.367288: step 11670, loss = 0.66 (332.1 examples/sec; 0.385 sec/batch)
2017-04-02 23:32:10.253198: step 11680, loss = 0.98 (329.4 examples/sec; 0.389 sec/batch)
2017-04-02 23:32:14.162955: step 11690, loss = 0.77 (327.4 examples/sec; 0.391 sec/batch)
2017-04-02 23:32:18.106490: step 11700, loss = 0.67 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 23:32:22.007379: step 11710, loss = 0.97 (328.1 examples/sec; 0.390 sec/batch)
2017-04-02 23:32:25.892242: step 11720, loss = 0.89 (329.5 examples/sec; 0.388 sec/batch)
2017-04-02 23:32:29.720629: step 11730, loss = 0.85 (334.3 examples/sec; 0.383 sec/batch)
2017-04-02 23:32:33.593874: step 11740, loss = 0.86 (330.5 examples/sec; 0.387 sec/batch)
2017-04-02 23:32:37.459076: step 11750, loss = 0.81 (331.2 examples/sec; 0.387 sec/batch)
2017-04-02 23:32:41.365815: step 11760, loss = 0.91 (327.6 examples/sec; 0.391 sec/batch)
2017-04-02 23:32:45.251209: step 11770, loss = 0.80 (329.4 examples/sec; 0.389 sec/batch)
2017-04-02 23:32:49.163219: step 11780, loss = 0.95 (327.2 examples/sec; 0.391 sec/batch)
2017-04-02 23:32:53.014209: step 11790, loss = 0.96 (332.4 examples/sec; 0.385 sec/batch)
2017-04-02 23:32:57.267634: step 11800, loss = 0.85 (300.9 examples/sec; 0.425 sec/batch)
2017-04-02 23:33:01.169983: step 11810, loss = 0.80 (328.0 examples/sec; 0.390 sec/batch)
2017-04-02 23:33:05.059272: step 11820, loss = 1.02 (329.1 examples/sec; 0.389 sec/batch)
2017-04-02 23:33:08.924541: step 11830, loss = 0.97 (331.2 examples/sec; 0.387 sec/batch)
2017-04-02 23:33:12.800907: step 11840, loss = 0.98 (330.2 examples/sec; 0.388 sec/batch)
2017-04-02 23:33:16.645412: step 11850, loss = 1.04 (332.9 examples/sec; 0.384 sec/batch)
2017-04-02 23:33:20.503353: step 11860, loss = 0.94 (331.8 examples/sec; 0.386 sec/batch)
2017-04-02 23:33:24.348132: step 11870, loss = 0.88 (332.9 examples/sec; 0.384 sec/batch)
2017-04-02 23:33:28.224991: step 11880, loss = 0.78 (330.2 examples/sec; 0.388 sec/batch)
2017-04-02 23:33:32.094049: step 11890, loss = 0.83 (330.8 examples/sec; 0.387 sec/batch)
2017-04-02 23:33:36.064930: step 11900, loss = 0.86 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 23:33:39.948779: step 11910, loss = 1.01 (329.6 examples/sec; 0.388 sec/batch)
2017-04-02 23:33:43.848808: step 11920, loss = 1.01 (328.2 examples/sec; 0.390 sec/batch)
2017-04-02 23:33:47.752229: step 11930, loss = 0.72 (327.9 examples/sec; 0.390 sec/batch)
2017-04-02 23:33:51.665193: step 11940, loss = 0.87 (327.1 examples/sec; 0.391 sec/batch)
2017-04-02 23:33:55.552077: step 11950, loss = 1.09 (329.3 examples/sec; 0.389 sec/batch)
2017-04-02 23:33:59.430773: step 11960, loss = 0.82 (330.0 examples/sec; 0.388 sec/batch)
2017-04-02 23:34:03.317504: step 11970, loss = 0.81 (329.3 examples/sec; 0.389 sec/batch)
2017-04-02 23:34:07.192165: step 11980, loss = 0.72 (330.4 examples/sec; 0.387 sec/batch)
2017-04-02 23:34:11.124103: step 11990, loss = 0.76 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 23:34:15.078172: step 12000, loss = 0.82 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 23:35:24.414415: precision @ 1 = 0.816

2017-04-02 23:36:20.625929: step 12010, loss = 1.00 (10.2 examples/sec; 12.555 sec/batch)
2017-04-02 23:36:24.515138: step 12020, loss = 0.70 (329.1 examples/sec; 0.389 sec/batch)
2017-04-02 23:36:28.397995: step 12030, loss = 0.89 (329.7 examples/sec; 0.388 sec/batch)
2017-04-02 23:36:32.244467: step 12040, loss = 0.92 (332.8 examples/sec; 0.385 sec/batch)
2017-04-02 23:36:36.145458: step 12050, loss = 0.84 (328.1 examples/sec; 0.390 sec/batch)
2017-04-02 23:36:40.028910: step 12060, loss = 0.88 (329.6 examples/sec; 0.388 sec/batch)
2017-04-02 23:36:43.901428: step 12070, loss = 1.04 (330.5 examples/sec; 0.387 sec/batch)
2017-04-02 23:36:47.797786: step 12080, loss = 0.79 (328.5 examples/sec; 0.390 sec/batch)
2017-04-02 23:36:51.695300: step 12090, loss = 0.81 (328.4 examples/sec; 0.390 sec/batch)
2017-04-02 23:36:55.607739: step 12100, loss = 0.79 (327.2 examples/sec; 0.391 sec/batch)
2017-04-02 23:36:59.484799: step 12110, loss = 0.70 (330.1 examples/sec; 0.388 sec/batch)
2017-04-02 23:37:03.376919: step 12120, loss = 0.80 (328.9 examples/sec; 0.389 sec/batch)
2017-04-02 23:37:07.262671: step 12130, loss = 0.90 (329.4 examples/sec; 0.389 sec/batch)
2017-04-02 23:37:11.160008: step 12140, loss = 1.24 (328.4 examples/sec; 0.390 sec/batch)
2017-04-02 23:37:15.050387: step 12150, loss = 0.82 (329.0 examples/sec; 0.389 sec/batch)
2017-04-02 23:37:18.960555: step 12160, loss = 1.09 (327.4 examples/sec; 0.391 sec/batch)
2017-04-02 23:37:22.802163: step 12170, loss = 0.79 (333.2 examples/sec; 0.384 sec/batch)
2017-04-02 23:37:26.657732: step 12180, loss = 0.81 (332.0 examples/sec; 0.386 sec/batch)
2017-04-02 23:37:30.542026: step 12190, loss = 0.81 (329.5 examples/sec; 0.388 sec/batch)
2017-04-02 23:37:34.491708: step 12200, loss = 0.87 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 23:37:38.379778: step 12210, loss = 0.86 (329.2 examples/sec; 0.389 sec/batch)
2017-04-02 23:37:42.293004: step 12220, loss = 0.93 (327.1 examples/sec; 0.391 sec/batch)
2017-04-02 23:37:46.168169: step 12230, loss = 0.84 (330.3 examples/sec; 0.388 sec/batch)
2017-04-02 23:37:50.067659: step 12240, loss = 0.75 (328.2 examples/sec; 0.390 sec/batch)
2017-04-02 23:37:53.918066: step 12250, loss = 0.81 (332.4 examples/sec; 0.385 sec/batch)
2017-04-02 23:37:57.820302: step 12260, loss = 0.78 (328.0 examples/sec; 0.390 sec/batch)
2017-04-02 23:38:01.695273: step 12270, loss = 0.98 (330.3 examples/sec; 0.387 sec/batch)
2017-04-02 23:38:05.556170: step 12280, loss = 0.72 (331.5 examples/sec; 0.386 sec/batch)
2017-04-02 23:38:09.452420: step 12290, loss = 1.01 (328.5 examples/sec; 0.390 sec/batch)
2017-04-02 23:38:13.380619: step 12300, loss = 0.95 (325.8 examples/sec; 0.393 sec/batch)
2017-04-02 23:38:17.237082: step 12310, loss = 0.85 (331.9 examples/sec; 0.386 sec/batch)
2017-04-02 23:38:21.081372: step 12320, loss = 0.71 (333.0 examples/sec; 0.384 sec/batch)
2017-04-02 23:38:24.936500: step 12330, loss = 0.99 (332.0 examples/sec; 0.386 sec/batch)
2017-04-02 23:38:28.807530: step 12340, loss = 0.85 (330.7 examples/sec; 0.387 sec/batch)
2017-04-02 23:38:32.630654: step 12350, loss = 0.91 (334.8 examples/sec; 0.382 sec/batch)
2017-04-02 23:38:36.491764: step 12360, loss = 0.89 (331.5 examples/sec; 0.386 sec/batch)
2017-04-02 23:38:40.409326: step 12370, loss = 0.85 (326.7 examples/sec; 0.392 sec/batch)
2017-04-02 23:38:44.292387: step 12380, loss = 0.87 (329.6 examples/sec; 0.388 sec/batch)
2017-04-02 23:38:48.182565: step 12390, loss = 0.91 (329.0 examples/sec; 0.389 sec/batch)
2017-04-02 23:38:52.131729: step 12400, loss = 0.96 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 23:38:55.992900: step 12410, loss = 0.87 (331.5 examples/sec; 0.386 sec/batch)
2017-04-02 23:38:59.834725: step 12420, loss = 0.91 (333.2 examples/sec; 0.384 sec/batch)
2017-04-02 23:39:03.721517: step 12430, loss = 0.91 (329.3 examples/sec; 0.389 sec/batch)
2017-04-02 23:39:07.597803: step 12440, loss = 0.73 (330.2 examples/sec; 0.388 sec/batch)
2017-04-02 23:39:11.478858: step 12450, loss = 0.75 (329.8 examples/sec; 0.388 sec/batch)
2017-04-02 23:39:15.339926: step 12460, loss = 0.85 (331.5 examples/sec; 0.386 sec/batch)
2017-04-02 23:39:19.238433: step 12470, loss = 0.73 (328.3 examples/sec; 0.390 sec/batch)
2017-04-02 23:39:23.081037: step 12480, loss = 0.74 (333.1 examples/sec; 0.384 sec/batch)
2017-04-02 23:39:26.910888: step 12490, loss = 0.90 (334.2 examples/sec; 0.383 sec/batch)
2017-04-02 23:39:30.810751: step 12500, loss = 1.03 (328.2 examples/sec; 0.390 sec/batch)
2017-04-02 23:39:34.718237: step 12510, loss = 0.89 (327.6 examples/sec; 0.391 sec/batch)
2017-04-02 23:39:38.586785: step 12520, loss = 0.71 (330.9 examples/sec; 0.387 sec/batch)
2017-04-02 23:39:42.464336: step 12530, loss = 0.85 (330.1 examples/sec; 0.388 sec/batch)
2017-04-02 23:39:46.389298: step 12540, loss = 0.84 (326.1 examples/sec; 0.392 sec/batch)
2017-04-02 23:39:50.226187: step 12550, loss = 0.75 (333.6 examples/sec; 0.384 sec/batch)
2017-04-02 23:39:54.085495: step 12560, loss = 0.90 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 23:39:57.980733: step 12570, loss = 0.91 (328.6 examples/sec; 0.390 sec/batch)
2017-04-02 23:40:01.880440: step 12580, loss = 0.97 (328.2 examples/sec; 0.390 sec/batch)
2017-04-02 23:40:05.756409: step 12590, loss = 0.80 (330.2 examples/sec; 0.388 sec/batch)
2017-04-02 23:40:09.722477: step 12600, loss = 0.78 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 23:40:13.637809: step 12610, loss = 0.77 (326.9 examples/sec; 0.392 sec/batch)
2017-04-02 23:40:17.505607: step 12620, loss = 0.88 (330.9 examples/sec; 0.387 sec/batch)
2017-04-02 23:40:21.374039: step 12630, loss = 0.89 (330.9 examples/sec; 0.387 sec/batch)
2017-04-02 23:40:25.273648: step 12640, loss = 0.97 (328.2 examples/sec; 0.390 sec/batch)
2017-04-02 23:40:29.118542: step 12650, loss = 0.92 (332.9 examples/sec; 0.384 sec/batch)
2017-04-02 23:40:32.970502: step 12660, loss = 0.89 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 23:40:36.864346: step 12670, loss = 0.81 (328.7 examples/sec; 0.389 sec/batch)
2017-04-02 23:40:40.744144: step 12680, loss = 0.73 (329.9 examples/sec; 0.388 sec/batch)
2017-04-02 23:40:44.666157: step 12690, loss = 0.96 (326.4 examples/sec; 0.392 sec/batch)
2017-04-02 23:40:48.631624: step 12700, loss = 0.63 (322.8 examples/sec; 0.397 sec/batch)
2017-04-02 23:40:52.543038: step 12710, loss = 0.96 (327.2 examples/sec; 0.391 sec/batch)
2017-04-02 23:40:56.398189: step 12720, loss = 0.98 (332.0 examples/sec; 0.386 sec/batch)
2017-04-02 23:41:00.303947: step 12730, loss = 0.94 (327.7 examples/sec; 0.391 sec/batch)
2017-04-02 23:41:04.182659: step 12740, loss = 0.72 (330.0 examples/sec; 0.388 sec/batch)
2017-04-02 23:41:08.066266: step 12750, loss = 0.81 (329.6 examples/sec; 0.388 sec/batch)
2017-04-02 23:41:11.990729: step 12760, loss = 0.77 (326.2 examples/sec; 0.392 sec/batch)
2017-04-02 23:41:15.856535: step 12770, loss = 0.99 (331.1 examples/sec; 0.387 sec/batch)
2017-04-02 23:41:19.749185: step 12780, loss = 0.84 (328.8 examples/sec; 0.389 sec/batch)
2017-04-02 23:41:23.644975: step 12790, loss = 1.07 (328.6 examples/sec; 0.390 sec/batch)
2017-04-02 23:41:27.561802: step 12800, loss = 0.66 (326.8 examples/sec; 0.392 sec/batch)
2017-04-02 23:41:31.465339: step 12810, loss = 0.95 (327.9 examples/sec; 0.390 sec/batch)
2017-04-02 23:41:35.363275: step 12820, loss = 0.89 (328.4 examples/sec; 0.390 sec/batch)
2017-04-02 23:41:39.258962: step 12830, loss = 0.89 (328.6 examples/sec; 0.390 sec/batch)
2017-04-02 23:41:43.146415: step 12840, loss = 0.88 (329.3 examples/sec; 0.389 sec/batch)
2017-04-02 23:41:47.070626: step 12850, loss = 0.87 (326.2 examples/sec; 0.392 sec/batch)
2017-04-02 23:41:50.971318: step 12860, loss = 0.72 (328.1 examples/sec; 0.390 sec/batch)
2017-04-02 23:41:54.903271: step 12870, loss = 0.83 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 23:41:58.790536: step 12880, loss = 0.84 (329.3 examples/sec; 0.389 sec/batch)
2017-04-02 23:42:02.704560: step 12890, loss = 0.90 (327.0 examples/sec; 0.391 sec/batch)
2017-04-02 23:42:06.648643: step 12900, loss = 0.88 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 23:42:10.543046: step 12910, loss = 0.95 (328.7 examples/sec; 0.389 sec/batch)
2017-04-02 23:42:14.453535: step 12920, loss = 0.88 (327.3 examples/sec; 0.391 sec/batch)
2017-04-02 23:42:18.330505: step 12930, loss = 0.70 (330.2 examples/sec; 0.388 sec/batch)
2017-04-02 23:42:22.215018: step 12940, loss = 0.73 (329.5 examples/sec; 0.388 sec/batch)
2017-04-02 23:42:26.124359: step 12950, loss = 0.88 (327.4 examples/sec; 0.391 sec/batch)
2017-04-02 23:42:30.000542: step 12960, loss = 0.80 (330.2 examples/sec; 0.388 sec/batch)
2017-04-02 23:42:33.888149: step 12970, loss = 0.77 (329.3 examples/sec; 0.389 sec/batch)
2017-04-02 23:42:37.789883: step 12980, loss = 0.79 (328.1 examples/sec; 0.390 sec/batch)
2017-04-02 23:42:41.695118: step 12990, loss = 0.95 (327.8 examples/sec; 0.391 sec/batch)
2017-04-02 23:42:45.640382: step 13000, loss = 0.74 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 23:42:49.557469: step 13010, loss = 0.79 (326.8 examples/sec; 0.392 sec/batch)
2017-04-02 23:42:53.467143: step 13020, loss = 0.84 (327.4 examples/sec; 0.391 sec/batch)
2017-04-02 23:42:57.355218: step 13030, loss = 0.80 (329.2 examples/sec; 0.389 sec/batch)
2017-04-02 23:43:01.240904: step 13040, loss = 0.89 (329.4 examples/sec; 0.389 sec/batch)
2017-04-02 23:43:05.099169: step 13050, loss = 0.95 (331.8 examples/sec; 0.386 sec/batch)
2017-04-02 23:43:08.949221: step 13060, loss = 0.92 (332.5 examples/sec; 0.385 sec/batch)
2017-04-02 23:43:12.833831: step 13070, loss = 0.76 (329.5 examples/sec; 0.388 sec/batch)
2017-04-02 23:43:16.699174: step 13080, loss = 0.82 (331.1 examples/sec; 0.387 sec/batch)
2017-04-02 23:43:20.562844: step 13090, loss = 0.88 (331.3 examples/sec; 0.386 sec/batch)
2017-04-02 23:43:24.529130: step 13100, loss = 0.76 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 23:43:28.413003: step 13110, loss = 0.72 (329.6 examples/sec; 0.388 sec/batch)
2017-04-02 23:43:32.319070: step 13120, loss = 0.92 (327.7 examples/sec; 0.391 sec/batch)
2017-04-02 23:43:36.241053: step 13130, loss = 0.82 (326.4 examples/sec; 0.392 sec/batch)
2017-04-02 23:43:40.102448: step 13140, loss = 0.61 (331.5 examples/sec; 0.386 sec/batch)
2017-04-02 23:43:44.001411: step 13150, loss = 0.89 (328.3 examples/sec; 0.390 sec/batch)
2017-04-02 23:43:47.885351: step 13160, loss = 0.92 (329.6 examples/sec; 0.388 sec/batch)
2017-04-02 23:43:51.774060: step 13170, loss = 0.75 (329.2 examples/sec; 0.389 sec/batch)
2017-04-02 23:43:55.657980: step 13180, loss = 0.74 (329.6 examples/sec; 0.388 sec/batch)
2017-04-02 23:43:59.538227: step 13190, loss = 1.06 (329.9 examples/sec; 0.388 sec/batch)
2017-04-02 23:44:03.452970: step 13200, loss = 0.77 (327.0 examples/sec; 0.391 sec/batch)
2017-04-02 23:44:07.335176: step 13210, loss = 1.00 (329.7 examples/sec; 0.388 sec/batch)
2017-04-02 23:44:11.185095: step 13220, loss = 1.01 (332.5 examples/sec; 0.385 sec/batch)
2017-04-02 23:44:15.053217: step 13230, loss = 0.91 (330.9 examples/sec; 0.387 sec/batch)
2017-04-02 23:44:18.917434: step 13240, loss = 0.77 (331.2 examples/sec; 0.386 sec/batch)
2017-04-02 23:44:22.830428: step 13250, loss = 0.83 (327.1 examples/sec; 0.391 sec/batch)
2017-04-02 23:44:26.720651: step 13260, loss = 0.86 (329.0 examples/sec; 0.389 sec/batch)
2017-04-02 23:44:30.581091: step 13270, loss = 0.80 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 23:44:34.471382: step 13280, loss = 0.68 (329.0 examples/sec; 0.389 sec/batch)
2017-04-02 23:44:38.345226: step 13290, loss = 0.95 (330.4 examples/sec; 0.387 sec/batch)
2017-04-02 23:44:42.288313: step 13300, loss = 0.89 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 23:44:46.181130: step 13310, loss = 0.83 (328.8 examples/sec; 0.389 sec/batch)
2017-04-02 23:44:50.096102: step 13320, loss = 1.01 (326.9 examples/sec; 0.391 sec/batch)
2017-04-02 23:44:53.978951: step 13330, loss = 0.94 (329.7 examples/sec; 0.388 sec/batch)
2017-04-02 23:44:57.961328: step 13340, loss = 0.90 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 23:45:01.853914: step 13350, loss = 1.01 (328.8 examples/sec; 0.389 sec/batch)
2017-04-02 23:45:05.743497: step 13360, loss = 0.89 (329.1 examples/sec; 0.389 sec/batch)
2017-04-02 23:45:09.637085: step 13370, loss = 0.91 (328.7 examples/sec; 0.389 sec/batch)
2017-04-02 23:45:13.511040: step 13380, loss = 0.76 (330.4 examples/sec; 0.387 sec/batch)
2017-04-02 23:45:17.372451: step 13390, loss = 0.77 (331.5 examples/sec; 0.386 sec/batch)
2017-04-02 23:45:21.286482: step 13400, loss = 1.03 (327.0 examples/sec; 0.391 sec/batch)
2017-04-02 23:45:25.154669: step 13410, loss = 0.85 (330.9 examples/sec; 0.387 sec/batch)
2017-04-02 23:45:29.030350: step 13420, loss = 0.91 (330.3 examples/sec; 0.388 sec/batch)
2017-04-02 23:45:32.918228: step 13430, loss = 0.79 (329.2 examples/sec; 0.389 sec/batch)
2017-04-02 23:45:36.787899: step 13440, loss = 0.92 (330.8 examples/sec; 0.387 sec/batch)
2017-04-02 23:45:40.670545: step 13450, loss = 0.74 (329.7 examples/sec; 0.388 sec/batch)
2017-04-02 23:45:44.551109: step 13460, loss = 0.87 (329.8 examples/sec; 0.388 sec/batch)
2017-04-02 23:45:48.430586: step 13470, loss = 0.94 (329.9 examples/sec; 0.388 sec/batch)
2017-04-02 23:45:52.312322: step 13480, loss = 0.92 (329.7 examples/sec; 0.388 sec/batch)
2017-04-02 23:45:56.217663: step 13490, loss = 1.00 (327.8 examples/sec; 0.391 sec/batch)
2017-04-02 23:46:00.148547: step 13500, loss = 0.85 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 23:46:04.054641: step 13510, loss = 0.86 (327.7 examples/sec; 0.391 sec/batch)
2017-04-02 23:46:07.972812: step 13520, loss = 1.14 (326.7 examples/sec; 0.392 sec/batch)
2017-04-02 23:46:19.918894: step 13530, loss = 0.98 (107.1 examples/sec; 1.195 sec/batch)
2017-04-02 23:46:23.786253: step 13540, loss = 0.77 (331.0 examples/sec; 0.387 sec/batch)
2017-04-02 23:46:27.622313: step 13550, loss = 0.76 (333.7 examples/sec; 0.384 sec/batch)
2017-04-02 23:46:31.501908: step 13560, loss = 0.88 (329.9 examples/sec; 0.388 sec/batch)
2017-04-02 23:46:35.354633: step 13570, loss = 0.80 (332.2 examples/sec; 0.385 sec/batch)
2017-04-02 23:46:39.249317: step 13580, loss = 0.74 (328.7 examples/sec; 0.389 sec/batch)
2017-04-02 23:46:43.098757: step 13590, loss = 0.68 (332.5 examples/sec; 0.385 sec/batch)
2017-04-02 23:46:47.081523: step 13600, loss = 0.88 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 23:46:50.959035: step 13610, loss = 0.72 (330.1 examples/sec; 0.388 sec/batch)
2017-04-02 23:46:54.853445: step 13620, loss = 0.93 (328.7 examples/sec; 0.389 sec/batch)
2017-04-02 23:46:58.708975: step 13630, loss = 0.97 (332.0 examples/sec; 0.386 sec/batch)
2017-04-02 23:47:02.605964: step 13640, loss = 0.97 (328.5 examples/sec; 0.390 sec/batch)
2017-04-02 23:47:06.495579: step 13650, loss = 1.00 (329.1 examples/sec; 0.389 sec/batch)
2017-04-02 23:47:10.358160: step 13660, loss = 0.78 (331.4 examples/sec; 0.386 sec/batch)
2017-04-02 23:47:14.238577: step 13670, loss = 0.76 (329.9 examples/sec; 0.388 sec/batch)
2017-04-02 23:47:18.101461: step 13680, loss = 0.80 (331.4 examples/sec; 0.386 sec/batch)
2017-04-02 23:47:22.011123: step 13690, loss = 0.91 (327.4 examples/sec; 0.391 sec/batch)
2017-04-02 23:47:25.939070: step 13700, loss = 0.70 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 23:47:29.827440: step 13710, loss = 0.89 (329.2 examples/sec; 0.389 sec/batch)
2017-04-02 23:47:33.710020: step 13720, loss = 0.84 (329.7 examples/sec; 0.388 sec/batch)
2017-04-02 23:47:37.594265: step 13730, loss = 0.84 (329.5 examples/sec; 0.388 sec/batch)
2017-04-02 23:47:41.461197: step 13740, loss = 0.71 (331.0 examples/sec; 0.387 sec/batch)
2017-04-02 23:47:45.338794: step 13750, loss = 0.64 (330.1 examples/sec; 0.388 sec/batch)
2017-04-02 23:47:49.205596: step 13760, loss = 0.89 (331.0 examples/sec; 0.387 sec/batch)
2017-04-02 23:47:53.070299: step 13770, loss = 0.77 (331.2 examples/sec; 0.386 sec/batch)
2017-04-02 23:47:56.964169: step 13780, loss = 0.83 (328.7 examples/sec; 0.389 sec/batch)
2017-04-02 23:48:00.833351: step 13790, loss = 0.93 (330.8 examples/sec; 0.387 sec/batch)
2017-04-02 23:48:04.756164: step 13800, loss = 1.00 (326.3 examples/sec; 0.392 sec/batch)
2017-04-02 23:48:08.622707: step 13810, loss = 0.86 (331.0 examples/sec; 0.387 sec/batch)
2017-04-02 23:48:12.483930: step 13820, loss = 0.82 (331.5 examples/sec; 0.386 sec/batch)
2017-04-02 23:48:16.366770: step 13830, loss = 0.81 (329.7 examples/sec; 0.388 sec/batch)
2017-04-02 23:48:20.227083: step 13840, loss = 0.78 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 23:48:24.101194: step 13850, loss = 0.87 (330.4 examples/sec; 0.387 sec/batch)
2017-04-02 23:48:27.993191: step 13860, loss = 0.74 (328.9 examples/sec; 0.389 sec/batch)
2017-04-02 23:48:31.823186: step 13870, loss = 0.75 (334.2 examples/sec; 0.383 sec/batch)
2017-04-02 23:48:35.677344: step 13880, loss = 0.85 (332.1 examples/sec; 0.385 sec/batch)
2017-04-02 23:48:39.553636: step 13890, loss = 0.76 (330.2 examples/sec; 0.388 sec/batch)
2017-04-02 23:48:43.497522: step 13900, loss = 1.03 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 23:48:47.445319: step 13910, loss = 0.95 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 23:48:51.367389: step 13920, loss = 0.79 (326.4 examples/sec; 0.392 sec/batch)
2017-04-02 23:48:55.226984: step 13930, loss = 0.76 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 23:48:59.098948: step 13940, loss = 0.75 (330.6 examples/sec; 0.387 sec/batch)
2017-04-02 23:49:02.994025: step 13950, loss = 0.86 (328.6 examples/sec; 0.390 sec/batch)
2017-04-02 23:49:06.888606: step 13960, loss = 0.75 (328.7 examples/sec; 0.389 sec/batch)
2017-04-02 23:49:10.816391: step 13970, loss = 0.97 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 23:49:14.662059: step 13980, loss = 0.67 (332.8 examples/sec; 0.385 sec/batch)
2017-04-02 23:49:18.548023: step 13990, loss = 0.89 (329.4 examples/sec; 0.389 sec/batch)
2017-04-02 23:49:22.504514: step 14000, loss = 0.80 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 23:51:49.320396: precision @ 1 = 0.824

2017-04-02 23:52:17.590208: step 14010, loss = 0.71 (7.3 examples/sec; 17.509 sec/batch)
2017-04-02 23:52:21.445352: step 14020, loss = 0.95 (332.0 examples/sec; 0.386 sec/batch)
2017-04-02 23:52:25.324499: step 14030, loss = 0.93 (330.0 examples/sec; 0.388 sec/batch)
2017-04-02 23:52:29.227950: step 14040, loss = 0.98 (327.9 examples/sec; 0.390 sec/batch)
2017-04-02 23:52:33.108349: step 14050, loss = 0.96 (329.9 examples/sec; 0.388 sec/batch)
2017-04-02 23:52:36.992291: step 14060, loss = 0.91 (329.6 examples/sec; 0.388 sec/batch)
2017-04-02 23:52:40.859665: step 14070, loss = 0.92 (331.0 examples/sec; 0.387 sec/batch)
2017-04-02 23:52:44.717398: step 14080, loss = 0.93 (331.8 examples/sec; 0.386 sec/batch)
2017-04-02 23:52:48.581821: step 14090, loss = 0.93 (331.2 examples/sec; 0.386 sec/batch)
2017-04-02 23:52:52.506526: step 14100, loss = 1.05 (326.1 examples/sec; 0.392 sec/batch)
2017-04-02 23:52:56.347632: step 14110, loss = 0.70 (333.2 examples/sec; 0.384 sec/batch)
2017-04-02 23:53:00.231790: step 14120, loss = 0.81 (329.5 examples/sec; 0.388 sec/batch)
2017-04-02 23:53:04.109380: step 14130, loss = 0.86 (330.1 examples/sec; 0.388 sec/batch)
2017-04-02 23:53:07.978215: step 14140, loss = 0.76 (330.8 examples/sec; 0.387 sec/batch)
2017-04-02 23:53:11.859545: step 14150, loss = 0.89 (329.8 examples/sec; 0.388 sec/batch)
2017-04-02 23:53:15.732532: step 14160, loss = 0.77 (330.5 examples/sec; 0.387 sec/batch)
2017-04-02 23:53:19.590140: step 14170, loss = 0.90 (331.8 examples/sec; 0.386 sec/batch)
2017-04-02 23:53:23.469117: step 14180, loss = 0.83 (330.0 examples/sec; 0.388 sec/batch)
2017-04-02 23:53:27.321945: step 14190, loss = 0.93 (332.2 examples/sec; 0.385 sec/batch)
2017-04-02 23:53:31.254836: step 14200, loss = 0.77 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 23:53:35.133911: step 14210, loss = 0.89 (330.0 examples/sec; 0.388 sec/batch)
2017-04-02 23:53:38.969776: step 14220, loss = 0.77 (333.7 examples/sec; 0.384 sec/batch)
2017-04-02 23:53:42.841212: step 14230, loss = 0.77 (330.6 examples/sec; 0.387 sec/batch)
2017-04-02 23:53:46.699720: step 14240, loss = 0.81 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 23:53:50.579444: step 14250, loss = 1.01 (329.9 examples/sec; 0.388 sec/batch)
2017-04-02 23:53:54.447383: step 14260, loss = 1.10 (330.9 examples/sec; 0.387 sec/batch)
2017-04-02 23:53:58.335515: step 14270, loss = 0.93 (329.2 examples/sec; 0.389 sec/batch)
2017-04-02 23:54:02.176227: step 14280, loss = 0.86 (333.3 examples/sec; 0.384 sec/batch)
2017-04-02 23:54:06.035480: step 14290, loss = 0.87 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 23:54:09.942877: step 14300, loss = 0.85 (327.6 examples/sec; 0.391 sec/batch)
2017-04-02 23:54:13.848944: step 14310, loss = 0.83 (327.7 examples/sec; 0.391 sec/batch)
2017-04-02 23:54:17.728351: step 14320, loss = 0.80 (329.9 examples/sec; 0.388 sec/batch)
2017-04-02 23:54:21.578514: step 14330, loss = 0.81 (332.5 examples/sec; 0.385 sec/batch)
2017-04-02 23:54:25.427126: step 14340, loss = 0.92 (332.6 examples/sec; 0.385 sec/batch)
2017-04-02 23:54:29.257021: step 14350, loss = 0.71 (334.2 examples/sec; 0.383 sec/batch)
2017-04-02 23:54:33.116122: step 14360, loss = 0.82 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 23:54:36.960129: step 14370, loss = 0.86 (333.0 examples/sec; 0.384 sec/batch)
2017-04-02 23:54:40.826233: step 14380, loss = 0.84 (331.1 examples/sec; 0.387 sec/batch)
2017-04-02 23:54:44.674882: step 14390, loss = 0.84 (332.6 examples/sec; 0.385 sec/batch)
2017-04-02 23:54:48.587521: step 14400, loss = 0.81 (327.1 examples/sec; 0.391 sec/batch)
2017-04-02 23:54:52.454366: step 14410, loss = 0.93 (331.0 examples/sec; 0.387 sec/batch)
2017-04-02 23:54:56.346562: step 14420, loss = 0.80 (328.9 examples/sec; 0.389 sec/batch)
2017-04-02 23:55:00.187053: step 14430, loss = 0.76 (333.3 examples/sec; 0.384 sec/batch)
2017-04-02 23:55:04.075736: step 14440, loss = 0.88 (329.2 examples/sec; 0.389 sec/batch)
2017-04-02 23:55:07.942637: step 14450, loss = 0.95 (331.0 examples/sec; 0.387 sec/batch)
2017-04-02 23:55:11.794043: step 14460, loss = 0.93 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 23:55:15.644052: step 14470, loss = 0.71 (332.5 examples/sec; 0.385 sec/batch)
2017-04-02 23:55:19.522965: step 14480, loss = 0.83 (330.0 examples/sec; 0.388 sec/batch)
2017-04-02 23:55:23.401310: step 14490, loss = 0.94 (330.0 examples/sec; 0.388 sec/batch)
2017-04-02 23:55:27.313862: step 14500, loss = 0.86 (327.2 examples/sec; 0.391 sec/batch)
2017-04-02 23:55:31.218581: step 14510, loss = 1.03 (327.8 examples/sec; 0.390 sec/batch)
2017-04-02 23:55:35.081459: step 14520, loss = 0.85 (331.4 examples/sec; 0.386 sec/batch)
2017-04-02 23:55:38.961483: step 14530, loss = 0.90 (329.9 examples/sec; 0.388 sec/batch)
2017-04-02 23:55:42.839932: step 14540, loss = 0.78 (330.0 examples/sec; 0.388 sec/batch)
2017-04-02 23:55:46.721507: step 14550, loss = 0.95 (329.8 examples/sec; 0.388 sec/batch)
2017-04-02 23:55:50.577905: step 14560, loss = 0.86 (331.9 examples/sec; 0.386 sec/batch)
2017-04-02 23:55:54.431274: step 14570, loss = 0.83 (332.2 examples/sec; 0.385 sec/batch)
2017-04-02 23:55:58.311666: step 14580, loss = 0.86 (329.9 examples/sec; 0.388 sec/batch)
2017-04-02 23:56:02.154359: step 14590, loss = 0.91 (333.1 examples/sec; 0.384 sec/batch)
2017-04-02 23:56:06.100127: step 14600, loss = 0.96 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 23:56:09.976558: step 14610, loss = 0.96 (330.2 examples/sec; 0.388 sec/batch)
2017-04-02 23:56:21.858454: step 14620, loss = 0.89 (107.7 examples/sec; 1.188 sec/batch)
2017-04-02 23:56:25.708520: step 14630, loss = 0.78 (332.5 examples/sec; 0.385 sec/batch)
2017-04-02 23:56:29.534611: step 14640, loss = 0.66 (334.5 examples/sec; 0.383 sec/batch)
2017-04-02 23:56:33.419361: step 14650, loss = 0.93 (329.5 examples/sec; 0.388 sec/batch)
2017-04-02 23:56:37.269584: step 14660, loss = 0.75 (332.4 examples/sec; 0.385 sec/batch)
2017-04-02 23:56:41.121690: step 14670, loss = 0.88 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 23:56:44.980375: step 14680, loss = 0.69 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 23:56:48.825215: step 14690, loss = 0.85 (332.9 examples/sec; 0.384 sec/batch)
2017-04-02 23:56:52.741396: step 14700, loss = 0.68 (326.8 examples/sec; 0.392 sec/batch)
2017-04-02 23:56:56.597342: step 14710, loss = 0.85 (332.0 examples/sec; 0.386 sec/batch)
2017-04-02 23:57:00.450767: step 14720, loss = 0.87 (332.2 examples/sec; 0.385 sec/batch)
2017-04-02 23:57:04.309847: step 14730, loss = 0.89 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 23:57:08.177525: step 14740, loss = 0.96 (330.9 examples/sec; 0.387 sec/batch)
2017-04-02 23:57:12.064857: step 14750, loss = 0.73 (329.3 examples/sec; 0.389 sec/batch)
2017-04-02 23:57:15.953959: step 14760, loss = 0.69 (329.1 examples/sec; 0.389 sec/batch)
2017-04-02 23:57:19.836310: step 14770, loss = 0.89 (329.7 examples/sec; 0.388 sec/batch)
2017-04-02 23:57:23.712758: step 14780, loss = 0.92 (330.2 examples/sec; 0.388 sec/batch)
2017-04-02 23:57:27.609629: step 14790, loss = 1.00 (328.5 examples/sec; 0.390 sec/batch)
2017-04-02 23:57:31.570419: step 14800, loss = 0.79 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 23:57:35.480614: step 14810, loss = 0.78 (327.3 examples/sec; 0.391 sec/batch)
2017-04-02 23:57:39.388547: step 14820, loss = 0.92 (327.5 examples/sec; 0.391 sec/batch)
2017-04-02 23:57:43.265635: step 14830, loss = 0.69 (330.1 examples/sec; 0.388 sec/batch)
2017-04-02 23:57:47.138104: step 14840, loss = 0.98 (330.5 examples/sec; 0.387 sec/batch)
2017-04-02 23:57:51.015641: step 14850, loss = 0.80 (330.1 examples/sec; 0.388 sec/batch)
2017-04-02 23:57:54.904956: step 14860, loss = 0.95 (329.1 examples/sec; 0.389 sec/batch)
2017-04-02 23:57:58.769432: step 14870, loss = 0.83 (331.2 examples/sec; 0.386 sec/batch)
2017-04-02 23:58:02.640721: step 14880, loss = 1.10 (330.6 examples/sec; 0.387 sec/batch)
2017-04-02 23:58:06.576603: step 14890, loss = 0.83 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 23:58:10.563491: step 14900, loss = 0.89 (321.1 examples/sec; 0.399 sec/batch)
2017-04-02 23:58:14.501210: step 14910, loss = 0.93 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 23:58:18.420516: step 14920, loss = 0.76 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 23:58:22.365212: step 14930, loss = 0.86 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 23:58:26.268238: step 14940, loss = 1.00 (328.0 examples/sec; 0.390 sec/batch)
2017-04-02 23:58:30.184916: step 14950, loss = 0.95 (326.8 examples/sec; 0.392 sec/batch)
2017-04-02 23:58:34.108400: step 14960, loss = 0.95 (326.2 examples/sec; 0.392 sec/batch)
2017-04-02 23:58:38.019809: step 14970, loss = 0.67 (327.2 examples/sec; 0.391 sec/batch)
2017-04-02 23:58:41.901851: step 14980, loss = 0.86 (329.7 examples/sec; 0.388 sec/batch)
2017-04-02 23:58:45.801472: step 14990, loss = 0.70 (328.2 examples/sec; 0.390 sec/batch)
2017-04-02 23:58:49.769964: step 15000, loss = 0.74 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 23:58:53.685174: step 15010, loss = 0.95 (326.9 examples/sec; 0.392 sec/batch)
2017-04-02 23:58:57.593320: step 15020, loss = 0.88 (327.5 examples/sec; 0.391 sec/batch)
2017-04-02 23:59:01.505743: step 15030, loss = 1.03 (327.2 examples/sec; 0.391 sec/batch)
2017-04-02 23:59:05.388787: step 15040, loss = 0.84 (329.6 examples/sec; 0.388 sec/batch)
2017-04-02 23:59:09.292437: step 15050, loss = 0.79 (327.9 examples/sec; 0.390 sec/batch)
2017-04-02 23:59:13.186220: step 15060, loss = 0.84 (328.7 examples/sec; 0.389 sec/batch)
2017-04-02 23:59:17.096506: step 15070, loss = 0.94 (327.3 examples/sec; 0.391 sec/batch)
2017-04-02 23:59:20.971132: step 15080, loss = 0.82 (330.4 examples/sec; 0.387 sec/batch)
2017-04-02 23:59:24.860765: step 15090, loss = 0.73 (329.1 examples/sec; 0.389 sec/batch)
2017-04-02 23:59:28.788932: step 15100, loss = 0.87 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 23:59:32.633261: step 15110, loss = 0.90 (333.0 examples/sec; 0.384 sec/batch)
2017-04-02 23:59:36.497597: step 15120, loss = 0.88 (331.2 examples/sec; 0.386 sec/batch)
2017-04-02 23:59:40.356511: step 15130, loss = 0.80 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 23:59:44.267925: step 15140, loss = 0.87 (327.2 examples/sec; 0.391 sec/batch)
2017-04-02 23:59:48.159586: step 15150, loss = 0.85 (328.9 examples/sec; 0.389 sec/batch)
2017-04-02 23:59:52.094501: step 15160, loss = 0.81 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 23:59:56.003529: step 15170, loss = 1.09 (327.4 examples/sec; 0.391 sec/batch)
2017-04-02 23:59:59.908743: step 15180, loss = 0.87 (327.8 examples/sec; 0.391 sec/batch)
2017-04-03 00:00:03.799352: step 15190, loss = 1.05 (329.0 examples/sec; 0.389 sec/batch)
2017-04-03 00:00:07.727792: step 15200, loss = 0.75 (325.8 examples/sec; 0.393 sec/batch)
2017-04-03 00:00:26.661963: step 15210, loss = 0.92 (67.6 examples/sec; 1.893 sec/batch)
2017-04-03 00:00:30.569098: step 15220, loss = 1.05 (327.6 examples/sec; 0.391 sec/batch)
2017-04-03 00:00:34.465001: step 15230, loss = 1.05 (328.6 examples/sec; 0.390 sec/batch)
2017-04-03 00:00:38.370921: step 15240, loss = 0.84 (327.7 examples/sec; 0.391 sec/batch)
2017-04-03 00:00:42.293705: step 15250, loss = 0.88 (326.3 examples/sec; 0.392 sec/batch)
2017-04-03 00:00:46.194491: step 15260, loss = 0.97 (328.1 examples/sec; 0.390 sec/batch)
2017-04-03 00:00:50.097031: step 15270, loss = 0.91 (328.0 examples/sec; 0.390 sec/batch)
2017-04-03 00:00:54.026736: step 15280, loss = 0.75 (325.7 examples/sec; 0.393 sec/batch)
2017-04-03 00:00:57.932061: step 15290, loss = 0.87 (327.8 examples/sec; 0.391 sec/batch)
2017-04-03 00:01:01.890363: step 15300, loss = 0.72 (323.4 examples/sec; 0.396 sec/batch)
2017-04-03 00:01:05.784591: step 15310, loss = 0.74 (328.7 examples/sec; 0.389 sec/batch)
2017-04-03 00:01:09.650227: step 15320, loss = 1.02 (331.1 examples/sec; 0.387 sec/batch)
2017-04-03 00:01:13.574799: step 15330, loss = 0.90 (326.2 examples/sec; 0.392 sec/batch)
2017-04-03 00:01:17.456953: step 15340, loss = 0.85 (329.7 examples/sec; 0.388 sec/batch)
2017-04-03 00:01:21.342862: step 15350, loss = 0.80 (329.4 examples/sec; 0.389 sec/batch)
2017-04-03 00:01:25.253424: step 15360, loss = 0.80 (327.3 examples/sec; 0.391 sec/batch)
2017-04-03 00:01:29.121056: step 15370, loss = 0.77 (331.0 examples/sec; 0.387 sec/batch)
2017-04-03 00:01:32.999132: step 15380, loss = 0.98 (330.1 examples/sec; 0.388 sec/batch)
2017-04-03 00:01:36.900729: step 15390, loss = 0.92 (328.1 examples/sec; 0.390 sec/batch)
2017-04-03 00:01:40.856456: step 15400, loss = 0.96 (323.6 examples/sec; 0.396 sec/batch)
2017-04-03 00:01:44.738964: step 15410, loss = 0.84 (329.7 examples/sec; 0.388 sec/batch)
2017-04-03 00:01:48.622931: step 15420, loss = 0.77 (329.6 examples/sec; 0.388 sec/batch)
2017-04-03 00:01:52.517116: step 15430, loss = 1.10 (328.7 examples/sec; 0.389 sec/batch)
2017-04-03 00:01:56.383055: step 15440, loss = 0.67 (331.1 examples/sec; 0.387 sec/batch)
2017-04-03 00:02:00.317418: step 15450, loss = 0.74 (325.3 examples/sec; 0.393 sec/batch)
2017-04-03 00:02:04.206443: step 15460, loss = 0.95 (329.1 examples/sec; 0.389 sec/batch)
2017-04-03 00:02:08.093207: step 15470, loss = 0.84 (329.3 examples/sec; 0.389 sec/batch)
2017-04-03 00:02:11.981816: step 15480, loss = 0.74 (329.2 examples/sec; 0.389 sec/batch)
2017-04-03 00:02:15.892483: step 15490, loss = 0.82 (327.3 examples/sec; 0.391 sec/batch)
2017-04-03 00:02:19.842908: step 15500, loss = 1.07 (324.0 examples/sec; 0.395 sec/batch)
2017-04-03 00:02:23.742196: step 15510, loss = 0.91 (328.3 examples/sec; 0.390 sec/batch)
2017-04-03 00:02:27.677364: step 15520, loss = 1.00 (325.3 examples/sec; 0.394 sec/batch)
2017-04-03 00:02:31.637353: step 15530, loss = 0.90 (323.2 examples/sec; 0.396 sec/batch)
2017-04-03 00:02:35.551376: step 15540, loss = 0.76 (327.0 examples/sec; 0.391 sec/batch)
2017-04-03 00:02:39.463925: step 15550, loss = 0.84 (327.2 examples/sec; 0.391 sec/batch)
2017-04-03 00:02:43.379737: step 15560, loss = 0.80 (326.9 examples/sec; 0.392 sec/batch)
2017-04-03 00:02:47.353461: step 15570, loss = 0.92 (322.1 examples/sec; 0.397 sec/batch)
2017-04-03 00:02:51.295921: step 15580, loss = 0.86 (324.7 examples/sec; 0.394 sec/batch)
2017-04-03 00:02:55.251423: step 15590, loss = 0.87 (323.6 examples/sec; 0.396 sec/batch)
2017-04-03 00:02:59.228369: step 15600, loss = 0.88 (321.9 examples/sec; 0.398 sec/batch)
2017-04-03 00:03:03.185604: step 15610, loss = 0.75 (323.5 examples/sec; 0.396 sec/batch)
2017-04-03 00:03:07.087218: step 15620, loss = 0.90 (328.1 examples/sec; 0.390 sec/batch)
2017-04-03 00:03:10.998009: step 15630, loss = 0.81 (327.3 examples/sec; 0.391 sec/batch)
2017-04-03 00:03:14.913189: step 15640, loss = 0.93 (326.9 examples/sec; 0.392 sec/batch)
2017-04-03 00:03:18.896684: step 15650, loss = 0.83 (321.3 examples/sec; 0.398 sec/batch)
2017-04-03 00:03:22.823533: step 15660, loss = 0.77 (326.0 examples/sec; 0.393 sec/batch)
2017-04-03 00:03:26.752398: step 15670, loss = 0.91 (325.8 examples/sec; 0.393 sec/batch)
2017-04-03 00:03:30.689480: step 15680, loss = 1.00 (325.1 examples/sec; 0.394 sec/batch)
2017-04-03 00:03:34.588793: step 15690, loss = 0.72 (328.3 examples/sec; 0.390 sec/batch)
2017-04-03 00:03:38.591452: step 15700, loss = 0.83 (319.8 examples/sec; 0.400 sec/batch)
2017-04-03 00:03:42.509503: step 15710, loss = 0.86 (326.7 examples/sec; 0.392 sec/batch)
2017-04-03 00:03:46.487794: step 15720, loss = 0.80 (321.7 examples/sec; 0.398 sec/batch)
2017-04-03 00:03:50.457147: step 15730, loss = 0.92 (322.5 examples/sec; 0.397 sec/batch)
2017-04-03 00:03:54.342717: step 15740, loss = 1.02 (329.4 examples/sec; 0.389 sec/batch)
2017-04-03 00:03:58.291224: step 15750, loss = 0.82 (324.2 examples/sec; 0.395 sec/batch)
2017-04-03 00:04:02.242667: step 15760, loss = 0.78 (323.9 examples/sec; 0.395 sec/batch)
2017-04-03 00:04:06.166186: step 15770, loss = 0.92 (326.2 examples/sec; 0.392 sec/batch)
2017-04-03 00:04:10.089258: step 15780, loss = 0.71 (326.3 examples/sec; 0.392 sec/batch)
2017-04-03 00:04:14.025210: step 15790, loss = 0.72 (325.2 examples/sec; 0.394 sec/batch)
2017-04-03 00:04:17.999938: step 15800, loss = 0.84 (322.0 examples/sec; 0.397 sec/batch)
2017-04-03 00:04:21.900544: step 15810, loss = 0.79 (328.2 examples/sec; 0.390 sec/batch)
2017-04-03 00:04:25.830740: step 15820, loss = 0.82 (325.7 examples/sec; 0.393 sec/batch)
2017-04-03 00:04:29.744114: step 15830, loss = 0.96 (327.1 examples/sec; 0.391 sec/batch)
2017-04-03 00:04:33.701608: step 15840, loss = 0.80 (323.4 examples/sec; 0.396 sec/batch)
2017-04-03 00:04:37.649766: step 15850, loss = 0.84 (324.2 examples/sec; 0.395 sec/batch)
2017-04-03 00:04:41.587756: step 15860, loss = 0.83 (325.0 examples/sec; 0.394 sec/batch)
2017-04-03 00:04:45.504171: step 15870, loss = 0.83 (326.8 examples/sec; 0.392 sec/batch)
2017-04-03 00:04:49.438230: step 15880, loss = 0.78 (325.4 examples/sec; 0.393 sec/batch)
2017-04-03 00:04:53.348230: step 15890, loss = 0.92 (327.4 examples/sec; 0.391 sec/batch)
2017-04-03 00:04:57.342243: step 15900, loss = 0.81 (320.5 examples/sec; 0.399 sec/batch)
2017-04-03 00:05:01.289258: step 15910, loss = 0.84 (324.3 examples/sec; 0.395 sec/batch)
2017-04-03 00:05:05.226011: step 15920, loss = 0.83 (325.1 examples/sec; 0.394 sec/batch)
2017-04-03 00:05:09.165144: step 15930, loss = 1.07 (324.9 examples/sec; 0.394 sec/batch)
2017-04-03 00:05:13.123543: step 15940, loss = 0.73 (323.4 examples/sec; 0.396 sec/batch)
2017-04-03 00:05:17.069541: step 15950, loss = 0.81 (324.4 examples/sec; 0.395 sec/batch)
2017-04-03 00:05:20.984773: step 15960, loss = 0.88 (326.9 examples/sec; 0.392 sec/batch)
2017-04-03 00:05:24.919442: step 15970, loss = 0.88 (325.3 examples/sec; 0.393 sec/batch)
2017-04-03 00:05:28.896808: step 15980, loss = 0.69 (321.8 examples/sec; 0.398 sec/batch)
2017-04-03 00:05:32.849857: step 15990, loss = 0.87 (323.8 examples/sec; 0.395 sec/batch)
2017-04-03 00:05:36.850855: step 16000, loss = 0.70 (319.9 examples/sec; 0.400 sec/batch)
2017-04-03 00:05:40.806367: step 16010, loss = 0.77 (323.6 examples/sec; 0.396 sec/batch)
2017-04-03 00:05:44.716840: step 16020, loss = 0.92 (327.3 examples/sec; 0.391 sec/batch)
2017-04-03 00:08:09.054921: precision @ 1 = 0.106

