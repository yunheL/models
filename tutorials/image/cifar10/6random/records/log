2017-04-02 21:24:35.026037: step 0, loss = 4.68 (310.8 examples/sec; 0.412 sec/batch)
2017-04-02 21:24:38.998315: step 10, loss = 4.61 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 21:24:42.914981: step 20, loss = 4.45 (326.8 examples/sec; 0.392 sec/batch)
2017-04-02 21:24:46.847421: step 30, loss = 4.39 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 21:24:50.795806: step 40, loss = 4.44 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 21:24:54.793955: step 50, loss = 4.22 (320.1 examples/sec; 0.400 sec/batch)
2017-04-02 21:24:58.775862: step 60, loss = 4.25 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 21:25:02.698364: step 70, loss = 4.31 (326.3 examples/sec; 0.392 sec/batch)
2017-04-02 21:25:06.638984: step 80, loss = 4.11 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 21:25:10.586369: step 90, loss = 4.14 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 21:25:14.570643: step 100, loss = 4.08 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 21:25:18.528369: step 110, loss = 4.09 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 21:25:22.438060: step 120, loss = 4.13 (327.4 examples/sec; 0.391 sec/batch)
2017-04-02 21:25:26.348775: step 130, loss = 4.00 (327.3 examples/sec; 0.391 sec/batch)
2017-04-02 21:25:30.277609: step 140, loss = 4.02 (325.8 examples/sec; 0.393 sec/batch)
2017-04-02 21:25:34.205074: step 150, loss = 3.96 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 21:25:38.148242: step 160, loss = 3.91 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 21:25:42.109257: step 170, loss = 4.05 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 21:25:46.054673: step 180, loss = 3.74 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 21:25:50.077565: step 190, loss = 3.91 (318.2 examples/sec; 0.402 sec/batch)
2017-04-02 21:25:54.091130: step 200, loss = 3.76 (318.9 examples/sec; 0.401 sec/batch)
2017-04-02 21:25:58.022249: step 210, loss = 3.91 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 21:26:01.941134: step 220, loss = 3.91 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 21:26:05.884869: step 230, loss = 4.02 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 21:26:09.786123: step 240, loss = 3.84 (328.1 examples/sec; 0.390 sec/batch)
2017-04-02 21:26:13.698398: step 250, loss = 3.85 (327.2 examples/sec; 0.391 sec/batch)
2017-04-02 21:26:17.608115: step 260, loss = 3.67 (327.4 examples/sec; 0.391 sec/batch)
2017-04-02 21:26:21.520106: step 270, loss = 3.71 (327.2 examples/sec; 0.391 sec/batch)
2017-04-02 21:26:25.467135: step 280, loss = 3.58 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 21:26:29.418602: step 290, loss = 3.61 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 21:26:33.399226: step 300, loss = 3.40 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 21:26:37.329725: step 310, loss = 3.45 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 21:26:41.221316: step 320, loss = 3.55 (328.9 examples/sec; 0.389 sec/batch)
2017-04-02 21:26:45.172204: step 330, loss = 3.27 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 21:26:49.150967: step 340, loss = 3.49 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 21:26:53.113482: step 350, loss = 3.52 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 21:26:57.070252: step 360, loss = 3.64 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 21:27:00.980918: step 370, loss = 3.48 (327.3 examples/sec; 0.391 sec/batch)
2017-04-02 21:27:04.929632: step 380, loss = 3.37 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 21:27:08.882688: step 390, loss = 3.33 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 21:27:12.922578: step 400, loss = 3.53 (316.8 examples/sec; 0.404 sec/batch)
2017-04-02 21:27:16.855087: step 410, loss = 3.24 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 21:27:20.792494: step 420, loss = 3.07 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 21:27:24.721610: step 430, loss = 3.27 (325.8 examples/sec; 0.393 sec/batch)
2017-04-02 21:27:28.648855: step 440, loss = 3.15 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 21:27:32.573602: step 450, loss = 3.17 (326.1 examples/sec; 0.392 sec/batch)
2017-04-02 21:27:36.464030: step 460, loss = 3.30 (329.0 examples/sec; 0.389 sec/batch)
2017-04-02 21:27:40.394078: step 470, loss = 3.21 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 21:27:44.318289: step 480, loss = 3.31 (326.2 examples/sec; 0.392 sec/batch)
2017-04-02 21:27:48.254650: step 490, loss = 3.13 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 21:27:52.253447: step 500, loss = 3.17 (320.1 examples/sec; 0.400 sec/batch)
2017-04-02 21:27:56.171063: step 510, loss = 3.42 (326.7 examples/sec; 0.392 sec/batch)
2017-04-02 21:28:00.058823: step 520, loss = 3.00 (329.2 examples/sec; 0.389 sec/batch)
2017-04-02 21:28:04.026308: step 530, loss = 3.17 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 21:28:07.933399: step 540, loss = 3.05 (327.6 examples/sec; 0.391 sec/batch)
2017-04-02 21:28:11.845793: step 550, loss = 3.00 (327.2 examples/sec; 0.391 sec/batch)
2017-04-02 21:28:15.755008: step 560, loss = 2.93 (327.4 examples/sec; 0.391 sec/batch)
2017-04-02 21:28:19.654129: step 570, loss = 3.00 (328.3 examples/sec; 0.390 sec/batch)
2017-04-02 21:28:23.552985: step 580, loss = 2.93 (328.3 examples/sec; 0.390 sec/batch)
2017-04-02 21:28:27.463868: step 590, loss = 2.98 (327.3 examples/sec; 0.391 sec/batch)
2017-04-02 21:28:31.454084: step 600, loss = 3.06 (320.8 examples/sec; 0.399 sec/batch)
2017-04-02 21:28:35.357417: step 610, loss = 3.10 (327.9 examples/sec; 0.390 sec/batch)
2017-04-02 21:28:39.253207: step 620, loss = 2.95 (328.6 examples/sec; 0.390 sec/batch)
2017-04-02 21:28:43.144869: step 630, loss = 2.81 (328.9 examples/sec; 0.389 sec/batch)
2017-04-02 21:28:47.028316: step 640, loss = 3.01 (329.6 examples/sec; 0.388 sec/batch)
2017-04-02 21:28:50.940943: step 650, loss = 2.84 (327.1 examples/sec; 0.391 sec/batch)
2017-04-02 21:28:54.827979: step 660, loss = 2.74 (329.3 examples/sec; 0.389 sec/batch)
2017-04-02 21:28:58.707706: step 670, loss = 3.02 (329.9 examples/sec; 0.388 sec/batch)
2017-04-02 21:29:02.604818: step 680, loss = 2.96 (328.4 examples/sec; 0.390 sec/batch)
2017-04-02 21:29:06.514378: step 690, loss = 2.98 (327.4 examples/sec; 0.391 sec/batch)
2017-04-02 21:29:10.486096: step 700, loss = 2.78 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 21:29:14.418371: step 710, loss = 2.82 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 21:29:18.336952: step 720, loss = 2.95 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 21:29:22.273805: step 730, loss = 2.95 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 21:29:26.171414: step 740, loss = 2.51 (328.4 examples/sec; 0.390 sec/batch)
2017-04-02 21:29:30.066558: step 750, loss = 2.81 (328.6 examples/sec; 0.390 sec/batch)
2017-04-02 21:29:33.979909: step 760, loss = 2.95 (327.1 examples/sec; 0.391 sec/batch)
2017-04-02 21:29:37.849624: step 770, loss = 2.61 (330.8 examples/sec; 0.387 sec/batch)
2017-04-02 21:29:41.741536: step 780, loss = 2.80 (328.9 examples/sec; 0.389 sec/batch)
2017-04-02 21:29:45.638936: step 790, loss = 2.79 (328.4 examples/sec; 0.390 sec/batch)
2017-04-02 21:29:49.564066: step 800, loss = 2.90 (326.1 examples/sec; 0.393 sec/batch)
2017-04-02 21:29:53.435643: step 810, loss = 2.73 (330.6 examples/sec; 0.387 sec/batch)
2017-04-02 21:29:57.323426: step 820, loss = 2.84 (329.2 examples/sec; 0.389 sec/batch)
2017-04-02 21:30:01.224791: step 830, loss = 2.58 (328.1 examples/sec; 0.390 sec/batch)
2017-04-02 21:30:05.160615: step 840, loss = 2.71 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 21:30:09.031412: step 850, loss = 2.37 (330.7 examples/sec; 0.387 sec/batch)
2017-04-02 21:30:12.930145: step 860, loss = 2.74 (328.3 examples/sec; 0.390 sec/batch)
2017-04-02 21:30:16.836546: step 870, loss = 2.59 (327.7 examples/sec; 0.391 sec/batch)
2017-04-02 21:30:20.723982: step 880, loss = 2.85 (329.3 examples/sec; 0.389 sec/batch)
2017-04-02 21:30:24.597987: step 890, loss = 2.62 (330.4 examples/sec; 0.387 sec/batch)
2017-04-02 21:30:28.565155: step 900, loss = 2.60 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 21:30:32.454787: step 910, loss = 2.70 (329.1 examples/sec; 0.389 sec/batch)
2017-04-02 21:30:36.313648: step 920, loss = 2.51 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 21:30:40.207396: step 930, loss = 2.37 (328.7 examples/sec; 0.389 sec/batch)
2017-04-02 21:30:44.086704: step 940, loss = 2.54 (330.0 examples/sec; 0.388 sec/batch)
2017-04-02 21:30:47.977295: step 950, loss = 2.56 (329.0 examples/sec; 0.389 sec/batch)
2017-04-02 21:30:51.891469: step 960, loss = 2.55 (327.0 examples/sec; 0.391 sec/batch)
2017-04-02 21:30:55.753657: step 970, loss = 2.62 (331.4 examples/sec; 0.386 sec/batch)
2017-04-02 21:30:59.640368: step 980, loss = 2.24 (329.3 examples/sec; 0.389 sec/batch)
2017-04-02 21:31:03.518385: step 990, loss = 2.31 (330.1 examples/sec; 0.388 sec/batch)
2017-04-02 21:31:07.455274: step 1000, loss = 2.29 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 21:31:11.350422: step 1010, loss = 2.16 (328.6 examples/sec; 0.390 sec/batch)
2017-04-02 21:31:15.234619: step 1020, loss = 2.52 (329.5 examples/sec; 0.388 sec/batch)
2017-04-02 21:31:19.110611: step 1030, loss = 2.28 (330.2 examples/sec; 0.388 sec/batch)
2017-04-02 21:31:22.997410: step 1040, loss = 2.47 (329.3 examples/sec; 0.389 sec/batch)
2017-04-02 21:31:26.900759: step 1050, loss = 2.27 (327.9 examples/sec; 0.390 sec/batch)
2017-04-02 21:31:30.777881: step 1060, loss = 2.27 (330.1 examples/sec; 0.388 sec/batch)
2017-04-02 21:31:34.675378: step 1070, loss = 2.51 (328.4 examples/sec; 0.390 sec/batch)
2017-04-02 21:31:38.528015: step 1080, loss = 2.39 (332.2 examples/sec; 0.385 sec/batch)
2017-04-02 21:31:42.393710: step 1090, loss = 2.19 (331.1 examples/sec; 0.387 sec/batch)
2017-04-02 21:31:46.369999: step 1100, loss = 2.32 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 21:31:50.237443: step 1110, loss = 2.43 (331.0 examples/sec; 0.387 sec/batch)
2017-04-02 21:31:54.137553: step 1120, loss = 2.53 (328.2 examples/sec; 0.390 sec/batch)
2017-04-02 21:31:58.016955: step 1130, loss = 2.42 (329.9 examples/sec; 0.388 sec/batch)
2017-04-02 21:32:01.925759: step 1140, loss = 2.12 (327.5 examples/sec; 0.391 sec/batch)
2017-04-02 21:32:05.815995: step 1150, loss = 2.12 (329.0 examples/sec; 0.389 sec/batch)
2017-04-02 21:32:09.674382: step 1160, loss = 2.17 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 21:32:13.521796: step 1170, loss = 2.24 (332.7 examples/sec; 0.385 sec/batch)
2017-04-02 21:32:17.391958: step 1180, loss = 2.03 (330.7 examples/sec; 0.387 sec/batch)
2017-04-02 21:32:21.268130: step 1190, loss = 2.31 (330.2 examples/sec; 0.388 sec/batch)
2017-04-02 21:32:25.201646: step 1200, loss = 2.18 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 21:32:29.090745: step 1210, loss = 2.23 (329.1 examples/sec; 0.389 sec/batch)
2017-04-02 21:32:32.982694: step 1220, loss = 2.33 (328.9 examples/sec; 0.389 sec/batch)
2017-04-02 21:32:36.829293: step 1230, loss = 2.26 (332.8 examples/sec; 0.385 sec/batch)
2017-04-02 21:32:40.692130: step 1240, loss = 2.10 (331.4 examples/sec; 0.386 sec/batch)
2017-04-02 21:32:44.574541: step 1250, loss = 2.18 (329.7 examples/sec; 0.388 sec/batch)
2017-04-02 21:32:48.488057: step 1260, loss = 2.06 (327.1 examples/sec; 0.391 sec/batch)
2017-04-02 21:32:52.375601: step 1270, loss = 2.15 (329.3 examples/sec; 0.389 sec/batch)
^Z
[1]  + 9737 suspended  python cifar10_train.py
(tensorflow) ➜  src git:(master) ✗ hostname
snares-06
(tensorflow) ➜  src git:(master) ✗ fg %1
[1]  + 9737 continued  python cifar10_train.py
2017-04-02 21:33:05.474139: step 1280, loss = 2.20 (97.7 examples/sec; 1.310 sec/batch)
2017-04-02 21:33:09.363815: step 1290, loss = 2.26 (329.1 examples/sec; 0.389 sec/batch)
2017-04-02 21:33:13.296174: step 1300, loss = 2.29 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 21:33:17.168733: step 1310, loss = 2.14 (330.5 examples/sec; 0.387 sec/batch)
2017-04-02 21:33:21.042281: step 1320, loss = 1.96 (330.4 examples/sec; 0.387 sec/batch)
2017-04-02 21:33:24.929547: step 1330, loss = 2.06 (329.3 examples/sec; 0.389 sec/batch)
2017-04-02 21:33:28.813602: step 1340, loss = 2.08 (329.6 examples/sec; 0.388 sec/batch)
2017-04-02 21:33:32.663381: step 1350, loss = 2.08 (332.5 examples/sec; 0.385 sec/batch)
2017-04-02 21:33:36.545927: step 1360, loss = 2.04 (329.7 examples/sec; 0.388 sec/batch)
2017-04-02 21:33:40.456750: step 1370, loss = 2.22 (327.3 examples/sec; 0.391 sec/batch)
2017-04-02 21:33:44.351819: step 1380, loss = 2.01 (328.6 examples/sec; 0.390 sec/batch)
2017-04-02 21:33:48.264285: step 1390, loss = 1.92 (327.2 examples/sec; 0.391 sec/batch)
2017-04-02 21:33:52.197727: step 1400, loss = 2.10 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 21:33:56.108785: step 1410, loss = 2.12 (327.3 examples/sec; 0.391 sec/batch)
2017-04-02 21:34:00.007997: step 1420, loss = 2.21 (328.3 examples/sec; 0.390 sec/batch)
2017-04-02 21:34:03.876930: step 1430, loss = 1.84 (330.8 examples/sec; 0.387 sec/batch)
2017-04-02 21:34:07.798019: step 1440, loss = 2.00 (326.4 examples/sec; 0.392 sec/batch)
2017-04-02 21:34:11.671585: step 1450, loss = 1.85 (330.4 examples/sec; 0.387 sec/batch)
2017-04-02 21:34:15.539348: step 1460, loss = 1.97 (330.9 examples/sec; 0.387 sec/batch)
2017-04-02 21:34:19.408843: step 1470, loss = 1.94 (330.8 examples/sec; 0.387 sec/batch)
2017-04-02 21:34:23.266176: step 1480, loss = 2.03 (331.8 examples/sec; 0.386 sec/batch)
2017-04-02 21:34:27.129979: step 1490, loss = 1.86 (331.3 examples/sec; 0.386 sec/batch)
2017-04-02 21:34:31.078650: step 1500, loss = 1.91 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 21:34:35.954608: step 1510, loss = 1.87 (262.5 examples/sec; 0.488 sec/batch)
2017-04-02 21:34:39.834018: step 1520, loss = 1.88 (329.9 examples/sec; 0.388 sec/batch)
2017-04-02 21:34:43.685744: step 1530, loss = 1.81 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 21:34:47.550018: step 1540, loss = 1.88 (331.2 examples/sec; 0.386 sec/batch)
2017-04-02 21:34:51.385071: step 1550, loss = 1.75 (333.8 examples/sec; 0.384 sec/batch)
2017-04-02 21:34:55.208605: step 1560, loss = 1.99 (334.8 examples/sec; 0.382 sec/batch)
2017-04-02 21:34:59.048170: step 1570, loss = 1.80 (333.4 examples/sec; 0.384 sec/batch)
2017-04-02 21:35:02.914948: step 1580, loss = 1.76 (331.0 examples/sec; 0.387 sec/batch)
2017-04-02 21:35:06.781962: step 1590, loss = 2.01 (331.0 examples/sec; 0.387 sec/batch)
2017-04-02 21:35:10.696947: step 1600, loss = 1.95 (326.9 examples/sec; 0.391 sec/batch)
2017-04-02 21:35:14.553306: step 1610, loss = 1.90 (331.9 examples/sec; 0.386 sec/batch)
2017-04-02 21:35:18.441702: step 1620, loss = 2.00 (329.2 examples/sec; 0.389 sec/batch)
2017-04-02 21:35:22.286353: step 1630, loss = 1.91 (332.9 examples/sec; 0.384 sec/batch)
2017-04-02 21:35:26.135502: step 1640, loss = 1.88 (332.5 examples/sec; 0.385 sec/batch)
2017-04-02 21:35:29.989087: step 1650, loss = 1.85 (332.2 examples/sec; 0.385 sec/batch)
2017-04-02 21:35:33.872369: step 1660, loss = 1.75 (329.6 examples/sec; 0.388 sec/batch)
2017-04-02 21:35:37.735597: step 1670, loss = 1.74 (331.3 examples/sec; 0.386 sec/batch)
2017-04-02 21:35:41.621645: step 1680, loss = 1.85 (329.4 examples/sec; 0.389 sec/batch)
2017-04-02 21:35:45.509453: step 1690, loss = 1.83 (329.2 examples/sec; 0.389 sec/batch)
2017-04-02 21:35:49.449553: step 1700, loss = 1.72 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 21:35:53.371058: step 1710, loss = 1.72 (326.4 examples/sec; 0.392 sec/batch)
2017-04-02 21:35:57.243390: step 1720, loss = 1.80 (330.6 examples/sec; 0.387 sec/batch)
2017-04-02 21:36:01.102641: step 1730, loss = 1.83 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 21:36:04.971863: step 1740, loss = 1.86 (330.8 examples/sec; 0.387 sec/batch)
2017-04-02 21:36:08.864043: step 1750, loss = 1.73 (328.9 examples/sec; 0.389 sec/batch)
2017-04-02 21:36:12.720715: step 1760, loss = 1.75 (331.9 examples/sec; 0.386 sec/batch)
2017-04-02 21:36:16.580953: step 1770, loss = 1.76 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 21:36:20.427999: step 1780, loss = 1.87 (332.7 examples/sec; 0.385 sec/batch)
2017-04-02 21:36:24.298738: step 1790, loss = 1.79 (330.7 examples/sec; 0.387 sec/batch)
2017-04-02 21:36:28.210980: step 1800, loss = 1.75 (327.2 examples/sec; 0.391 sec/batch)
2017-04-02 21:36:32.064770: step 1810, loss = 1.65 (332.1 examples/sec; 0.385 sec/batch)
2017-04-02 21:36:35.925546: step 1820, loss = 1.80 (331.5 examples/sec; 0.386 sec/batch)
2017-04-02 21:36:39.802465: step 1830, loss = 1.56 (330.2 examples/sec; 0.388 sec/batch)
2017-04-02 21:36:43.661758: step 1840, loss = 1.67 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 21:36:47.522079: step 1850, loss = 1.81 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 21:36:51.369072: step 1860, loss = 1.54 (332.7 examples/sec; 0.385 sec/batch)
2017-04-02 21:36:55.220400: step 1870, loss = 1.54 (332.4 examples/sec; 0.385 sec/batch)
2017-04-02 21:36:59.070293: step 1880, loss = 1.72 (332.5 examples/sec; 0.385 sec/batch)
2017-04-02 21:37:02.961731: step 1890, loss = 1.72 (328.9 examples/sec; 0.389 sec/batch)
2017-04-02 21:37:06.842364: step 1900, loss = 1.88 (329.8 examples/sec; 0.388 sec/batch)
2017-04-02 21:37:10.730683: step 1910, loss = 1.65 (329.2 examples/sec; 0.389 sec/batch)
2017-04-02 21:37:14.613105: step 1920, loss = 1.62 (329.7 examples/sec; 0.388 sec/batch)
2017-04-02 21:37:18.494669: step 1930, loss = 1.66 (329.8 examples/sec; 0.388 sec/batch)
2017-04-02 21:37:22.425528: step 1940, loss = 1.66 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 21:37:26.265911: step 1950, loss = 1.81 (333.3 examples/sec; 0.384 sec/batch)
2017-04-02 21:37:30.122681: step 1960, loss = 1.70 (331.9 examples/sec; 0.386 sec/batch)
2017-04-02 21:37:33.990997: step 1970, loss = 1.65 (330.9 examples/sec; 0.387 sec/batch)
2017-04-02 21:37:37.882280: step 1980, loss = 1.57 (328.9 examples/sec; 0.389 sec/batch)
2017-04-02 21:37:41.735203: step 1990, loss = 1.69 (332.2 examples/sec; 0.385 sec/batch)
2017-04-02 21:37:45.634978: step 2000, loss = 1.70 (328.2 examples/sec; 0.390 sec/batch)
2017-04-02 21:38:35.970439: precision @ 1 = 0.104
2017-04-02 21:38:54.362787: step 2010, loss = 1.52 (18.6 examples/sec; 6.873 sec/batch)
^Z
[1]  + 9737 suspended  python cifar10_train.py
(tensorflow) ➜  src git:(master) ✗ vim ../records/log     
(tensorflow) ➜  src git:(master) ✗ fg %1             
[1]  - 9737 continued  python cifar10_train.py
2017-04-02 21:39:31.365442: step 2020, loss = 1.61 (34.6 examples/sec; 3.700 sec/batch)
2017-04-02 21:39:35.234923: step 2030, loss = 1.77 (330.8 examples/sec; 0.387 sec/batch)
2017-04-02 21:39:39.089362: step 2040, loss = 1.60 (332.1 examples/sec; 0.385 sec/batch)
2017-04-02 21:39:42.950931: step 2050, loss = 1.77 (331.5 examples/sec; 0.386 sec/batch)
2017-04-02 21:39:46.827440: step 2060, loss = 1.58 (330.2 examples/sec; 0.388 sec/batch)
2017-04-02 21:39:50.688186: step 2070, loss = 1.68 (331.5 examples/sec; 0.386 sec/batch)
2017-04-02 21:39:54.576456: step 2080, loss = 1.67 (329.2 examples/sec; 0.389 sec/batch)
2017-04-02 21:39:58.445710: step 2090, loss = 1.69 (330.8 examples/sec; 0.387 sec/batch)
2017-04-02 21:40:02.361804: step 2100, loss = 1.63 (326.9 examples/sec; 0.392 sec/batch)
2017-04-02 21:40:06.235323: step 2110, loss = 1.66 (330.4 examples/sec; 0.387 sec/batch)
2017-04-02 21:40:10.144958: step 2120, loss = 1.61 (327.4 examples/sec; 0.391 sec/batch)
2017-04-02 21:40:14.027020: step 2130, loss = 1.52 (329.7 examples/sec; 0.388 sec/batch)
2017-04-02 21:40:17.878928: step 2140, loss = 1.65 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 21:40:21.715192: step 2150, loss = 1.50 (333.7 examples/sec; 0.384 sec/batch)
2017-04-02 21:40:25.577064: step 2160, loss = 1.79 (331.4 examples/sec; 0.386 sec/batch)
2017-04-02 21:40:29.452926: step 2170, loss = 1.55 (330.2 examples/sec; 0.388 sec/batch)
2017-04-02 21:40:33.297617: step 2180, loss = 1.52 (332.9 examples/sec; 0.384 sec/batch)
2017-04-02 21:40:37.154305: step 2190, loss = 1.78 (331.9 examples/sec; 0.386 sec/batch)
2017-04-02 21:40:41.067046: step 2200, loss = 1.43 (327.1 examples/sec; 0.391 sec/batch)
2017-04-02 21:40:44.921129: step 2210, loss = 1.42 (332.1 examples/sec; 0.385 sec/batch)
2017-04-02 21:40:48.798780: step 2220, loss = 1.45 (330.1 examples/sec; 0.388 sec/batch)
2017-04-02 21:40:52.629504: step 2230, loss = 1.47 (334.1 examples/sec; 0.383 sec/batch)
2017-04-02 21:40:56.489090: step 2240, loss = 1.51 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 21:41:00.361678: step 2250, loss = 1.73 (330.5 examples/sec; 0.387 sec/batch)
2017-04-02 21:41:04.227985: step 2260, loss = 1.47 (331.1 examples/sec; 0.387 sec/batch)
2017-04-02 21:41:08.083069: step 2270, loss = 1.44 (332.0 examples/sec; 0.386 sec/batch)
2017-04-02 21:41:11.930877: step 2280, loss = 1.64 (332.7 examples/sec; 0.385 sec/batch)
2017-04-02 21:41:15.798397: step 2290, loss = 1.51 (331.0 examples/sec; 0.387 sec/batch)
2017-04-02 21:41:19.752622: step 2300, loss = 1.43 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 21:41:23.629715: step 2310, loss = 1.61 (330.1 examples/sec; 0.388 sec/batch)
2017-04-02 21:41:27.493885: step 2320, loss = 1.52 (331.2 examples/sec; 0.386 sec/batch)
2017-04-02 21:41:31.343763: step 2330, loss = 1.28 (332.5 examples/sec; 0.385 sec/batch)
2017-04-02 21:41:35.190556: step 2340, loss = 1.41 (332.7 examples/sec; 0.385 sec/batch)
2017-04-02 21:41:39.032405: step 2350, loss = 1.49 (333.2 examples/sec; 0.384 sec/batch)
2017-04-02 21:41:42.903994: step 2360, loss = 1.62 (330.6 examples/sec; 0.387 sec/batch)
2017-04-02 21:41:46.796469: step 2370, loss = 1.62 (328.8 examples/sec; 0.389 sec/batch)
2017-04-02 21:41:50.678379: step 2380, loss = 1.41 (329.7 examples/sec; 0.388 sec/batch)
2017-04-02 21:41:54.545004: step 2390, loss = 1.49 (331.0 examples/sec; 0.387 sec/batch)
2017-04-02 21:41:58.451625: step 2400, loss = 1.34 (327.6 examples/sec; 0.391 sec/batch)
2017-04-02 21:42:02.318122: step 2410, loss = 1.31 (331.0 examples/sec; 0.387 sec/batch)
2017-04-02 21:42:06.179128: step 2420, loss = 1.66 (331.5 examples/sec; 0.386 sec/batch)
2017-04-02 21:42:10.043991: step 2430, loss = 1.58 (331.2 examples/sec; 0.386 sec/batch)
2017-04-02 21:42:13.909261: step 2440, loss = 1.36 (331.2 examples/sec; 0.387 sec/batch)
2017-04-02 21:42:17.770164: step 2450, loss = 1.32 (331.5 examples/sec; 0.386 sec/batch)
2017-04-02 21:42:21.623326: step 2460, loss = 1.45 (332.2 examples/sec; 0.385 sec/batch)
2017-04-02 21:42:25.492252: step 2470, loss = 1.35 (330.8 examples/sec; 0.387 sec/batch)
2017-04-02 21:42:29.358415: step 2480, loss = 1.28 (331.1 examples/sec; 0.387 sec/batch)
2017-04-02 21:42:33.214675: step 2490, loss = 1.37 (331.9 examples/sec; 0.386 sec/batch)
2017-04-02 21:42:37.129631: step 2500, loss = 1.40 (327.0 examples/sec; 0.391 sec/batch)
2017-04-02 21:42:40.950328: step 2510, loss = 1.25 (335.0 examples/sec; 0.382 sec/batch)
2017-04-02 21:42:44.826153: step 2520, loss = 1.56 (330.3 examples/sec; 0.388 sec/batch)
2017-04-02 21:42:48.706034: step 2530, loss = 1.26 (329.9 examples/sec; 0.388 sec/batch)
2017-04-02 21:42:52.562050: step 2540, loss = 1.47 (331.9 examples/sec; 0.386 sec/batch)
2017-04-02 21:42:56.444103: step 2550, loss = 1.43 (329.7 examples/sec; 0.388 sec/batch)
2017-04-02 21:43:00.304121: step 2560, loss = 1.37 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 21:43:04.153280: step 2570, loss = 1.28 (332.5 examples/sec; 0.385 sec/batch)
2017-04-02 21:43:07.992804: step 2580, loss = 1.32 (333.4 examples/sec; 0.384 sec/batch)
2017-04-02 21:43:11.854612: step 2590, loss = 1.37 (331.5 examples/sec; 0.386 sec/batch)
2017-04-02 21:43:15.806481: step 2600, loss = 1.40 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 21:43:19.652268: step 2610, loss = 1.42 (332.8 examples/sec; 0.385 sec/batch)
2017-04-02 21:43:23.511118: step 2620, loss = 1.15 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 21:43:27.357662: step 2630, loss = 1.46 (332.8 examples/sec; 0.385 sec/batch)
2017-04-02 21:43:31.198918: step 2640, loss = 1.28 (333.2 examples/sec; 0.384 sec/batch)
2017-04-02 21:43:35.072842: step 2650, loss = 1.51 (330.4 examples/sec; 0.387 sec/batch)
2017-04-02 21:43:38.925432: step 2660, loss = 1.39 (332.2 examples/sec; 0.385 sec/batch)
2017-04-02 21:43:42.784333: step 2670, loss = 1.32 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 21:43:46.645878: step 2680, loss = 1.55 (331.5 examples/sec; 0.386 sec/batch)
2017-04-02 21:43:50.524950: step 2690, loss = 1.18 (330.0 examples/sec; 0.388 sec/batch)
2017-04-02 21:43:54.454031: step 2700, loss = 1.67 (325.8 examples/sec; 0.393 sec/batch)
2017-04-02 21:43:58.309396: step 2710, loss = 1.43 (332.0 examples/sec; 0.386 sec/batch)
2017-04-02 21:44:02.161386: step 2720, loss = 1.20 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 21:44:06.000158: step 2730, loss = 1.38 (333.4 examples/sec; 0.384 sec/batch)
2017-04-02 21:44:09.847114: step 2740, loss = 1.30 (332.7 examples/sec; 0.385 sec/batch)
2017-04-02 21:44:13.689307: step 2750, loss = 1.27 (333.1 examples/sec; 0.384 sec/batch)
2017-04-02 21:44:17.570096: step 2760, loss = 1.32 (329.8 examples/sec; 0.388 sec/batch)
2017-04-02 21:44:21.417103: step 2770, loss = 1.61 (332.7 examples/sec; 0.385 sec/batch)
2017-04-02 21:44:25.275982: step 2780, loss = 1.34 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 21:44:29.107738: step 2790, loss = 1.37 (334.1 examples/sec; 0.383 sec/batch)
2017-04-02 21:44:33.033823: step 2800, loss = 1.40 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 21:44:38.009180: step 2810, loss = 1.33 (257.3 examples/sec; 0.498 sec/batch)
2017-04-02 21:44:41.846581: step 2820, loss = 1.41 (333.6 examples/sec; 0.384 sec/batch)
2017-04-02 21:44:45.672723: step 2830, loss = 1.43 (334.5 examples/sec; 0.383 sec/batch)
2017-04-02 21:44:49.563833: step 2840, loss = 1.39 (329.0 examples/sec; 0.389 sec/batch)
2017-04-02 21:44:53.426837: step 2850, loss = 1.29 (331.3 examples/sec; 0.386 sec/batch)
2017-04-02 21:44:57.256383: step 2860, loss = 1.27 (334.2 examples/sec; 0.383 sec/batch)
2017-04-02 21:45:01.093277: step 2870, loss = 1.25 (333.6 examples/sec; 0.384 sec/batch)
2017-04-02 21:45:04.934680: step 2880, loss = 1.18 (333.2 examples/sec; 0.384 sec/batch)
2017-04-02 21:45:08.812436: step 2890, loss = 1.23 (330.1 examples/sec; 0.388 sec/batch)
2017-04-02 21:45:12.724454: step 2900, loss = 1.28 (327.2 examples/sec; 0.391 sec/batch)
2017-04-02 21:45:16.612781: step 2910, loss = 1.30 (329.2 examples/sec; 0.389 sec/batch)
2017-04-02 21:45:20.470573: step 2920, loss = 1.36 (331.8 examples/sec; 0.386 sec/batch)
2017-04-02 21:45:24.309356: step 2930, loss = 1.29 (333.4 examples/sec; 0.384 sec/batch)
2017-04-02 21:45:28.144096: step 2940, loss = 1.14 (333.8 examples/sec; 0.383 sec/batch)
2017-04-02 21:45:31.993005: step 2950, loss = 1.18 (332.6 examples/sec; 0.385 sec/batch)
2017-04-02 21:45:35.823174: step 2960, loss = 1.19 (334.2 examples/sec; 0.383 sec/batch)
2017-04-02 21:45:39.689917: step 2970, loss = 1.28 (331.0 examples/sec; 0.387 sec/batch)
2017-04-02 21:45:43.525400: step 2980, loss = 1.38 (333.7 examples/sec; 0.384 sec/batch)
2017-04-02 21:45:47.352716: step 2990, loss = 1.28 (334.4 examples/sec; 0.383 sec/batch)
2017-04-02 21:45:51.269317: step 3000, loss = 1.12 (326.8 examples/sec; 0.392 sec/batch)
2017-04-02 21:45:55.148899: step 3010, loss = 1.22 (329.9 examples/sec; 0.388 sec/batch)
2017-04-02 21:45:59.011255: step 3020, loss = 1.22 (331.4 examples/sec; 0.386 sec/batch)
2017-04-02 21:46:02.869318: step 3030, loss = 1.22 (331.8 examples/sec; 0.386 sec/batch)
2017-04-02 21:46:06.707399: step 3040, loss = 1.14 (333.5 examples/sec; 0.384 sec/batch)
2017-04-02 21:46:10.572873: step 3050, loss = 1.37 (331.1 examples/sec; 0.387 sec/batch)
2017-04-02 21:46:14.445478: step 3060, loss = 1.31 (330.5 examples/sec; 0.387 sec/batch)
2017-04-02 21:46:18.330125: step 3070, loss = 1.12 (329.5 examples/sec; 0.388 sec/batch)
2017-04-02 21:46:22.171379: step 3080, loss = 1.23 (333.2 examples/sec; 0.384 sec/batch)
2017-04-02 21:46:26.019055: step 3090, loss = 1.27 (332.7 examples/sec; 0.385 sec/batch)
2017-04-02 21:46:29.911846: step 3100, loss = 1.26 (328.8 examples/sec; 0.389 sec/batch)
2017-04-02 21:46:33.793395: step 3110, loss = 1.30 (329.8 examples/sec; 0.388 sec/batch)
2017-04-02 21:46:37.661520: step 3120, loss = 1.24 (330.9 examples/sec; 0.387 sec/batch)
2017-04-02 21:46:41.545147: step 3130, loss = 1.14 (329.6 examples/sec; 0.388 sec/batch)
2017-04-02 21:46:45.395870: step 3140, loss = 1.52 (332.4 examples/sec; 0.385 sec/batch)
2017-04-02 21:46:49.257588: step 3150, loss = 1.12 (331.5 examples/sec; 0.386 sec/batch)
2017-04-02 21:46:53.123679: step 3160, loss = 1.18 (331.1 examples/sec; 0.387 sec/batch)
2017-04-02 21:46:56.963539: step 3170, loss = 1.26 (333.3 examples/sec; 0.384 sec/batch)
2017-04-02 21:47:00.827737: step 3180, loss = 1.47 (331.2 examples/sec; 0.386 sec/batch)
2017-04-02 21:47:04.672852: step 3190, loss = 1.08 (332.9 examples/sec; 0.385 sec/batch)
2017-04-02 21:47:08.573399: step 3200, loss = 1.25 (328.2 examples/sec; 0.390 sec/batch)
2017-04-02 21:47:12.431397: step 3210, loss = 1.24 (331.8 examples/sec; 0.386 sec/batch)
2017-04-02 21:47:16.281175: step 3220, loss = 1.05 (332.5 examples/sec; 0.385 sec/batch)
2017-04-02 21:47:20.140204: step 3230, loss = 1.28 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 21:47:24.002543: step 3240, loss = 1.30 (331.4 examples/sec; 0.386 sec/batch)
2017-04-02 21:47:27.891594: step 3250, loss = 1.32 (329.1 examples/sec; 0.389 sec/batch)
2017-04-02 21:47:31.765656: step 3260, loss = 1.29 (330.4 examples/sec; 0.387 sec/batch)
2017-04-02 21:47:35.608823: step 3270, loss = 1.15 (333.1 examples/sec; 0.384 sec/batch)
2017-04-02 21:47:39.472131: step 3280, loss = 1.19 (331.3 examples/sec; 0.386 sec/batch)
2017-04-02 21:47:43.324815: step 3290, loss = 1.11 (332.2 examples/sec; 0.385 sec/batch)
2017-04-02 21:47:47.241230: step 3300, loss = 1.25 (326.8 examples/sec; 0.392 sec/batch)
2017-04-02 21:47:51.059398: step 3310, loss = 1.22 (335.2 examples/sec; 0.382 sec/batch)
2017-04-02 21:47:54.887121: step 3320, loss = 1.34 (334.4 examples/sec; 0.383 sec/batch)
2017-04-02 21:47:58.731280: step 3330, loss = 1.25 (333.0 examples/sec; 0.384 sec/batch)
2017-04-02 21:48:02.580833: step 3340, loss = 1.46 (332.5 examples/sec; 0.385 sec/batch)
2017-04-02 21:48:06.411102: step 3350, loss = 1.19 (334.2 examples/sec; 0.383 sec/batch)
2017-04-02 21:48:10.270814: step 3360, loss = 1.21 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 21:48:14.138631: step 3370, loss = 1.23 (330.9 examples/sec; 0.387 sec/batch)
2017-04-02 21:48:17.985935: step 3380, loss = 1.29 (332.7 examples/sec; 0.385 sec/batch)
2017-04-02 21:48:21.816930: step 3390, loss = 1.23 (334.1 examples/sec; 0.383 sec/batch)
2017-04-02 21:48:25.703954: step 3400, loss = 1.37 (329.3 examples/sec; 0.389 sec/batch)
2017-04-02 21:48:29.534412: step 3410, loss = 1.28 (334.2 examples/sec; 0.383 sec/batch)
2017-04-02 21:48:33.398074: step 3420, loss = 1.01 (331.3 examples/sec; 0.386 sec/batch)
2017-04-02 21:48:37.271545: step 3430, loss = 1.29 (330.5 examples/sec; 0.387 sec/batch)
2017-04-02 21:48:41.104626: step 3440, loss = 1.12 (333.9 examples/sec; 0.383 sec/batch)
2017-04-02 21:48:44.944470: step 3450, loss = 0.99 (333.3 examples/sec; 0.384 sec/batch)
2017-04-02 21:48:48.820521: step 3460, loss = 1.13 (330.2 examples/sec; 0.388 sec/batch)
2017-04-02 21:48:52.671613: step 3470, loss = 1.21 (332.4 examples/sec; 0.385 sec/batch)
2017-04-02 21:48:56.520004: step 3480, loss = 1.18 (332.6 examples/sec; 0.385 sec/batch)
2017-04-02 21:49:00.379982: step 3490, loss = 0.93 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 21:49:04.309937: step 3500, loss = 1.28 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 21:49:08.175200: step 3510, loss = 1.35 (331.2 examples/sec; 0.387 sec/batch)
2017-04-02 21:49:12.027330: step 3520, loss = 1.23 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 21:49:15.889369: step 3530, loss = 1.17 (331.4 examples/sec; 0.386 sec/batch)
2017-04-02 21:49:19.748225: step 3540, loss = 1.22 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 21:49:23.578226: step 3550, loss = 1.12 (334.2 examples/sec; 0.383 sec/batch)
2017-04-02 21:49:27.408107: step 3560, loss = 1.33 (334.2 examples/sec; 0.383 sec/batch)
2017-04-02 21:49:31.280916: step 3570, loss = 0.99 (330.5 examples/sec; 0.387 sec/batch)
2017-04-02 21:49:35.133872: step 3580, loss = 1.27 (332.2 examples/sec; 0.385 sec/batch)
2017-04-02 21:49:38.980831: step 3590, loss = 1.15 (332.7 examples/sec; 0.385 sec/batch)
2017-04-02 21:49:42.864335: step 3600, loss = 1.10 (329.6 examples/sec; 0.388 sec/batch)
2017-04-02 21:49:46.705698: step 3610, loss = 1.13 (333.2 examples/sec; 0.384 sec/batch)
2017-04-02 21:49:50.542028: step 3620, loss = 1.17 (333.7 examples/sec; 0.384 sec/batch)
2017-04-02 21:49:54.414155: step 3630, loss = 1.21 (330.6 examples/sec; 0.387 sec/batch)
2017-04-02 21:49:58.292418: step 3640, loss = 1.18 (330.0 examples/sec; 0.388 sec/batch)
2017-04-02 21:50:02.113925: step 3650, loss = 1.18 (334.9 examples/sec; 0.382 sec/batch)
2017-04-02 21:50:06.017652: step 3660, loss = 1.22 (327.9 examples/sec; 0.390 sec/batch)
2017-04-02 21:50:09.889026: step 3670, loss = 1.24 (330.6 examples/sec; 0.387 sec/batch)
2017-04-02 21:50:13.722767: step 3680, loss = 1.26 (333.9 examples/sec; 0.383 sec/batch)
2017-04-02 21:50:17.572913: step 3690, loss = 1.07 (332.5 examples/sec; 0.385 sec/batch)
2017-04-02 21:50:21.507231: step 3700, loss = 1.10 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 21:50:25.366692: step 3710, loss = 1.02 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 21:50:29.257741: step 3720, loss = 1.13 (329.0 examples/sec; 0.389 sec/batch)
2017-04-02 21:50:33.137377: step 3730, loss = 1.11 (329.9 examples/sec; 0.388 sec/batch)
2017-04-02 21:50:36.984939: step 3740, loss = 1.03 (332.7 examples/sec; 0.385 sec/batch)
2017-04-02 21:50:40.849378: step 3750, loss = 1.16 (331.2 examples/sec; 0.386 sec/batch)
2017-04-02 21:50:44.701514: step 3760, loss = 1.35 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 21:50:48.554399: step 3770, loss = 1.19 (332.2 examples/sec; 0.385 sec/batch)
2017-04-02 21:50:52.385998: step 3780, loss = 0.98 (334.1 examples/sec; 0.383 sec/batch)
2017-04-02 21:50:56.209192: step 3790, loss = 1.10 (334.8 examples/sec; 0.382 sec/batch)
2017-04-02 21:51:00.131802: step 3800, loss = 1.29 (326.3 examples/sec; 0.392 sec/batch)
2017-04-02 21:51:03.960418: step 3810, loss = 1.19 (334.3 examples/sec; 0.383 sec/batch)
2017-04-02 21:51:07.823905: step 3820, loss = 1.07 (331.3 examples/sec; 0.386 sec/batch)
2017-04-02 21:51:11.663895: step 3830, loss = 0.95 (333.3 examples/sec; 0.384 sec/batch)
2017-04-02 21:51:15.493187: step 3840, loss = 1.11 (334.3 examples/sec; 0.383 sec/batch)
2017-04-02 21:51:19.344833: step 3850, loss = 1.25 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 21:51:23.189522: step 3860, loss = 1.17 (332.9 examples/sec; 0.384 sec/batch)
2017-04-02 21:51:27.041227: step 3870, loss = 1.10 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 21:51:30.901650: step 3880, loss = 1.27 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 21:51:34.776278: step 3890, loss = 1.20 (330.4 examples/sec; 0.387 sec/batch)
2017-04-02 21:51:38.663170: step 3900, loss = 1.24 (329.3 examples/sec; 0.389 sec/batch)
2017-04-02 21:51:42.496733: step 3910, loss = 1.06 (333.9 examples/sec; 0.383 sec/batch)
2017-04-02 21:51:46.353968: step 3920, loss = 1.06 (331.8 examples/sec; 0.386 sec/batch)
2017-04-02 21:51:50.201657: step 3930, loss = 1.34 (332.7 examples/sec; 0.385 sec/batch)
2017-04-02 21:51:54.044806: step 3940, loss = 1.10 (333.1 examples/sec; 0.384 sec/batch)
2017-04-02 21:51:57.876551: step 3950, loss = 1.00 (334.1 examples/sec; 0.383 sec/batch)
2017-04-02 21:52:01.750369: step 3960, loss = 0.93 (330.4 examples/sec; 0.387 sec/batch)
2017-04-02 21:52:05.567845: step 3970, loss = 1.12 (335.3 examples/sec; 0.382 sec/batch)
2017-04-02 21:52:09.419492: step 3980, loss = 1.05 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 21:52:13.254004: step 3990, loss = 0.96 (333.8 examples/sec; 0.383 sec/batch)
2017-04-02 21:52:17.182872: step 4000, loss = 1.06 (325.8 examples/sec; 0.393 sec/batch)
2017-04-02 21:52:21.054572: step 4010, loss = 1.12 (330.6 examples/sec; 0.387 sec/batch)
2017-04-02 21:52:24.876508: step 4020, loss = 1.30 (334.9 examples/sec; 0.382 sec/batch)
2017-04-02 21:52:28.715174: step 4030, loss = 1.12 (333.4 examples/sec; 0.384 sec/batch)
2017-04-02 21:52:32.591820: step 4040, loss = 1.07 (330.2 examples/sec; 0.388 sec/batch)
2017-04-02 21:52:36.456086: step 4050, loss = 1.00 (331.2 examples/sec; 0.386 sec/batch)
2017-04-02 21:54:03.743962: precision @ 1 = 0.104

2017-04-02 21:54:39.834227: step 4060, loss = 0.98 (10.4 examples/sec; 12.338 sec/batch)
2017-04-02 21:54:43.648638: step 4070, loss = 1.02 (335.6 examples/sec; 0.381 sec/batch)
2017-04-02 21:54:47.481439: step 4080, loss = 1.09 (334.0 examples/sec; 0.383 sec/batch)
2017-04-02 21:54:51.297990: step 4090, loss = 0.99 (335.4 examples/sec; 0.382 sec/batch)
2017-04-02 21:54:55.210351: step 4100, loss = 1.09 (327.2 examples/sec; 0.391 sec/batch)
2017-04-02 21:54:59.061904: step 4110, loss = 1.05 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 21:55:02.900666: step 4120, loss = 1.15 (333.4 examples/sec; 0.384 sec/batch)
2017-04-02 21:55:06.744286: step 4130, loss = 1.16 (333.0 examples/sec; 0.384 sec/batch)
2017-04-02 21:55:10.572358: step 4140, loss = 1.12 (334.4 examples/sec; 0.383 sec/batch)
2017-04-02 21:55:14.447797: step 4150, loss = 1.17 (330.3 examples/sec; 0.388 sec/batch)
2017-04-02 21:55:18.285460: step 4160, loss = 1.11 (333.5 examples/sec; 0.384 sec/batch)
2017-04-02 21:55:22.120317: step 4170, loss = 1.27 (333.8 examples/sec; 0.383 sec/batch)
2017-04-02 21:55:25.942692: step 4180, loss = 1.21 (334.9 examples/sec; 0.382 sec/batch)
2017-04-02 21:55:29.793688: step 4190, loss = 1.04 (332.4 examples/sec; 0.385 sec/batch)
2017-04-02 21:55:33.700407: step 4200, loss = 1.06 (327.6 examples/sec; 0.391 sec/batch)
2017-04-02 21:55:37.556972: step 4210, loss = 0.87 (331.9 examples/sec; 0.386 sec/batch)
2017-04-02 21:55:41.395678: step 4220, loss = 1.14 (333.4 examples/sec; 0.384 sec/batch)
2017-04-02 21:55:45.249618: step 4230, loss = 1.04 (332.1 examples/sec; 0.385 sec/batch)
2017-04-02 21:55:49.116633: step 4240, loss = 1.09 (331.0 examples/sec; 0.387 sec/batch)
2017-04-02 21:55:53.061972: step 4250, loss = 1.31 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 21:55:56.924106: step 4260, loss = 1.05 (331.4 examples/sec; 0.386 sec/batch)
2017-04-02 21:56:00.772931: step 4270, loss = 1.00 (332.6 examples/sec; 0.385 sec/batch)
2017-04-02 21:56:04.617475: step 4280, loss = 1.03 (332.9 examples/sec; 0.384 sec/batch)
2017-04-02 21:56:08.486888: step 4290, loss = 1.14 (330.8 examples/sec; 0.387 sec/batch)
2017-04-02 21:56:12.386176: step 4300, loss = 1.02 (328.3 examples/sec; 0.390 sec/batch)
2017-04-02 21:56:16.217469: step 4310, loss = 0.89 (334.1 examples/sec; 0.383 sec/batch)
2017-04-02 21:56:20.031600: step 4320, loss = 1.41 (335.6 examples/sec; 0.381 sec/batch)
2017-04-02 21:56:23.883625: step 4330, loss = 1.11 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 21:56:27.733659: step 4340, loss = 1.05 (332.5 examples/sec; 0.385 sec/batch)
2017-04-02 21:56:31.561325: step 4350, loss = 1.05 (334.4 examples/sec; 0.383 sec/batch)
2017-04-02 21:56:35.441827: step 4360, loss = 0.94 (329.9 examples/sec; 0.388 sec/batch)
2017-04-02 21:56:39.315401: step 4370, loss = 1.01 (330.4 examples/sec; 0.387 sec/batch)
2017-04-02 21:56:43.141368: step 4380, loss = 0.84 (334.6 examples/sec; 0.383 sec/batch)
2017-04-02 21:56:47.003359: step 4390, loss = 1.13 (331.4 examples/sec; 0.386 sec/batch)
2017-04-02 21:56:50.904334: step 4400, loss = 1.14 (328.1 examples/sec; 0.390 sec/batch)
2017-04-02 21:56:54.743950: step 4410, loss = 1.04 (333.4 examples/sec; 0.384 sec/batch)
2017-04-02 21:56:58.621072: step 4420, loss = 1.16 (330.1 examples/sec; 0.388 sec/batch)
2017-04-02 21:57:02.479935: step 4430, loss = 0.98 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 21:57:06.324536: step 4440, loss = 1.06 (332.9 examples/sec; 0.384 sec/batch)
2017-04-02 21:57:10.204486: step 4450, loss = 0.99 (329.9 examples/sec; 0.388 sec/batch)
2017-04-02 21:57:14.055350: step 4460, loss = 0.97 (332.4 examples/sec; 0.385 sec/batch)
2017-04-02 21:57:17.902133: step 4470, loss = 1.09 (332.7 examples/sec; 0.385 sec/batch)
2017-04-02 21:57:21.768896: step 4480, loss = 1.09 (331.0 examples/sec; 0.387 sec/batch)
2017-04-02 21:57:25.605000: step 4490, loss = 1.15 (333.7 examples/sec; 0.384 sec/batch)
2017-04-02 21:57:29.507610: step 4500, loss = 1.17 (328.0 examples/sec; 0.390 sec/batch)
2017-04-02 21:57:33.367124: step 4510, loss = 1.01 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 21:57:37.190036: step 4520, loss = 1.10 (334.8 examples/sec; 0.382 sec/batch)
2017-04-02 21:57:41.032590: step 4530, loss = 0.98 (333.1 examples/sec; 0.384 sec/batch)
2017-04-02 21:57:44.880402: step 4540, loss = 1.17 (332.7 examples/sec; 0.385 sec/batch)
2017-04-02 21:57:48.735153: step 4550, loss = 0.93 (332.1 examples/sec; 0.385 sec/batch)
2017-04-02 21:57:52.560424: step 4560, loss = 1.11 (334.6 examples/sec; 0.383 sec/batch)
2017-04-02 21:57:56.392496: step 4570, loss = 1.10 (334.0 examples/sec; 0.383 sec/batch)
2017-04-02 21:58:00.219979: step 4580, loss = 1.03 (334.4 examples/sec; 0.383 sec/batch)
2017-04-02 21:58:04.104055: step 4590, loss = 0.98 (329.6 examples/sec; 0.388 sec/batch)
2017-04-02 21:58:08.004236: step 4600, loss = 1.00 (328.2 examples/sec; 0.390 sec/batch)
2017-04-02 21:58:11.823760: step 4610, loss = 1.05 (335.1 examples/sec; 0.382 sec/batch)
2017-04-02 21:58:15.684292: step 4620, loss = 1.15 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 21:58:19.524530: step 4630, loss = 0.93 (333.3 examples/sec; 0.384 sec/batch)
2017-04-02 21:58:23.422756: step 4640, loss = 1.06 (328.4 examples/sec; 0.390 sec/batch)
2017-04-02 21:58:27.285126: step 4650, loss = 1.06 (331.4 examples/sec; 0.386 sec/batch)
2017-04-02 21:58:31.153598: step 4660, loss = 1.07 (330.9 examples/sec; 0.387 sec/batch)
2017-04-02 21:58:34.994024: step 4670, loss = 0.83 (333.3 examples/sec; 0.384 sec/batch)
2017-04-02 21:58:38.853492: step 4680, loss = 0.98 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 21:58:42.685540: step 4690, loss = 1.06 (334.0 examples/sec; 0.383 sec/batch)
2017-04-02 21:58:46.566884: step 4700, loss = 0.87 (329.8 examples/sec; 0.388 sec/batch)
2017-04-02 21:58:50.383001: step 4710, loss = 1.08 (335.4 examples/sec; 0.382 sec/batch)
2017-04-02 21:58:54.217201: step 4720, loss = 1.13 (333.8 examples/sec; 0.383 sec/batch)
2017-04-02 21:58:58.059572: step 4730, loss = 1.00 (333.1 examples/sec; 0.384 sec/batch)
2017-04-02 21:59:01.886397: step 4740, loss = 0.91 (334.5 examples/sec; 0.383 sec/batch)
2017-04-02 21:59:05.720489: step 4750, loss = 1.03 (333.8 examples/sec; 0.383 sec/batch)
2017-04-02 21:59:09.520238: step 4760, loss = 0.88 (336.9 examples/sec; 0.380 sec/batch)
2017-04-02 21:59:13.351910: step 4770, loss = 0.98 (334.1 examples/sec; 0.383 sec/batch)
2017-04-02 21:59:17.208917: step 4780, loss = 1.09 (331.9 examples/sec; 0.386 sec/batch)
2017-04-02 21:59:21.017199: step 4790, loss = 1.11 (336.1 examples/sec; 0.381 sec/batch)
2017-04-02 21:59:24.930206: step 4800, loss = 0.96 (327.1 examples/sec; 0.391 sec/batch)
2017-04-02 21:59:28.764734: step 4810, loss = 1.05 (333.8 examples/sec; 0.383 sec/batch)
2017-04-02 21:59:32.583199: step 4820, loss = 1.05 (335.2 examples/sec; 0.382 sec/batch)
2017-04-02 21:59:36.447106: step 4830, loss = 1.05 (331.3 examples/sec; 0.386 sec/batch)
2017-04-02 21:59:40.247318: step 4840, loss = 1.01 (336.8 examples/sec; 0.380 sec/batch)
2017-04-02 21:59:44.087306: step 4850, loss = 1.17 (333.3 examples/sec; 0.384 sec/batch)
2017-04-02 21:59:47.894879: step 4860, loss = 1.13 (336.2 examples/sec; 0.381 sec/batch)
2017-04-02 21:59:51.719402: step 4870, loss = 1.06 (334.7 examples/sec; 0.382 sec/batch)
2017-04-02 21:59:55.543980: step 4880, loss = 1.15 (334.7 examples/sec; 0.382 sec/batch)
2017-04-02 21:59:59.395419: step 4890, loss = 1.03 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 22:00:03.303665: step 4900, loss = 0.86 (327.5 examples/sec; 0.391 sec/batch)
2017-04-02 22:00:07.187327: step 4910, loss = 1.06 (329.6 examples/sec; 0.388 sec/batch)
2017-04-02 22:00:11.004743: step 4920, loss = 1.04 (335.3 examples/sec; 0.382 sec/batch)
2017-04-02 22:00:14.821477: step 4930, loss = 1.09 (335.4 examples/sec; 0.382 sec/batch)
2017-04-02 22:00:18.664897: step 4940, loss = 1.01 (333.0 examples/sec; 0.384 sec/batch)
2017-04-02 22:00:22.486095: step 4950, loss = 1.05 (335.0 examples/sec; 0.382 sec/batch)
2017-04-02 22:00:26.315525: step 4960, loss = 1.04 (334.3 examples/sec; 0.383 sec/batch)
2017-04-02 22:00:30.130607: step 4970, loss = 1.15 (335.5 examples/sec; 0.382 sec/batch)
2017-04-02 22:00:33.960761: step 4980, loss = 0.91 (334.2 examples/sec; 0.383 sec/batch)
2017-04-02 22:00:37.801028: step 4990, loss = 1.14 (333.3 examples/sec; 0.384 sec/batch)
2017-04-02 22:00:41.741440: step 5000, loss = 0.99 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 22:00:45.593920: step 5010, loss = 0.93 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 22:00:49.427599: step 5020, loss = 1.05 (333.9 examples/sec; 0.383 sec/batch)
2017-04-02 22:00:53.292331: step 5030, loss = 0.86 (331.2 examples/sec; 0.386 sec/batch)
2017-04-02 22:00:57.145324: step 5040, loss = 0.89 (332.2 examples/sec; 0.385 sec/batch)
2017-04-02 22:01:00.999630: step 5050, loss = 1.00 (332.1 examples/sec; 0.385 sec/batch)
2017-04-02 22:01:04.807963: step 5060, loss = 0.86 (336.1 examples/sec; 0.381 sec/batch)
2017-04-02 22:01:08.631833: step 5070, loss = 1.10 (334.7 examples/sec; 0.382 sec/batch)
2017-04-02 22:01:12.503312: step 5080, loss = 1.19 (330.6 examples/sec; 0.387 sec/batch)
2017-04-02 22:01:16.395418: step 5090, loss = 1.03 (328.9 examples/sec; 0.389 sec/batch)
2017-04-02 22:01:20.300023: step 5100, loss = 0.98 (327.8 examples/sec; 0.390 sec/batch)
2017-04-02 22:01:24.152488: step 5110, loss = 1.15 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 22:01:27.964837: step 5120, loss = 0.91 (335.8 examples/sec; 0.381 sec/batch)
2017-04-02 22:01:31.824189: step 5130, loss = 1.07 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 22:01:35.671850: step 5140, loss = 0.97 (332.7 examples/sec; 0.385 sec/batch)
2017-04-02 22:01:39.512865: step 5150, loss = 0.87 (333.2 examples/sec; 0.384 sec/batch)
2017-04-02 22:01:43.368156: step 5160, loss = 0.97 (332.0 examples/sec; 0.386 sec/batch)
2017-04-02 22:01:47.217682: step 5170, loss = 0.99 (332.5 examples/sec; 0.385 sec/batch)
2017-04-02 22:01:51.060069: step 5180, loss = 1.17 (333.1 examples/sec; 0.384 sec/batch)
2017-04-02 22:01:54.934711: step 5190, loss = 0.92 (330.4 examples/sec; 0.387 sec/batch)
2017-04-02 22:01:58.869183: step 5200, loss = 1.15 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 22:02:02.727784: step 5210, loss = 1.19 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 22:02:06.589686: step 5220, loss = 0.96 (331.4 examples/sec; 0.386 sec/batch)
2017-04-02 22:02:10.462336: step 5230, loss = 0.93 (330.5 examples/sec; 0.387 sec/batch)
2017-04-02 22:02:14.352313: step 5240, loss = 0.93 (329.1 examples/sec; 0.389 sec/batch)
2017-04-02 22:02:18.240310: step 5250, loss = 1.14 (329.2 examples/sec; 0.389 sec/batch)
2017-04-02 22:02:22.107314: step 5260, loss = 1.11 (331.0 examples/sec; 0.387 sec/batch)
2017-04-02 22:02:25.954481: step 5270, loss = 0.89 (332.7 examples/sec; 0.385 sec/batch)
2017-04-02 22:02:29.834282: step 5280, loss = 0.89 (329.9 examples/sec; 0.388 sec/batch)
2017-04-02 22:02:33.638575: step 5290, loss = 0.94 (336.5 examples/sec; 0.380 sec/batch)
2017-04-02 22:02:37.542627: step 5300, loss = 1.16 (327.9 examples/sec; 0.390 sec/batch)
2017-04-02 22:02:41.412407: step 5310, loss = 0.75 (330.8 examples/sec; 0.387 sec/batch)
2017-04-02 22:02:45.376575: step 5320, loss = 0.94 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 22:02:49.666281: step 5330, loss = 0.92 (298.4 examples/sec; 0.429 sec/batch)
2017-04-02 22:02:53.541737: step 5340, loss = 0.96 (330.3 examples/sec; 0.388 sec/batch)
2017-04-02 22:02:57.413499: step 5350, loss = 0.99 (330.6 examples/sec; 0.387 sec/batch)
2017-04-02 22:03:01.548008: step 5360, loss = 1.07 (309.6 examples/sec; 0.413 sec/batch)
2017-04-02 22:03:05.591246: step 5370, loss = 1.09 (316.6 examples/sec; 0.404 sec/batch)
2017-04-02 22:03:09.441708: step 5380, loss = 0.96 (332.4 examples/sec; 0.385 sec/batch)
2017-04-02 22:03:13.293847: step 5390, loss = 1.08 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 22:03:17.226843: step 5400, loss = 0.85 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 22:03:21.105774: step 5410, loss = 0.90 (330.0 examples/sec; 0.388 sec/batch)
2017-04-02 22:03:24.953386: step 5420, loss = 1.01 (332.7 examples/sec; 0.385 sec/batch)
2017-04-02 22:03:28.802222: step 5430, loss = 1.41 (332.6 examples/sec; 0.385 sec/batch)
2017-04-02 22:03:32.683371: step 5440, loss = 1.00 (329.8 examples/sec; 0.388 sec/batch)
2017-04-02 22:03:36.534831: step 5450, loss = 0.96 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 22:03:40.390006: step 5460, loss = 1.07 (332.0 examples/sec; 0.386 sec/batch)
2017-04-02 22:03:44.211013: step 5470, loss = 1.02 (335.0 examples/sec; 0.382 sec/batch)
2017-04-02 22:03:48.076865: step 5480, loss = 1.02 (331.1 examples/sec; 0.387 sec/batch)
2017-04-02 22:03:51.902638: step 5490, loss = 1.08 (334.6 examples/sec; 0.383 sec/batch)
2017-04-02 22:03:55.809549: step 5500, loss = 1.06 (327.6 examples/sec; 0.391 sec/batch)
2017-04-02 22:03:59.665742: step 5510, loss = 0.88 (331.9 examples/sec; 0.386 sec/batch)
2017-04-02 22:04:03.516215: step 5520, loss = 1.19 (332.4 examples/sec; 0.385 sec/batch)
2017-04-02 22:04:07.396681: step 5530, loss = 1.03 (329.9 examples/sec; 0.388 sec/batch)
2017-04-02 22:04:11.249959: step 5540, loss = 0.91 (332.2 examples/sec; 0.385 sec/batch)
2017-04-02 22:04:15.108435: step 5550, loss = 1.11 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 22:04:18.957713: step 5560, loss = 0.92 (332.5 examples/sec; 0.385 sec/batch)
2017-04-02 22:04:22.820026: step 5570, loss = 0.91 (331.4 examples/sec; 0.386 sec/batch)
2017-04-02 22:04:26.693942: step 5580, loss = 0.98 (330.4 examples/sec; 0.387 sec/batch)
2017-04-02 22:04:30.566792: step 5590, loss = 0.92 (330.5 examples/sec; 0.387 sec/batch)
2017-04-02 22:04:34.494539: step 5600, loss = 1.12 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 22:04:39.103434: step 5610, loss = 0.96 (277.7 examples/sec; 0.461 sec/batch)
2017-04-02 22:04:42.973617: step 5620, loss = 0.92 (330.7 examples/sec; 0.387 sec/batch)
2017-04-02 22:04:46.812773: step 5630, loss = 0.92 (333.4 examples/sec; 0.384 sec/batch)
2017-04-02 22:04:50.642433: step 5640, loss = 0.91 (334.2 examples/sec; 0.383 sec/batch)
2017-04-02 22:04:54.450638: step 5650, loss = 0.78 (336.1 examples/sec; 0.381 sec/batch)
2017-04-02 22:04:58.305589: step 5660, loss = 0.93 (332.0 examples/sec; 0.385 sec/batch)
2017-04-02 22:05:02.165089: step 5670, loss = 0.90 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 22:05:06.027198: step 5680, loss = 0.98 (331.4 examples/sec; 0.386 sec/batch)
2017-04-02 22:05:09.928307: step 5690, loss = 0.98 (328.1 examples/sec; 0.390 sec/batch)
2017-04-02 22:05:13.850934: step 5700, loss = 1.04 (326.3 examples/sec; 0.392 sec/batch)
2017-04-02 22:05:17.696743: step 5710, loss = 1.18 (332.8 examples/sec; 0.385 sec/batch)
2017-04-02 22:05:21.557117: step 5720, loss = 0.94 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 22:05:25.385252: step 5730, loss = 0.97 (334.4 examples/sec; 0.383 sec/batch)
2017-04-02 22:05:29.209040: step 5740, loss = 0.88 (334.7 examples/sec; 0.382 sec/batch)
2017-04-02 22:05:33.028980: step 5750, loss = 1.00 (335.1 examples/sec; 0.382 sec/batch)
2017-04-02 22:05:36.855916: step 5760, loss = 1.00 (334.5 examples/sec; 0.383 sec/batch)
2017-04-02 22:05:40.681101: step 5770, loss = 0.99 (334.6 examples/sec; 0.383 sec/batch)
2017-04-02 22:05:44.534725: step 5780, loss = 1.07 (332.2 examples/sec; 0.385 sec/batch)
2017-04-02 22:05:48.373632: step 5790, loss = 0.95 (333.4 examples/sec; 0.384 sec/batch)
2017-04-02 22:05:52.276147: step 5800, loss = 0.86 (328.0 examples/sec; 0.390 sec/batch)
2017-04-02 22:05:56.114446: step 5810, loss = 1.04 (333.5 examples/sec; 0.384 sec/batch)
2017-04-02 22:05:59.918007: step 5820, loss = 0.92 (336.5 examples/sec; 0.380 sec/batch)
2017-04-02 22:06:03.750568: step 5830, loss = 1.06 (334.0 examples/sec; 0.383 sec/batch)
2017-04-02 22:06:07.624918: step 5840, loss = 1.03 (330.4 examples/sec; 0.387 sec/batch)
2017-04-02 22:06:11.464114: step 5850, loss = 0.97 (333.4 examples/sec; 0.384 sec/batch)
2017-04-02 22:06:15.249942: step 5860, loss = 0.88 (338.1 examples/sec; 0.379 sec/batch)
2017-04-02 22:06:19.065850: step 5870, loss = 1.00 (335.4 examples/sec; 0.382 sec/batch)
2017-04-02 22:06:22.884739: step 5880, loss = 0.93 (335.2 examples/sec; 0.382 sec/batch)
2017-04-02 22:06:26.737127: step 5890, loss = 0.87 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 22:06:30.618757: step 5900, loss = 0.87 (329.8 examples/sec; 0.388 sec/batch)
2017-04-02 22:06:34.446375: step 5910, loss = 1.05 (334.4 examples/sec; 0.383 sec/batch)
2017-04-02 22:06:38.258612: step 5920, loss = 1.14 (335.8 examples/sec; 0.381 sec/batch)
2017-04-02 22:06:42.060646: step 5930, loss = 0.91 (336.7 examples/sec; 0.380 sec/batch)
2017-04-02 22:06:45.870301: step 5940, loss = 1.07 (336.0 examples/sec; 0.381 sec/batch)
2017-04-02 22:06:49.722397: step 5950, loss = 1.05 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 22:06:53.557693: step 5960, loss = 1.07 (333.7 examples/sec; 0.384 sec/batch)
2017-04-02 22:06:57.416413: step 5970, loss = 0.90 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 22:07:01.223278: step 5980, loss = 0.99 (336.2 examples/sec; 0.381 sec/batch)
2017-04-02 22:07:05.078469: step 5990, loss = 1.02 (332.0 examples/sec; 0.386 sec/batch)
2017-04-02 22:07:08.966593: step 6000, loss = 0.95 (329.2 examples/sec; 0.389 sec/batch)
2017-04-02 22:07:12.812172: step 6010, loss = 1.02 (332.8 examples/sec; 0.385 sec/batch)
2017-04-02 22:07:16.657369: step 6020, loss = 1.03 (332.9 examples/sec; 0.385 sec/batch)
2017-04-02 22:07:20.497001: step 6030, loss = 0.97 (333.4 examples/sec; 0.384 sec/batch)
2017-04-02 22:07:24.350029: step 6040, loss = 1.11 (332.2 examples/sec; 0.385 sec/batch)
2017-04-02 22:07:28.199350: step 6050, loss = 1.01 (332.5 examples/sec; 0.385 sec/batch)
2017-04-02 22:07:32.019532: step 6060, loss = 0.87 (335.1 examples/sec; 0.382 sec/batch)
2017-04-02 22:07:35.848113: step 6070, loss = 1.00 (334.3 examples/sec; 0.383 sec/batch)
2017-04-02 22:08:40.499059: precision @ 1 = 0.786

2017-04-02 22:10:10.988260: step 6080, loss = 0.92 (8.3 examples/sec; 15.514 sec/batch)
2017-04-02 22:10:14.825052: step 6090, loss = 0.96 (333.6 examples/sec; 0.384 sec/batch)
2017-04-02 22:10:18.741156: step 6100, loss = 1.11 (326.9 examples/sec; 0.392 sec/batch)
2017-04-02 22:10:22.570638: step 6110, loss = 1.00 (334.2 examples/sec; 0.383 sec/batch)
2017-04-02 22:10:26.428052: step 6120, loss = 1.13 (331.8 examples/sec; 0.386 sec/batch)
2017-04-02 22:10:30.275913: step 6130, loss = 0.84 (332.7 examples/sec; 0.385 sec/batch)
2017-04-02 22:10:34.112926: step 6140, loss = 1.12 (333.6 examples/sec; 0.384 sec/batch)
2017-04-02 22:10:38.003885: step 6150, loss = 0.80 (329.0 examples/sec; 0.389 sec/batch)
2017-04-02 22:10:41.850649: step 6160, loss = 0.99 (332.7 examples/sec; 0.385 sec/batch)
2017-04-02 22:10:45.687681: step 6170, loss = 0.92 (333.6 examples/sec; 0.384 sec/batch)
2017-04-02 22:10:49.653515: step 6180, loss = 0.90 (322.8 examples/sec; 0.397 sec/batch)
2017-04-02 22:10:53.821924: step 6190, loss = 0.98 (307.1 examples/sec; 0.417 sec/batch)
2017-04-02 22:10:57.716176: step 6200, loss = 1.02 (328.7 examples/sec; 0.389 sec/batch)
2017-04-02 22:11:01.541917: step 6210, loss = 1.02 (334.6 examples/sec; 0.383 sec/batch)
2017-04-02 22:11:05.378713: step 6220, loss = 0.88 (333.6 examples/sec; 0.384 sec/batch)
2017-04-02 22:11:09.278789: step 6230, loss = 1.02 (328.2 examples/sec; 0.390 sec/batch)
2017-04-02 22:11:13.152697: step 6240, loss = 0.94 (330.4 examples/sec; 0.387 sec/batch)
2017-04-02 22:11:17.011498: step 6250, loss = 1.05 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 22:11:20.861202: step 6260, loss = 1.07 (332.5 examples/sec; 0.385 sec/batch)
2017-04-02 22:11:24.718861: step 6270, loss = 0.93 (331.8 examples/sec; 0.386 sec/batch)
2017-04-02 22:11:28.572992: step 6280, loss = 0.94 (332.1 examples/sec; 0.385 sec/batch)
2017-04-02 22:11:32.409031: step 6290, loss = 0.87 (333.7 examples/sec; 0.384 sec/batch)
2017-04-02 22:11:36.294469: step 6300, loss = 1.15 (329.4 examples/sec; 0.389 sec/batch)
2017-04-02 22:11:40.133423: step 6310, loss = 0.99 (333.4 examples/sec; 0.384 sec/batch)
2017-04-02 22:11:44.220421: step 6320, loss = 0.98 (313.2 examples/sec; 0.409 sec/batch)
2017-04-02 22:11:48.070366: step 6330, loss = 0.84 (332.5 examples/sec; 0.385 sec/batch)
2017-04-02 22:11:51.897102: step 6340, loss = 1.06 (334.5 examples/sec; 0.383 sec/batch)
2017-04-02 22:11:55.742442: step 6350, loss = 1.00 (332.9 examples/sec; 0.385 sec/batch)
2017-04-02 22:11:59.563251: step 6360, loss = 0.90 (335.0 examples/sec; 0.382 sec/batch)
2017-04-02 22:12:03.371476: step 6370, loss = 1.14 (336.1 examples/sec; 0.381 sec/batch)
2017-04-02 22:12:07.229384: step 6380, loss = 0.97 (331.8 examples/sec; 0.386 sec/batch)
2017-04-02 22:12:11.081698: step 6390, loss = 0.99 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 22:12:14.979759: step 6400, loss = 0.81 (328.4 examples/sec; 0.390 sec/batch)
2017-04-02 22:12:18.774161: step 6410, loss = 0.91 (337.3 examples/sec; 0.379 sec/batch)
2017-04-02 22:12:22.613777: step 6420, loss = 0.90 (333.4 examples/sec; 0.384 sec/batch)
2017-04-02 22:12:26.445806: step 6430, loss = 1.00 (334.0 examples/sec; 0.383 sec/batch)
2017-04-02 22:12:30.526844: step 6440, loss = 1.00 (313.6 examples/sec; 0.408 sec/batch)
2017-04-02 22:12:34.373673: step 6450, loss = 0.94 (332.7 examples/sec; 0.385 sec/batch)
2017-04-02 22:12:38.225238: step 6460, loss = 0.94 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 22:12:42.060091: step 6470, loss = 0.91 (333.8 examples/sec; 0.383 sec/batch)
2017-04-02 22:12:45.887680: step 6480, loss = 1.00 (334.4 examples/sec; 0.383 sec/batch)
2017-04-02 22:12:49.757054: step 6490, loss = 1.03 (330.8 examples/sec; 0.387 sec/batch)
2017-04-02 22:12:53.642920: step 6500, loss = 0.97 (329.4 examples/sec; 0.389 sec/batch)
2017-04-02 22:12:57.486238: step 6510, loss = 0.98 (333.0 examples/sec; 0.384 sec/batch)
2017-04-02 22:13:01.304120: step 6520, loss = 0.93 (335.3 examples/sec; 0.382 sec/batch)
2017-04-02 22:13:05.092846: step 6530, loss = 0.86 (337.8 examples/sec; 0.379 sec/batch)
2017-04-02 22:13:08.927898: step 6540, loss = 1.07 (333.8 examples/sec; 0.384 sec/batch)
2017-04-02 22:13:12.759089: step 6550, loss = 0.78 (334.1 examples/sec; 0.383 sec/batch)
2017-04-02 22:13:16.582040: step 6560, loss = 1.10 (334.8 examples/sec; 0.382 sec/batch)
2017-04-02 22:13:20.426529: step 6570, loss = 0.95 (332.9 examples/sec; 0.384 sec/batch)
2017-04-02 22:13:24.216583: step 6580, loss = 1.08 (337.7 examples/sec; 0.379 sec/batch)
2017-04-02 22:13:28.075603: step 6590, loss = 1.10 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 22:13:31.970144: step 6600, loss = 0.95 (328.7 examples/sec; 0.389 sec/batch)
2017-04-02 22:13:35.795483: step 6610, loss = 0.82 (334.6 examples/sec; 0.383 sec/batch)
2017-04-02 22:13:39.622864: step 6620, loss = 0.93 (334.4 examples/sec; 0.383 sec/batch)
2017-04-02 22:13:43.456297: step 6630, loss = 1.00 (333.9 examples/sec; 0.383 sec/batch)
2017-04-02 22:13:47.295261: step 6640, loss = 0.82 (333.4 examples/sec; 0.384 sec/batch)
2017-04-02 22:13:51.116075: step 6650, loss = 1.16 (335.0 examples/sec; 0.382 sec/batch)
2017-04-02 22:13:54.936753: step 6660, loss = 0.96 (335.0 examples/sec; 0.382 sec/batch)
2017-04-02 22:13:58.777271: step 6670, loss = 0.95 (333.3 examples/sec; 0.384 sec/batch)
2017-04-02 22:14:02.644563: step 6680, loss = 1.00 (331.0 examples/sec; 0.387 sec/batch)
2017-04-02 22:14:06.467438: step 6690, loss = 0.99 (334.8 examples/sec; 0.382 sec/batch)
2017-04-02 22:14:10.395286: step 6700, loss = 0.76 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 22:14:14.250962: step 6710, loss = 0.90 (332.0 examples/sec; 0.386 sec/batch)
2017-04-02 22:14:18.105636: step 6720, loss = 0.91 (332.1 examples/sec; 0.385 sec/batch)
2017-04-02 22:14:21.936398: step 6730, loss = 0.96 (334.1 examples/sec; 0.383 sec/batch)
2017-04-02 22:14:25.768952: step 6740, loss = 0.96 (334.0 examples/sec; 0.383 sec/batch)
2017-04-02 22:14:29.606223: step 6750, loss = 0.99 (333.6 examples/sec; 0.384 sec/batch)
2017-04-02 22:14:33.426941: step 6760, loss = 1.01 (335.0 examples/sec; 0.382 sec/batch)
2017-04-02 22:14:37.277531: step 6770, loss = 0.91 (332.4 examples/sec; 0.385 sec/batch)
2017-04-02 22:14:42.048617: step 6780, loss = 0.83 (268.3 examples/sec; 0.477 sec/batch)
2017-04-02 22:14:45.870274: step 6790, loss = 0.90 (334.9 examples/sec; 0.382 sec/batch)
2017-04-02 22:14:49.763620: step 6800, loss = 1.04 (328.8 examples/sec; 0.389 sec/batch)
2017-04-02 22:14:53.611710: step 6810, loss = 0.94 (332.6 examples/sec; 0.385 sec/batch)
2017-04-02 22:14:57.450453: step 6820, loss = 0.92 (333.4 examples/sec; 0.384 sec/batch)
2017-04-02 22:15:01.266101: step 6830, loss = 1.05 (335.5 examples/sec; 0.382 sec/batch)
2017-04-02 22:15:05.077385: step 6840, loss = 1.05 (335.8 examples/sec; 0.381 sec/batch)
2017-04-02 22:15:08.908706: step 6850, loss = 0.87 (334.1 examples/sec; 0.383 sec/batch)
2017-04-02 22:15:12.705907: step 6860, loss = 0.97 (337.1 examples/sec; 0.380 sec/batch)
2017-04-02 22:15:16.540781: step 6870, loss = 0.95 (333.8 examples/sec; 0.383 sec/batch)
2017-04-02 22:15:20.382548: step 6880, loss = 0.94 (333.2 examples/sec; 0.384 sec/batch)
2017-04-02 22:15:24.205875: step 6890, loss = 0.91 (334.8 examples/sec; 0.382 sec/batch)
2017-04-02 22:15:28.071301: step 6900, loss = 0.82 (331.1 examples/sec; 0.387 sec/batch)
2017-04-02 22:15:31.933489: step 6910, loss = 0.95 (331.4 examples/sec; 0.386 sec/batch)
2017-04-02 22:15:35.771817: step 6920, loss = 0.91 (333.5 examples/sec; 0.384 sec/batch)
2017-04-02 22:15:39.593792: step 6930, loss = 0.87 (334.9 examples/sec; 0.382 sec/batch)
2017-04-02 22:15:43.413279: step 6940, loss = 0.90 (335.1 examples/sec; 0.382 sec/batch)
2017-04-02 22:15:47.293725: step 6950, loss = 0.98 (329.9 examples/sec; 0.388 sec/batch)
2017-04-02 22:15:51.142268: step 6960, loss = 0.87 (332.6 examples/sec; 0.385 sec/batch)
2017-04-02 22:15:55.010082: step 6970, loss = 0.88 (330.9 examples/sec; 0.387 sec/batch)
2017-04-02 22:15:58.859173: step 6980, loss = 1.01 (332.5 examples/sec; 0.385 sec/batch)
2017-04-02 22:16:02.710699: step 6990, loss = 0.86 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 22:16:06.587719: step 7000, loss = 0.87 (330.2 examples/sec; 0.388 sec/batch)
2017-04-02 22:16:10.445813: step 7010, loss = 0.81 (331.8 examples/sec; 0.386 sec/batch)
2017-04-02 22:16:14.354559: step 7020, loss = 0.98 (327.5 examples/sec; 0.391 sec/batch)
2017-04-02 22:16:18.217970: step 7030, loss = 1.01 (331.3 examples/sec; 0.386 sec/batch)
2017-04-02 22:16:22.080432: step 7040, loss = 0.99 (331.4 examples/sec; 0.386 sec/batch)
2017-04-02 22:16:25.966577: step 7050, loss = 1.04 (329.4 examples/sec; 0.389 sec/batch)
2017-04-02 22:16:29.808009: step 7060, loss = 0.90 (333.2 examples/sec; 0.384 sec/batch)
2017-04-02 22:16:33.750049: step 7070, loss = 1.00 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 22:16:37.609978: step 7080, loss = 0.96 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 22:16:41.472202: step 7090, loss = 0.96 (331.4 examples/sec; 0.386 sec/batch)
2017-04-02 22:16:45.392160: step 7100, loss = 0.97 (326.5 examples/sec; 0.392 sec/batch)
2017-04-02 22:16:49.230538: step 7110, loss = 1.10 (333.5 examples/sec; 0.384 sec/batch)
2017-04-02 22:16:53.064634: step 7120, loss = 1.04 (333.8 examples/sec; 0.383 sec/batch)
2017-04-02 22:16:56.939062: step 7130, loss = 1.00 (330.4 examples/sec; 0.387 sec/batch)
2017-04-02 22:17:00.789472: step 7140, loss = 1.07 (332.4 examples/sec; 0.385 sec/batch)
2017-04-02 22:17:04.668864: step 7150, loss = 0.88 (329.9 examples/sec; 0.388 sec/batch)
2017-04-02 22:17:08.521448: step 7160, loss = 0.93 (332.2 examples/sec; 0.385 sec/batch)
2017-04-02 22:17:12.368146: step 7170, loss = 0.93 (332.8 examples/sec; 0.385 sec/batch)
2017-04-02 22:17:16.213407: step 7180, loss = 0.77 (332.9 examples/sec; 0.385 sec/batch)
2017-04-02 22:17:20.051755: step 7190, loss = 0.91 (333.5 examples/sec; 0.384 sec/batch)
2017-04-02 22:17:23.954563: step 7200, loss = 0.96 (328.0 examples/sec; 0.390 sec/batch)
2017-04-02 22:17:27.836251: step 7210, loss = 1.00 (329.8 examples/sec; 0.388 sec/batch)
2017-04-02 22:17:31.701095: step 7220, loss = 1.04 (331.2 examples/sec; 0.386 sec/batch)
2017-04-02 22:17:35.576655: step 7230, loss = 1.00 (330.3 examples/sec; 0.388 sec/batch)
2017-04-02 22:17:39.445813: step 7240, loss = 0.66 (330.8 examples/sec; 0.387 sec/batch)
2017-04-02 22:17:43.284869: step 7250, loss = 0.95 (333.4 examples/sec; 0.384 sec/batch)
2017-04-02 22:17:47.137156: step 7260, loss = 1.01 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 22:17:51.002992: step 7270, loss = 0.77 (331.1 examples/sec; 0.387 sec/batch)
2017-04-02 22:17:54.844063: step 7280, loss = 0.83 (333.2 examples/sec; 0.384 sec/batch)
2017-04-02 22:17:58.734548: step 7290, loss = 1.11 (329.0 examples/sec; 0.389 sec/batch)
2017-04-02 22:18:02.679265: step 7300, loss = 0.91 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 22:18:06.559090: step 7310, loss = 1.10 (329.9 examples/sec; 0.388 sec/batch)
2017-04-02 22:18:10.397662: step 7320, loss = 1.02 (333.5 examples/sec; 0.384 sec/batch)
2017-04-02 22:18:14.214417: step 7330, loss = 0.91 (335.4 examples/sec; 0.382 sec/batch)
2017-04-02 22:18:18.050729: step 7340, loss = 0.98 (333.7 examples/sec; 0.384 sec/batch)
2017-04-02 22:18:21.885054: step 7350, loss = 0.91 (333.8 examples/sec; 0.383 sec/batch)
2017-04-02 22:18:25.717061: step 7360, loss = 0.88 (334.0 examples/sec; 0.383 sec/batch)
2017-04-02 22:18:29.566664: step 7370, loss = 0.78 (332.5 examples/sec; 0.385 sec/batch)
2017-04-02 22:18:33.422543: step 7380, loss = 0.94 (332.0 examples/sec; 0.386 sec/batch)
2017-04-02 22:18:37.293973: step 7390, loss = 0.84 (330.6 examples/sec; 0.387 sec/batch)
2017-04-02 22:18:41.218876: step 7400, loss = 0.80 (326.1 examples/sec; 0.392 sec/batch)
2017-04-02 22:18:45.040443: step 7410, loss = 0.80 (334.9 examples/sec; 0.382 sec/batch)
2017-04-02 22:18:48.870032: step 7420, loss = 0.81 (334.2 examples/sec; 0.383 sec/batch)
2017-04-02 22:18:52.725643: step 7430, loss = 0.76 (332.0 examples/sec; 0.386 sec/batch)
2017-04-02 22:18:56.566428: step 7440, loss = 0.92 (333.3 examples/sec; 0.384 sec/batch)
2017-04-02 22:19:00.415990: step 7450, loss = 1.05 (332.5 examples/sec; 0.385 sec/batch)
2017-04-02 22:19:04.274211: step 7460, loss = 0.86 (331.8 examples/sec; 0.386 sec/batch)
2017-04-02 22:19:08.092689: step 7470, loss = 1.02 (335.2 examples/sec; 0.382 sec/batch)
2017-04-02 22:19:11.958668: step 7480, loss = 1.04 (331.1 examples/sec; 0.387 sec/batch)
2017-04-02 22:19:15.807532: step 7490, loss = 0.99 (332.6 examples/sec; 0.385 sec/batch)
2017-04-02 22:19:19.741673: step 7500, loss = 0.92 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 22:19:23.559013: step 7510, loss = 0.95 (335.3 examples/sec; 0.382 sec/batch)
2017-04-02 22:19:27.397747: step 7520, loss = 0.85 (333.4 examples/sec; 0.384 sec/batch)
2017-04-02 22:19:31.288529: step 7530, loss = 0.95 (329.0 examples/sec; 0.389 sec/batch)
2017-04-02 22:19:35.172463: step 7540, loss = 0.81 (329.6 examples/sec; 0.388 sec/batch)
2017-04-02 22:19:39.056870: step 7550, loss = 0.87 (329.5 examples/sec; 0.388 sec/batch)
2017-04-02 22:19:42.885320: step 7560, loss = 1.17 (334.3 examples/sec; 0.383 sec/batch)
2017-04-02 22:19:46.762850: step 7570, loss = 1.02 (330.1 examples/sec; 0.388 sec/batch)
2017-04-02 22:19:50.654367: step 7580, loss = 0.88 (328.9 examples/sec; 0.389 sec/batch)
2017-04-02 22:19:54.533504: step 7590, loss = 0.81 (330.0 examples/sec; 0.388 sec/batch)
2017-04-02 22:19:58.427499: step 7600, loss = 1.07 (328.7 examples/sec; 0.389 sec/batch)
2017-04-02 22:20:02.293461: step 7610, loss = 0.91 (331.1 examples/sec; 0.387 sec/batch)
2017-04-02 22:20:06.153661: step 7620, loss = 0.80 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 22:20:10.005860: step 7630, loss = 0.97 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 22:20:13.889501: step 7640, loss = 1.13 (329.6 examples/sec; 0.388 sec/batch)
2017-04-02 22:20:17.758962: step 7650, loss = 0.94 (330.8 examples/sec; 0.387 sec/batch)
2017-04-02 22:20:21.619226: step 7660, loss = 1.05 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 22:20:25.493679: step 7670, loss = 0.83 (330.4 examples/sec; 0.387 sec/batch)
2017-04-02 22:20:29.370204: step 7680, loss = 1.00 (330.2 examples/sec; 0.388 sec/batch)
2017-04-02 22:20:33.255023: step 7690, loss = 0.91 (329.5 examples/sec; 0.388 sec/batch)
2017-04-02 22:20:37.170299: step 7700, loss = 1.02 (326.9 examples/sec; 0.392 sec/batch)
2017-04-02 22:20:41.015166: step 7710, loss = 0.77 (332.9 examples/sec; 0.384 sec/batch)
2017-04-02 22:20:44.826544: step 7720, loss = 1.01 (335.8 examples/sec; 0.381 sec/batch)
2017-04-02 22:20:48.665699: step 7730, loss = 0.74 (333.4 examples/sec; 0.384 sec/batch)
2017-04-02 22:20:52.525097: step 7740, loss = 0.80 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 22:20:56.383473: step 7750, loss = 0.80 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 22:21:00.242497: step 7760, loss = 0.97 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 22:21:04.121769: step 7770, loss = 0.99 (330.0 examples/sec; 0.388 sec/batch)
2017-04-02 22:21:07.973431: step 7780, loss = 0.90 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 22:21:11.801904: step 7790, loss = 1.15 (334.3 examples/sec; 0.383 sec/batch)
2017-04-02 22:21:15.729744: step 7800, loss = 0.87 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 22:21:19.612877: step 7810, loss = 0.80 (329.6 examples/sec; 0.388 sec/batch)
2017-04-02 22:21:23.499394: step 7820, loss = 1.06 (329.3 examples/sec; 0.389 sec/batch)
2017-04-02 22:21:27.385237: step 7830, loss = 1.00 (329.4 examples/sec; 0.389 sec/batch)
2017-04-02 22:21:31.232792: step 7840, loss = 1.05 (332.7 examples/sec; 0.385 sec/batch)
2017-04-02 22:21:35.093511: step 7850, loss = 0.97 (331.5 examples/sec; 0.386 sec/batch)
2017-04-02 22:21:38.991135: step 7860, loss = 0.75 (328.4 examples/sec; 0.390 sec/batch)
2017-04-02 22:21:42.856659: step 7870, loss = 0.99 (331.1 examples/sec; 0.387 sec/batch)
2017-04-02 22:21:46.733795: step 7880, loss = 1.00 (330.1 examples/sec; 0.388 sec/batch)
2017-04-02 22:21:50.628529: step 7890, loss = 0.89 (328.6 examples/sec; 0.389 sec/batch)
2017-04-02 22:21:54.570808: step 7900, loss = 0.93 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 22:21:58.444410: step 7910, loss = 0.92 (330.4 examples/sec; 0.387 sec/batch)
2017-04-02 22:22:02.326339: step 7920, loss = 0.87 (329.7 examples/sec; 0.388 sec/batch)
2017-04-02 22:22:06.169127: step 7930, loss = 0.84 (333.1 examples/sec; 0.384 sec/batch)
2017-04-02 22:22:10.019861: step 7940, loss = 1.12 (332.4 examples/sec; 0.385 sec/batch)
2017-04-02 22:22:13.857642: step 7950, loss = 1.03 (333.5 examples/sec; 0.384 sec/batch)
2017-04-02 22:22:17.676831: step 7960, loss = 0.79 (335.1 examples/sec; 0.382 sec/batch)
2017-04-02 22:22:21.528530: step 7970, loss = 0.83 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 22:22:25.372616: step 7980, loss = 0.87 (333.0 examples/sec; 0.384 sec/batch)
2017-04-02 22:22:29.241888: step 7990, loss = 0.74 (330.8 examples/sec; 0.387 sec/batch)
2017-04-02 22:22:33.155793: step 8000, loss = 0.86 (327.0 examples/sec; 0.391 sec/batch)
2017-04-02 22:25:17.394131: precision @ 1 = 0.793

2017-04-02 22:26:12.497738: step 8010, loss = 1.17 (5.8 examples/sec; 21.934 sec/batch)
2017-04-02 22:26:16.352214: step 8020, loss = 0.87 (332.1 examples/sec; 0.385 sec/batch)
2017-04-02 22:26:20.174640: step 8030, loss = 0.98 (334.9 examples/sec; 0.382 sec/batch)
2017-04-02 22:26:24.047122: step 8040, loss = 0.99 (330.5 examples/sec; 0.387 sec/batch)
2017-04-02 22:26:27.915333: step 8050, loss = 0.93 (330.9 examples/sec; 0.387 sec/batch)
2017-04-02 22:26:31.725103: step 8060, loss = 0.92 (336.0 examples/sec; 0.381 sec/batch)
2017-04-02 22:26:35.546477: step 8070, loss = 0.85 (335.0 examples/sec; 0.382 sec/batch)
2017-04-02 22:26:39.369088: step 8080, loss = 0.91 (334.8 examples/sec; 0.382 sec/batch)
2017-04-02 22:26:43.208670: step 8090, loss = 1.01 (333.4 examples/sec; 0.384 sec/batch)
2017-04-02 22:26:47.114520: step 8100, loss = 0.93 (327.7 examples/sec; 0.391 sec/batch)
2017-04-02 22:26:50.965320: step 8110, loss = 0.88 (332.4 examples/sec; 0.385 sec/batch)
2017-04-02 22:26:54.782298: step 8120, loss = 1.07 (335.3 examples/sec; 0.382 sec/batch)
2017-04-02 22:26:58.629313: step 8130, loss = 1.04 (332.7 examples/sec; 0.385 sec/batch)
2017-04-02 22:27:02.448436: step 8140, loss = 0.88 (335.2 examples/sec; 0.382 sec/batch)
2017-04-02 22:27:06.303902: step 8150, loss = 0.85 (332.0 examples/sec; 0.386 sec/batch)
2017-04-02 22:27:10.158317: step 8160, loss = 1.18 (332.1 examples/sec; 0.385 sec/batch)
2017-04-02 22:27:13.989560: step 8170, loss = 1.05 (334.1 examples/sec; 0.383 sec/batch)
2017-04-02 22:27:17.862626: step 8180, loss = 0.95 (330.5 examples/sec; 0.387 sec/batch)
2017-04-02 22:27:21.719761: step 8190, loss = 0.96 (331.9 examples/sec; 0.386 sec/batch)
2017-04-02 22:27:25.658037: step 8200, loss = 0.74 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 22:27:29.519572: step 8210, loss = 0.97 (331.5 examples/sec; 0.386 sec/batch)
2017-04-02 22:27:33.334084: step 8220, loss = 0.85 (335.6 examples/sec; 0.381 sec/batch)
2017-04-02 22:27:37.190746: step 8230, loss = 0.84 (331.9 examples/sec; 0.386 sec/batch)
2017-04-02 22:27:41.012375: step 8240, loss = 1.03 (334.9 examples/sec; 0.382 sec/batch)
2017-04-02 22:27:44.852732: step 8250, loss = 0.91 (333.3 examples/sec; 0.384 sec/batch)
2017-04-02 22:27:48.698713: step 8260, loss = 1.06 (332.8 examples/sec; 0.385 sec/batch)
2017-04-02 22:27:52.604178: step 8270, loss = 0.69 (327.7 examples/sec; 0.391 sec/batch)
2017-04-02 22:27:56.418011: step 8280, loss = 1.02 (335.6 examples/sec; 0.381 sec/batch)
2017-04-02 22:28:00.284297: step 8290, loss = 0.92 (331.1 examples/sec; 0.387 sec/batch)
2017-04-02 22:28:04.187540: step 8300, loss = 0.84 (327.9 examples/sec; 0.390 sec/batch)
2017-04-02 22:28:07.992193: step 8310, loss = 1.04 (336.4 examples/sec; 0.380 sec/batch)
2017-04-02 22:28:11.817542: step 8320, loss = 0.92 (334.6 examples/sec; 0.383 sec/batch)
2017-04-02 22:28:15.683330: step 8330, loss = 1.04 (331.1 examples/sec; 0.387 sec/batch)
2017-04-02 22:28:19.520826: step 8340, loss = 0.81 (333.6 examples/sec; 0.384 sec/batch)
2017-04-02 22:28:23.363639: step 8350, loss = 0.89 (333.1 examples/sec; 0.384 sec/batch)
2017-04-02 22:28:27.227028: step 8360, loss = 0.97 (331.3 examples/sec; 0.386 sec/batch)
2017-04-02 22:28:31.080697: step 8370, loss = 0.90 (332.2 examples/sec; 0.385 sec/batch)
2017-04-02 22:28:34.960810: step 8380, loss = 0.81 (329.9 examples/sec; 0.388 sec/batch)
2017-04-02 22:28:38.841816: step 8390, loss = 0.76 (329.8 examples/sec; 0.388 sec/batch)
2017-04-02 22:28:42.748783: step 8400, loss = 0.92 (327.6 examples/sec; 0.391 sec/batch)
2017-04-02 22:28:46.582437: step 8410, loss = 1.05 (333.9 examples/sec; 0.383 sec/batch)
2017-04-02 22:28:50.410616: step 8420, loss = 0.85 (334.4 examples/sec; 0.383 sec/batch)
2017-04-02 22:28:54.282839: step 8430, loss = 0.85 (330.6 examples/sec; 0.387 sec/batch)
2017-04-02 22:28:58.157701: step 8440, loss = 1.03 (330.3 examples/sec; 0.387 sec/batch)
2017-04-02 22:29:01.972670: step 8450, loss = 0.76 (335.5 examples/sec; 0.381 sec/batch)
2017-04-02 22:29:05.821280: step 8460, loss = 0.88 (332.6 examples/sec; 0.385 sec/batch)
2017-04-02 22:29:09.659260: step 8470, loss = 1.01 (333.5 examples/sec; 0.384 sec/batch)
2017-04-02 22:29:13.471418: step 8480, loss = 0.72 (335.8 examples/sec; 0.381 sec/batch)
2017-04-02 22:29:17.280181: step 8490, loss = 1.01 (336.1 examples/sec; 0.381 sec/batch)
2017-04-02 22:29:21.173900: step 8500, loss = 0.87 (328.7 examples/sec; 0.389 sec/batch)
2017-04-02 22:29:25.009754: step 8510, loss = 0.89 (333.7 examples/sec; 0.384 sec/batch)
2017-04-02 22:29:28.830862: step 8520, loss = 0.86 (335.0 examples/sec; 0.382 sec/batch)
2017-04-02 22:29:32.655141: step 8530, loss = 0.81 (334.7 examples/sec; 0.382 sec/batch)
2017-04-02 22:29:36.463192: step 8540, loss = 0.92 (336.1 examples/sec; 0.381 sec/batch)
2017-04-02 22:29:40.301254: step 8550, loss = 0.73 (333.5 examples/sec; 0.384 sec/batch)
2017-04-02 22:29:44.101480: step 8560, loss = 0.95 (336.8 examples/sec; 0.380 sec/batch)
2017-04-02 22:29:47.926073: step 8570, loss = 0.86 (334.7 examples/sec; 0.382 sec/batch)
2017-04-02 22:29:51.768605: step 8580, loss = 0.96 (333.1 examples/sec; 0.384 sec/batch)
2017-04-02 22:29:55.621106: step 8590, loss = 0.96 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 22:29:59.526035: step 8600, loss = 1.03 (327.8 examples/sec; 0.390 sec/batch)
2017-04-02 22:30:03.386876: step 8610, loss = 0.98 (331.5 examples/sec; 0.386 sec/batch)
2017-04-02 22:30:07.234863: step 8620, loss = 0.76 (332.6 examples/sec; 0.385 sec/batch)
2017-04-02 22:30:11.086646: step 8630, loss = 0.99 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 22:30:14.889639: step 8640, loss = 1.01 (336.6 examples/sec; 0.380 sec/batch)
2017-04-02 22:30:18.705308: step 8650, loss = 1.00 (335.5 examples/sec; 0.382 sec/batch)
2017-04-02 22:30:22.513618: step 8660, loss = 0.84 (336.1 examples/sec; 0.381 sec/batch)
2017-04-02 22:30:26.361751: step 8670, loss = 0.98 (332.6 examples/sec; 0.385 sec/batch)
2017-04-02 22:30:30.165885: step 8680, loss = 0.84 (336.5 examples/sec; 0.380 sec/batch)
2017-04-02 22:30:33.957227: step 8690, loss = 0.81 (337.6 examples/sec; 0.379 sec/batch)
2017-04-02 22:30:37.827853: step 8700, loss = 1.06 (330.7 examples/sec; 0.387 sec/batch)
2017-04-02 22:30:41.649166: step 8710, loss = 0.91 (335.0 examples/sec; 0.382 sec/batch)
2017-04-02 22:30:45.490809: step 8720, loss = 0.77 (333.2 examples/sec; 0.384 sec/batch)
2017-04-02 22:30:49.341933: step 8730, loss = 0.80 (332.4 examples/sec; 0.385 sec/batch)
2017-04-02 22:30:53.175041: step 8740, loss = 0.97 (333.9 examples/sec; 0.383 sec/batch)
2017-04-02 22:30:57.001576: step 8750, loss = 0.89 (334.5 examples/sec; 0.383 sec/batch)
2017-04-02 22:31:00.864021: step 8760, loss = 0.88 (331.4 examples/sec; 0.386 sec/batch)
2017-04-02 22:31:04.705042: step 8770, loss = 0.94 (333.2 examples/sec; 0.384 sec/batch)
2017-04-02 22:31:08.564989: step 8780, loss = 0.78 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 22:31:12.423792: step 8790, loss = 1.13 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 22:31:16.317071: step 8800, loss = 0.97 (328.8 examples/sec; 0.389 sec/batch)
2017-04-02 22:31:20.193119: step 8810, loss = 0.93 (330.2 examples/sec; 0.388 sec/batch)
2017-04-02 22:31:24.031278: step 8820, loss = 0.86 (333.5 examples/sec; 0.384 sec/batch)
2017-04-02 22:31:27.858642: step 8830, loss = 0.84 (334.4 examples/sec; 0.383 sec/batch)
2017-04-02 22:31:31.699896: step 8840, loss = 0.78 (333.2 examples/sec; 0.384 sec/batch)
2017-04-02 22:31:35.547410: step 8850, loss = 0.84 (332.7 examples/sec; 0.385 sec/batch)
2017-04-02 22:31:39.388540: step 8860, loss = 0.90 (333.2 examples/sec; 0.384 sec/batch)
2017-04-02 22:31:43.200918: step 8870, loss = 0.82 (335.7 examples/sec; 0.381 sec/batch)
2017-04-02 22:31:47.025358: step 8880, loss = 0.90 (334.7 examples/sec; 0.382 sec/batch)
2017-04-02 22:31:50.828372: step 8890, loss = 0.93 (336.6 examples/sec; 0.380 sec/batch)
2017-04-02 22:31:54.713637: step 8900, loss = 0.79 (329.4 examples/sec; 0.389 sec/batch)
2017-04-02 22:31:58.533298: step 8910, loss = 0.93 (335.1 examples/sec; 0.382 sec/batch)
2017-04-02 22:32:02.413997: step 8920, loss = 1.11 (329.8 examples/sec; 0.388 sec/batch)
2017-04-02 22:32:06.220443: step 8930, loss = 0.93 (336.3 examples/sec; 0.381 sec/batch)
2017-04-02 22:32:10.085550: step 8940, loss = 0.92 (331.2 examples/sec; 0.387 sec/batch)
2017-04-02 22:32:13.925984: step 8950, loss = 0.76 (333.3 examples/sec; 0.384 sec/batch)
2017-04-02 22:32:17.774454: step 8960, loss = 0.92 (332.6 examples/sec; 0.385 sec/batch)
2017-04-02 22:32:21.613728: step 8970, loss = 0.84 (333.4 examples/sec; 0.384 sec/batch)
2017-04-02 22:32:25.466583: step 8980, loss = 1.03 (332.2 examples/sec; 0.385 sec/batch)
2017-04-02 22:32:29.332778: step 8990, loss = 0.96 (331.1 examples/sec; 0.387 sec/batch)
2017-04-02 22:32:33.245612: step 9000, loss = 0.81 (327.1 examples/sec; 0.391 sec/batch)
2017-04-02 22:32:37.076539: step 9010, loss = 1.11 (334.1 examples/sec; 0.383 sec/batch)
2017-04-02 22:32:40.935084: step 9020, loss = 0.88 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 22:32:44.773718: step 9030, loss = 1.02 (333.5 examples/sec; 0.384 sec/batch)
2017-04-02 22:32:48.613723: step 9040, loss = 0.93 (333.3 examples/sec; 0.384 sec/batch)
2017-04-02 22:32:52.413789: step 9050, loss = 0.93 (336.8 examples/sec; 0.380 sec/batch)
2017-04-02 22:32:56.215752: step 9060, loss = 0.94 (336.7 examples/sec; 0.380 sec/batch)
2017-04-02 22:33:00.043180: step 9070, loss = 0.88 (334.4 examples/sec; 0.383 sec/batch)
2017-04-02 22:33:03.868825: step 9080, loss = 0.85 (334.6 examples/sec; 0.383 sec/batch)
2017-04-02 22:33:07.728425: step 9090, loss = 0.94 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 22:33:11.627247: step 9100, loss = 1.00 (328.3 examples/sec; 0.390 sec/batch)
2017-04-02 22:33:15.440471: step 9110, loss = 0.77 (335.7 examples/sec; 0.381 sec/batch)
2017-04-02 22:33:19.318326: step 9120, loss = 0.88 (330.1 examples/sec; 0.388 sec/batch)
2017-04-02 22:33:23.136928: step 9130, loss = 0.94 (335.2 examples/sec; 0.382 sec/batch)
2017-04-02 22:33:26.967849: step 9140, loss = 0.93 (334.1 examples/sec; 0.383 sec/batch)
2017-04-02 22:33:30.797434: step 9150, loss = 0.79 (334.2 examples/sec; 0.383 sec/batch)
2017-04-02 22:33:34.613795: step 9160, loss = 0.88 (335.4 examples/sec; 0.382 sec/batch)
2017-04-02 22:33:38.409671: step 9170, loss = 0.83 (337.2 examples/sec; 0.380 sec/batch)
2017-04-02 22:33:42.236933: step 9180, loss = 0.86 (334.4 examples/sec; 0.383 sec/batch)
2017-04-02 22:33:46.089416: step 9190, loss = 0.85 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 22:33:49.975682: step 9200, loss = 0.85 (329.4 examples/sec; 0.389 sec/batch)
2017-04-02 22:33:53.836755: step 9210, loss = 0.94 (331.5 examples/sec; 0.386 sec/batch)
2017-04-02 22:33:57.654108: step 9220, loss = 0.91 (335.3 examples/sec; 0.382 sec/batch)
2017-04-02 22:34:01.475472: step 9230, loss = 1.00 (335.0 examples/sec; 0.382 sec/batch)
2017-04-02 22:34:05.303826: step 9240, loss = 0.86 (334.3 examples/sec; 0.383 sec/batch)
2017-04-02 22:34:09.108390: step 9250, loss = 1.03 (336.4 examples/sec; 0.380 sec/batch)
2017-04-02 22:34:12.950791: step 9260, loss = 0.81 (333.1 examples/sec; 0.384 sec/batch)
2017-04-02 22:34:16.783909: step 9270, loss = 0.86 (333.9 examples/sec; 0.383 sec/batch)
2017-04-02 22:34:20.648652: step 9280, loss = 0.86 (331.2 examples/sec; 0.386 sec/batch)
2017-04-02 22:34:24.491385: step 9290, loss = 0.90 (333.1 examples/sec; 0.384 sec/batch)
2017-04-02 22:34:28.377950: step 9300, loss = 0.92 (329.3 examples/sec; 0.389 sec/batch)
2017-04-02 22:34:32.199067: step 9310, loss = 0.93 (335.0 examples/sec; 0.382 sec/batch)
2017-04-02 22:34:36.026207: step 9320, loss = 0.82 (334.5 examples/sec; 0.383 sec/batch)
2017-04-02 22:34:39.841499: step 9330, loss = 0.92 (335.5 examples/sec; 0.382 sec/batch)
2017-04-02 22:34:43.701001: step 9340, loss = 1.02 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 22:34:47.501338: step 9350, loss = 0.79 (336.8 examples/sec; 0.380 sec/batch)
2017-04-02 22:34:51.334221: step 9360, loss = 0.90 (334.0 examples/sec; 0.383 sec/batch)
2017-04-02 22:34:55.184295: step 9370, loss = 0.76 (332.5 examples/sec; 0.385 sec/batch)
2017-04-02 22:34:59.026461: step 9380, loss = 0.85 (333.1 examples/sec; 0.384 sec/batch)
2017-04-02 22:35:02.856238: step 9390, loss = 0.88 (334.2 examples/sec; 0.383 sec/batch)
2017-04-02 22:35:06.746989: step 9400, loss = 0.86 (329.0 examples/sec; 0.389 sec/batch)
2017-04-02 22:35:10.600815: step 9410, loss = 0.91 (332.1 examples/sec; 0.385 sec/batch)
2017-04-02 22:35:14.426675: step 9420, loss = 0.92 (334.6 examples/sec; 0.383 sec/batch)
2017-04-02 22:35:18.233351: step 9430, loss = 0.89 (336.3 examples/sec; 0.381 sec/batch)
2017-04-02 22:35:22.049760: step 9440, loss = 0.99 (335.4 examples/sec; 0.382 sec/batch)
2017-04-02 22:35:25.837516: step 9450, loss = 0.80 (337.9 examples/sec; 0.379 sec/batch)
2017-04-02 22:35:29.669410: step 9460, loss = 0.95 (334.0 examples/sec; 0.383 sec/batch)
2017-04-02 22:35:33.500933: step 9470, loss = 0.85 (334.1 examples/sec; 0.383 sec/batch)
2017-04-02 22:35:37.345699: step 9480, loss = 0.90 (332.9 examples/sec; 0.384 sec/batch)
2017-04-02 22:35:41.164590: step 9490, loss = 0.89 (335.2 examples/sec; 0.382 sec/batch)
2017-04-02 22:35:45.055054: step 9500, loss = 0.97 (329.0 examples/sec; 0.389 sec/batch)
2017-04-02 22:35:48.874594: step 9510, loss = 0.69 (335.1 examples/sec; 0.382 sec/batch)
2017-04-02 22:35:52.751899: step 9520, loss = 0.99 (330.1 examples/sec; 0.388 sec/batch)
2017-04-02 22:35:56.540892: step 9530, loss = 0.79 (337.8 examples/sec; 0.379 sec/batch)
2017-04-02 22:36:00.352683: step 9540, loss = 0.93 (335.8 examples/sec; 0.381 sec/batch)
2017-04-02 22:36:04.170265: step 9550, loss = 0.89 (335.3 examples/sec; 0.382 sec/batch)
2017-04-02 22:36:07.990746: step 9560, loss = 0.91 (335.0 examples/sec; 0.382 sec/batch)
2017-04-02 22:36:12.563561: step 9570, loss = 1.00 (279.9 examples/sec; 0.457 sec/batch)
2017-04-02 22:36:16.385273: step 9580, loss = 0.88 (334.9 examples/sec; 0.382 sec/batch)
2017-04-02 22:36:20.222094: step 9590, loss = 0.88 (333.6 examples/sec; 0.384 sec/batch)
2017-04-02 22:36:24.142381: step 9600, loss = 0.76 (326.5 examples/sec; 0.392 sec/batch)
2017-04-02 22:36:27.954381: step 9610, loss = 0.98 (335.8 examples/sec; 0.381 sec/batch)
2017-04-02 22:36:31.801584: step 9620, loss = 0.93 (332.7 examples/sec; 0.385 sec/batch)
2017-04-02 22:36:35.665407: step 9630, loss = 0.77 (331.3 examples/sec; 0.386 sec/batch)
2017-04-02 22:36:39.479487: step 9640, loss = 0.76 (335.6 examples/sec; 0.381 sec/batch)
2017-04-02 22:36:43.305571: step 9650, loss = 0.80 (334.5 examples/sec; 0.383 sec/batch)
2017-04-02 22:36:47.151418: step 9660, loss = 1.09 (332.8 examples/sec; 0.385 sec/batch)
2017-04-02 22:36:50.970121: step 9670, loss = 0.93 (335.2 examples/sec; 0.382 sec/batch)
2017-04-02 22:36:54.792613: step 9680, loss = 0.86 (334.9 examples/sec; 0.382 sec/batch)
2017-04-02 22:36:58.581097: step 9690, loss = 0.84 (337.9 examples/sec; 0.379 sec/batch)
2017-04-02 22:37:02.470050: step 9700, loss = 0.79 (329.1 examples/sec; 0.389 sec/batch)
2017-04-02 22:37:06.332559: step 9710, loss = 0.81 (331.4 examples/sec; 0.386 sec/batch)
2017-04-02 22:37:10.149339: step 9720, loss = 1.04 (335.4 examples/sec; 0.382 sec/batch)
2017-04-02 22:37:13.970862: step 9730, loss = 0.88 (334.9 examples/sec; 0.382 sec/batch)
2017-04-02 22:37:17.794482: step 9740, loss = 0.87 (334.8 examples/sec; 0.382 sec/batch)
2017-04-02 22:37:21.634052: step 9750, loss = 0.92 (333.4 examples/sec; 0.384 sec/batch)
2017-04-02 22:37:25.449097: step 9760, loss = 0.83 (335.5 examples/sec; 0.382 sec/batch)
2017-04-02 22:37:29.286923: step 9770, loss = 0.88 (333.5 examples/sec; 0.384 sec/batch)
2017-04-02 22:37:33.125381: step 9780, loss = 0.96 (333.5 examples/sec; 0.384 sec/batch)
2017-04-02 22:37:36.944100: step 9790, loss = 1.14 (335.2 examples/sec; 0.382 sec/batch)
2017-04-02 22:37:40.818101: step 9800, loss = 0.78 (330.4 examples/sec; 0.387 sec/batch)
2017-04-02 22:37:44.654195: step 9810, loss = 0.79 (333.7 examples/sec; 0.384 sec/batch)
2017-04-02 22:37:48.469258: step 9820, loss = 0.97 (335.5 examples/sec; 0.382 sec/batch)
2017-04-02 22:37:52.283653: step 9830, loss = 0.82 (335.6 examples/sec; 0.381 sec/batch)
2017-04-02 22:37:56.095186: step 9840, loss = 0.80 (335.8 examples/sec; 0.381 sec/batch)
2017-04-02 22:37:59.933450: step 9850, loss = 0.82 (333.5 examples/sec; 0.384 sec/batch)
2017-04-02 22:38:03.780784: step 9860, loss = 0.79 (332.7 examples/sec; 0.385 sec/batch)
2017-04-02 22:38:07.601406: step 9870, loss = 0.98 (335.0 examples/sec; 0.382 sec/batch)
2017-04-02 22:38:11.407035: step 9880, loss = 0.98 (336.3 examples/sec; 0.381 sec/batch)
2017-04-02 22:38:15.206525: step 9890, loss = 0.74 (336.9 examples/sec; 0.380 sec/batch)
2017-04-02 22:38:19.088936: step 9900, loss = 0.91 (329.7 examples/sec; 0.388 sec/batch)
2017-04-02 22:38:22.908626: step 9910, loss = 0.89 (335.1 examples/sec; 0.382 sec/batch)
2017-04-02 22:38:26.726068: step 9920, loss = 0.97 (335.3 examples/sec; 0.382 sec/batch)
2017-04-02 22:38:30.571531: step 9930, loss = 0.86 (332.9 examples/sec; 0.385 sec/batch)
2017-04-02 22:38:34.403675: step 9940, loss = 0.75 (334.0 examples/sec; 0.383 sec/batch)
2017-04-02 22:38:38.224499: step 9950, loss = 0.80 (335.0 examples/sec; 0.382 sec/batch)
2017-04-02 22:38:42.060891: step 9960, loss = 0.79 (333.6 examples/sec; 0.384 sec/batch)
2017-04-02 22:38:45.866097: step 9970, loss = 1.01 (336.4 examples/sec; 0.381 sec/batch)
2017-04-02 22:38:49.698731: step 9980, loss = 1.04 (334.0 examples/sec; 0.383 sec/batch)
2017-04-02 22:38:53.529998: step 9990, loss = 0.76 (334.1 examples/sec; 0.383 sec/batch)
2017-04-02 22:38:57.423417: step 10000, loss = 1.08 (328.8 examples/sec; 0.389 sec/batch)
2017-04-02 22:39:01.254759: step 10010, loss = 0.78 (334.1 examples/sec; 0.383 sec/batch)
2017-04-02 22:39:05.072495: step 10020, loss = 0.84 (335.3 examples/sec; 0.382 sec/batch)
2017-04-02 22:39:08.883251: step 10030, loss = 0.84 (335.9 examples/sec; 0.381 sec/batch)
2017-04-02 22:39:12.679571: step 10040, loss = 0.79 (337.2 examples/sec; 0.380 sec/batch)
2017-04-02 22:39:16.526400: step 10050, loss = 1.02 (332.7 examples/sec; 0.385 sec/batch)
2017-04-02 22:39:20.344614: step 10060, loss = 1.03 (335.2 examples/sec; 0.382 sec/batch)
2017-04-02 22:39:24.200673: step 10070, loss = 0.91 (331.9 examples/sec; 0.386 sec/batch)
2017-04-02 22:39:28.034414: step 10080, loss = 0.84 (333.9 examples/sec; 0.383 sec/batch)
2017-04-02 22:39:31.859992: step 10090, loss = 0.88 (334.6 examples/sec; 0.383 sec/batch)
2017-04-02 22:39:35.727797: step 10100, loss = 0.88 (330.9 examples/sec; 0.387 sec/batch)
2017-04-02 22:39:39.577633: step 10110, loss = 0.93 (332.5 examples/sec; 0.385 sec/batch)
2017-04-02 22:39:43.403034: step 10120, loss = 0.77 (334.6 examples/sec; 0.383 sec/batch)
2017-04-02 22:39:47.230727: step 10130, loss = 0.83 (334.4 examples/sec; 0.383 sec/batch)
2017-04-02 22:39:51.067984: step 10140, loss = 0.89 (333.6 examples/sec; 0.384 sec/batch)
2017-04-02 22:39:54.841601: step 10150, loss = 0.84 (339.2 examples/sec; 0.377 sec/batch)
2017-04-02 22:39:58.679842: step 10160, loss = 0.71 (333.5 examples/sec; 0.384 sec/batch)
2017-04-02 22:40:02.481396: step 10170, loss = 0.95 (336.7 examples/sec; 0.380 sec/batch)
2017-04-02 22:40:06.348146: step 10180, loss = 0.87 (331.0 examples/sec; 0.387 sec/batch)
2017-04-02 22:41:45.151011: precision @ 1 = 0.814

2017-04-02 22:43:10.617403: step 10190, loss = 0.89 (6.9 examples/sec; 18.427 sec/batch)
2017-04-02 22:43:14.519712: step 10200, loss = 0.82 (328.0 examples/sec; 0.390 sec/batch)
2017-04-02 22:43:18.336066: step 10210, loss = 0.94 (335.4 examples/sec; 0.382 sec/batch)
2017-04-02 22:43:22.154597: step 10220, loss = 0.94 (335.2 examples/sec; 0.382 sec/batch)
2017-04-02 22:43:26.007526: step 10230, loss = 0.80 (332.2 examples/sec; 0.385 sec/batch)
2017-04-02 22:43:29.817659: step 10240, loss = 0.84 (335.9 examples/sec; 0.381 sec/batch)
2017-04-02 22:43:33.698691: step 10250, loss = 0.88 (329.8 examples/sec; 0.388 sec/batch)
2017-04-02 22:43:37.533813: step 10260, loss = 1.01 (333.8 examples/sec; 0.384 sec/batch)
2017-04-02 22:43:41.325712: step 10270, loss = 0.82 (337.6 examples/sec; 0.379 sec/batch)
2017-04-02 22:43:45.156078: step 10280, loss = 0.91 (334.2 examples/sec; 0.383 sec/batch)
2017-04-02 22:43:48.995689: step 10290, loss = 0.96 (333.4 examples/sec; 0.384 sec/batch)
2017-04-02 22:43:52.877675: step 10300, loss = 0.83 (329.7 examples/sec; 0.388 sec/batch)
2017-04-02 22:43:56.677835: step 10310, loss = 0.92 (336.8 examples/sec; 0.380 sec/batch)
2017-04-02 22:44:00.518121: step 10320, loss = 0.97 (333.3 examples/sec; 0.384 sec/batch)
2017-04-02 22:44:04.352857: step 10330, loss = 1.13 (333.8 examples/sec; 0.383 sec/batch)
2017-04-02 22:44:08.156450: step 10340, loss = 0.86 (336.5 examples/sec; 0.380 sec/batch)
2017-04-02 22:44:11.997160: step 10350, loss = 0.85 (333.3 examples/sec; 0.384 sec/batch)
2017-04-02 22:44:15.826634: step 10360, loss = 0.81 (334.2 examples/sec; 0.383 sec/batch)
2017-04-02 22:44:19.653560: step 10370, loss = 0.77 (334.5 examples/sec; 0.383 sec/batch)
2017-04-02 22:44:23.517844: step 10380, loss = 0.85 (331.2 examples/sec; 0.386 sec/batch)
2017-04-02 22:44:27.349919: step 10390, loss = 0.92 (334.0 examples/sec; 0.383 sec/batch)
2017-04-02 22:44:31.177197: step 10400, loss = 0.91 (334.4 examples/sec; 0.383 sec/batch)
2017-04-02 22:44:34.979090: step 10410, loss = 0.69 (336.7 examples/sec; 0.380 sec/batch)
2017-04-02 22:44:38.799257: step 10420, loss = 0.92 (335.1 examples/sec; 0.382 sec/batch)
2017-04-02 22:44:42.627424: step 10430, loss = 0.84 (334.4 examples/sec; 0.383 sec/batch)
2017-04-02 22:44:46.457162: step 10440, loss = 0.84 (334.2 examples/sec; 0.383 sec/batch)
2017-04-02 22:44:50.302934: step 10450, loss = 0.89 (332.8 examples/sec; 0.385 sec/batch)
2017-04-02 22:44:54.121342: step 10460, loss = 0.92 (335.2 examples/sec; 0.382 sec/batch)
2017-04-02 22:44:57.938076: step 10470, loss = 0.81 (335.4 examples/sec; 0.382 sec/batch)
2017-04-02 22:45:01.755704: step 10480, loss = 0.84 (335.3 examples/sec; 0.382 sec/batch)
2017-04-02 22:45:05.567594: step 10490, loss = 0.78 (335.8 examples/sec; 0.381 sec/batch)
2017-04-02 22:45:09.449840: step 10500, loss = 0.87 (329.7 examples/sec; 0.388 sec/batch)
2017-04-02 22:45:13.274110: step 10510, loss = 0.89 (334.7 examples/sec; 0.382 sec/batch)
2017-04-02 22:45:17.085474: step 10520, loss = 0.90 (335.8 examples/sec; 0.381 sec/batch)
2017-04-02 22:45:20.893533: step 10530, loss = 0.78 (336.1 examples/sec; 0.381 sec/batch)
2017-04-02 22:45:24.721775: step 10540, loss = 0.84 (334.4 examples/sec; 0.383 sec/batch)
2017-04-02 22:45:28.534679: step 10550, loss = 0.79 (335.7 examples/sec; 0.381 sec/batch)
2017-04-02 22:45:32.345147: step 10560, loss = 0.90 (335.9 examples/sec; 0.381 sec/batch)
2017-04-02 22:45:36.157692: step 10570, loss = 0.84 (335.7 examples/sec; 0.381 sec/batch)
2017-04-02 22:45:39.985573: step 10580, loss = 0.92 (334.4 examples/sec; 0.383 sec/batch)
2017-04-02 22:45:43.785979: step 10590, loss = 0.91 (336.8 examples/sec; 0.380 sec/batch)
2017-04-02 22:45:47.671112: step 10600, loss = 0.90 (329.5 examples/sec; 0.389 sec/batch)
2017-04-02 22:45:51.487692: step 10610, loss = 0.94 (335.4 examples/sec; 0.382 sec/batch)
2017-04-02 22:45:55.290325: step 10620, loss = 0.88 (336.6 examples/sec; 0.380 sec/batch)
2017-04-02 22:45:59.098624: step 10630, loss = 0.82 (336.1 examples/sec; 0.381 sec/batch)
2017-04-02 22:46:02.921559: step 10640, loss = 0.90 (334.8 examples/sec; 0.382 sec/batch)
2017-04-02 22:46:06.737449: step 10650, loss = 0.83 (335.4 examples/sec; 0.382 sec/batch)
2017-04-02 22:46:11.272507: step 10660, loss = 0.90 (282.2 examples/sec; 0.454 sec/batch)
2017-04-02 22:46:15.079752: step 10670, loss = 0.88 (336.2 examples/sec; 0.381 sec/batch)
2017-04-02 22:46:18.910320: step 10680, loss = 0.84 (334.2 examples/sec; 0.383 sec/batch)
2017-04-02 22:46:22.703688: step 10690, loss = 0.93 (337.4 examples/sec; 0.379 sec/batch)
2017-04-02 22:46:26.597829: step 10700, loss = 1.00 (328.7 examples/sec; 0.389 sec/batch)
2017-04-02 22:46:30.420160: step 10710, loss = 0.95 (334.9 examples/sec; 0.382 sec/batch)
2017-04-02 22:46:34.252613: step 10720, loss = 0.88 (334.0 examples/sec; 0.383 sec/batch)
2017-04-02 22:46:38.071790: step 10730, loss = 1.01 (335.2 examples/sec; 0.382 sec/batch)
2017-04-02 22:46:41.898292: step 10740, loss = 0.94 (334.5 examples/sec; 0.383 sec/batch)
2017-04-02 22:46:46.122217: step 10750, loss = 0.97 (303.0 examples/sec; 0.422 sec/batch)
2017-04-02 22:46:49.980238: step 10760, loss = 0.78 (331.8 examples/sec; 0.386 sec/batch)
2017-04-02 22:46:53.813611: step 10770, loss = 1.05 (333.9 examples/sec; 0.383 sec/batch)
2017-04-02 22:46:57.637490: step 10780, loss = 0.88 (334.7 examples/sec; 0.382 sec/batch)
2017-04-02 22:47:01.466837: step 10790, loss = 0.92 (334.3 examples/sec; 0.383 sec/batch)
2017-04-02 22:47:05.394236: step 10800, loss = 0.83 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 22:47:09.198890: step 10810, loss = 0.76 (336.4 examples/sec; 0.380 sec/batch)
2017-04-02 22:47:13.036377: step 10820, loss = 0.78 (333.6 examples/sec; 0.384 sec/batch)
2017-04-02 22:47:16.873428: step 10830, loss = 0.78 (333.6 examples/sec; 0.384 sec/batch)
2017-04-02 22:47:20.698520: step 10840, loss = 0.82 (334.6 examples/sec; 0.383 sec/batch)
2017-04-02 22:47:24.529922: step 10850, loss = 0.72 (334.1 examples/sec; 0.383 sec/batch)
2017-04-02 22:47:28.355390: step 10860, loss = 0.85 (334.6 examples/sec; 0.383 sec/batch)
2017-04-02 22:47:32.209773: step 10870, loss = 0.78 (332.1 examples/sec; 0.385 sec/batch)
2017-04-02 22:47:36.056188: step 10880, loss = 0.88 (332.8 examples/sec; 0.385 sec/batch)
2017-04-02 22:47:39.876184: step 10890, loss = 0.84 (335.1 examples/sec; 0.382 sec/batch)
2017-04-02 22:47:43.763794: step 10900, loss = 0.88 (329.3 examples/sec; 0.389 sec/batch)
2017-04-02 22:47:47.572012: step 10910, loss = 0.76 (336.1 examples/sec; 0.381 sec/batch)
2017-04-02 22:47:51.354876: step 10920, loss = 1.05 (338.4 examples/sec; 0.378 sec/batch)
2017-04-02 22:47:55.211593: step 10930, loss = 0.80 (331.9 examples/sec; 0.386 sec/batch)
2017-04-02 22:47:59.037401: step 10940, loss = 0.94 (334.6 examples/sec; 0.383 sec/batch)
2017-04-02 22:48:02.880881: step 10950, loss = 0.92 (333.0 examples/sec; 0.384 sec/batch)
2017-04-02 22:48:06.689140: step 10960, loss = 0.87 (336.1 examples/sec; 0.381 sec/batch)
2017-04-02 22:48:10.525022: step 10970, loss = 0.93 (333.7 examples/sec; 0.384 sec/batch)
2017-04-02 22:48:14.387317: step 10980, loss = 0.89 (331.4 examples/sec; 0.386 sec/batch)
2017-04-02 22:48:18.222039: step 10990, loss = 0.98 (333.8 examples/sec; 0.383 sec/batch)
2017-04-02 22:48:22.097587: step 11000, loss = 0.68 (330.3 examples/sec; 0.388 sec/batch)
2017-04-02 22:48:25.952639: step 11010, loss = 0.73 (332.0 examples/sec; 0.386 sec/batch)
2017-04-02 22:48:29.794711: step 11020, loss = 0.93 (333.2 examples/sec; 0.384 sec/batch)
2017-04-02 22:48:33.648429: step 11030, loss = 0.80 (332.1 examples/sec; 0.385 sec/batch)
2017-04-02 22:48:37.471581: step 11040, loss = 0.83 (334.8 examples/sec; 0.382 sec/batch)
2017-04-02 22:48:41.294081: step 11050, loss = 0.95 (334.9 examples/sec; 0.382 sec/batch)
2017-04-02 22:48:45.227377: step 11060, loss = 0.96 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 22:48:49.040747: step 11070, loss = 0.87 (335.7 examples/sec; 0.381 sec/batch)
2017-04-02 22:48:52.853469: step 11080, loss = 0.89 (335.7 examples/sec; 0.381 sec/batch)
2017-04-02 22:48:56.720192: step 11090, loss = 0.85 (331.0 examples/sec; 0.387 sec/batch)
2017-04-02 22:49:00.606284: step 11100, loss = 0.81 (329.4 examples/sec; 0.389 sec/batch)
2017-04-02 22:49:04.453051: step 11110, loss = 1.01 (332.7 examples/sec; 0.385 sec/batch)
2017-04-02 22:49:08.269988: step 11120, loss = 0.87 (335.3 examples/sec; 0.382 sec/batch)
2017-04-02 22:49:12.117271: step 11130, loss = 0.89 (332.7 examples/sec; 0.385 sec/batch)
2017-04-02 22:49:15.942785: step 11140, loss = 0.77 (334.6 examples/sec; 0.383 sec/batch)
2017-04-02 22:49:19.817681: step 11150, loss = 0.84 (330.3 examples/sec; 0.387 sec/batch)
2017-04-02 22:49:23.650762: step 11160, loss = 0.85 (333.9 examples/sec; 0.383 sec/batch)
2017-04-02 22:49:27.481509: step 11170, loss = 0.89 (334.1 examples/sec; 0.383 sec/batch)
2017-04-02 22:49:31.333411: step 11180, loss = 0.86 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 22:49:35.148595: step 11190, loss = 0.87 (335.5 examples/sec; 0.382 sec/batch)
2017-04-02 22:49:39.039838: step 11200, loss = 0.87 (328.9 examples/sec; 0.389 sec/batch)
2017-04-02 22:49:42.849785: step 11210, loss = 0.85 (336.0 examples/sec; 0.381 sec/batch)
2017-04-02 22:49:46.690377: step 11220, loss = 0.82 (333.3 examples/sec; 0.384 sec/batch)
2017-04-02 22:49:50.515560: step 11230, loss = 0.89 (334.6 examples/sec; 0.383 sec/batch)
2017-04-02 22:49:54.323810: step 11240, loss = 1.13 (336.1 examples/sec; 0.381 sec/batch)
2017-04-02 22:49:58.138402: step 11250, loss = 0.87 (335.6 examples/sec; 0.381 sec/batch)
2017-04-02 22:50:01.953202: step 11260, loss = 0.86 (335.5 examples/sec; 0.381 sec/batch)
2017-04-02 22:50:05.801349: step 11270, loss = 0.92 (332.6 examples/sec; 0.385 sec/batch)
2017-04-02 22:50:09.622595: step 11280, loss = 0.72 (335.0 examples/sec; 0.382 sec/batch)
2017-04-02 22:50:13.425628: step 11290, loss = 0.87 (336.6 examples/sec; 0.380 sec/batch)
2017-04-02 22:50:17.279852: step 11300, loss = 0.69 (332.1 examples/sec; 0.385 sec/batch)
2017-04-02 22:50:21.110654: step 11310, loss = 0.82 (334.1 examples/sec; 0.383 sec/batch)
2017-04-02 22:50:24.923772: step 11320, loss = 1.00 (335.7 examples/sec; 0.381 sec/batch)
2017-04-02 22:50:28.754375: step 11330, loss = 1.03 (334.2 examples/sec; 0.383 sec/batch)
2017-04-02 22:50:32.586901: step 11340, loss = 0.83 (334.0 examples/sec; 0.383 sec/batch)
2017-04-02 22:50:36.425296: step 11350, loss = 0.82 (333.5 examples/sec; 0.384 sec/batch)
2017-04-02 22:50:40.210354: step 11360, loss = 0.84 (338.2 examples/sec; 0.379 sec/batch)
2017-04-02 22:50:44.045861: step 11370, loss = 0.81 (333.7 examples/sec; 0.384 sec/batch)
2017-04-02 22:50:47.867918: step 11380, loss = 0.89 (334.9 examples/sec; 0.382 sec/batch)
2017-04-02 22:50:51.704997: step 11390, loss = 0.94 (333.6 examples/sec; 0.384 sec/batch)
2017-04-02 22:50:55.589968: step 11400, loss = 0.95 (329.5 examples/sec; 0.388 sec/batch)
2017-04-02 22:50:59.441512: step 11410, loss = 0.95 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 22:51:03.276322: step 11420, loss = 0.90 (333.8 examples/sec; 0.383 sec/batch)
2017-04-02 22:51:07.088646: step 11430, loss = 0.71 (335.8 examples/sec; 0.381 sec/batch)
2017-04-02 22:51:10.924543: step 11440, loss = 1.06 (333.7 examples/sec; 0.384 sec/batch)
2017-04-02 22:51:14.710366: step 11450, loss = 0.82 (338.1 examples/sec; 0.379 sec/batch)
2017-04-02 22:51:18.512131: step 11460, loss = 0.83 (336.7 examples/sec; 0.380 sec/batch)
2017-04-02 22:51:22.345066: step 11470, loss = 0.92 (333.9 examples/sec; 0.383 sec/batch)
2017-04-02 22:51:26.133693: step 11480, loss = 1.05 (337.9 examples/sec; 0.379 sec/batch)
2017-04-02 22:51:29.933782: step 11490, loss = 1.14 (336.8 examples/sec; 0.380 sec/batch)
2017-04-02 22:51:33.787393: step 11500, loss = 0.95 (332.2 examples/sec; 0.385 sec/batch)
2017-04-02 22:51:37.608635: step 11510, loss = 0.76 (335.0 examples/sec; 0.382 sec/batch)
2017-04-02 22:51:41.415915: step 11520, loss = 0.96 (336.2 examples/sec; 0.381 sec/batch)
2017-04-02 22:51:45.264556: step 11530, loss = 1.07 (332.6 examples/sec; 0.385 sec/batch)
2017-04-02 22:51:49.096030: step 11540, loss = 1.03 (334.1 examples/sec; 0.383 sec/batch)
2017-04-02 22:51:52.908361: step 11550, loss = 0.98 (335.8 examples/sec; 0.381 sec/batch)
2017-04-02 22:51:56.744714: step 11560, loss = 0.89 (333.7 examples/sec; 0.384 sec/batch)
2017-04-02 22:52:00.548876: step 11570, loss = 0.78 (336.5 examples/sec; 0.380 sec/batch)
2017-04-02 22:52:04.380646: step 11580, loss = 0.92 (334.0 examples/sec; 0.383 sec/batch)
2017-04-02 22:52:08.186344: step 11590, loss = 0.82 (336.3 examples/sec; 0.381 sec/batch)
2017-04-02 22:52:12.077000: step 11600, loss = 0.86 (329.0 examples/sec; 0.389 sec/batch)
2017-04-02 22:52:15.910673: step 11610, loss = 0.81 (333.9 examples/sec; 0.383 sec/batch)
2017-04-02 22:52:19.762545: step 11620, loss = 0.85 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 22:52:23.574905: step 11630, loss = 0.99 (335.8 examples/sec; 0.381 sec/batch)
2017-04-02 22:52:27.406676: step 11640, loss = 0.82 (334.0 examples/sec; 0.383 sec/batch)
2017-04-02 22:52:31.223832: step 11650, loss = 0.86 (335.3 examples/sec; 0.382 sec/batch)
2017-04-02 22:52:35.035232: step 11660, loss = 0.88 (335.8 examples/sec; 0.381 sec/batch)
2017-04-02 22:52:38.870671: step 11670, loss = 0.81 (333.7 examples/sec; 0.384 sec/batch)
2017-04-02 22:52:42.697321: step 11680, loss = 0.81 (334.5 examples/sec; 0.383 sec/batch)
2017-04-02 22:52:46.558185: step 11690, loss = 0.77 (331.5 examples/sec; 0.386 sec/batch)
2017-04-02 22:52:50.438266: step 11700, loss = 0.94 (329.9 examples/sec; 0.388 sec/batch)
2017-04-02 22:52:54.236811: step 11710, loss = 1.09 (337.0 examples/sec; 0.380 sec/batch)
2017-04-02 22:52:58.038842: step 11720, loss = 0.84 (336.7 examples/sec; 0.380 sec/batch)
2017-04-02 22:53:02.001640: step 11730, loss = 0.87 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 22:53:06.124981: step 11740, loss = 0.88 (310.4 examples/sec; 0.412 sec/batch)
2017-04-02 22:53:09.943888: step 11750, loss = 0.76 (335.2 examples/sec; 0.382 sec/batch)
2017-04-02 22:53:13.792965: step 11760, loss = 0.77 (332.5 examples/sec; 0.385 sec/batch)
2017-04-02 22:53:17.638931: step 11770, loss = 0.92 (332.8 examples/sec; 0.385 sec/batch)
2017-04-02 22:53:21.477470: step 11780, loss = 0.99 (333.5 examples/sec; 0.384 sec/batch)
2017-04-02 22:53:25.274678: step 11790, loss = 0.78 (337.1 examples/sec; 0.380 sec/batch)
2017-04-02 22:53:29.157152: step 11800, loss = 1.00 (329.7 examples/sec; 0.388 sec/batch)
2017-04-02 22:53:33.008315: step 11810, loss = 0.88 (332.4 examples/sec; 0.385 sec/batch)
2017-04-02 22:53:36.868800: step 11820, loss = 0.86 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 22:53:40.718465: step 11830, loss = 0.84 (332.5 examples/sec; 0.385 sec/batch)
2017-04-02 22:53:44.558378: step 11840, loss = 0.90 (333.3 examples/sec; 0.384 sec/batch)
2017-04-02 22:53:48.404984: step 11850, loss = 0.72 (332.8 examples/sec; 0.385 sec/batch)
2017-04-02 22:53:52.239634: step 11860, loss = 0.80 (333.8 examples/sec; 0.383 sec/batch)
2017-04-02 22:53:56.036291: step 11870, loss = 0.92 (337.1 examples/sec; 0.380 sec/batch)
2017-04-02 22:53:59.829070: step 11880, loss = 1.00 (337.5 examples/sec; 0.379 sec/batch)
2017-04-02 22:54:03.657228: step 11890, loss = 0.86 (334.4 examples/sec; 0.383 sec/batch)
2017-04-02 22:54:07.529954: step 11900, loss = 0.77 (330.5 examples/sec; 0.387 sec/batch)
2017-04-02 22:54:11.364458: step 11910, loss = 1.10 (333.8 examples/sec; 0.383 sec/batch)
2017-04-02 22:54:15.179651: step 11920, loss = 0.89 (335.5 examples/sec; 0.382 sec/batch)
2017-04-02 22:54:19.004908: step 11930, loss = 0.70 (334.6 examples/sec; 0.383 sec/batch)
2017-04-02 22:54:22.833750: step 11940, loss = 0.85 (334.3 examples/sec; 0.383 sec/batch)
2017-04-02 22:54:26.690404: step 11950, loss = 0.98 (331.9 examples/sec; 0.386 sec/batch)
2017-04-02 22:54:30.506074: step 11960, loss = 0.84 (335.5 examples/sec; 0.382 sec/batch)
2017-04-02 22:54:34.347282: step 11970, loss = 0.84 (333.2 examples/sec; 0.384 sec/batch)
2017-04-02 22:54:38.160150: step 11980, loss = 0.81 (335.7 examples/sec; 0.381 sec/batch)
2017-04-02 22:54:41.980615: step 11990, loss = 0.84 (335.0 examples/sec; 0.382 sec/batch)
2017-04-02 22:54:45.879300: step 12000, loss = 0.92 (328.3 examples/sec; 0.390 sec/batch)
2017-04-02 22:54:49.712569: step 12010, loss = 0.77 (333.9 examples/sec; 0.383 sec/batch)
2017-04-02 22:54:53.531780: step 12020, loss = 0.77 (335.1 examples/sec; 0.382 sec/batch)
2017-04-02 22:54:57.377776: step 12030, loss = 0.92 (332.8 examples/sec; 0.385 sec/batch)
2017-04-02 22:55:01.192371: step 12040, loss = 0.87 (335.6 examples/sec; 0.381 sec/batch)
2017-04-02 22:55:05.013530: step 12050, loss = 0.82 (335.0 examples/sec; 0.382 sec/batch)
2017-04-02 22:55:45.913789: precision @ 1 = 0.820

2017-04-02 22:56:15.116362: step 12060, loss = 0.92 (18.3 examples/sec; 7.010 sec/batch)
2017-04-02 22:56:18.961666: step 12070, loss = 0.98 (332.9 examples/sec; 0.385 sec/batch)
2017-04-02 22:56:22.800181: step 12080, loss = 0.80 (333.5 examples/sec; 0.384 sec/batch)
2017-04-02 22:56:26.643363: step 12090, loss = 0.78 (333.1 examples/sec; 0.384 sec/batch)
2017-04-02 22:56:30.570479: step 12100, loss = 0.96 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 22:56:34.424339: step 12110, loss = 0.63 (332.1 examples/sec; 0.385 sec/batch)
2017-04-02 22:56:38.266037: step 12120, loss = 0.72 (333.2 examples/sec; 0.384 sec/batch)
2017-04-02 22:56:42.110072: step 12130, loss = 0.73 (333.0 examples/sec; 0.384 sec/batch)
2017-04-02 22:56:45.925856: step 12140, loss = 0.86 (335.4 examples/sec; 0.382 sec/batch)
2017-04-02 22:56:49.760665: step 12150, loss = 0.90 (333.8 examples/sec; 0.383 sec/batch)
2017-04-02 22:56:53.594116: step 12160, loss = 0.78 (333.9 examples/sec; 0.383 sec/batch)
2017-04-02 22:56:57.444944: step 12170, loss = 1.00 (332.4 examples/sec; 0.385 sec/batch)
2017-04-02 22:57:01.304168: step 12180, loss = 0.81 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 22:57:05.160918: step 12190, loss = 0.90 (331.9 examples/sec; 0.386 sec/batch)
2017-04-02 22:57:09.080438: step 12200, loss = 0.65 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 22:57:12.921882: step 12210, loss = 0.83 (333.2 examples/sec; 0.384 sec/batch)
2017-04-02 22:57:16.764522: step 12220, loss = 0.94 (333.1 examples/sec; 0.384 sec/batch)
2017-04-02 22:57:20.632885: step 12230, loss = 0.84 (330.9 examples/sec; 0.387 sec/batch)
2017-04-02 22:57:24.497508: step 12240, loss = 0.83 (331.2 examples/sec; 0.386 sec/batch)
2017-04-02 22:57:28.359379: step 12250, loss = 0.97 (331.4 examples/sec; 0.386 sec/batch)
2017-04-02 22:57:32.196532: step 12260, loss = 0.94 (333.6 examples/sec; 0.384 sec/batch)
2017-04-02 22:57:36.038106: step 12270, loss = 0.82 (333.2 examples/sec; 0.384 sec/batch)
2017-04-02 22:57:39.890361: step 12280, loss = 0.81 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 22:57:43.713089: step 12290, loss = 0.87 (334.8 examples/sec; 0.382 sec/batch)
2017-04-02 22:57:47.637715: step 12300, loss = 0.92 (326.1 examples/sec; 0.392 sec/batch)
2017-04-02 22:57:51.489196: step 12310, loss = 0.89 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 22:57:55.406414: step 12320, loss = 0.87 (326.8 examples/sec; 0.392 sec/batch)
2017-04-02 22:57:59.628570: step 12330, loss = 0.99 (303.2 examples/sec; 0.422 sec/batch)
2017-04-02 22:58:03.445669: step 12340, loss = 0.92 (335.3 examples/sec; 0.382 sec/batch)
2017-04-02 22:58:07.272655: step 12350, loss = 0.98 (334.5 examples/sec; 0.383 sec/batch)
2017-04-02 22:58:11.064966: step 12360, loss = 0.72 (337.5 examples/sec; 0.379 sec/batch)
2017-04-02 22:58:14.894400: step 12370, loss = 0.90 (334.3 examples/sec; 0.383 sec/batch)
2017-04-02 22:58:18.711509: step 12380, loss = 0.83 (335.3 examples/sec; 0.382 sec/batch)
2017-04-02 22:58:22.525056: step 12390, loss = 0.95 (335.6 examples/sec; 0.381 sec/batch)
2017-04-02 22:58:26.443753: step 12400, loss = 0.96 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 22:58:30.321893: step 12410, loss = 0.80 (330.1 examples/sec; 0.388 sec/batch)
2017-04-02 22:58:34.157298: step 12420, loss = 0.78 (333.7 examples/sec; 0.384 sec/batch)
2017-04-02 22:58:37.987045: step 12430, loss = 0.90 (334.2 examples/sec; 0.383 sec/batch)
2017-04-02 22:58:41.807456: step 12440, loss = 0.82 (335.0 examples/sec; 0.382 sec/batch)
2017-04-02 22:58:45.633189: step 12450, loss = 0.84 (334.6 examples/sec; 0.383 sec/batch)
2017-04-02 22:58:49.448594: step 12460, loss = 0.81 (335.5 examples/sec; 0.382 sec/batch)
2017-04-02 22:58:53.292422: step 12470, loss = 0.91 (333.0 examples/sec; 0.384 sec/batch)
2017-04-02 22:58:57.119621: step 12480, loss = 0.88 (334.4 examples/sec; 0.383 sec/batch)
2017-04-02 22:59:00.938051: step 12490, loss = 0.77 (335.2 examples/sec; 0.382 sec/batch)
2017-04-02 22:59:04.857467: step 12500, loss = 0.77 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 22:59:08.698757: step 12510, loss = 0.83 (333.2 examples/sec; 0.384 sec/batch)
2017-04-02 22:59:12.544605: step 12520, loss = 0.90 (332.8 examples/sec; 0.385 sec/batch)
2017-04-02 22:59:16.351920: step 12530, loss = 0.91 (336.2 examples/sec; 0.381 sec/batch)
2017-04-02 22:59:20.210548: step 12540, loss = 0.78 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 22:59:24.046100: step 12550, loss = 0.85 (333.7 examples/sec; 0.384 sec/batch)
2017-04-02 22:59:27.892857: step 12560, loss = 1.02 (332.7 examples/sec; 0.385 sec/batch)
2017-04-02 22:59:31.740331: step 12570, loss = 0.77 (332.7 examples/sec; 0.385 sec/batch)
2017-04-02 22:59:35.580945: step 12580, loss = 0.66 (333.3 examples/sec; 0.384 sec/batch)
2017-04-02 22:59:39.391542: step 12590, loss = 0.72 (335.9 examples/sec; 0.381 sec/batch)
2017-04-02 22:59:43.312429: step 12600, loss = 0.94 (326.5 examples/sec; 0.392 sec/batch)
2017-04-02 22:59:47.152336: step 12610, loss = 0.76 (333.3 examples/sec; 0.384 sec/batch)
2017-04-02 22:59:50.937386: step 12620, loss = 0.81 (338.2 examples/sec; 0.379 sec/batch)
2017-04-02 22:59:54.769628: step 12630, loss = 0.82 (334.0 examples/sec; 0.383 sec/batch)
2017-04-02 22:59:58.587950: step 12640, loss = 0.79 (335.2 examples/sec; 0.382 sec/batch)
2017-04-02 23:00:02.421340: step 12650, loss = 1.11 (333.9 examples/sec; 0.383 sec/batch)
2017-04-02 23:00:06.236987: step 12660, loss = 0.88 (335.5 examples/sec; 0.382 sec/batch)
2017-04-02 23:00:10.072673: step 12670, loss = 0.79 (333.7 examples/sec; 0.384 sec/batch)
2017-04-02 23:00:13.907012: step 12680, loss = 0.87 (333.8 examples/sec; 0.383 sec/batch)
2017-04-02 23:00:17.730504: step 12690, loss = 0.84 (334.8 examples/sec; 0.382 sec/batch)
2017-04-02 23:00:21.622665: step 12700, loss = 0.84 (328.9 examples/sec; 0.389 sec/batch)
2017-04-02 23:00:25.459144: step 12710, loss = 0.91 (333.6 examples/sec; 0.384 sec/batch)
2017-04-02 23:00:29.279354: step 12720, loss = 0.74 (335.1 examples/sec; 0.382 sec/batch)
2017-04-02 23:00:33.073431: step 12730, loss = 0.70 (337.4 examples/sec; 0.379 sec/batch)
2017-04-02 23:00:36.900798: step 12740, loss = 0.91 (334.4 examples/sec; 0.383 sec/batch)
2017-04-02 23:00:40.708419: step 12750, loss = 0.79 (336.2 examples/sec; 0.381 sec/batch)
2017-04-02 23:00:44.537512: step 12760, loss = 0.76 (334.3 examples/sec; 0.383 sec/batch)
2017-04-02 23:00:48.352937: step 12770, loss = 0.95 (335.5 examples/sec; 0.382 sec/batch)
2017-04-02 23:00:52.189918: step 12780, loss = 0.79 (333.6 examples/sec; 0.384 sec/batch)
2017-04-02 23:00:56.018726: step 12790, loss = 0.75 (334.3 examples/sec; 0.383 sec/batch)
2017-04-02 23:00:59.918116: step 12800, loss = 0.93 (328.3 examples/sec; 0.390 sec/batch)
2017-04-02 23:01:03.745873: step 12810, loss = 0.80 (334.4 examples/sec; 0.383 sec/batch)
2017-04-02 23:01:07.577635: step 12820, loss = 0.94 (334.0 examples/sec; 0.383 sec/batch)
2017-04-02 23:01:11.398594: step 12830, loss = 0.87 (335.0 examples/sec; 0.382 sec/batch)
2017-04-02 23:01:15.202399: step 12840, loss = 0.96 (336.5 examples/sec; 0.380 sec/batch)
2017-04-02 23:01:19.022254: step 12850, loss = 1.04 (335.1 examples/sec; 0.382 sec/batch)
2017-04-02 23:01:22.848435: step 12860, loss = 0.85 (334.5 examples/sec; 0.383 sec/batch)
2017-04-02 23:01:26.666037: step 12870, loss = 0.72 (335.3 examples/sec; 0.382 sec/batch)
2017-04-02 23:01:30.503678: step 12880, loss = 0.84 (333.5 examples/sec; 0.384 sec/batch)
2017-04-02 23:01:34.346994: step 12890, loss = 0.95 (333.0 examples/sec; 0.384 sec/batch)
2017-04-02 23:01:38.232602: step 12900, loss = 0.93 (329.4 examples/sec; 0.389 sec/batch)
2017-04-02 23:01:42.091723: step 12910, loss = 0.82 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 23:01:45.901139: step 12920, loss = 0.91 (336.0 examples/sec; 0.381 sec/batch)
2017-04-02 23:01:49.719998: step 12930, loss = 0.78 (335.2 examples/sec; 0.382 sec/batch)
2017-04-02 23:01:53.518758: step 12940, loss = 0.66 (337.0 examples/sec; 0.380 sec/batch)
2017-04-02 23:01:57.317839: step 12950, loss = 0.89 (336.9 examples/sec; 0.380 sec/batch)
2017-04-02 23:02:01.134436: step 12960, loss = 0.76 (335.4 examples/sec; 0.382 sec/batch)
2017-04-02 23:02:04.958452: step 12970, loss = 0.68 (334.7 examples/sec; 0.382 sec/batch)
2017-04-02 23:02:08.806464: step 12980, loss = 1.12 (332.6 examples/sec; 0.385 sec/batch)
2017-04-02 23:02:12.620984: step 12990, loss = 0.85 (335.6 examples/sec; 0.381 sec/batch)
2017-04-02 23:02:16.482966: step 13000, loss = 0.91 (331.4 examples/sec; 0.386 sec/batch)
2017-04-02 23:02:20.323368: step 13010, loss = 0.96 (333.3 examples/sec; 0.384 sec/batch)
2017-04-02 23:02:24.139753: step 13020, loss = 0.95 (335.4 examples/sec; 0.382 sec/batch)
2017-04-02 23:02:27.962568: step 13030, loss = 1.01 (334.8 examples/sec; 0.382 sec/batch)
2017-04-02 23:02:31.804035: step 13040, loss = 0.73 (333.2 examples/sec; 0.384 sec/batch)
2017-04-02 23:02:35.641749: step 13050, loss = 0.83 (333.5 examples/sec; 0.384 sec/batch)
2017-04-02 23:02:39.460622: step 13060, loss = 0.76 (335.2 examples/sec; 0.382 sec/batch)
2017-04-02 23:02:43.294728: step 13070, loss = 0.84 (333.8 examples/sec; 0.383 sec/batch)
2017-04-02 23:02:47.110856: step 13080, loss = 1.03 (335.4 examples/sec; 0.382 sec/batch)
2017-04-02 23:02:50.929436: step 13090, loss = 0.85 (335.2 examples/sec; 0.382 sec/batch)
2017-04-02 23:02:54.826152: step 13100, loss = 0.83 (328.5 examples/sec; 0.390 sec/batch)
2017-04-02 23:02:58.678678: step 13110, loss = 1.02 (332.2 examples/sec; 0.385 sec/batch)
2017-04-02 23:03:02.520252: step 13120, loss = 1.01 (333.2 examples/sec; 0.384 sec/batch)
2017-04-02 23:03:06.354285: step 13130, loss = 0.86 (333.9 examples/sec; 0.383 sec/batch)
2017-04-02 23:03:10.192505: step 13140, loss = 0.83 (333.5 examples/sec; 0.384 sec/batch)
2017-04-02 23:03:14.026402: step 13150, loss = 0.92 (333.9 examples/sec; 0.383 sec/batch)
2017-04-02 23:03:17.854887: step 13160, loss = 0.88 (334.3 examples/sec; 0.383 sec/batch)
2017-04-02 23:03:21.714425: step 13170, loss = 0.80 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 23:03:25.545155: step 13180, loss = 0.82 (334.1 examples/sec; 0.383 sec/batch)
2017-04-02 23:03:29.373183: step 13190, loss = 0.68 (334.4 examples/sec; 0.383 sec/batch)
2017-04-02 23:03:33.258866: step 13200, loss = 0.78 (329.4 examples/sec; 0.389 sec/batch)
2017-04-02 23:03:37.113314: step 13210, loss = 0.75 (332.1 examples/sec; 0.385 sec/batch)
2017-04-02 23:03:40.927541: step 13220, loss = 0.73 (335.6 examples/sec; 0.381 sec/batch)
2017-04-02 23:03:44.755537: step 13230, loss = 0.81 (334.4 examples/sec; 0.383 sec/batch)
2017-04-02 23:03:48.579890: step 13240, loss = 0.65 (334.7 examples/sec; 0.382 sec/batch)
2017-04-02 23:03:52.381126: step 13250, loss = 0.87 (336.7 examples/sec; 0.380 sec/batch)
2017-04-02 23:03:56.219573: step 13260, loss = 0.66 (333.5 examples/sec; 0.384 sec/batch)
2017-04-02 23:04:00.073761: step 13270, loss = 0.77 (332.1 examples/sec; 0.385 sec/batch)
2017-04-02 23:04:03.922591: step 13280, loss = 0.74 (332.6 examples/sec; 0.385 sec/batch)
2017-04-02 23:04:07.750019: step 13290, loss = 0.97 (334.4 examples/sec; 0.383 sec/batch)
2017-04-02 23:04:11.638482: step 13300, loss = 1.14 (329.2 examples/sec; 0.389 sec/batch)
2017-04-02 23:04:15.517326: step 13310, loss = 0.91 (330.0 examples/sec; 0.388 sec/batch)
2017-04-02 23:04:19.318764: step 13320, loss = 0.85 (336.7 examples/sec; 0.380 sec/batch)
2017-04-02 23:04:23.147157: step 13330, loss = 0.73 (334.3 examples/sec; 0.383 sec/batch)
2017-04-02 23:04:26.927004: step 13340, loss = 0.86 (338.6 examples/sec; 0.378 sec/batch)
2017-04-02 23:04:30.768918: step 13350, loss = 0.97 (333.2 examples/sec; 0.384 sec/batch)
2017-04-02 23:04:34.599964: step 13360, loss = 1.00 (334.1 examples/sec; 0.383 sec/batch)
2017-04-02 23:04:38.434147: step 13370, loss = 0.81 (333.8 examples/sec; 0.383 sec/batch)
2017-04-02 23:04:42.251245: step 13380, loss = 0.88 (335.3 examples/sec; 0.382 sec/batch)
2017-04-02 23:04:46.100886: step 13390, loss = 0.93 (332.5 examples/sec; 0.385 sec/batch)
2017-04-02 23:04:50.009873: step 13400, loss = 0.92 (327.5 examples/sec; 0.391 sec/batch)
2017-04-02 23:04:53.854363: step 13410, loss = 0.71 (332.9 examples/sec; 0.384 sec/batch)
2017-04-02 23:04:57.693647: step 13420, loss = 0.82 (333.4 examples/sec; 0.384 sec/batch)
2017-04-02 23:05:01.477814: step 13430, loss = 0.68 (338.3 examples/sec; 0.378 sec/batch)
2017-04-02 23:05:05.300416: step 13440, loss = 0.86 (334.9 examples/sec; 0.382 sec/batch)
2017-04-02 23:05:09.107420: step 13450, loss = 0.70 (336.2 examples/sec; 0.381 sec/batch)
2017-04-02 23:05:12.949487: step 13460, loss = 0.80 (333.2 examples/sec; 0.384 sec/batch)
2017-04-02 23:05:16.770812: step 13470, loss = 0.84 (335.0 examples/sec; 0.382 sec/batch)
2017-04-02 23:05:20.607046: step 13480, loss = 0.97 (333.7 examples/sec; 0.384 sec/batch)
2017-04-02 23:05:24.419904: step 13490, loss = 0.81 (335.7 examples/sec; 0.381 sec/batch)
2017-04-02 23:05:28.285728: step 13500, loss = 0.87 (331.1 examples/sec; 0.387 sec/batch)
2017-04-02 23:05:32.113621: step 13510, loss = 0.73 (334.4 examples/sec; 0.383 sec/batch)
2017-04-02 23:05:35.935404: step 13520, loss = 0.75 (334.9 examples/sec; 0.382 sec/batch)
2017-04-02 23:05:39.768644: step 13530, loss = 0.73 (333.9 examples/sec; 0.383 sec/batch)
2017-04-02 23:05:43.612726: step 13540, loss = 0.81 (333.0 examples/sec; 0.384 sec/batch)
2017-04-02 23:05:47.447237: step 13550, loss = 0.90 (333.8 examples/sec; 0.383 sec/batch)
2017-04-02 23:05:51.266031: step 13560, loss = 0.81 (335.2 examples/sec; 0.382 sec/batch)
2017-04-02 23:05:55.083471: step 13570, loss = 0.74 (335.3 examples/sec; 0.382 sec/batch)
2017-04-02 23:05:58.902841: step 13580, loss = 0.65 (335.1 examples/sec; 0.382 sec/batch)
2017-04-02 23:06:02.726563: step 13590, loss = 0.88 (334.8 examples/sec; 0.382 sec/batch)
2017-04-02 23:06:06.634748: step 13600, loss = 1.00 (327.5 examples/sec; 0.391 sec/batch)
2017-04-02 23:06:10.470143: step 13610, loss = 0.91 (333.7 examples/sec; 0.384 sec/batch)
2017-04-02 23:06:14.990622: step 13620, loss = 0.76 (283.2 examples/sec; 0.452 sec/batch)
2017-04-02 23:06:18.801924: step 13630, loss = 0.92 (335.8 examples/sec; 0.381 sec/batch)
2017-04-02 23:06:22.641443: step 13640, loss = 1.01 (333.4 examples/sec; 0.384 sec/batch)
2017-04-02 23:06:26.458130: step 13650, loss = 0.74 (335.4 examples/sec; 0.382 sec/batch)
2017-04-02 23:06:30.287145: step 13660, loss = 0.82 (334.3 examples/sec; 0.383 sec/batch)
2017-04-02 23:06:34.078887: step 13670, loss = 0.84 (337.6 examples/sec; 0.379 sec/batch)
2017-04-02 23:06:37.910706: step 13680, loss = 0.87 (334.0 examples/sec; 0.383 sec/batch)
2017-04-02 23:06:41.756799: step 13690, loss = 0.97 (332.8 examples/sec; 0.385 sec/batch)
2017-04-02 23:06:45.627078: step 13700, loss = 0.97 (330.7 examples/sec; 0.387 sec/batch)
2017-04-02 23:06:49.469752: step 13710, loss = 0.86 (333.1 examples/sec; 0.384 sec/batch)
2017-04-02 23:06:53.296654: step 13720, loss = 0.97 (334.5 examples/sec; 0.383 sec/batch)
2017-04-02 23:06:57.160227: step 13730, loss = 0.78 (331.3 examples/sec; 0.386 sec/batch)
2017-04-02 23:07:01.019830: step 13740, loss = 0.78 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 23:07:04.863248: step 13750, loss = 0.91 (333.0 examples/sec; 0.384 sec/batch)
2017-04-02 23:07:08.677558: step 13760, loss = 0.90 (335.6 examples/sec; 0.381 sec/batch)
2017-04-02 23:07:12.497432: step 13770, loss = 0.84 (335.1 examples/sec; 0.382 sec/batch)
2017-04-02 23:07:16.321144: step 13780, loss = 0.89 (334.8 examples/sec; 0.382 sec/batch)
2017-04-02 23:07:20.140500: step 13790, loss = 0.93 (335.1 examples/sec; 0.382 sec/batch)
2017-04-02 23:07:24.030806: step 13800, loss = 0.76 (329.0 examples/sec; 0.389 sec/batch)
2017-04-02 23:07:27.874793: step 13810, loss = 0.86 (333.0 examples/sec; 0.384 sec/batch)
2017-04-02 23:07:31.705943: step 13820, loss = 0.95 (334.1 examples/sec; 0.383 sec/batch)
2017-04-02 23:07:35.544513: step 13830, loss = 0.99 (333.5 examples/sec; 0.384 sec/batch)
2017-04-02 23:07:39.340866: step 13840, loss = 0.94 (337.2 examples/sec; 0.380 sec/batch)
2017-04-02 23:07:43.147019: step 13850, loss = 0.83 (336.3 examples/sec; 0.381 sec/batch)
2017-04-02 23:07:46.957298: step 13860, loss = 0.73 (335.9 examples/sec; 0.381 sec/batch)
2017-04-02 23:07:50.801281: step 13870, loss = 0.95 (333.0 examples/sec; 0.384 sec/batch)
2017-04-02 23:07:54.614933: step 13880, loss = 0.92 (335.6 examples/sec; 0.381 sec/batch)
2017-04-02 23:07:58.456539: step 13890, loss = 0.88 (333.2 examples/sec; 0.384 sec/batch)
2017-04-02 23:08:02.353453: step 13900, loss = 0.72 (328.5 examples/sec; 0.390 sec/batch)
2017-04-02 23:08:06.217559: step 13910, loss = 0.85 (331.3 examples/sec; 0.386 sec/batch)
2017-04-02 23:08:10.021123: step 13920, loss = 1.02 (336.5 examples/sec; 0.380 sec/batch)
2017-04-02 23:08:13.833792: step 13930, loss = 0.86 (335.7 examples/sec; 0.381 sec/batch)
2017-04-02 23:08:17.705993: step 13940, loss = 0.85 (330.6 examples/sec; 0.387 sec/batch)
2017-04-02 23:08:21.534083: step 13950, loss = 0.85 (334.4 examples/sec; 0.383 sec/batch)
2017-04-02 23:08:25.368028: step 13960, loss = 0.85 (333.9 examples/sec; 0.383 sec/batch)
2017-04-02 23:08:29.156961: step 13970, loss = 0.94 (337.8 examples/sec; 0.379 sec/batch)
2017-04-02 23:08:33.006448: step 13980, loss = 0.89 (332.5 examples/sec; 0.385 sec/batch)
2017-04-02 23:08:36.841990: step 13990, loss = 0.89 (333.7 examples/sec; 0.384 sec/batch)
2017-04-02 23:08:40.711982: step 14000, loss = 0.73 (330.7 examples/sec; 0.387 sec/batch)
2017-04-02 23:08:44.530925: step 14010, loss = 0.76 (335.2 examples/sec; 0.382 sec/batch)
2017-04-02 23:08:48.356302: step 14020, loss = 0.89 (334.6 examples/sec; 0.383 sec/batch)
2017-04-02 23:08:52.189620: step 14030, loss = 0.92 (333.9 examples/sec; 0.383 sec/batch)
2017-04-02 23:08:56.028709: step 14040, loss = 0.92 (333.4 examples/sec; 0.384 sec/batch)
2017-04-02 23:08:59.841668: step 14050, loss = 0.83 (335.7 examples/sec; 0.381 sec/batch)
2017-04-02 23:09:03.648065: step 14060, loss = 0.95 (336.3 examples/sec; 0.381 sec/batch)
2017-04-02 23:09:07.448834: step 14070, loss = 1.07 (336.8 examples/sec; 0.380 sec/batch)
2017-04-02 23:09:11.242674: step 14080, loss = 0.69 (337.4 examples/sec; 0.379 sec/batch)
2017-04-02 23:09:15.083377: step 14090, loss = 1.02 (333.3 examples/sec; 0.384 sec/batch)
2017-04-02 23:09:18.967222: step 14100, loss = 0.90 (329.6 examples/sec; 0.388 sec/batch)
2017-04-02 23:09:22.825719: step 14110, loss = 0.75 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 23:09:26.649756: step 14120, loss = 0.84 (334.7 examples/sec; 0.382 sec/batch)
2017-04-02 23:09:30.480784: step 14130, loss = 0.89 (334.1 examples/sec; 0.383 sec/batch)
2017-04-02 23:09:34.312915: step 14140, loss = 0.88 (334.0 examples/sec; 0.383 sec/batch)
2017-04-02 23:09:38.148052: step 14150, loss = 0.85 (333.8 examples/sec; 0.384 sec/batch)
2017-04-02 23:09:41.966899: step 14160, loss = 0.96 (335.2 examples/sec; 0.382 sec/batch)
2017-04-02 23:09:45.784824: step 14170, loss = 0.90 (335.3 examples/sec; 0.382 sec/batch)
2017-04-02 23:09:49.598452: step 14180, loss = 0.84 (335.6 examples/sec; 0.381 sec/batch)
2017-04-02 23:09:53.443148: step 14190, loss = 0.80 (332.9 examples/sec; 0.384 sec/batch)
2017-04-02 23:09:57.315617: step 14200, loss = 0.72 (330.5 examples/sec; 0.387 sec/batch)
2017-04-02 23:10:01.142680: step 14210, loss = 0.87 (334.5 examples/sec; 0.383 sec/batch)
2017-04-02 23:10:04.961912: step 14220, loss = 0.83 (335.1 examples/sec; 0.382 sec/batch)
2017-04-02 23:10:08.800219: step 14230, loss = 0.81 (333.5 examples/sec; 0.384 sec/batch)
2017-04-02 23:10:12.619578: step 14240, loss = 0.87 (335.1 examples/sec; 0.382 sec/batch)
2017-04-02 23:10:16.487764: step 14250, loss = 0.69 (330.9 examples/sec; 0.387 sec/batch)
2017-04-02 23:10:20.331146: step 14260, loss = 0.83 (333.0 examples/sec; 0.384 sec/batch)
2017-04-02 23:10:24.174268: step 14270, loss = 0.86 (333.1 examples/sec; 0.384 sec/batch)
2017-04-02 23:10:27.999946: step 14280, loss = 0.79 (334.6 examples/sec; 0.383 sec/batch)
2017-04-02 23:10:31.828805: step 14290, loss = 0.71 (334.3 examples/sec; 0.383 sec/batch)
2017-04-02 23:10:35.697234: step 14300, loss = 0.94 (330.9 examples/sec; 0.387 sec/batch)
2017-04-02 23:10:39.509897: step 14310, loss = 0.88 (335.7 examples/sec; 0.381 sec/batch)
2017-04-02 23:10:43.326609: step 14320, loss = 0.94 (335.4 examples/sec; 0.382 sec/batch)
2017-04-02 23:10:47.150467: step 14330, loss = 0.90 (334.7 examples/sec; 0.382 sec/batch)
2017-04-02 23:10:50.961601: step 14340, loss = 0.88 (335.9 examples/sec; 0.381 sec/batch)
2017-04-02 23:12:34.267271: precision @ 1 = 0.107

2017-04-02 23:13:29.179449: step 14350, loss = 0.70 (8.1 examples/sec; 15.822 sec/batch)
2017-04-02 23:13:33.054695: step 14360, loss = 0.87 (330.3 examples/sec; 0.388 sec/batch)
2017-04-02 23:13:36.938447: step 14370, loss = 0.76 (329.6 examples/sec; 0.388 sec/batch)
2017-04-02 23:13:40.831262: step 14380, loss = 0.92 (328.8 examples/sec; 0.389 sec/batch)
2017-04-02 23:13:44.673406: step 14390, loss = 0.94 (333.1 examples/sec; 0.384 sec/batch)
2017-04-02 23:13:48.612810: step 14400, loss = 0.71 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 23:13:52.484036: step 14410, loss = 0.96 (330.6 examples/sec; 0.387 sec/batch)
2017-04-02 23:13:56.344246: step 14420, loss = 0.65 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 23:14:00.211461: step 14430, loss = 0.75 (331.0 examples/sec; 0.387 sec/batch)
2017-04-02 23:14:04.046008: step 14440, loss = 0.90 (333.8 examples/sec; 0.383 sec/batch)
2017-04-02 23:14:07.928206: step 14450, loss = 0.69 (329.7 examples/sec; 0.388 sec/batch)
2017-04-02 23:14:11.807504: step 14460, loss = 0.75 (330.0 examples/sec; 0.388 sec/batch)
2017-04-02 23:14:15.668148: step 14470, loss = 0.71 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 23:14:19.525719: step 14480, loss = 1.06 (331.8 examples/sec; 0.386 sec/batch)
2017-04-02 23:14:23.369406: step 14490, loss = 0.90 (333.0 examples/sec; 0.384 sec/batch)
2017-04-02 23:14:27.286929: step 14500, loss = 0.83 (326.7 examples/sec; 0.392 sec/batch)
2017-04-02 23:14:31.140449: step 14510, loss = 0.77 (332.2 examples/sec; 0.385 sec/batch)
2017-04-02 23:14:34.988988: step 14520, loss = 0.67 (332.6 examples/sec; 0.385 sec/batch)
2017-04-02 23:14:38.844929: step 14530, loss = 0.88 (332.0 examples/sec; 0.386 sec/batch)
2017-04-02 23:14:42.684094: step 14540, loss = 0.84 (333.4 examples/sec; 0.384 sec/batch)
2017-04-02 23:14:46.514330: step 14550, loss = 0.69 (334.2 examples/sec; 0.383 sec/batch)
2017-04-02 23:14:50.338004: step 14560, loss = 0.74 (334.8 examples/sec; 0.382 sec/batch)
2017-04-02 23:14:54.156935: step 14570, loss = 0.75 (335.2 examples/sec; 0.382 sec/batch)
2017-04-02 23:14:58.008726: step 14580, loss = 0.87 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 23:15:01.861275: step 14590, loss = 1.02 (332.2 examples/sec; 0.385 sec/batch)
2017-04-02 23:15:05.761498: step 14600, loss = 0.85 (328.2 examples/sec; 0.390 sec/batch)
2017-04-02 23:15:09.613130: step 14610, loss = 0.83 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 23:15:13.472138: step 14620, loss = 0.96 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 23:15:17.300614: step 14630, loss = 0.88 (334.3 examples/sec; 0.383 sec/batch)
2017-04-02 23:15:21.122083: step 14640, loss = 0.92 (334.9 examples/sec; 0.382 sec/batch)
2017-04-02 23:15:24.977983: step 14650, loss = 0.78 (332.0 examples/sec; 0.386 sec/batch)
2017-04-02 23:15:28.822706: step 14660, loss = 0.76 (332.9 examples/sec; 0.384 sec/batch)
2017-04-02 23:15:32.671377: step 14670, loss = 0.86 (332.6 examples/sec; 0.385 sec/batch)
2017-04-02 23:15:36.532374: step 14680, loss = 0.99 (331.5 examples/sec; 0.386 sec/batch)
2017-04-02 23:15:40.441304: step 14690, loss = 0.77 (327.5 examples/sec; 0.391 sec/batch)
2017-04-02 23:15:44.822770: step 14700, loss = 0.85 (292.1 examples/sec; 0.438 sec/batch)
2017-04-02 23:15:48.679069: step 14710, loss = 0.81 (331.9 examples/sec; 0.386 sec/batch)
2017-04-02 23:15:52.522060: step 14720, loss = 0.91 (333.1 examples/sec; 0.384 sec/batch)
2017-04-02 23:15:56.363992: step 14730, loss = 1.00 (333.2 examples/sec; 0.384 sec/batch)
2017-04-02 23:16:00.211095: step 14740, loss = 0.76 (332.7 examples/sec; 0.385 sec/batch)
2017-04-02 23:16:04.093554: step 14750, loss = 0.89 (329.7 examples/sec; 0.388 sec/batch)
2017-04-02 23:16:07.944951: step 14760, loss = 0.70 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 23:16:11.770030: step 14770, loss = 1.13 (334.6 examples/sec; 0.383 sec/batch)
2017-04-02 23:16:16.277496: step 14780, loss = 0.79 (284.0 examples/sec; 0.451 sec/batch)
2017-04-02 23:16:20.086427: step 14790, loss = 0.94 (336.1 examples/sec; 0.381 sec/batch)
2017-04-02 23:16:23.989386: step 14800, loss = 0.76 (328.0 examples/sec; 0.390 sec/batch)
2017-04-02 23:16:27.825902: step 14810, loss = 0.77 (333.6 examples/sec; 0.384 sec/batch)
2017-04-02 23:16:31.654668: step 14820, loss = 0.83 (334.3 examples/sec; 0.383 sec/batch)
2017-04-02 23:16:35.507193: step 14830, loss = 0.83 (332.2 examples/sec; 0.385 sec/batch)
2017-04-02 23:16:39.323018: step 14840, loss = 0.83 (335.4 examples/sec; 0.382 sec/batch)
2017-04-02 23:16:43.149331: step 14850, loss = 0.75 (334.5 examples/sec; 0.383 sec/batch)
2017-04-02 23:16:46.973493: step 14860, loss = 0.73 (334.7 examples/sec; 0.382 sec/batch)
2017-04-02 23:16:50.828821: step 14870, loss = 0.83 (332.0 examples/sec; 0.386 sec/batch)
2017-04-02 23:16:54.680173: step 14880, loss = 0.64 (332.4 examples/sec; 0.385 sec/batch)
2017-04-02 23:16:58.525902: step 14890, loss = 0.79 (332.8 examples/sec; 0.385 sec/batch)
2017-04-02 23:17:02.428783: step 14900, loss = 0.81 (328.0 examples/sec; 0.390 sec/batch)
2017-04-02 23:17:06.282932: step 14910, loss = 0.90 (332.1 examples/sec; 0.385 sec/batch)
2017-04-02 23:17:10.143381: step 14920, loss = 0.95 (331.6 examples/sec; 0.386 sec/batch)
2017-04-02 23:17:13.970132: step 14930, loss = 0.82 (334.5 examples/sec; 0.383 sec/batch)
2017-04-02 23:17:17.809677: step 14940, loss = 0.98 (333.4 examples/sec; 0.384 sec/batch)
2017-04-02 23:17:21.623176: step 14950, loss = 0.78 (335.6 examples/sec; 0.381 sec/batch)
2017-04-02 23:17:25.428754: step 14960, loss = 0.86 (336.3 examples/sec; 0.381 sec/batch)
2017-04-02 23:17:29.282926: step 14970, loss = 1.03 (332.1 examples/sec; 0.385 sec/batch)
2017-04-02 23:17:33.093006: step 14980, loss = 0.91 (336.0 examples/sec; 0.381 sec/batch)
2017-04-02 23:17:36.906643: step 14990, loss = 0.77 (335.6 examples/sec; 0.381 sec/batch)
2017-04-02 23:17:40.787451: step 15000, loss = 0.84 (329.8 examples/sec; 0.388 sec/batch)
2017-04-02 23:17:44.604502: step 15010, loss = 0.76 (335.3 examples/sec; 0.382 sec/batch)
2017-04-02 23:17:48.430089: step 15020, loss = 0.83 (334.6 examples/sec; 0.383 sec/batch)
2017-04-02 23:17:52.258089: step 15030, loss = 0.82 (334.4 examples/sec; 0.383 sec/batch)
2017-04-02 23:17:56.070439: step 15040, loss = 0.85 (335.8 examples/sec; 0.381 sec/batch)
2017-04-02 23:17:59.897986: step 15050, loss = 0.73 (334.4 examples/sec; 0.383 sec/batch)
2017-04-02 23:18:03.682813: step 15060, loss = 0.72 (338.2 examples/sec; 0.378 sec/batch)
2017-04-02 23:18:07.524921: step 15070, loss = 0.74 (333.2 examples/sec; 0.384 sec/batch)
2017-04-02 23:18:11.344948: step 15080, loss = 0.91 (335.1 examples/sec; 0.382 sec/batch)
2017-04-02 23:18:15.150110: step 15090, loss = 0.67 (336.4 examples/sec; 0.381 sec/batch)
2017-04-02 23:18:18.995258: step 15100, loss = 0.85 (332.9 examples/sec; 0.385 sec/batch)
2017-04-02 23:18:22.827483: step 15110, loss = 0.80 (334.0 examples/sec; 0.383 sec/batch)
2017-04-02 23:18:26.666588: step 15120, loss = 0.80 (333.4 examples/sec; 0.384 sec/batch)
2017-04-02 23:18:30.491485: step 15130, loss = 0.68 (334.6 examples/sec; 0.382 sec/batch)
2017-04-02 23:18:34.312382: step 15140, loss = 0.96 (335.0 examples/sec; 0.382 sec/batch)
2017-04-02 23:18:38.104199: step 15150, loss = 0.74 (337.6 examples/sec; 0.379 sec/batch)
2017-04-02 23:18:41.914107: step 15160, loss = 0.81 (336.0 examples/sec; 0.381 sec/batch)
2017-04-02 23:18:45.718550: step 15170, loss = 0.95 (336.4 examples/sec; 0.380 sec/batch)
2017-04-02 23:18:49.534355: step 15180, loss = 0.76 (335.4 examples/sec; 0.382 sec/batch)
2017-04-02 23:18:53.365867: step 15190, loss = 0.83 (334.1 examples/sec; 0.383 sec/batch)
2017-04-02 23:18:57.234549: step 15200, loss = 0.83 (330.9 examples/sec; 0.387 sec/batch)
2017-04-02 23:19:01.081847: step 15210, loss = 0.82 (332.7 examples/sec; 0.385 sec/batch)
2017-04-02 23:19:04.912729: step 15220, loss = 0.73 (334.1 examples/sec; 0.383 sec/batch)
2017-04-02 23:19:08.725301: step 15230, loss = 0.80 (335.7 examples/sec; 0.381 sec/batch)
2017-04-02 23:19:12.560728: step 15240, loss = 0.89 (333.7 examples/sec; 0.384 sec/batch)
2017-04-02 23:19:16.359217: step 15250, loss = 0.87 (337.0 examples/sec; 0.380 sec/batch)
2017-04-02 23:19:20.200782: step 15260, loss = 0.73 (333.2 examples/sec; 0.384 sec/batch)
2017-04-02 23:19:24.033860: step 15270, loss = 0.73 (333.9 examples/sec; 0.383 sec/batch)
2017-04-02 23:19:27.869520: step 15280, loss = 0.88 (333.7 examples/sec; 0.384 sec/batch)
2017-04-02 23:19:31.705840: step 15290, loss = 0.78 (333.7 examples/sec; 0.384 sec/batch)
2017-04-02 23:19:35.567723: step 15300, loss = 0.93 (331.4 examples/sec; 0.386 sec/batch)
2017-04-02 23:19:39.361123: step 15310, loss = 0.77 (337.4 examples/sec; 0.379 sec/batch)
2017-04-02 23:19:43.217956: step 15320, loss = 0.78 (331.9 examples/sec; 0.386 sec/batch)
2017-04-02 23:19:47.045257: step 15330, loss = 0.72 (334.4 examples/sec; 0.383 sec/batch)
2017-04-02 23:19:50.854941: step 15340, loss = 0.86 (336.0 examples/sec; 0.381 sec/batch)
2017-04-02 23:19:54.660836: step 15350, loss = 0.71 (336.3 examples/sec; 0.381 sec/batch)
2017-04-02 23:19:58.473551: step 15360, loss = 0.99 (335.7 examples/sec; 0.381 sec/batch)
2017-04-02 23:20:02.313160: step 15370, loss = 0.99 (333.4 examples/sec; 0.384 sec/batch)
2017-04-02 23:20:06.142075: step 15380, loss = 0.82 (334.3 examples/sec; 0.383 sec/batch)
2017-04-02 23:20:09.956990: step 15390, loss = 0.79 (335.5 examples/sec; 0.381 sec/batch)
2017-04-02 23:20:13.812218: step 15400, loss = 0.99 (332.0 examples/sec; 0.386 sec/batch)
2017-04-02 23:20:17.660981: step 15410, loss = 0.82 (332.6 examples/sec; 0.385 sec/batch)
2017-04-02 23:20:21.495291: step 15420, loss = 0.81 (333.8 examples/sec; 0.383 sec/batch)
2017-04-02 23:20:25.333029: step 15430, loss = 0.92 (333.5 examples/sec; 0.384 sec/batch)
2017-04-02 23:20:29.159538: step 15440, loss = 0.87 (334.5 examples/sec; 0.383 sec/batch)
2017-04-02 23:20:32.954701: step 15450, loss = 0.85 (337.3 examples/sec; 0.380 sec/batch)
2017-04-02 23:20:36.780879: step 15460, loss = 0.58 (334.5 examples/sec; 0.383 sec/batch)
2017-04-02 23:20:40.617298: step 15470, loss = 0.83 (333.6 examples/sec; 0.384 sec/batch)
2017-04-02 23:20:44.437019: step 15480, loss = 0.68 (335.1 examples/sec; 0.382 sec/batch)
2017-04-02 23:20:48.276996: step 15490, loss = 0.90 (333.3 examples/sec; 0.384 sec/batch)
2017-04-02 23:20:52.151177: step 15500, loss = 0.71 (330.4 examples/sec; 0.387 sec/batch)
2017-04-02 23:20:55.996675: step 15510, loss = 0.81 (332.9 examples/sec; 0.385 sec/batch)
2017-04-02 23:20:59.831787: step 15520, loss = 0.96 (333.8 examples/sec; 0.384 sec/batch)
2017-04-02 23:21:03.658689: step 15530, loss = 0.69 (334.5 examples/sec; 0.383 sec/batch)
2017-04-02 23:21:07.486623: step 15540, loss = 0.75 (334.4 examples/sec; 0.383 sec/batch)
2017-04-02 23:21:11.293854: step 15550, loss = 0.85 (336.2 examples/sec; 0.381 sec/batch)
2017-04-02 23:21:15.126526: step 15560, loss = 0.84 (334.0 examples/sec; 0.383 sec/batch)
2017-04-02 23:21:18.949074: step 15570, loss = 0.81 (334.9 examples/sec; 0.382 sec/batch)
2017-04-02 23:21:22.761167: step 15580, loss = 0.78 (335.8 examples/sec; 0.381 sec/batch)
2017-04-02 23:21:26.593321: step 15590, loss = 0.85 (334.0 examples/sec; 0.383 sec/batch)
2017-04-02 23:21:30.446399: step 15600, loss = 0.96 (332.2 examples/sec; 0.385 sec/batch)
2017-04-02 23:21:34.289106: step 15610, loss = 0.66 (333.1 examples/sec; 0.384 sec/batch)
2017-04-02 23:21:38.145067: step 15620, loss = 0.78 (332.0 examples/sec; 0.386 sec/batch)
2017-04-02 23:21:41.965333: step 15630, loss = 0.91 (335.1 examples/sec; 0.382 sec/batch)
2017-04-02 23:21:45.766113: step 15640, loss = 0.98 (336.8 examples/sec; 0.380 sec/batch)
2017-04-02 23:21:49.600808: step 15650, loss = 0.72 (333.8 examples/sec; 0.383 sec/batch)
2017-04-02 23:21:53.428155: step 15660, loss = 0.75 (334.4 examples/sec; 0.383 sec/batch)
2017-04-02 23:21:57.253369: step 15670, loss = 0.72 (334.6 examples/sec; 0.383 sec/batch)
2017-04-02 23:22:01.071778: step 15680, loss = 0.97 (335.2 examples/sec; 0.382 sec/batch)
2017-04-02 23:22:04.930432: step 15690, loss = 0.84 (331.7 examples/sec; 0.386 sec/batch)
2017-04-02 23:22:08.817601: step 15700, loss = 0.89 (329.3 examples/sec; 0.389 sec/batch)
2017-04-02 23:22:12.610083: step 15710, loss = 0.79 (337.5 examples/sec; 0.379 sec/batch)
2017-04-02 23:22:16.476173: step 15720, loss = 0.86 (331.1 examples/sec; 0.387 sec/batch)
2017-04-02 23:22:20.309879: step 15730, loss = 0.61 (333.9 examples/sec; 0.383 sec/batch)
2017-04-02 23:22:24.141951: step 15740, loss = 0.67 (334.0 examples/sec; 0.383 sec/batch)
2017-04-02 23:22:27.978983: step 15750, loss = 0.80 (333.6 examples/sec; 0.384 sec/batch)
2017-04-02 23:22:31.843684: step 15760, loss = 0.92 (331.2 examples/sec; 0.386 sec/batch)
2017-04-02 23:22:35.692048: step 15770, loss = 0.85 (332.6 examples/sec; 0.385 sec/batch)
2017-04-02 23:22:39.490381: step 15780, loss = 0.93 (337.0 examples/sec; 0.380 sec/batch)
2017-04-02 23:22:43.352404: step 15790, loss = 0.84 (331.4 examples/sec; 0.386 sec/batch)
2017-04-02 23:22:47.258766: step 15800, loss = 0.87 (327.7 examples/sec; 0.391 sec/batch)
2017-04-02 23:22:51.122691: step 15810, loss = 0.95 (331.3 examples/sec; 0.386 sec/batch)
2017-04-02 23:22:54.980405: step 15820, loss = 0.92 (331.8 examples/sec; 0.386 sec/batch)
2017-04-02 23:22:58.837194: step 15830, loss = 0.76 (331.9 examples/sec; 0.386 sec/batch)
2017-04-02 23:23:02.697998: step 15840, loss = 0.84 (331.5 examples/sec; 0.386 sec/batch)
2017-04-02 23:23:06.540217: step 15850, loss = 0.74 (333.1 examples/sec; 0.384 sec/batch)
2017-04-02 23:23:10.355829: step 15860, loss = 0.80 (335.5 examples/sec; 0.382 sec/batch)
2017-04-02 23:23:14.197403: step 15870, loss = 0.72 (333.2 examples/sec; 0.384 sec/batch)
2017-04-02 23:23:18.049018: step 15880, loss = 0.85 (332.3 examples/sec; 0.385 sec/batch)
2017-04-02 23:23:21.890387: step 15890, loss = 0.69 (333.2 examples/sec; 0.384 sec/batch)
2017-04-02 23:23:25.796010: step 15900, loss = 0.97 (327.7 examples/sec; 0.391 sec/batch)
2017-04-02 23:23:29.625231: step 15910, loss = 0.89 (334.3 examples/sec; 0.383 sec/batch)
2017-04-02 23:23:33.462216: step 15920, loss = 0.96 (333.6 examples/sec; 0.384 sec/batch)
2017-04-02 23:23:37.281402: step 15930, loss = 1.14 (335.1 examples/sec; 0.382 sec/batch)
2017-04-02 23:23:41.102229: step 15940, loss = 0.85 (335.0 examples/sec; 0.382 sec/batch)
2017-04-02 23:23:44.964348: step 15950, loss = 0.97 (331.4 examples/sec; 0.386 sec/batch)
2017-04-02 23:23:48.781183: step 15960, loss = 0.81 (335.4 examples/sec; 0.382 sec/batch)
2017-04-02 23:23:52.627347: step 15970, loss = 0.76 (332.8 examples/sec; 0.385 sec/batch)
2017-04-02 23:23:56.451204: step 15980, loss = 0.94 (334.7 examples/sec; 0.382 sec/batch)
2017-04-02 23:24:00.279341: step 15990, loss = 0.81 (334.4 examples/sec; 0.383 sec/batch)
2017-04-02 23:24:04.147946: step 16000, loss = 0.84 (330.9 examples/sec; 0.387 sec/batch)
2017-04-02 23:27:36.654188: precision @ 1 = 0.831

