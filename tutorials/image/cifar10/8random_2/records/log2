2017-04-02 22:38:55.039721: step 0, loss = 4.68 (364.0 examples/sec; 0.352 sec/batch)
2017-04-02 22:38:59.111796: step 10, loss = 4.66 (314.3 examples/sec; 0.407 sec/batch)
2017-04-02 22:39:03.213930: step 20, loss = 4.65 (312.0 examples/sec; 0.410 sec/batch)
2017-04-02 22:39:07.287311: step 30, loss = 4.62 (314.2 examples/sec; 0.407 sec/batch)
2017-04-02 22:39:11.333696: step 40, loss = 4.61 (316.3 examples/sec; 0.405 sec/batch)
2017-04-02 22:39:15.424120: step 50, loss = 4.60 (312.9 examples/sec; 0.409 sec/batch)
2017-04-02 22:39:19.458872: step 60, loss = 4.56 (317.2 examples/sec; 0.403 sec/batch)
2017-04-02 22:39:23.586715: step 70, loss = 4.55 (310.1 examples/sec; 0.413 sec/batch)
2017-04-02 22:39:27.623914: step 80, loss = 4.53 (317.1 examples/sec; 0.404 sec/batch)
2017-04-02 22:39:31.684847: step 90, loss = 4.51 (315.2 examples/sec; 0.406 sec/batch)
2017-04-02 22:39:35.787993: step 100, loss = 4.50 (312.0 examples/sec; 0.410 sec/batch)
2017-04-02 22:39:39.930320: step 110, loss = 4.48 (309.0 examples/sec; 0.414 sec/batch)
2017-04-02 22:39:44.045379: step 120, loss = 4.46 (311.1 examples/sec; 0.412 sec/batch)
2017-04-02 22:39:48.251809: step 130, loss = 4.44 (304.3 examples/sec; 0.421 sec/batch)
2017-04-02 22:39:52.335934: step 140, loss = 4.43 (313.4 examples/sec; 0.408 sec/batch)
2017-04-02 22:39:56.397336: step 150, loss = 4.40 (315.2 examples/sec; 0.406 sec/batch)
2017-04-02 22:40:00.487885: step 160, loss = 4.40 (312.9 examples/sec; 0.409 sec/batch)
2017-04-02 22:40:04.553131: step 170, loss = 4.38 (314.9 examples/sec; 0.407 sec/batch)
2017-04-02 22:40:08.624335: step 180, loss = 4.36 (314.4 examples/sec; 0.407 sec/batch)
2017-04-02 22:40:12.714121: step 190, loss = 4.35 (313.0 examples/sec; 0.409 sec/batch)
2017-04-02 22:40:16.849115: step 200, loss = 4.33 (309.6 examples/sec; 0.413 sec/batch)
2017-04-02 22:40:20.940907: step 210, loss = 4.32 (312.8 examples/sec; 0.409 sec/batch)
2017-04-02 22:40:24.989612: step 220, loss = 4.30 (316.2 examples/sec; 0.405 sec/batch)
2017-04-02 22:40:29.068713: step 230, loss = 4.28 (313.8 examples/sec; 0.408 sec/batch)
2017-04-02 22:40:33.127315: step 240, loss = 4.27 (315.4 examples/sec; 0.406 sec/batch)
2017-04-02 22:40:37.197507: step 250, loss = 4.25 (314.5 examples/sec; 0.407 sec/batch)
2017-04-02 22:40:41.237083: step 260, loss = 4.24 (316.9 examples/sec; 0.404 sec/batch)
2017-04-02 22:40:45.310774: step 270, loss = 4.22 (314.2 examples/sec; 0.407 sec/batch)
2017-04-02 22:40:49.364251: step 280, loss = 4.20 (315.8 examples/sec; 0.405 sec/batch)
2017-04-02 22:40:53.431203: step 290, loss = 4.19 (314.7 examples/sec; 0.407 sec/batch)
2017-04-02 22:40:57.581974: step 300, loss = 4.17 (308.4 examples/sec; 0.415 sec/batch)
2017-04-02 22:41:01.637078: step 310, loss = 4.16 (315.7 examples/sec; 0.406 sec/batch)
2017-04-02 22:41:05.732424: step 320, loss = 4.15 (312.5 examples/sec; 0.410 sec/batch)
2017-04-02 22:41:09.841456: step 330, loss = 4.13 (311.5 examples/sec; 0.411 sec/batch)
2017-04-02 22:41:13.880037: step 340, loss = 4.12 (316.9 examples/sec; 0.404 sec/batch)
2017-04-02 22:41:17.976952: step 350, loss = 4.10 (312.4 examples/sec; 0.410 sec/batch)
2017-04-02 22:41:22.039800: step 360, loss = 4.09 (315.1 examples/sec; 0.406 sec/batch)
2017-04-02 22:41:26.120821: step 370, loss = 4.07 (313.6 examples/sec; 0.408 sec/batch)
2017-04-02 22:41:30.182152: step 380, loss = 4.06 (315.2 examples/sec; 0.406 sec/batch)
2017-04-02 22:41:34.278513: step 390, loss = 4.04 (312.5 examples/sec; 0.410 sec/batch)
2017-04-02 22:41:38.370660: step 400, loss = 4.03 (312.8 examples/sec; 0.409 sec/batch)
2017-04-02 22:41:42.418191: step 410, loss = 4.02 (316.2 examples/sec; 0.405 sec/batch)
2017-04-02 22:41:46.518485: step 420, loss = 4.00 (312.2 examples/sec; 0.410 sec/batch)
2017-04-02 22:41:50.599779: step 430, loss = 3.99 (313.6 examples/sec; 0.408 sec/batch)
2017-04-02 22:41:54.643002: step 440, loss = 3.97 (316.6 examples/sec; 0.404 sec/batch)
2017-04-02 22:41:58.739430: step 450, loss = 3.96 (312.5 examples/sec; 0.410 sec/batch)
2017-04-02 22:42:02.796076: step 460, loss = 3.95 (315.5 examples/sec; 0.406 sec/batch)
2017-04-02 22:42:06.860416: step 470, loss = 3.94 (314.9 examples/sec; 0.406 sec/batch)
2017-04-02 22:42:10.935507: step 480, loss = 3.93 (314.1 examples/sec; 0.408 sec/batch)
2017-04-02 22:42:15.027212: step 490, loss = 3.91 (312.8 examples/sec; 0.409 sec/batch)
2017-04-02 22:42:19.177758: step 500, loss = 3.90 (308.4 examples/sec; 0.415 sec/batch)
2017-04-02 22:42:23.235306: step 510, loss = 3.88 (315.5 examples/sec; 0.406 sec/batch)
2017-04-02 22:42:27.318153: step 520, loss = 3.87 (313.5 examples/sec; 0.408 sec/batch)
2017-04-02 22:42:31.389438: step 530, loss = 3.86 (314.4 examples/sec; 0.407 sec/batch)
2017-04-02 22:42:35.479136: step 540, loss = 3.85 (313.0 examples/sec; 0.409 sec/batch)
2017-04-02 22:42:39.531830: step 550, loss = 3.83 (315.8 examples/sec; 0.405 sec/batch)
2017-04-02 22:42:43.596247: step 560, loss = 3.82 (314.9 examples/sec; 0.406 sec/batch)
2017-04-02 22:42:47.670678: step 570, loss = 3.81 (314.2 examples/sec; 0.407 sec/batch)
2017-04-02 22:42:51.746637: step 580, loss = 3.80 (314.0 examples/sec; 0.408 sec/batch)
2017-04-02 22:42:55.833037: step 590, loss = 3.79 (313.2 examples/sec; 0.409 sec/batch)
2017-04-02 22:43:00.036721: step 600, loss = 3.78 (304.5 examples/sec; 0.420 sec/batch)
2017-04-02 22:43:04.115614: step 610, loss = 3.76 (313.8 examples/sec; 0.408 sec/batch)
2017-04-02 22:43:08.155591: step 620, loss = 3.75 (316.8 examples/sec; 0.404 sec/batch)
2017-04-02 22:43:12.244795: step 630, loss = 3.74 (313.0 examples/sec; 0.409 sec/batch)
2017-04-02 22:43:16.331078: step 640, loss = 3.73 (313.2 examples/sec; 0.409 sec/batch)
2017-04-02 22:43:20.426208: step 650, loss = 3.71 (312.6 examples/sec; 0.410 sec/batch)
2017-04-02 22:43:24.512407: step 660, loss = 3.71 (313.2 examples/sec; 0.409 sec/batch)
2017-04-02 22:43:28.603336: step 670, loss = 3.70 (312.9 examples/sec; 0.409 sec/batch)
2017-04-02 22:43:32.689535: step 680, loss = 3.68 (313.2 examples/sec; 0.409 sec/batch)
2017-04-02 22:43:36.785820: step 690, loss = 3.67 (312.5 examples/sec; 0.410 sec/batch)
2017-04-02 22:43:41.056613: step 700, loss = 3.66 (299.7 examples/sec; 0.427 sec/batch)
2017-04-02 22:43:45.127497: step 710, loss = 3.65 (314.4 examples/sec; 0.407 sec/batch)
2017-04-02 22:43:49.214603: step 720, loss = 3.64 (313.2 examples/sec; 0.409 sec/batch)
2017-04-02 22:43:53.299391: step 730, loss = 3.63 (313.4 examples/sec; 0.408 sec/batch)
2017-04-02 22:43:57.365915: step 740, loss = 3.62 (314.8 examples/sec; 0.407 sec/batch)
2017-04-02 22:44:01.425877: step 750, loss = 3.61 (315.3 examples/sec; 0.406 sec/batch)
2017-04-02 22:44:05.465750: step 760, loss = 3.60 (316.8 examples/sec; 0.404 sec/batch)
2017-04-02 22:44:09.535135: step 770, loss = 3.58 (314.5 examples/sec; 0.407 sec/batch)
2017-04-02 22:44:13.597954: step 780, loss = 3.58 (315.1 examples/sec; 0.406 sec/batch)
2017-04-02 22:44:17.691240: step 790, loss = 3.57 (312.7 examples/sec; 0.409 sec/batch)
2017-04-02 22:44:21.827047: step 800, loss = 3.57 (309.5 examples/sec; 0.414 sec/batch)
2017-04-02 22:44:25.888866: step 810, loss = 3.54 (315.1 examples/sec; 0.406 sec/batch)
2017-04-02 22:44:29.972232: step 820, loss = 3.54 (313.5 examples/sec; 0.408 sec/batch)
2017-04-02 22:44:34.035918: step 830, loss = 3.53 (315.0 examples/sec; 0.406 sec/batch)
2017-04-02 22:44:38.105613: step 840, loss = 3.52 (314.5 examples/sec; 0.407 sec/batch)
2017-04-02 22:44:42.185847: step 850, loss = 3.51 (313.7 examples/sec; 0.408 sec/batch)
2017-04-02 22:44:46.253260: step 860, loss = 3.50 (314.7 examples/sec; 0.407 sec/batch)
2017-04-02 22:44:50.373589: step 870, loss = 3.49 (310.7 examples/sec; 0.412 sec/batch)
2017-04-02 22:44:54.425886: step 880, loss = 3.48 (315.9 examples/sec; 0.405 sec/batch)
2017-04-02 22:44:58.473949: step 890, loss = 3.47 (316.2 examples/sec; 0.405 sec/batch)
2017-04-02 22:45:02.608160: step 900, loss = 3.46 (309.6 examples/sec; 0.413 sec/batch)
2017-04-02 22:45:06.669808: step 910, loss = 3.45 (315.1 examples/sec; 0.406 sec/batch)
2017-04-02 22:45:10.712139: step 920, loss = 3.44 (316.6 examples/sec; 0.404 sec/batch)
2017-04-02 22:45:14.749976: step 930, loss = 3.43 (317.0 examples/sec; 0.404 sec/batch)
2017-04-02 22:45:18.796975: step 940, loss = 3.42 (316.3 examples/sec; 0.405 sec/batch)
2017-04-02 22:45:22.853317: step 950, loss = 3.41 (315.6 examples/sec; 0.406 sec/batch)
2017-04-02 22:45:26.910084: step 960, loss = 3.40 (315.5 examples/sec; 0.406 sec/batch)
2017-04-02 22:45:30.977777: step 970, loss = 3.40 (314.7 examples/sec; 0.407 sec/batch)
2017-04-02 22:45:35.060942: step 980, loss = 3.38 (313.5 examples/sec; 0.408 sec/batch)
2017-04-02 22:45:39.139816: step 990, loss = 3.38 (313.8 examples/sec; 0.408 sec/batch)
2017-04-02 22:45:43.279492: step 1000, loss = 3.37 (309.2 examples/sec; 0.414 sec/batch)
2017-04-02 22:45:47.363719: step 1010, loss = 3.36 (313.4 examples/sec; 0.408 sec/batch)
2017-04-02 22:45:51.462407: step 1020, loss = 3.36 (312.3 examples/sec; 0.410 sec/batch)
2017-04-02 22:45:55.552131: step 1030, loss = 3.35 (313.0 examples/sec; 0.409 sec/batch)
2017-04-02 22:45:59.617693: step 1040, loss = 3.34 (314.8 examples/sec; 0.407 sec/batch)
2017-04-02 22:46:03.655606: step 1050, loss = 3.33 (317.0 examples/sec; 0.404 sec/batch)
2017-04-02 22:46:07.855105: step 1060, loss = 3.32 (304.8 examples/sec; 0.420 sec/batch)
2017-04-02 22:46:11.934662: step 1070, loss = 3.31 (313.8 examples/sec; 0.408 sec/batch)
2017-04-02 22:46:16.010978: step 1080, loss = 3.30 (314.0 examples/sec; 0.408 sec/batch)
2017-04-02 22:46:20.029468: step 1090, loss = 3.30 (318.5 examples/sec; 0.402 sec/batch)
2017-04-02 22:46:24.164965: step 1100, loss = 3.29 (309.5 examples/sec; 0.414 sec/batch)
2017-04-02 22:46:28.211599: step 1110, loss = 3.28 (316.3 examples/sec; 0.405 sec/batch)
2017-04-02 22:46:32.265928: step 1120, loss = 3.27 (315.7 examples/sec; 0.405 sec/batch)
2017-04-02 22:46:36.313936: step 1130, loss = 3.26 (316.2 examples/sec; 0.405 sec/batch)
2017-04-02 22:46:40.374579: step 1140, loss = 3.25 (315.2 examples/sec; 0.406 sec/batch)
2017-04-02 22:46:44.441669: step 1150, loss = 3.25 (314.7 examples/sec; 0.407 sec/batch)
2017-04-02 22:46:48.534677: step 1160, loss = 3.24 (312.7 examples/sec; 0.409 sec/batch)
2017-04-02 22:46:52.618475: step 1170, loss = 3.24 (313.4 examples/sec; 0.408 sec/batch)
2017-04-02 22:46:56.674080: step 1180, loss = 3.23 (315.6 examples/sec; 0.406 sec/batch)
2017-04-02 22:47:00.756661: step 1190, loss = 3.22 (313.5 examples/sec; 0.408 sec/batch)
2017-04-02 22:47:04.897932: step 1200, loss = 3.21 (309.1 examples/sec; 0.414 sec/batch)
2017-04-02 22:47:08.960155: step 1210, loss = 3.20 (315.1 examples/sec; 0.406 sec/batch)
2017-04-02 22:47:13.030446: step 1220, loss = 3.20 (314.5 examples/sec; 0.407 sec/batch)
2017-04-02 22:47:17.087655: step 1230, loss = 3.19 (315.5 examples/sec; 0.406 sec/batch)
2017-04-02 22:47:21.168222: step 1240, loss = 3.18 (313.7 examples/sec; 0.408 sec/batch)
2017-04-02 22:47:25.219378: step 1250, loss = 3.18 (316.0 examples/sec; 0.405 sec/batch)
2017-04-02 22:47:29.262908: step 1260, loss = 3.17 (316.6 examples/sec; 0.404 sec/batch)
2017-04-02 22:47:33.318600: step 1270, loss = 3.16 (315.6 examples/sec; 0.406 sec/batch)
2017-04-02 22:47:37.388935: step 1280, loss = 3.16 (314.5 examples/sec; 0.407 sec/batch)
2017-04-02 22:47:41.438568: step 1290, loss = 3.14 (316.1 examples/sec; 0.405 sec/batch)
2017-04-02 22:47:45.570701: step 1300, loss = 3.14 (309.8 examples/sec; 0.413 sec/batch)
2017-04-02 22:47:49.640238: step 1310, loss = 3.14 (314.5 examples/sec; 0.407 sec/batch)
2017-04-02 22:47:53.702881: step 1320, loss = 3.13 (315.1 examples/sec; 0.406 sec/batch)
2017-04-02 22:47:57.798251: step 1330, loss = 3.12 (312.5 examples/sec; 0.410 sec/batch)
2017-04-02 22:48:01.847744: step 1340, loss = 3.12 (316.1 examples/sec; 0.405 sec/batch)
2017-04-02 22:48:05.909157: step 1350, loss = 3.11 (315.2 examples/sec; 0.406 sec/batch)
2017-04-02 22:48:09.991659: step 1360, loss = 3.10 (313.5 examples/sec; 0.408 sec/batch)
2017-04-02 22:48:14.048629: step 1370, loss = 3.10 (315.5 examples/sec; 0.406 sec/batch)
2017-04-02 22:48:18.139637: step 1380, loss = 3.09 (312.9 examples/sec; 0.409 sec/batch)
2017-04-02 22:48:22.266502: step 1390, loss = 3.08 (310.2 examples/sec; 0.413 sec/batch)
2017-04-02 22:48:26.434124: step 1400, loss = 3.08 (307.1 examples/sec; 0.417 sec/batch)
2017-04-02 22:48:30.497283: step 1410, loss = 3.07 (315.0 examples/sec; 0.406 sec/batch)
2017-04-02 22:48:34.627457: step 1420, loss = 3.07 (309.9 examples/sec; 0.413 sec/batch)
2017-04-02 22:48:38.683707: step 1430, loss = 3.06 (315.6 examples/sec; 0.406 sec/batch)
2017-04-02 22:48:42.773950: step 1440, loss = 3.05 (312.9 examples/sec; 0.409 sec/batch)
2017-04-02 22:48:46.807758: step 1450, loss = 3.05 (317.3 examples/sec; 0.403 sec/batch)
2017-04-02 22:48:50.865868: step 1460, loss = 3.04 (315.4 examples/sec; 0.406 sec/batch)
2017-04-02 22:48:55.717895: step 1470, loss = 3.04 (263.8 examples/sec; 0.485 sec/batch)
2017-04-02 22:48:59.782136: step 1480, loss = 3.03 (314.9 examples/sec; 0.406 sec/batch)
2017-04-02 22:49:03.846916: step 1490, loss = 3.02 (314.9 examples/sec; 0.406 sec/batch)
2017-04-02 22:49:07.990127: step 1500, loss = 3.02 (308.9 examples/sec; 0.414 sec/batch)
2017-04-02 22:49:12.059256: step 1510, loss = 3.01 (314.6 examples/sec; 0.407 sec/batch)
2017-04-02 22:49:16.115739: step 1520, loss = 3.01 (315.5 examples/sec; 0.406 sec/batch)
2017-04-02 22:49:20.193460: step 1530, loss = 3.00 (313.9 examples/sec; 0.408 sec/batch)
2017-04-02 22:49:24.286263: step 1540, loss = 3.00 (312.7 examples/sec; 0.409 sec/batch)
2017-04-02 22:49:28.351804: step 1550, loss = 2.99 (314.8 examples/sec; 0.407 sec/batch)
2017-04-02 22:49:32.434480: step 1560, loss = 2.98 (313.5 examples/sec; 0.408 sec/batch)
2017-04-02 22:49:36.522792: step 1570, loss = 2.98 (313.1 examples/sec; 0.409 sec/batch)
2017-04-02 22:49:40.597368: step 1580, loss = 2.97 (314.1 examples/sec; 0.407 sec/batch)
2017-04-02 22:49:44.683743: step 1590, loss = 2.97 (313.2 examples/sec; 0.409 sec/batch)
2017-04-02 22:49:48.803968: step 1600, loss = 2.96 (310.7 examples/sec; 0.412 sec/batch)
2017-04-02 22:49:52.876924: step 1610, loss = 2.96 (314.3 examples/sec; 0.407 sec/batch)
2017-04-02 22:49:56.958729: step 1620, loss = 2.95 (313.6 examples/sec; 0.408 sec/batch)
2017-04-02 22:50:01.033593: step 1630, loss = 2.95 (314.1 examples/sec; 0.407 sec/batch)
2017-04-02 22:50:05.166276: step 1640, loss = 2.95 (309.7 examples/sec; 0.413 sec/batch)
2017-04-02 22:50:09.234776: step 1650, loss = 2.94 (314.6 examples/sec; 0.407 sec/batch)
2017-04-02 22:50:13.286972: step 1660, loss = 2.93 (315.9 examples/sec; 0.405 sec/batch)
2017-04-02 22:50:17.348552: step 1670, loss = 2.93 (315.1 examples/sec; 0.406 sec/batch)
2017-04-02 22:50:21.394570: step 1680, loss = 2.92 (316.4 examples/sec; 0.405 sec/batch)
2017-04-02 22:50:25.434746: step 1690, loss = 2.92 (316.8 examples/sec; 0.404 sec/batch)
2017-04-02 22:50:29.622233: step 1700, loss = 2.91 (305.7 examples/sec; 0.419 sec/batch)
2017-04-02 22:50:33.655367: step 1710, loss = 2.91 (317.4 examples/sec; 0.403 sec/batch)
2017-04-02 22:50:37.742785: step 1720, loss = 2.90 (313.2 examples/sec; 0.409 sec/batch)
2017-04-02 22:50:41.790377: step 1730, loss = 2.90 (316.2 examples/sec; 0.405 sec/batch)
2017-04-02 22:50:45.846832: step 1740, loss = 2.89 (315.5 examples/sec; 0.406 sec/batch)
2017-04-02 22:50:49.919981: step 1750, loss = 2.89 (314.3 examples/sec; 0.407 sec/batch)
2017-04-02 22:50:53.977834: step 1760, loss = 2.89 (315.4 examples/sec; 0.406 sec/batch)
2017-04-02 22:50:58.014943: step 1770, loss = 2.88 (317.1 examples/sec; 0.404 sec/batch)
2017-04-02 22:51:02.117281: step 1780, loss = 2.87 (312.0 examples/sec; 0.410 sec/batch)
2017-04-02 22:51:06.220667: step 1790, loss = 2.87 (311.9 examples/sec; 0.410 sec/batch)
2017-04-02 22:51:10.359924: step 1800, loss = 2.86 (309.2 examples/sec; 0.414 sec/batch)
2017-04-02 22:51:14.430421: step 1810, loss = 2.86 (314.5 examples/sec; 0.407 sec/batch)
2017-04-02 22:51:18.483955: step 1820, loss = 2.86 (315.8 examples/sec; 0.405 sec/batch)
2017-04-02 22:51:22.575780: step 1830, loss = 2.85 (312.8 examples/sec; 0.409 sec/batch)
2017-04-02 22:51:26.688052: step 1840, loss = 2.85 (311.3 examples/sec; 0.411 sec/batch)
2017-04-02 22:51:30.787016: step 1850, loss = 2.84 (312.3 examples/sec; 0.410 sec/batch)
2017-04-02 22:51:34.863357: step 1860, loss = 2.84 (314.0 examples/sec; 0.408 sec/batch)
2017-04-02 22:51:38.976963: step 1870, loss = 2.84 (311.2 examples/sec; 0.411 sec/batch)
2017-04-02 22:51:43.095524: step 1880, loss = 2.83 (310.8 examples/sec; 0.412 sec/batch)
2017-04-02 22:51:47.205781: step 1890, loss = 2.83 (311.4 examples/sec; 0.411 sec/batch)
2017-04-02 22:51:51.355775: step 1900, loss = 2.82 (308.4 examples/sec; 0.415 sec/batch)
2017-04-02 22:51:55.464664: step 1910, loss = 2.82 (311.5 examples/sec; 0.411 sec/batch)
2017-04-02 22:51:59.542593: step 1920, loss = 2.82 (313.9 examples/sec; 0.408 sec/batch)
2017-04-02 22:52:03.606627: step 1930, loss = 2.81 (315.0 examples/sec; 0.406 sec/batch)
2017-04-02 22:52:07.658542: step 1940, loss = 2.81 (315.9 examples/sec; 0.405 sec/batch)
2017-04-02 22:52:11.746038: step 1950, loss = 2.80 (313.2 examples/sec; 0.409 sec/batch)
2017-04-02 22:52:15.788596: step 1960, loss = 2.80 (316.6 examples/sec; 0.404 sec/batch)
2017-04-02 22:52:19.870840: step 1970, loss = 2.79 (313.6 examples/sec; 0.408 sec/batch)
2017-04-02 22:52:23.917605: step 1980, loss = 2.79 (316.3 examples/sec; 0.405 sec/batch)
2017-04-02 22:52:27.999725: step 1990, loss = 2.79 (313.6 examples/sec; 0.408 sec/batch)
2017-04-02 22:52:32.171088: step 2000, loss = 2.78 (306.9 examples/sec; 0.417 sec/batch)
2017-04-02 22:53:32.848857: precision @ 1 = 0.144

2017-04-02 22:54:44.061241: step 2010, loss = 2.78 (9.7 examples/sec; 13.189 sec/batch)
2017-04-02 22:54:48.167721: step 2020, loss = 2.77 (311.7 examples/sec; 0.411 sec/batch)
2017-04-02 22:54:52.273422: step 2030, loss = 2.77 (311.8 examples/sec; 0.411 sec/batch)
2017-04-02 22:54:56.386851: step 2040, loss = 2.77 (311.2 examples/sec; 0.411 sec/batch)
2017-04-02 22:55:00.500565: step 2050, loss = 2.77 (311.2 examples/sec; 0.411 sec/batch)
2017-04-02 22:55:04.626545: step 2060, loss = 2.76 (310.2 examples/sec; 0.413 sec/batch)
2017-04-02 22:55:08.748621: step 2070, loss = 2.76 (310.5 examples/sec; 0.412 sec/batch)
2017-04-02 22:55:12.842937: step 2080, loss = 2.75 (312.6 examples/sec; 0.409 sec/batch)
2017-04-02 22:55:16.949572: step 2090, loss = 2.75 (311.7 examples/sec; 0.411 sec/batch)
2017-04-02 22:55:21.128959: step 2100, loss = 2.75 (306.3 examples/sec; 0.418 sec/batch)
2017-04-02 22:55:25.264415: step 2110, loss = 2.74 (309.5 examples/sec; 0.414 sec/batch)
2017-04-02 22:55:29.374361: step 2120, loss = 2.74 (311.4 examples/sec; 0.411 sec/batch)
2017-04-02 22:55:33.502368: step 2130, loss = 2.73 (310.1 examples/sec; 0.413 sec/batch)
2017-04-02 22:55:37.623208: step 2140, loss = 2.73 (310.6 examples/sec; 0.412 sec/batch)
2017-04-02 22:55:41.733843: step 2150, loss = 2.73 (311.4 examples/sec; 0.411 sec/batch)
2017-04-02 22:55:45.812150: step 2160, loss = 2.72 (313.9 examples/sec; 0.408 sec/batch)
2017-04-02 22:55:49.938297: step 2170, loss = 2.72 (310.2 examples/sec; 0.413 sec/batch)
2017-04-02 22:55:54.023699: step 2180, loss = 2.72 (313.3 examples/sec; 0.409 sec/batch)
2017-04-02 22:55:58.118424: step 2190, loss = 2.71 (312.6 examples/sec; 0.409 sec/batch)
2017-04-02 22:56:02.268935: step 2200, loss = 2.71 (308.4 examples/sec; 0.415 sec/batch)
2017-04-02 22:56:06.350396: step 2210, loss = 2.71 (313.6 examples/sec; 0.408 sec/batch)
2017-04-02 22:56:10.435088: step 2220, loss = 2.70 (313.4 examples/sec; 0.408 sec/batch)
2017-04-02 22:56:14.559038: step 2230, loss = 2.70 (310.4 examples/sec; 0.412 sec/batch)
2017-04-02 22:56:18.673345: step 2240, loss = 2.70 (311.1 examples/sec; 0.411 sec/batch)
2017-04-02 22:56:22.739153: step 2250, loss = 2.70 (314.8 examples/sec; 0.407 sec/batch)
2017-04-02 22:56:26.807858: step 2260, loss = 2.69 (314.6 examples/sec; 0.407 sec/batch)
2017-04-02 22:56:30.890232: step 2270, loss = 2.69 (313.5 examples/sec; 0.408 sec/batch)
2017-04-02 22:56:35.005763: step 2280, loss = 2.69 (311.0 examples/sec; 0.412 sec/batch)
2017-04-02 22:56:39.089211: step 2290, loss = 2.68 (313.5 examples/sec; 0.408 sec/batch)
2017-04-02 22:56:43.239253: step 2300, loss = 2.68 (308.4 examples/sec; 0.415 sec/batch)
2017-04-02 22:56:47.327445: step 2310, loss = 2.68 (313.1 examples/sec; 0.409 sec/batch)
2017-04-02 22:56:51.436587: step 2320, loss = 2.68 (311.5 examples/sec; 0.411 sec/batch)
2017-04-02 22:56:55.555693: step 2330, loss = 2.67 (310.7 examples/sec; 0.412 sec/batch)
2017-04-02 22:56:59.681331: step 2340, loss = 2.67 (310.3 examples/sec; 0.413 sec/batch)
2017-04-02 22:57:03.778957: step 2350, loss = 2.66 (312.4 examples/sec; 0.410 sec/batch)
2017-04-02 22:57:07.867188: step 2360, loss = 2.66 (313.1 examples/sec; 0.409 sec/batch)
2017-04-02 22:57:11.997772: step 2370, loss = 2.66 (309.9 examples/sec; 0.413 sec/batch)
2017-04-02 22:57:16.091545: step 2380, loss = 2.66 (312.7 examples/sec; 0.409 sec/batch)
2017-04-02 22:57:20.185279: step 2390, loss = 2.66 (312.7 examples/sec; 0.409 sec/batch)
2017-04-02 22:57:24.368005: step 2400, loss = 2.65 (306.0 examples/sec; 0.418 sec/batch)
2017-04-02 22:57:28.485849: step 2410, loss = 2.65 (310.8 examples/sec; 0.412 sec/batch)
2017-04-02 22:57:32.569121: step 2420, loss = 2.64 (313.5 examples/sec; 0.408 sec/batch)
2017-04-02 22:57:36.655944: step 2430, loss = 2.64 (313.2 examples/sec; 0.409 sec/batch)
2017-04-02 22:57:40.748302: step 2440, loss = 2.64 (312.8 examples/sec; 0.409 sec/batch)
2017-04-02 22:57:44.885514: step 2450, loss = 2.64 (309.4 examples/sec; 0.414 sec/batch)
2017-04-02 22:57:48.973351: step 2460, loss = 2.64 (313.1 examples/sec; 0.409 sec/batch)
2017-04-02 22:57:53.034642: step 2470, loss = 2.63 (315.2 examples/sec; 0.406 sec/batch)
2017-04-02 22:57:57.079658: step 2480, loss = 2.63 (316.4 examples/sec; 0.405 sec/batch)
2017-04-02 22:58:01.183836: step 2490, loss = 2.62 (311.9 examples/sec; 0.410 sec/batch)
2017-04-02 22:58:05.344334: step 2500, loss = 2.63 (307.7 examples/sec; 0.416 sec/batch)
2017-04-02 22:58:09.423536: step 2510, loss = 2.62 (313.8 examples/sec; 0.408 sec/batch)
2017-04-02 22:58:13.534906: step 2520, loss = 2.62 (311.3 examples/sec; 0.411 sec/batch)
2017-04-02 22:58:17.686860: step 2530, loss = 2.62 (308.3 examples/sec; 0.415 sec/batch)
2017-04-02 22:58:21.798927: step 2540, loss = 2.61 (311.3 examples/sec; 0.411 sec/batch)
2017-04-02 22:58:25.902240: step 2550, loss = 2.61 (311.9 examples/sec; 0.410 sec/batch)
2017-04-02 22:58:29.998328: step 2560, loss = 2.61 (312.5 examples/sec; 0.410 sec/batch)
2017-04-02 22:58:34.100418: step 2570, loss = 2.61 (312.0 examples/sec; 0.410 sec/batch)
2017-04-02 22:58:38.210837: step 2580, loss = 2.60 (311.4 examples/sec; 0.411 sec/batch)
2017-04-02 22:58:42.295034: step 2590, loss = 2.60 (313.4 examples/sec; 0.408 sec/batch)
2017-04-02 22:58:46.460185: step 2600, loss = 2.60 (307.3 examples/sec; 0.417 sec/batch)
2017-04-02 22:58:50.579952: step 2610, loss = 2.60 (310.7 examples/sec; 0.412 sec/batch)
2017-04-02 22:58:55.274040: step 2620, loss = 2.60 (272.7 examples/sec; 0.469 sec/batch)
2017-04-02 22:58:59.336439: step 2630, loss = 2.59 (315.1 examples/sec; 0.406 sec/batch)
2017-04-02 22:59:03.414407: step 2640, loss = 2.59 (313.9 examples/sec; 0.408 sec/batch)
2017-04-02 22:59:07.489652: step 2650, loss = 2.58 (314.1 examples/sec; 0.408 sec/batch)
2017-04-02 22:59:11.570952: step 2660, loss = 2.58 (313.6 examples/sec; 0.408 sec/batch)
2017-04-02 22:59:15.676228: step 2670, loss = 2.58 (311.8 examples/sec; 0.411 sec/batch)
2017-04-02 22:59:19.751905: step 2680, loss = 2.58 (314.1 examples/sec; 0.408 sec/batch)
2017-04-02 22:59:23.834854: step 2690, loss = 2.58 (313.5 examples/sec; 0.408 sec/batch)
2017-04-02 22:59:27.994942: step 2700, loss = 2.58 (307.7 examples/sec; 0.416 sec/batch)
2017-04-02 22:59:32.065236: step 2710, loss = 2.58 (314.5 examples/sec; 0.407 sec/batch)
2017-04-02 22:59:36.124156: step 2720, loss = 2.57 (315.4 examples/sec; 0.406 sec/batch)
2017-04-02 22:59:40.189872: step 2730, loss = 2.57 (314.8 examples/sec; 0.407 sec/batch)
2017-04-02 22:59:44.225935: step 2740, loss = 2.57 (317.1 examples/sec; 0.404 sec/batch)
2017-04-02 22:59:48.268285: step 2750, loss = 2.57 (316.6 examples/sec; 0.404 sec/batch)
2017-04-02 22:59:52.318983: step 2760, loss = 2.56 (316.0 examples/sec; 0.405 sec/batch)
2017-04-02 22:59:56.393873: step 2770, loss = 2.56 (314.1 examples/sec; 0.407 sec/batch)
2017-04-02 23:00:00.493828: step 2780, loss = 2.56 (312.2 examples/sec; 0.410 sec/batch)
2017-04-02 23:00:04.561031: step 2790, loss = 2.56 (314.7 examples/sec; 0.407 sec/batch)
2017-04-02 23:00:08.644165: step 2800, loss = 2.56 (313.5 examples/sec; 0.408 sec/batch)
2017-04-02 23:00:12.713895: step 2810, loss = 2.55 (314.5 examples/sec; 0.407 sec/batch)
2017-04-02 23:00:16.819230: step 2820, loss = 2.55 (311.8 examples/sec; 0.411 sec/batch)
2017-04-02 23:00:20.888347: step 2830, loss = 2.55 (314.6 examples/sec; 0.407 sec/batch)
2017-04-02 23:00:24.982156: step 2840, loss = 2.55 (312.7 examples/sec; 0.409 sec/batch)
2017-04-02 23:00:29.062286: step 2850, loss = 2.55 (313.7 examples/sec; 0.408 sec/batch)
2017-04-02 23:00:33.239266: step 2860, loss = 2.54 (306.4 examples/sec; 0.418 sec/batch)
2017-04-02 23:00:37.302247: step 2870, loss = 2.54 (315.0 examples/sec; 0.406 sec/batch)
2017-04-02 23:00:41.381371: step 2880, loss = 2.54 (313.8 examples/sec; 0.408 sec/batch)
2017-04-02 23:00:45.560414: step 2890, loss = 2.54 (306.3 examples/sec; 0.418 sec/batch)
2017-04-02 23:00:49.667296: step 2900, loss = 2.54 (311.7 examples/sec; 0.411 sec/batch)
2017-04-02 23:00:53.755742: step 2910, loss = 2.54 (313.1 examples/sec; 0.409 sec/batch)
2017-04-02 23:00:57.798673: step 2920, loss = 2.54 (316.6 examples/sec; 0.404 sec/batch)
2017-04-02 23:01:01.909846: step 2930, loss = 2.53 (311.3 examples/sec; 0.411 sec/batch)
2017-04-02 23:01:05.985811: step 2940, loss = 2.53 (314.0 examples/sec; 0.408 sec/batch)
2017-04-02 23:01:10.041180: step 2950, loss = 2.53 (315.6 examples/sec; 0.406 sec/batch)
2017-04-02 23:01:14.086346: step 2960, loss = 2.52 (316.4 examples/sec; 0.405 sec/batch)
2017-04-02 23:01:18.136886: step 2970, loss = 2.52 (316.0 examples/sec; 0.405 sec/batch)
2017-04-02 23:01:22.189286: step 2980, loss = 2.52 (315.9 examples/sec; 0.405 sec/batch)
2017-04-02 23:01:26.280436: step 2990, loss = 2.52 (312.9 examples/sec; 0.409 sec/batch)
2017-04-02 23:01:30.392253: step 3000, loss = 2.52 (311.3 examples/sec; 0.411 sec/batch)
2017-04-02 23:01:34.479007: step 3010, loss = 2.52 (313.2 examples/sec; 0.409 sec/batch)
2017-04-02 23:01:38.549515: step 3020, loss = 2.52 (314.5 examples/sec; 0.407 sec/batch)
2017-04-02 23:01:42.597341: step 3030, loss = 2.51 (316.2 examples/sec; 0.405 sec/batch)
2017-04-02 23:01:46.647188: step 3040, loss = 2.51 (316.1 examples/sec; 0.405 sec/batch)
2017-04-02 23:01:50.699770: step 3050, loss = 2.51 (315.8 examples/sec; 0.405 sec/batch)
2017-04-02 23:01:54.784314: step 3060, loss = 2.51 (313.4 examples/sec; 0.408 sec/batch)
2017-04-02 23:01:58.855323: step 3070, loss = 2.51 (314.4 examples/sec; 0.407 sec/batch)
2017-04-02 23:02:02.934951: step 3080, loss = 2.51 (313.8 examples/sec; 0.408 sec/batch)
2017-04-02 23:02:06.985718: step 3090, loss = 2.50 (316.0 examples/sec; 0.405 sec/batch)
2017-04-02 23:02:11.120104: step 3100, loss = 2.50 (309.6 examples/sec; 0.413 sec/batch)
2017-04-02 23:02:15.182295: step 3110, loss = 2.50 (315.1 examples/sec; 0.406 sec/batch)
2017-04-02 23:02:19.224806: step 3120, loss = 2.50 (316.6 examples/sec; 0.404 sec/batch)
2017-04-02 23:02:23.284142: step 3130, loss = 2.50 (315.3 examples/sec; 0.406 sec/batch)
2017-04-02 23:02:27.305928: step 3140, loss = 2.50 (318.3 examples/sec; 0.402 sec/batch)
2017-04-02 23:02:31.357589: step 3150, loss = 2.49 (315.9 examples/sec; 0.405 sec/batch)
2017-04-02 23:02:35.442268: step 3160, loss = 2.49 (313.4 examples/sec; 0.408 sec/batch)
2017-04-02 23:02:39.490400: step 3170, loss = 2.49 (316.2 examples/sec; 0.405 sec/batch)
2017-04-02 23:02:43.555786: step 3180, loss = 2.49 (314.9 examples/sec; 0.407 sec/batch)
2017-04-02 23:02:47.616129: step 3190, loss = 2.49 (315.2 examples/sec; 0.406 sec/batch)
2017-04-02 23:02:51.742819: step 3200, loss = 2.49 (310.2 examples/sec; 0.413 sec/batch)
2017-04-02 23:02:55.832657: step 3210, loss = 2.49 (313.0 examples/sec; 0.409 sec/batch)
2017-04-02 23:02:59.909671: step 3220, loss = 2.49 (314.0 examples/sec; 0.408 sec/batch)
2017-04-02 23:03:03.967208: step 3230, loss = 2.48 (315.5 examples/sec; 0.406 sec/batch)
2017-04-02 23:03:08.010154: step 3240, loss = 2.48 (316.6 examples/sec; 0.404 sec/batch)
2017-04-02 23:03:12.091043: step 3250, loss = 2.48 (313.7 examples/sec; 0.408 sec/batch)
2017-04-02 23:03:16.205426: step 3260, loss = 2.48 (311.1 examples/sec; 0.411 sec/batch)
2017-04-02 23:03:20.287047: step 3270, loss = 2.48 (313.6 examples/sec; 0.408 sec/batch)
2017-04-02 23:03:24.371148: step 3280, loss = 2.48 (313.4 examples/sec; 0.408 sec/batch)
2017-04-02 23:03:28.422709: step 3290, loss = 2.48 (315.9 examples/sec; 0.405 sec/batch)
2017-04-02 23:03:32.559822: step 3300, loss = 2.47 (309.4 examples/sec; 0.414 sec/batch)
2017-04-02 23:03:36.625756: step 3310, loss = 2.47 (314.8 examples/sec; 0.407 sec/batch)
2017-04-02 23:03:40.694144: step 3320, loss = 2.47 (314.6 examples/sec; 0.407 sec/batch)
2017-04-02 23:03:44.801808: step 3330, loss = 2.46 (311.6 examples/sec; 0.411 sec/batch)
2017-04-02 23:03:48.894344: step 3340, loss = 2.46 (312.8 examples/sec; 0.409 sec/batch)
2017-04-02 23:03:52.997991: step 3350, loss = 2.46 (311.9 examples/sec; 0.410 sec/batch)
2017-04-02 23:03:57.147978: step 3360, loss = 2.47 (308.4 examples/sec; 0.415 sec/batch)
2017-04-02 23:04:01.256188: step 3370, loss = 2.46 (311.6 examples/sec; 0.411 sec/batch)
2017-04-02 23:04:05.394523: step 3380, loss = 2.46 (309.3 examples/sec; 0.414 sec/batch)
2017-04-02 23:04:09.519352: step 3390, loss = 2.47 (310.3 examples/sec; 0.412 sec/batch)
2017-04-02 23:04:13.670289: step 3400, loss = 2.46 (308.4 examples/sec; 0.415 sec/batch)
2017-04-02 23:04:17.791112: step 3410, loss = 2.46 (310.6 examples/sec; 0.412 sec/batch)
2017-04-02 23:04:21.893776: step 3420, loss = 2.46 (312.0 examples/sec; 0.410 sec/batch)
2017-04-02 23:04:25.991641: step 3430, loss = 2.45 (312.4 examples/sec; 0.410 sec/batch)
2017-04-02 23:04:30.083708: step 3440, loss = 2.45 (312.8 examples/sec; 0.409 sec/batch)
2017-04-02 23:04:34.211358: step 3450, loss = 2.45 (310.1 examples/sec; 0.413 sec/batch)
2017-04-02 23:04:38.345290: step 3460, loss = 2.45 (309.6 examples/sec; 0.413 sec/batch)
2017-04-02 23:04:42.446732: step 3470, loss = 2.45 (312.1 examples/sec; 0.410 sec/batch)
2017-04-02 23:04:46.569749: step 3480, loss = 2.45 (310.5 examples/sec; 0.412 sec/batch)
2017-04-02 23:04:50.699738: step 3490, loss = 2.45 (309.9 examples/sec; 0.413 sec/batch)
2017-04-02 23:04:54.978494: step 3500, loss = 2.45 (299.2 examples/sec; 0.428 sec/batch)
2017-04-02 23:04:59.574336: step 3510, loss = 2.44 (278.5 examples/sec; 0.460 sec/batch)
2017-04-02 23:05:03.669286: step 3520, loss = 2.44 (312.6 examples/sec; 0.409 sec/batch)
2017-04-02 23:05:07.782388: step 3530, loss = 2.44 (311.2 examples/sec; 0.411 sec/batch)
2017-04-02 23:05:11.910487: step 3540, loss = 2.44 (310.1 examples/sec; 0.413 sec/batch)
2017-04-02 23:05:15.972625: step 3550, loss = 2.44 (315.1 examples/sec; 0.406 sec/batch)
2017-04-02 23:05:20.083745: step 3560, loss = 2.44 (311.4 examples/sec; 0.411 sec/batch)
2017-04-02 23:05:24.198714: step 3570, loss = 2.44 (311.1 examples/sec; 0.411 sec/batch)
2017-04-02 23:05:28.334849: step 3580, loss = 2.44 (309.5 examples/sec; 0.414 sec/batch)
2017-04-02 23:05:32.466334: step 3590, loss = 2.43 (309.8 examples/sec; 0.413 sec/batch)
2017-04-02 23:05:36.655524: step 3600, loss = 2.43 (305.5 examples/sec; 0.419 sec/batch)
2017-04-02 23:05:40.757899: step 3610, loss = 2.43 (312.0 examples/sec; 0.410 sec/batch)
2017-04-02 23:05:44.841963: step 3620, loss = 2.43 (313.4 examples/sec; 0.408 sec/batch)
2017-04-02 23:05:49.082125: step 3630, loss = 2.44 (301.9 examples/sec; 0.424 sec/batch)
2017-04-02 23:05:53.613897: step 3640, loss = 2.43 (282.5 examples/sec; 0.453 sec/batch)
2017-04-02 23:05:57.764414: step 3650, loss = 2.43 (308.4 examples/sec; 0.415 sec/batch)
2017-04-02 23:06:01.923865: step 3660, loss = 2.43 (307.7 examples/sec; 0.416 sec/batch)
2017-04-02 23:06:06.034945: step 3670, loss = 2.43 (311.4 examples/sec; 0.411 sec/batch)
2017-04-02 23:06:10.206826: step 3680, loss = 2.43 (306.8 examples/sec; 0.417 sec/batch)
2017-04-02 23:06:14.346235: step 3690, loss = 2.43 (309.2 examples/sec; 0.414 sec/batch)
2017-04-02 23:06:18.527564: step 3700, loss = 2.43 (306.1 examples/sec; 0.418 sec/batch)
2017-04-02 23:06:22.658870: step 3710, loss = 2.42 (309.8 examples/sec; 0.413 sec/batch)
2017-04-02 23:06:26.759036: step 3720, loss = 2.43 (312.2 examples/sec; 0.410 sec/batch)
2017-04-02 23:06:30.881289: step 3730, loss = 2.42 (310.5 examples/sec; 0.412 sec/batch)
2017-04-02 23:06:34.985402: step 3740, loss = 2.42 (311.9 examples/sec; 0.410 sec/batch)
2017-04-02 23:06:39.117553: step 3750, loss = 2.42 (309.8 examples/sec; 0.413 sec/batch)
2017-04-02 23:06:43.235060: step 3760, loss = 2.42 (310.9 examples/sec; 0.412 sec/batch)
2017-04-02 23:06:47.362970: step 3770, loss = 2.42 (310.1 examples/sec; 0.413 sec/batch)
2017-04-02 23:06:51.471401: step 3780, loss = 2.42 (311.6 examples/sec; 0.411 sec/batch)
2017-04-02 23:06:55.579791: step 3790, loss = 2.42 (311.6 examples/sec; 0.411 sec/batch)
2017-04-02 23:06:59.738097: step 3800, loss = 2.42 (307.8 examples/sec; 0.416 sec/batch)
2017-04-02 23:07:03.860280: step 3810, loss = 2.41 (310.5 examples/sec; 0.412 sec/batch)
2017-04-02 23:07:07.967590: step 3820, loss = 2.42 (311.6 examples/sec; 0.411 sec/batch)
2017-04-02 23:07:12.095124: step 3830, loss = 2.41 (310.1 examples/sec; 0.413 sec/batch)
2017-04-02 23:07:16.189189: step 3840, loss = 2.41 (312.6 examples/sec; 0.409 sec/batch)
2017-04-02 23:07:20.316573: step 3850, loss = 2.41 (310.1 examples/sec; 0.413 sec/batch)
2017-04-02 23:07:24.415621: step 3860, loss = 2.41 (312.3 examples/sec; 0.410 sec/batch)
2017-04-02 23:07:28.532054: step 3870, loss = 2.41 (310.9 examples/sec; 0.412 sec/batch)
2017-04-02 23:07:32.651281: step 3880, loss = 2.41 (310.7 examples/sec; 0.412 sec/batch)
2017-04-02 23:07:36.752559: step 3890, loss = 2.41 (312.1 examples/sec; 0.410 sec/batch)
2017-04-02 23:07:40.923880: step 3900, loss = 2.41 (306.9 examples/sec; 0.417 sec/batch)
2017-04-02 23:07:45.065788: step 3910, loss = 2.40 (309.0 examples/sec; 0.414 sec/batch)
2017-04-02 23:07:49.160415: step 3920, loss = 2.41 (312.6 examples/sec; 0.409 sec/batch)
2017-04-02 23:07:53.269725: step 3930, loss = 2.41 (311.5 examples/sec; 0.411 sec/batch)
2017-04-02 23:07:57.368153: step 3940, loss = 2.41 (312.3 examples/sec; 0.410 sec/batch)
2017-04-02 23:08:01.462177: step 3950, loss = 2.40 (312.7 examples/sec; 0.409 sec/batch)
2017-04-02 23:08:05.572933: step 3960, loss = 2.40 (311.4 examples/sec; 0.411 sec/batch)
2017-04-02 23:08:09.667707: step 3970, loss = 2.40 (312.6 examples/sec; 0.409 sec/batch)
2017-04-02 23:08:13.793767: step 3980, loss = 2.40 (310.2 examples/sec; 0.413 sec/batch)
2017-04-02 23:08:17.892977: step 3990, loss = 2.40 (312.3 examples/sec; 0.410 sec/batch)
2017-04-02 23:08:22.059126: step 4000, loss = 2.40 (307.2 examples/sec; 0.417 sec/batch)
2017-04-02 23:08:26.153725: step 4010, loss = 2.40 (312.6 examples/sec; 0.409 sec/batch)
2017-04-02 23:08:30.271048: step 4020, loss = 2.40 (310.9 examples/sec; 0.412 sec/batch)
2017-04-02 23:08:34.382000: step 4030, loss = 2.40 (311.4 examples/sec; 0.411 sec/batch)
2017-04-02 23:08:38.478059: step 4040, loss = 2.40 (312.5 examples/sec; 0.410 sec/batch)
2017-04-02 23:10:21.768608: precision @ 1 = 0.103

2017-04-02 23:11:37.190547: step 4050, loss = 2.40 (7.2 examples/sec; 17.871 sec/batch)
2017-04-02 23:11:41.278256: step 4060, loss = 2.40 (313.1 examples/sec; 0.409 sec/batch)
2017-04-02 23:11:45.353797: step 4070, loss = 2.40 (314.1 examples/sec; 0.408 sec/batch)
2017-04-02 23:11:49.438625: step 4080, loss = 2.39 (313.4 examples/sec; 0.408 sec/batch)
2017-04-02 23:11:53.542252: step 4090, loss = 2.39 (311.9 examples/sec; 0.410 sec/batch)
2017-04-02 23:11:57.698024: step 4100, loss = 2.39 (308.0 examples/sec; 0.416 sec/batch)
2017-04-02 23:12:01.786203: step 4110, loss = 2.39 (313.1 examples/sec; 0.409 sec/batch)
2017-04-02 23:12:05.874935: step 4120, loss = 2.39 (313.1 examples/sec; 0.409 sec/batch)
2017-04-02 23:12:09.936862: step 4130, loss = 2.39 (315.1 examples/sec; 0.406 sec/batch)
2017-04-02 23:12:14.029057: step 4140, loss = 2.39 (312.8 examples/sec; 0.409 sec/batch)
2017-04-02 23:12:18.091479: step 4150, loss = 2.39 (315.1 examples/sec; 0.406 sec/batch)
2017-04-02 23:12:22.148460: step 4160, loss = 2.39 (315.5 examples/sec; 0.406 sec/batch)
2017-04-02 23:12:26.266033: step 4170, loss = 2.39 (310.9 examples/sec; 0.412 sec/batch)
2017-04-02 23:12:30.350887: step 4180, loss = 2.39 (313.4 examples/sec; 0.408 sec/batch)
2017-04-02 23:12:34.457286: step 4190, loss = 2.38 (311.7 examples/sec; 0.411 sec/batch)
2017-04-02 23:12:38.598901: step 4200, loss = 2.39 (309.1 examples/sec; 0.414 sec/batch)
2017-04-02 23:12:42.666875: step 4210, loss = 2.39 (314.7 examples/sec; 0.407 sec/batch)
2017-04-02 23:12:46.702670: step 4220, loss = 2.38 (317.2 examples/sec; 0.404 sec/batch)
2017-04-02 23:12:50.761419: step 4230, loss = 2.38 (315.4 examples/sec; 0.406 sec/batch)
2017-04-02 23:12:54.822447: step 4240, loss = 2.38 (315.2 examples/sec; 0.406 sec/batch)
2017-04-02 23:12:58.906248: step 4250, loss = 2.38 (313.4 examples/sec; 0.408 sec/batch)
2017-04-02 23:13:03.013881: step 4260, loss = 2.38 (311.6 examples/sec; 0.411 sec/batch)
2017-04-02 23:13:07.077260: step 4270, loss = 2.38 (315.0 examples/sec; 0.406 sec/batch)
2017-04-02 23:13:11.153102: step 4280, loss = 2.38 (314.0 examples/sec; 0.408 sec/batch)
2017-04-02 23:13:15.255016: step 4290, loss = 2.38 (312.0 examples/sec; 0.410 sec/batch)
2017-04-02 23:13:19.400100: step 4300, loss = 2.38 (308.8 examples/sec; 0.415 sec/batch)
2017-04-02 23:13:23.445338: step 4310, loss = 2.38 (316.4 examples/sec; 0.405 sec/batch)
2017-04-02 23:13:27.534184: step 4320, loss = 2.38 (313.0 examples/sec; 0.409 sec/batch)
2017-04-02 23:13:31.650066: step 4330, loss = 2.38 (311.0 examples/sec; 0.412 sec/batch)
2017-04-02 23:13:35.683864: step 4340, loss = 2.38 (317.3 examples/sec; 0.403 sec/batch)
2017-04-02 23:13:40.129738: step 4350, loss = 2.38 (287.9 examples/sec; 0.445 sec/batch)
2017-04-02 23:13:44.262075: step 4360, loss = 2.37 (309.8 examples/sec; 0.413 sec/batch)
2017-04-02 23:13:48.389984: step 4370, loss = 2.37 (310.1 examples/sec; 0.413 sec/batch)
2017-04-02 23:13:52.447302: step 4380, loss = 2.38 (315.5 examples/sec; 0.406 sec/batch)
2017-04-02 23:13:56.529804: step 4390, loss = 2.37 (313.5 examples/sec; 0.408 sec/batch)
2017-04-02 23:14:00.667536: step 4400, loss = 2.37 (309.3 examples/sec; 0.414 sec/batch)
2017-04-02 23:14:04.782525: step 4410, loss = 2.37 (311.1 examples/sec; 0.411 sec/batch)
2017-04-02 23:14:08.863506: step 4420, loss = 2.37 (313.7 examples/sec; 0.408 sec/batch)
2017-04-02 23:14:12.922788: step 4430, loss = 2.37 (315.3 examples/sec; 0.406 sec/batch)
2017-04-02 23:14:16.999944: step 4440, loss = 2.37 (313.9 examples/sec; 0.408 sec/batch)
2017-04-02 23:14:21.082397: step 4450, loss = 2.37 (313.5 examples/sec; 0.408 sec/batch)
2017-04-02 23:14:25.145762: step 4460, loss = 2.37 (315.0 examples/sec; 0.406 sec/batch)
2017-04-02 23:14:29.213962: step 4470, loss = 2.37 (314.6 examples/sec; 0.407 sec/batch)
2017-04-02 23:14:33.279919: step 4480, loss = 2.37 (314.8 examples/sec; 0.407 sec/batch)
2017-04-02 23:14:37.353568: step 4490, loss = 2.37 (314.2 examples/sec; 0.407 sec/batch)
2017-04-02 23:14:41.501038: step 4500, loss = 2.37 (308.6 examples/sec; 0.415 sec/batch)
2017-04-02 23:14:45.613394: step 4510, loss = 2.37 (311.3 examples/sec; 0.411 sec/batch)
2017-04-02 23:14:49.695425: step 4520, loss = 2.37 (313.6 examples/sec; 0.408 sec/batch)
2017-04-02 23:14:53.799941: step 4530, loss = 2.36 (311.9 examples/sec; 0.410 sec/batch)
2017-04-02 23:14:57.883229: step 4540, loss = 2.37 (313.5 examples/sec; 0.408 sec/batch)
2017-04-02 23:15:01.942553: step 4550, loss = 2.37 (315.3 examples/sec; 0.406 sec/batch)
2017-04-02 23:15:06.032637: step 4560, loss = 2.37 (313.0 examples/sec; 0.409 sec/batch)
2017-04-02 23:15:10.109981: step 4570, loss = 2.37 (313.9 examples/sec; 0.408 sec/batch)
2017-04-02 23:15:14.238943: step 4580, loss = 2.36 (310.0 examples/sec; 0.413 sec/batch)
2017-04-02 23:15:18.349972: step 4590, loss = 2.36 (311.4 examples/sec; 0.411 sec/batch)
2017-04-02 23:15:22.500994: step 4600, loss = 2.36 (308.4 examples/sec; 0.415 sec/batch)
2017-04-02 23:15:26.602138: step 4610, loss = 2.36 (312.1 examples/sec; 0.410 sec/batch)
2017-04-02 23:15:30.714473: step 4620, loss = 2.36 (311.3 examples/sec; 0.411 sec/batch)
2017-04-02 23:15:34.823413: step 4630, loss = 2.36 (311.5 examples/sec; 0.411 sec/batch)
2017-04-02 23:15:38.891603: step 4640, loss = 2.36 (314.6 examples/sec; 0.407 sec/batch)
2017-04-02 23:15:43.017727: step 4650, loss = 2.36 (310.2 examples/sec; 0.413 sec/batch)
2017-04-02 23:15:47.084248: step 4660, loss = 2.36 (314.8 examples/sec; 0.407 sec/batch)
2017-04-02 23:15:51.158239: step 4670, loss = 2.36 (314.2 examples/sec; 0.407 sec/batch)
2017-04-02 23:15:55.224920: step 4680, loss = 2.36 (314.8 examples/sec; 0.407 sec/batch)
2017-04-02 23:15:59.321181: step 4690, loss = 2.36 (312.5 examples/sec; 0.410 sec/batch)
2017-04-02 23:16:03.501021: step 4700, loss = 2.36 (306.2 examples/sec; 0.418 sec/batch)
2017-04-02 23:16:07.597213: step 4710, loss = 2.36 (312.5 examples/sec; 0.410 sec/batch)
2017-04-02 23:16:11.674883: step 4720, loss = 2.36 (313.9 examples/sec; 0.408 sec/batch)
2017-04-02 23:16:15.763364: step 4730, loss = 2.36 (313.1 examples/sec; 0.409 sec/batch)
2017-04-02 23:16:19.842507: step 4740, loss = 2.36 (313.8 examples/sec; 0.408 sec/batch)
2017-04-02 23:16:23.910568: step 4750, loss = 2.36 (314.6 examples/sec; 0.407 sec/batch)
2017-04-02 23:16:27.999860: step 4760, loss = 2.36 (313.0 examples/sec; 0.409 sec/batch)
2017-04-02 23:16:32.093845: step 4770, loss = 2.36 (312.7 examples/sec; 0.409 sec/batch)
2017-04-02 23:16:36.181221: step 4780, loss = 2.36 (313.2 examples/sec; 0.409 sec/batch)
2017-04-02 23:16:40.256628: step 4790, loss = 2.35 (314.1 examples/sec; 0.408 sec/batch)
2017-04-02 23:16:44.430697: step 4800, loss = 2.36 (306.7 examples/sec; 0.417 sec/batch)
2017-04-02 23:16:48.533621: step 4810, loss = 2.35 (312.0 examples/sec; 0.410 sec/batch)
2017-04-02 23:16:52.614608: step 4820, loss = 2.35 (313.6 examples/sec; 0.408 sec/batch)
2017-04-02 23:16:56.688519: step 4830, loss = 2.35 (314.2 examples/sec; 0.407 sec/batch)
2017-04-02 23:17:00.811075: step 4840, loss = 2.35 (310.5 examples/sec; 0.412 sec/batch)
2017-04-02 23:17:04.930212: step 4850, loss = 2.35 (310.7 examples/sec; 0.412 sec/batch)
2017-04-02 23:17:09.004094: step 4860, loss = 2.35 (314.2 examples/sec; 0.407 sec/batch)
2017-04-02 23:17:13.102900: step 4870, loss = 2.35 (312.3 examples/sec; 0.410 sec/batch)
2017-04-02 23:17:17.167099: step 4880, loss = 2.35 (314.9 examples/sec; 0.406 sec/batch)
2017-04-02 23:17:21.236292: step 4890, loss = 2.35 (314.6 examples/sec; 0.407 sec/batch)
2017-04-02 23:17:25.379662: step 4900, loss = 2.35 (308.9 examples/sec; 0.414 sec/batch)
2017-04-02 23:17:29.497868: step 4910, loss = 2.35 (310.8 examples/sec; 0.412 sec/batch)
2017-04-02 23:17:33.597608: step 4920, loss = 2.35 (312.2 examples/sec; 0.410 sec/batch)
2017-04-02 23:17:37.681100: step 4930, loss = 2.35 (313.5 examples/sec; 0.408 sec/batch)
2017-04-02 23:17:41.810251: step 4940, loss = 2.35 (310.0 examples/sec; 0.413 sec/batch)
2017-04-02 23:17:45.888884: step 4950, loss = 2.35 (313.8 examples/sec; 0.408 sec/batch)
2017-04-02 23:17:49.981407: step 4960, loss = 2.35 (312.8 examples/sec; 0.409 sec/batch)
2017-04-02 23:17:54.066614: step 4970, loss = 2.35 (313.3 examples/sec; 0.409 sec/batch)
2017-04-02 23:17:58.171602: step 4980, loss = 2.35 (311.8 examples/sec; 0.410 sec/batch)
2017-04-02 23:18:02.263121: step 4990, loss = 2.35 (312.8 examples/sec; 0.409 sec/batch)
2017-04-02 23:18:06.403603: step 5000, loss = 2.35 (309.1 examples/sec; 0.414 sec/batch)
2017-04-02 23:18:10.470715: step 5010, loss = 2.35 (314.7 examples/sec; 0.407 sec/batch)
2017-04-02 23:18:14.543544: step 5020, loss = 2.35 (314.3 examples/sec; 0.407 sec/batch)
2017-04-02 23:18:18.631632: step 5030, loss = 2.34 (313.1 examples/sec; 0.409 sec/batch)
2017-04-02 23:18:22.730947: step 5040, loss = 2.34 (312.2 examples/sec; 0.410 sec/batch)
2017-04-02 23:18:26.807151: step 5050, loss = 2.34 (314.0 examples/sec; 0.408 sec/batch)
2017-04-02 23:18:30.898014: step 5060, loss = 2.35 (312.9 examples/sec; 0.409 sec/batch)
2017-04-02 23:18:34.999022: step 5070, loss = 2.34 (312.1 examples/sec; 0.410 sec/batch)
2017-04-02 23:18:39.105098: step 5080, loss = 2.34 (311.7 examples/sec; 0.411 sec/batch)
2017-04-02 23:18:43.229455: step 5090, loss = 2.34 (310.4 examples/sec; 0.412 sec/batch)
2017-04-02 23:18:47.399356: step 5100, loss = 2.34 (307.0 examples/sec; 0.417 sec/batch)
2017-04-02 23:18:51.549718: step 5110, loss = 2.34 (308.4 examples/sec; 0.415 sec/batch)
2017-04-02 23:18:55.662717: step 5120, loss = 2.34 (311.2 examples/sec; 0.411 sec/batch)
2017-04-02 23:18:59.761016: step 5130, loss = 2.34 (312.3 examples/sec; 0.410 sec/batch)
2017-04-02 23:19:03.873679: step 5140, loss = 2.34 (311.2 examples/sec; 0.411 sec/batch)
2017-04-02 23:19:07.956709: step 5150, loss = 2.34 (313.5 examples/sec; 0.408 sec/batch)
2017-04-02 23:19:12.091639: step 5160, loss = 2.35 (309.6 examples/sec; 0.413 sec/batch)
2017-04-02 23:19:16.229094: step 5170, loss = 2.34 (309.4 examples/sec; 0.414 sec/batch)
2017-04-02 23:19:20.342855: step 5180, loss = 2.34 (311.2 examples/sec; 0.411 sec/batch)
2017-04-02 23:19:24.410772: step 5190, loss = 2.34 (314.7 examples/sec; 0.407 sec/batch)
2017-04-02 23:19:28.584457: step 5200, loss = 2.34 (306.7 examples/sec; 0.417 sec/batch)
2017-04-02 23:19:32.708142: step 5210, loss = 2.34 (310.4 examples/sec; 0.412 sec/batch)
2017-04-02 23:19:36.794387: step 5220, loss = 2.34 (313.2 examples/sec; 0.409 sec/batch)
2017-04-02 23:19:40.890707: step 5230, loss = 2.34 (312.5 examples/sec; 0.410 sec/batch)
2017-04-02 23:19:45.017303: step 5240, loss = 2.34 (310.2 examples/sec; 0.413 sec/batch)
2017-04-02 23:19:49.074557: step 5250, loss = 2.34 (315.5 examples/sec; 0.406 sec/batch)
2017-04-02 23:19:53.157667: step 5260, loss = 2.34 (313.5 examples/sec; 0.408 sec/batch)
2017-04-02 23:19:57.291559: step 5270, loss = 2.34 (309.6 examples/sec; 0.413 sec/batch)
2017-04-02 23:20:01.403884: step 5280, loss = 2.34 (311.3 examples/sec; 0.411 sec/batch)
2017-04-02 23:20:05.491788: step 5290, loss = 2.34 (313.1 examples/sec; 0.409 sec/batch)
2017-04-02 23:20:09.757808: step 5300, loss = 2.34 (300.0 examples/sec; 0.427 sec/batch)
2017-04-02 23:20:13.883100: step 5310, loss = 2.34 (310.3 examples/sec; 0.413 sec/batch)
2017-04-02 23:20:17.995059: step 5320, loss = 2.33 (311.3 examples/sec; 0.411 sec/batch)
2017-04-02 23:20:22.126107: step 5330, loss = 2.33 (309.8 examples/sec; 0.413 sec/batch)
2017-04-02 23:20:26.265306: step 5340, loss = 2.34 (309.2 examples/sec; 0.414 sec/batch)
2017-04-02 23:20:30.361585: step 5350, loss = 2.33 (312.5 examples/sec; 0.410 sec/batch)
2017-04-02 23:20:34.443839: step 5360, loss = 2.33 (313.6 examples/sec; 0.408 sec/batch)
2017-04-02 23:20:38.562049: step 5370, loss = 2.33 (310.8 examples/sec; 0.412 sec/batch)
2017-04-02 23:20:42.620174: step 5380, loss = 2.34 (315.4 examples/sec; 0.406 sec/batch)
2017-04-02 23:20:46.666420: step 5390, loss = 2.33 (316.3 examples/sec; 0.405 sec/batch)
2017-04-02 23:20:50.946314: step 5400, loss = 2.33 (299.1 examples/sec; 0.428 sec/batch)
2017-04-02 23:20:55.449738: step 5410, loss = 2.34 (284.2 examples/sec; 0.450 sec/batch)
2017-04-02 23:20:59.568661: step 5420, loss = 2.33 (310.8 examples/sec; 0.412 sec/batch)
2017-04-02 23:21:03.683437: step 5430, loss = 2.33 (311.1 examples/sec; 0.411 sec/batch)
2017-04-02 23:21:07.809727: step 5440, loss = 2.33 (310.2 examples/sec; 0.413 sec/batch)
2017-04-02 23:21:11.912075: step 5450, loss = 2.33 (312.0 examples/sec; 0.410 sec/batch)
2017-04-02 23:21:16.011315: step 5460, loss = 2.33 (312.3 examples/sec; 0.410 sec/batch)
2017-04-02 23:21:20.077788: step 5470, loss = 2.33 (314.8 examples/sec; 0.407 sec/batch)
2017-04-02 23:21:24.181088: step 5480, loss = 2.33 (311.9 examples/sec; 0.410 sec/batch)
2017-04-02 23:21:28.320889: step 5490, loss = 2.33 (309.2 examples/sec; 0.414 sec/batch)
2017-04-02 23:21:32.506378: step 5500, loss = 2.33 (305.8 examples/sec; 0.419 sec/batch)
2017-04-02 23:21:37.409368: step 5510, loss = 2.33 (261.1 examples/sec; 0.490 sec/batch)
2017-04-02 23:21:41.526421: step 5520, loss = 2.33 (310.9 examples/sec; 0.412 sec/batch)
2017-04-02 23:21:45.609630: step 5530, loss = 2.33 (313.5 examples/sec; 0.408 sec/batch)
2017-04-02 23:21:49.689640: step 5540, loss = 2.33 (313.7 examples/sec; 0.408 sec/batch)
2017-04-02 23:21:53.826313: step 5550, loss = 2.33 (309.4 examples/sec; 0.414 sec/batch)
2017-04-02 23:21:57.929256: step 5560, loss = 2.33 (312.0 examples/sec; 0.410 sec/batch)
2017-04-02 23:22:02.022353: step 5570, loss = 2.33 (312.7 examples/sec; 0.409 sec/batch)
2017-04-02 23:22:06.129792: step 5580, loss = 2.33 (311.6 examples/sec; 0.411 sec/batch)
2017-04-02 23:22:10.241466: step 5590, loss = 2.33 (311.3 examples/sec; 0.411 sec/batch)
2017-04-02 23:22:14.407400: step 5600, loss = 2.33 (307.3 examples/sec; 0.417 sec/batch)
2017-04-02 23:22:18.518778: step 5610, loss = 2.33 (311.3 examples/sec; 0.411 sec/batch)
2017-04-02 23:22:22.614030: step 5620, loss = 2.33 (312.6 examples/sec; 0.410 sec/batch)
2017-04-02 23:22:26.731967: step 5630, loss = 2.33 (310.8 examples/sec; 0.412 sec/batch)
2017-04-02 23:22:30.847906: step 5640, loss = 2.33 (311.0 examples/sec; 0.412 sec/batch)
2017-04-02 23:22:34.994173: step 5650, loss = 2.33 (308.7 examples/sec; 0.415 sec/batch)
2017-04-02 23:22:39.074723: step 5660, loss = 2.33 (313.7 examples/sec; 0.408 sec/batch)
2017-04-02 23:22:43.207965: step 5670, loss = 2.33 (309.7 examples/sec; 0.413 sec/batch)
2017-04-02 23:22:47.307925: step 5680, loss = 2.33 (312.2 examples/sec; 0.410 sec/batch)
2017-04-02 23:22:51.416073: step 5690, loss = 2.33 (311.6 examples/sec; 0.411 sec/batch)
2017-04-02 23:22:55.603891: step 5700, loss = 2.33 (305.6 examples/sec; 0.419 sec/batch)
2017-04-02 23:22:59.762260: step 5710, loss = 2.33 (307.8 examples/sec; 0.416 sec/batch)
2017-04-02 23:23:03.879858: step 5720, loss = 2.33 (310.9 examples/sec; 0.412 sec/batch)
2017-04-02 23:23:07.999982: step 5730, loss = 2.33 (310.7 examples/sec; 0.412 sec/batch)
2017-04-02 23:23:12.110973: step 5740, loss = 2.33 (311.4 examples/sec; 0.411 sec/batch)
2017-04-02 23:23:16.221674: step 5750, loss = 2.33 (311.4 examples/sec; 0.411 sec/batch)
2017-04-02 23:23:20.373345: step 5760, loss = 2.33 (308.3 examples/sec; 0.415 sec/batch)
2017-04-02 23:23:24.481167: step 5770, loss = 2.33 (311.6 examples/sec; 0.411 sec/batch)
2017-04-02 23:23:28.602406: step 5780, loss = 2.33 (310.6 examples/sec; 0.412 sec/batch)
2017-04-02 23:23:32.685035: step 5790, loss = 2.33 (313.5 examples/sec; 0.408 sec/batch)
2017-04-02 23:23:36.819943: step 5800, loss = 2.33 (309.6 examples/sec; 0.413 sec/batch)
2017-04-02 23:23:40.925735: step 5810, loss = 2.33 (311.8 examples/sec; 0.411 sec/batch)
2017-04-02 23:23:45.029589: step 5820, loss = 2.33 (311.9 examples/sec; 0.410 sec/batch)
2017-04-02 23:23:49.135454: step 5830, loss = 2.32 (311.7 examples/sec; 0.411 sec/batch)
2017-04-02 23:23:53.214149: step 5840, loss = 2.33 (313.8 examples/sec; 0.408 sec/batch)
2017-04-02 23:23:57.333503: step 5850, loss = 2.32 (310.7 examples/sec; 0.412 sec/batch)
2017-04-02 23:24:01.433724: step 5860, loss = 2.32 (312.2 examples/sec; 0.410 sec/batch)
2017-04-02 23:24:05.883997: step 5870, loss = 2.33 (287.6 examples/sec; 0.445 sec/batch)
2017-04-02 23:24:09.992977: step 5880, loss = 2.33 (311.5 examples/sec; 0.411 sec/batch)
2017-04-02 23:24:14.097334: step 5890, loss = 2.32 (311.9 examples/sec; 0.410 sec/batch)
2017-04-02 23:24:18.270538: step 5900, loss = 2.32 (306.7 examples/sec; 0.417 sec/batch)
2017-04-02 23:24:22.375641: step 5910, loss = 2.32 (311.8 examples/sec; 0.411 sec/batch)
2017-04-02 23:24:26.494510: step 5920, loss = 2.32 (310.8 examples/sec; 0.412 sec/batch)
2017-04-02 23:24:30.598534: step 5930, loss = 2.32 (311.9 examples/sec; 0.410 sec/batch)
2017-04-02 23:24:34.724008: step 5940, loss = 2.32 (310.3 examples/sec; 0.413 sec/batch)
2017-04-02 23:24:38.846171: step 5950, loss = 2.32 (310.5 examples/sec; 0.412 sec/batch)
2017-04-02 23:24:42.931290: step 5960, loss = 2.32 (313.3 examples/sec; 0.409 sec/batch)
2017-04-02 23:24:47.013480: step 5970, loss = 2.32 (313.6 examples/sec; 0.408 sec/batch)
2017-04-02 23:24:51.135327: step 5980, loss = 2.32 (310.5 examples/sec; 0.412 sec/batch)
2017-04-02 23:24:55.246896: step 5990, loss = 2.32 (311.3 examples/sec; 0.411 sec/batch)
2017-04-02 23:24:59.406534: step 6000, loss = 2.32 (307.7 examples/sec; 0.416 sec/batch)
2017-04-02 23:25:03.489986: step 6010, loss = 2.32 (313.5 examples/sec; 0.408 sec/batch)
2017-04-02 23:25:57.229811: precision @ 1 = 0.102

2017-04-02 23:26:28.455837: step 6020, loss = 2.32 (15.1 examples/sec; 8.497 sec/batch)
2017-04-02 23:26:32.553441: step 6030, loss = 2.32 (312.4 examples/sec; 0.410 sec/batch)
2017-04-02 23:26:36.691527: step 6040, loss = 2.32 (309.3 examples/sec; 0.414 sec/batch)
2017-04-02 23:26:40.807312: step 6050, loss = 2.32 (311.0 examples/sec; 0.412 sec/batch)
2017-04-02 23:26:44.899478: step 6060, loss = 2.32 (312.8 examples/sec; 0.409 sec/batch)
2017-04-02 23:26:49.014332: step 6070, loss = 2.32 (311.1 examples/sec; 0.411 sec/batch)
2017-04-02 23:26:53.165646: step 6080, loss = 2.32 (308.3 examples/sec; 0.415 sec/batch)
2017-04-02 23:26:57.275769: step 6090, loss = 2.32 (311.4 examples/sec; 0.411 sec/batch)
2017-04-02 23:27:01.409909: step 6100, loss = 2.32 (309.6 examples/sec; 0.413 sec/batch)
2017-04-02 23:27:05.547402: step 6110, loss = 2.32 (309.4 examples/sec; 0.414 sec/batch)
2017-04-02 23:27:09.641498: step 6120, loss = 2.32 (312.6 examples/sec; 0.409 sec/batch)
2017-04-02 23:27:13.713042: step 6130, loss = 2.32 (314.4 examples/sec; 0.407 sec/batch)
2017-04-02 23:27:17.854924: step 6140, loss = 2.32 (309.0 examples/sec; 0.414 sec/batch)
2017-04-02 23:27:21.966231: step 6150, loss = 2.32 (311.3 examples/sec; 0.411 sec/batch)
2017-04-02 23:27:26.059730: step 6160, loss = 2.32 (312.7 examples/sec; 0.409 sec/batch)
2017-04-02 23:27:30.198953: step 6170, loss = 2.32 (309.2 examples/sec; 0.414 sec/batch)
2017-04-02 23:27:34.298309: step 6180, loss = 2.32 (312.2 examples/sec; 0.410 sec/batch)
2017-04-02 23:27:38.422332: step 6190, loss = 2.32 (310.4 examples/sec; 0.412 sec/batch)
2017-04-02 23:27:42.632186: step 6200, loss = 2.32 (304.0 examples/sec; 0.421 sec/batch)
2017-04-02 23:27:46.758577: step 6210, loss = 2.32 (310.2 examples/sec; 0.413 sec/batch)
2017-04-02 23:27:50.897541: step 6220, loss = 2.32 (309.3 examples/sec; 0.414 sec/batch)
2017-04-02 23:27:55.038301: step 6230, loss = 2.32 (309.1 examples/sec; 0.414 sec/batch)
2017-04-02 23:27:59.141875: step 6240, loss = 2.32 (311.9 examples/sec; 0.410 sec/batch)
2017-04-02 23:28:03.262477: step 6250, loss = 2.32 (310.6 examples/sec; 0.412 sec/batch)
2017-04-02 23:28:07.365051: step 6260, loss = 2.32 (312.0 examples/sec; 0.410 sec/batch)
2017-04-02 23:28:11.501905: step 6270, loss = 2.32 (309.4 examples/sec; 0.414 sec/batch)
2017-04-02 23:28:15.634263: step 6280, loss = 2.32 (309.8 examples/sec; 0.413 sec/batch)
2017-04-02 23:28:19.746605: step 6290, loss = 2.32 (311.3 examples/sec; 0.411 sec/batch)
2017-04-02 23:28:23.929175: step 6300, loss = 2.32 (306.0 examples/sec; 0.418 sec/batch)
2017-04-02 23:28:28.075906: step 6310, loss = 2.32 (308.7 examples/sec; 0.415 sec/batch)
2017-04-02 23:28:32.213983: step 6320, loss = 2.32 (309.3 examples/sec; 0.414 sec/batch)
2017-04-02 23:28:36.355517: step 6330, loss = 2.32 (309.1 examples/sec; 0.414 sec/batch)
2017-04-02 23:28:40.495698: step 6340, loss = 2.32 (309.2 examples/sec; 0.414 sec/batch)
2017-04-02 23:28:44.649472: step 6350, loss = 2.32 (308.2 examples/sec; 0.415 sec/batch)
2017-04-02 23:28:48.840870: step 6360, loss = 2.32 (305.4 examples/sec; 0.419 sec/batch)
2017-04-02 23:28:52.974684: step 6370, loss = 2.32 (309.6 examples/sec; 0.413 sec/batch)
2017-04-02 23:28:57.134548: step 6380, loss = 2.32 (307.7 examples/sec; 0.416 sec/batch)
2017-04-02 23:29:01.274891: step 6390, loss = 2.32 (309.2 examples/sec; 0.414 sec/batch)
2017-04-02 23:29:05.439584: step 6400, loss = 2.32 (307.3 examples/sec; 0.416 sec/batch)
2017-04-02 23:29:09.576837: step 6410, loss = 2.32 (309.4 examples/sec; 0.414 sec/batch)
2017-04-02 23:29:13.722765: step 6420, loss = 2.32 (308.7 examples/sec; 0.415 sec/batch)
2017-04-02 23:29:17.840660: step 6430, loss = 2.32 (310.8 examples/sec; 0.412 sec/batch)
2017-04-02 23:29:21.966697: step 6440, loss = 2.31 (310.2 examples/sec; 0.413 sec/batch)
2017-04-02 23:29:26.137683: step 6450, loss = 2.32 (306.9 examples/sec; 0.417 sec/batch)
2017-04-02 23:29:30.260420: step 6460, loss = 2.31 (310.5 examples/sec; 0.412 sec/batch)
2017-04-02 23:29:34.432558: step 6470, loss = 2.31 (306.8 examples/sec; 0.417 sec/batch)
2017-04-02 23:29:38.567075: step 6480, loss = 2.31 (309.6 examples/sec; 0.413 sec/batch)
2017-04-02 23:29:42.693680: step 6490, loss = 2.32 (310.2 examples/sec; 0.413 sec/batch)
2017-04-02 23:29:46.879944: step 6500, loss = 2.32 (305.8 examples/sec; 0.419 sec/batch)
2017-04-02 23:29:50.972833: step 6510, loss = 2.32 (312.7 examples/sec; 0.409 sec/batch)
2017-04-02 23:29:55.093659: step 6520, loss = 2.32 (310.6 examples/sec; 0.412 sec/batch)
2017-04-02 23:29:59.174756: step 6530, loss = 2.31 (313.6 examples/sec; 0.408 sec/batch)
2017-04-02 23:30:03.321946: step 6540, loss = 2.32 (308.6 examples/sec; 0.415 sec/batch)
2017-04-02 23:30:07.435071: step 6550, loss = 2.32 (311.2 examples/sec; 0.411 sec/batch)
2017-04-02 23:30:11.530616: step 6560, loss = 2.32 (312.5 examples/sec; 0.410 sec/batch)
2017-04-02 23:30:15.654791: step 6570, loss = 2.32 (310.4 examples/sec; 0.412 sec/batch)
2017-04-02 23:30:19.759851: step 6580, loss = 2.31 (311.8 examples/sec; 0.411 sec/batch)
2017-04-02 23:30:23.891058: step 6590, loss = 2.32 (309.8 examples/sec; 0.413 sec/batch)
2017-04-02 23:30:28.092297: step 6600, loss = 2.32 (304.7 examples/sec; 0.420 sec/batch)
2017-04-02 23:30:32.160495: step 6610, loss = 2.32 (314.6 examples/sec; 0.407 sec/batch)
2017-04-02 23:30:36.253594: step 6620, loss = 2.31 (312.7 examples/sec; 0.409 sec/batch)
2017-04-02 23:30:40.361952: step 6630, loss = 2.31 (311.6 examples/sec; 0.411 sec/batch)
2017-04-02 23:30:44.426441: step 6640, loss = 2.32 (314.9 examples/sec; 0.406 sec/batch)
2017-04-02 23:30:48.535352: step 6650, loss = 2.32 (311.5 examples/sec; 0.411 sec/batch)
2017-04-02 23:30:52.638673: step 6660, loss = 2.32 (311.9 examples/sec; 0.410 sec/batch)
2017-04-02 23:30:56.744266: step 6670, loss = 2.32 (311.8 examples/sec; 0.411 sec/batch)
2017-04-02 23:31:00.857679: step 6680, loss = 2.31 (311.2 examples/sec; 0.411 sec/batch)
2017-04-02 23:31:04.986625: step 6690, loss = 2.31 (310.0 examples/sec; 0.413 sec/batch)
2017-04-02 23:31:09.140560: step 6700, loss = 2.31 (308.1 examples/sec; 0.415 sec/batch)
2017-04-02 23:31:13.242726: step 6710, loss = 2.31 (312.0 examples/sec; 0.410 sec/batch)
2017-04-02 23:31:17.349961: step 6720, loss = 2.31 (311.6 examples/sec; 0.411 sec/batch)
2017-04-02 23:31:21.464084: step 6730, loss = 2.32 (311.1 examples/sec; 0.411 sec/batch)
2017-04-02 23:31:25.582147: step 6740, loss = 2.31 (310.8 examples/sec; 0.412 sec/batch)
2017-04-02 23:31:29.662013: step 6750, loss = 2.32 (313.7 examples/sec; 0.408 sec/batch)
2017-04-02 23:31:33.755760: step 6760, loss = 2.31 (312.7 examples/sec; 0.409 sec/batch)
2017-04-02 23:31:38.642741: step 6770, loss = 2.31 (261.9 examples/sec; 0.489 sec/batch)
2017-04-02 23:31:42.801169: step 6780, loss = 2.31 (307.8 examples/sec; 0.416 sec/batch)
2017-04-02 23:31:46.878521: step 6790, loss = 2.31 (313.9 examples/sec; 0.408 sec/batch)
2017-04-02 23:31:51.026246: step 6800, loss = 2.31 (308.6 examples/sec; 0.415 sec/batch)
2017-04-02 23:31:55.126327: step 6810, loss = 2.31 (312.2 examples/sec; 0.410 sec/batch)
2017-04-02 23:31:59.233529: step 6820, loss = 2.31 (311.6 examples/sec; 0.411 sec/batch)
2017-04-02 23:32:03.331200: step 6830, loss = 2.31 (312.4 examples/sec; 0.410 sec/batch)
2017-04-02 23:32:07.435629: step 6840, loss = 2.31 (311.9 examples/sec; 0.410 sec/batch)
2017-04-02 23:32:11.540216: step 6850, loss = 2.31 (311.8 examples/sec; 0.410 sec/batch)
2017-04-02 23:32:15.636514: step 6860, loss = 2.31 (312.5 examples/sec; 0.410 sec/batch)
2017-04-02 23:32:19.725920: step 6870, loss = 2.31 (313.0 examples/sec; 0.409 sec/batch)
2017-04-02 23:32:23.839302: step 6880, loss = 2.31 (311.2 examples/sec; 0.411 sec/batch)
2017-04-02 23:32:27.942214: step 6890, loss = 2.31 (312.0 examples/sec; 0.410 sec/batch)
2017-04-02 23:32:32.150208: step 6900, loss = 2.31 (304.2 examples/sec; 0.421 sec/batch)
2017-04-02 23:32:36.225411: step 6910, loss = 2.31 (314.1 examples/sec; 0.408 sec/batch)
2017-04-02 23:32:40.313633: step 6920, loss = 2.31 (313.1 examples/sec; 0.409 sec/batch)
2017-04-02 23:32:44.394702: step 6930, loss = 2.31 (313.6 examples/sec; 0.408 sec/batch)
2017-04-02 23:32:48.509433: step 6940, loss = 2.31 (311.1 examples/sec; 0.411 sec/batch)
2017-04-02 23:32:52.615894: step 6950, loss = 2.31 (311.7 examples/sec; 0.411 sec/batch)
2017-04-02 23:32:56.671958: step 6960, loss = 2.31 (315.6 examples/sec; 0.406 sec/batch)
2017-04-02 23:33:00.789602: step 6970, loss = 2.32 (310.9 examples/sec; 0.412 sec/batch)
2017-04-02 23:33:04.877806: step 6980, loss = 2.31 (313.1 examples/sec; 0.409 sec/batch)
2017-04-02 23:33:08.994245: step 6990, loss = 2.31 (310.9 examples/sec; 0.412 sec/batch)
2017-04-02 23:33:13.230969: step 7000, loss = 2.31 (302.1 examples/sec; 0.424 sec/batch)
2017-04-02 23:33:17.337598: step 7010, loss = 2.31 (311.7 examples/sec; 0.411 sec/batch)
2017-04-02 23:33:21.454165: step 7020, loss = 2.31 (310.9 examples/sec; 0.412 sec/batch)
2017-04-02 23:33:25.582607: step 7030, loss = 2.31 (310.0 examples/sec; 0.413 sec/batch)
2017-04-02 23:33:29.695743: step 7040, loss = 2.31 (311.2 examples/sec; 0.411 sec/batch)
2017-04-02 23:33:33.801673: step 7050, loss = 2.31 (311.7 examples/sec; 0.411 sec/batch)
2017-04-02 23:33:37.964387: step 7060, loss = 2.31 (307.5 examples/sec; 0.416 sec/batch)
2017-04-02 23:33:42.105475: step 7070, loss = 2.31 (309.1 examples/sec; 0.414 sec/batch)
2017-04-02 23:33:46.205562: step 7080, loss = 2.31 (312.2 examples/sec; 0.410 sec/batch)
2017-04-02 23:33:50.347709: step 7090, loss = 2.31 (309.0 examples/sec; 0.414 sec/batch)
2017-04-02 23:33:54.536177: step 7100, loss = 2.31 (305.6 examples/sec; 0.419 sec/batch)
2017-04-02 23:33:58.637777: step 7110, loss = 2.31 (312.1 examples/sec; 0.410 sec/batch)
2017-04-02 23:34:02.743861: step 7120, loss = 2.31 (311.7 examples/sec; 0.411 sec/batch)
2017-04-02 23:34:06.838857: step 7130, loss = 2.31 (312.6 examples/sec; 0.409 sec/batch)
2017-04-02 23:34:10.920963: step 7140, loss = 2.31 (313.6 examples/sec; 0.408 sec/batch)
2017-04-02 23:34:15.018504: step 7150, loss = 2.31 (312.4 examples/sec; 0.410 sec/batch)
2017-04-02 23:34:19.133457: step 7160, loss = 2.31 (311.1 examples/sec; 0.411 sec/batch)
2017-04-02 23:34:23.282614: step 7170, loss = 2.31 (308.5 examples/sec; 0.415 sec/batch)
2017-04-02 23:34:27.417621: step 7180, loss = 2.31 (309.6 examples/sec; 0.414 sec/batch)
2017-04-02 23:34:31.514862: step 7190, loss = 2.31 (312.4 examples/sec; 0.410 sec/batch)
2017-04-02 23:34:35.740415: step 7200, loss = 2.31 (302.9 examples/sec; 0.423 sec/batch)
2017-04-02 23:34:39.893743: step 7210, loss = 2.31 (308.2 examples/sec; 0.415 sec/batch)
2017-04-02 23:34:44.022139: step 7220, loss = 2.31 (310.0 examples/sec; 0.413 sec/batch)
2017-04-02 23:34:48.145714: step 7230, loss = 2.31 (310.4 examples/sec; 0.412 sec/batch)
2017-04-02 23:34:52.327544: step 7240, loss = 2.31 (306.1 examples/sec; 0.418 sec/batch)
2017-04-02 23:34:56.430198: step 7250, loss = 2.31 (312.0 examples/sec; 0.410 sec/batch)
2017-04-02 23:35:00.531517: step 7260, loss = 2.31 (312.1 examples/sec; 0.410 sec/batch)
2017-04-02 23:35:04.627852: step 7270, loss = 2.31 (312.5 examples/sec; 0.410 sec/batch)
2017-04-02 23:35:08.773247: step 7280, loss = 2.31 (308.8 examples/sec; 0.415 sec/batch)
2017-04-02 23:35:12.889440: step 7290, loss = 2.31 (311.0 examples/sec; 0.412 sec/batch)
2017-04-02 23:35:17.086145: step 7300, loss = 2.31 (305.0 examples/sec; 0.420 sec/batch)
2017-04-02 23:35:21.212223: step 7310, loss = 2.31 (310.2 examples/sec; 0.413 sec/batch)
2017-04-02 23:35:25.313425: step 7320, loss = 2.31 (312.1 examples/sec; 0.410 sec/batch)
2017-04-02 23:35:29.396008: step 7330, loss = 2.31 (313.5 examples/sec; 0.408 sec/batch)
2017-04-02 23:35:33.538944: step 7340, loss = 2.31 (309.0 examples/sec; 0.414 sec/batch)
2017-04-02 23:35:37.653201: step 7350, loss = 2.31 (311.1 examples/sec; 0.411 sec/batch)
2017-04-02 23:35:41.821801: step 7360, loss = 2.31 (307.1 examples/sec; 0.417 sec/batch)
2017-04-02 23:35:45.953318: step 7370, loss = 2.31 (309.8 examples/sec; 0.413 sec/batch)
2017-04-02 23:35:50.077903: step 7380, loss = 2.31 (310.3 examples/sec; 0.412 sec/batch)
2017-04-02 23:35:54.224662: step 7390, loss = 2.31 (308.7 examples/sec; 0.415 sec/batch)
2017-04-02 23:35:58.426043: step 7400, loss = 2.31 (304.7 examples/sec; 0.420 sec/batch)
2017-04-02 23:36:02.574622: step 7410, loss = 2.31 (308.5 examples/sec; 0.415 sec/batch)
2017-04-02 23:36:06.708564: step 7420, loss = 2.31 (309.6 examples/sec; 0.413 sec/batch)
2017-04-02 23:36:11.236480: step 7430, loss = 2.31 (282.7 examples/sec; 0.453 sec/batch)
2017-04-02 23:36:15.367455: step 7440, loss = 2.31 (309.9 examples/sec; 0.413 sec/batch)
2017-04-02 23:36:19.514167: step 7450, loss = 2.31 (308.7 examples/sec; 0.415 sec/batch)
2017-04-02 23:36:23.659696: step 7460, loss = 2.31 (308.8 examples/sec; 0.415 sec/batch)
2017-04-02 23:36:27.769193: step 7470, loss = 2.31 (311.5 examples/sec; 0.411 sec/batch)
2017-04-02 23:36:31.912983: step 7480, loss = 2.31 (308.9 examples/sec; 0.414 sec/batch)
2017-04-02 23:36:36.053028: step 7490, loss = 2.30 (309.2 examples/sec; 0.414 sec/batch)
2017-04-02 23:36:40.254539: step 7500, loss = 2.31 (304.7 examples/sec; 0.420 sec/batch)
2017-04-02 23:36:44.359173: step 7510, loss = 2.31 (311.8 examples/sec; 0.410 sec/batch)
2017-04-02 23:36:48.524350: step 7520, loss = 2.31 (307.3 examples/sec; 0.417 sec/batch)
2017-04-02 23:36:52.670067: step 7530, loss = 2.31 (308.8 examples/sec; 0.415 sec/batch)
2017-04-02 23:36:56.816115: step 7540, loss = 2.31 (308.7 examples/sec; 0.415 sec/batch)
2017-04-02 23:37:00.936288: step 7550, loss = 2.31 (310.7 examples/sec; 0.412 sec/batch)
2017-04-02 23:37:05.070313: step 7560, loss = 2.31 (309.6 examples/sec; 0.413 sec/batch)
2017-04-02 23:37:09.202386: step 7570, loss = 2.31 (309.8 examples/sec; 0.413 sec/batch)
2017-04-02 23:37:13.383157: step 7580, loss = 2.31 (306.2 examples/sec; 0.418 sec/batch)
2017-04-02 23:37:17.497070: step 7590, loss = 2.31 (311.1 examples/sec; 0.411 sec/batch)
2017-04-02 23:37:21.654618: step 7600, loss = 2.31 (307.9 examples/sec; 0.416 sec/batch)
2017-04-02 23:37:25.836732: step 7610, loss = 2.31 (306.1 examples/sec; 0.418 sec/batch)
2017-04-02 23:37:29.983220: step 7620, loss = 2.30 (308.7 examples/sec; 0.415 sec/batch)
2017-04-02 23:37:34.093715: step 7630, loss = 2.31 (311.4 examples/sec; 0.411 sec/batch)
2017-04-02 23:37:38.260174: step 7640, loss = 2.30 (307.2 examples/sec; 0.417 sec/batch)
2017-04-02 23:37:42.367852: step 7650, loss = 2.31 (311.6 examples/sec; 0.411 sec/batch)
2017-04-02 23:37:46.479337: step 7660, loss = 2.31 (311.3 examples/sec; 0.411 sec/batch)
2017-04-02 23:37:50.574452: step 7670, loss = 2.31 (312.6 examples/sec; 0.410 sec/batch)
2017-04-02 23:37:54.715492: step 7680, loss = 2.30 (309.1 examples/sec; 0.414 sec/batch)
2017-04-02 23:37:58.861914: step 7690, loss = 2.30 (308.7 examples/sec; 0.415 sec/batch)
2017-04-02 23:38:03.032263: step 7700, loss = 2.31 (306.9 examples/sec; 0.417 sec/batch)
2017-04-02 23:38:07.175957: step 7710, loss = 2.31 (308.9 examples/sec; 0.414 sec/batch)
2017-04-02 23:38:11.386801: step 7720, loss = 2.31 (304.0 examples/sec; 0.421 sec/batch)
2017-04-02 23:38:15.507925: step 7730, loss = 2.31 (310.6 examples/sec; 0.412 sec/batch)
2017-04-02 23:38:19.652237: step 7740, loss = 2.31 (308.9 examples/sec; 0.414 sec/batch)
2017-04-02 23:38:23.767709: step 7750, loss = 2.31 (311.0 examples/sec; 0.412 sec/batch)
2017-04-02 23:38:27.920411: step 7760, loss = 2.31 (308.2 examples/sec; 0.415 sec/batch)
2017-04-02 23:38:32.059691: step 7770, loss = 2.31 (309.2 examples/sec; 0.414 sec/batch)
2017-04-02 23:38:36.181295: step 7780, loss = 2.31 (310.6 examples/sec; 0.412 sec/batch)
2017-04-02 23:38:40.315690: step 7790, loss = 2.31 (309.6 examples/sec; 0.413 sec/batch)
2017-04-02 23:38:44.530955: step 7800, loss = 2.31 (303.7 examples/sec; 0.422 sec/batch)
2017-04-02 23:38:48.694636: step 7810, loss = 2.31 (307.4 examples/sec; 0.416 sec/batch)
2017-04-02 23:38:52.851462: step 7820, loss = 2.31 (307.9 examples/sec; 0.416 sec/batch)
2017-04-02 23:38:57.009707: step 7830, loss = 2.31 (307.8 examples/sec; 0.416 sec/batch)
2017-04-02 23:39:01.126029: step 7840, loss = 2.31 (311.0 examples/sec; 0.412 sec/batch)
2017-04-02 23:39:05.268821: step 7850, loss = 2.31 (309.0 examples/sec; 0.414 sec/batch)
2017-04-02 23:39:09.408730: step 7860, loss = 2.31 (309.2 examples/sec; 0.414 sec/batch)
2017-04-02 23:39:13.543645: step 7870, loss = 2.30 (309.6 examples/sec; 0.413 sec/batch)
2017-04-02 23:39:17.637657: step 7880, loss = 2.31 (312.7 examples/sec; 0.409 sec/batch)
2017-04-02 23:39:21.748164: step 7890, loss = 2.31 (311.4 examples/sec; 0.411 sec/batch)
2017-04-02 23:39:25.941190: step 7900, loss = 2.31 (305.3 examples/sec; 0.419 sec/batch)
2017-04-02 23:39:30.069362: step 7910, loss = 2.31 (310.1 examples/sec; 0.413 sec/batch)
2017-04-02 23:39:34.208559: step 7920, loss = 2.31 (309.2 examples/sec; 0.414 sec/batch)
2017-04-02 23:39:38.311182: step 7930, loss = 2.31 (312.0 examples/sec; 0.410 sec/batch)
2017-04-02 23:39:42.466289: step 7940, loss = 2.31 (308.1 examples/sec; 0.416 sec/batch)
2017-04-02 23:39:46.605926: step 7950, loss = 2.31 (309.2 examples/sec; 0.414 sec/batch)
2017-04-02 23:39:50.843369: step 7960, loss = 2.31 (302.1 examples/sec; 0.424 sec/batch)
2017-04-02 23:39:54.980963: step 7970, loss = 2.31 (309.4 examples/sec; 0.414 sec/batch)
2017-04-02 23:39:59.142751: step 7980, loss = 2.31 (307.6 examples/sec; 0.416 sec/batch)
2017-04-02 23:40:03.263016: step 7990, loss = 2.31 (310.7 examples/sec; 0.412 sec/batch)
2017-04-02 23:40:07.464463: step 8000, loss = 2.31 (304.7 examples/sec; 0.420 sec/batch)
2017-04-02 23:41:20.479646: precision @ 1 = 0.101

2017-04-02 23:42:34.038027: step 8010, loss = 2.31 (8.7 examples/sec; 14.657 sec/batch)
2017-04-02 23:42:38.144275: step 8020, loss = 2.30 (311.7 examples/sec; 0.411 sec/batch)
2017-04-02 23:42:42.260503: step 8030, loss = 2.31 (311.0 examples/sec; 0.412 sec/batch)
2017-04-02 23:42:46.350394: step 8040, loss = 2.31 (313.0 examples/sec; 0.409 sec/batch)
2017-04-02 23:42:50.455330: step 8050, loss = 2.31 (311.8 examples/sec; 0.410 sec/batch)
2017-04-02 23:42:54.574747: step 8060, loss = 2.31 (310.7 examples/sec; 0.412 sec/batch)
2017-04-02 23:42:58.690228: step 8070, loss = 2.31 (311.0 examples/sec; 0.412 sec/batch)
2017-04-02 23:43:02.810380: step 8080, loss = 2.30 (310.7 examples/sec; 0.412 sec/batch)
2017-04-02 23:43:06.890086: step 8090, loss = 2.31 (313.7 examples/sec; 0.408 sec/batch)
2017-04-02 23:43:11.063796: step 8100, loss = 2.30 (306.7 examples/sec; 0.417 sec/batch)
2017-04-02 23:43:15.174470: step 8110, loss = 2.30 (311.4 examples/sec; 0.411 sec/batch)
2017-04-02 23:43:19.268261: step 8120, loss = 2.30 (312.7 examples/sec; 0.409 sec/batch)
2017-04-02 23:43:23.391948: step 8130, loss = 2.31 (310.4 examples/sec; 0.412 sec/batch)
2017-04-02 23:43:27.487066: step 8140, loss = 2.30 (312.6 examples/sec; 0.410 sec/batch)
2017-04-02 23:43:31.590096: step 8150, loss = 2.31 (312.0 examples/sec; 0.410 sec/batch)
2017-04-02 23:43:35.721966: step 8160, loss = 2.31 (309.8 examples/sec; 0.413 sec/batch)
2017-04-02 23:43:39.834544: step 8170, loss = 2.31 (311.2 examples/sec; 0.411 sec/batch)
2017-04-02 23:43:43.983557: step 8180, loss = 2.31 (308.5 examples/sec; 0.415 sec/batch)
2017-04-02 23:43:48.114721: step 8190, loss = 2.31 (309.8 examples/sec; 0.413 sec/batch)
2017-04-02 23:43:52.282947: step 8200, loss = 2.31 (307.1 examples/sec; 0.417 sec/batch)
2017-04-02 23:43:56.370409: step 8210, loss = 2.31 (313.2 examples/sec; 0.409 sec/batch)
2017-04-02 23:44:00.515507: step 8220, loss = 2.31 (308.8 examples/sec; 0.415 sec/batch)
2017-04-02 23:44:04.652319: step 8230, loss = 2.30 (309.4 examples/sec; 0.414 sec/batch)
2017-04-02 23:44:08.778768: step 8240, loss = 2.31 (310.2 examples/sec; 0.413 sec/batch)
2017-04-02 23:44:13.013833: step 8250, loss = 2.30 (302.2 examples/sec; 0.424 sec/batch)
2017-04-02 23:44:17.101815: step 8260, loss = 2.31 (313.1 examples/sec; 0.409 sec/batch)
2017-04-02 23:44:21.210571: step 8270, loss = 2.31 (311.5 examples/sec; 0.411 sec/batch)
2017-04-02 23:44:25.325553: step 8280, loss = 2.31 (311.1 examples/sec; 0.411 sec/batch)
2017-04-02 23:44:29.422385: step 8290, loss = 2.30 (312.4 examples/sec; 0.410 sec/batch)
2017-04-02 23:44:33.620118: step 8300, loss = 2.31 (304.9 examples/sec; 0.420 sec/batch)
2017-04-02 23:44:37.743183: step 8310, loss = 2.31 (310.4 examples/sec; 0.412 sec/batch)
2017-04-02 23:44:41.836135: step 8320, loss = 2.31 (312.7 examples/sec; 0.409 sec/batch)
2017-04-02 23:44:45.961451: step 8330, loss = 2.30 (310.3 examples/sec; 0.413 sec/batch)
2017-04-02 23:44:50.070607: step 8340, loss = 2.30 (311.5 examples/sec; 0.411 sec/batch)
2017-04-02 23:44:54.197977: step 8350, loss = 2.31 (310.1 examples/sec; 0.413 sec/batch)
2017-04-02 23:44:58.303178: step 8360, loss = 2.31 (311.8 examples/sec; 0.411 sec/batch)
2017-04-02 23:45:02.401578: step 8370, loss = 2.30 (312.3 examples/sec; 0.410 sec/batch)
2017-04-02 23:45:06.524395: step 8380, loss = 2.30 (310.5 examples/sec; 0.412 sec/batch)
2017-04-02 23:45:10.677552: step 8390, loss = 2.31 (308.2 examples/sec; 0.415 sec/batch)
2017-04-02 23:45:14.921043: step 8400, loss = 2.30 (301.6 examples/sec; 0.424 sec/batch)
2017-04-02 23:45:19.147395: step 8410, loss = 2.31 (302.9 examples/sec; 0.423 sec/batch)
2017-04-02 23:45:23.251347: step 8420, loss = 2.30 (311.9 examples/sec; 0.410 sec/batch)
2017-04-02 23:45:27.331385: step 8430, loss = 2.31 (313.7 examples/sec; 0.408 sec/batch)
2017-04-02 23:45:31.470405: step 8440, loss = 2.31 (309.3 examples/sec; 0.414 sec/batch)
2017-04-02 23:45:35.578288: step 8450, loss = 2.30 (311.6 examples/sec; 0.411 sec/batch)
2017-04-02 23:45:39.706712: step 8460, loss = 2.31 (310.0 examples/sec; 0.413 sec/batch)
2017-04-02 23:45:43.817210: step 8470, loss = 2.31 (311.4 examples/sec; 0.411 sec/batch)
2017-04-02 23:45:47.923264: step 8480, loss = 2.31 (311.7 examples/sec; 0.411 sec/batch)
2017-04-02 23:45:52.013373: step 8490, loss = 2.30 (313.0 examples/sec; 0.409 sec/batch)
2017-04-02 23:45:56.164448: step 8500, loss = 2.30 (308.4 examples/sec; 0.415 sec/batch)
2017-04-02 23:46:00.297875: step 8510, loss = 2.31 (309.7 examples/sec; 0.413 sec/batch)
2017-04-02 23:46:04.422336: step 8520, loss = 2.31 (310.3 examples/sec; 0.412 sec/batch)
2017-04-02 23:46:08.528129: step 8530, loss = 2.30 (311.8 examples/sec; 0.411 sec/batch)
2017-04-02 23:46:12.622292: step 8540, loss = 2.30 (312.6 examples/sec; 0.409 sec/batch)
2017-04-02 23:46:16.747376: step 8550, loss = 2.30 (310.3 examples/sec; 0.413 sec/batch)
2017-04-02 23:46:20.877499: step 8560, loss = 2.31 (309.9 examples/sec; 0.413 sec/batch)
2017-04-02 23:46:25.008041: step 8570, loss = 2.31 (309.9 examples/sec; 0.413 sec/batch)
2017-04-02 23:46:29.140275: step 8580, loss = 2.31 (309.8 examples/sec; 0.413 sec/batch)
2017-04-02 23:46:33.297404: step 8590, loss = 2.31 (307.9 examples/sec; 0.416 sec/batch)
2017-04-02 23:46:37.469245: step 8600, loss = 2.30 (306.8 examples/sec; 0.417 sec/batch)
2017-04-02 23:46:41.579516: step 8610, loss = 2.31 (311.4 examples/sec; 0.411 sec/batch)
2017-04-02 23:46:45.670138: step 8620, loss = 2.31 (312.9 examples/sec; 0.409 sec/batch)
2017-04-02 23:46:49.784822: step 8630, loss = 2.31 (311.1 examples/sec; 0.411 sec/batch)
2017-04-02 23:46:53.910709: step 8640, loss = 2.31 (310.2 examples/sec; 0.413 sec/batch)
2017-04-02 23:46:58.008042: step 8650, loss = 2.31 (312.4 examples/sec; 0.410 sec/batch)
2017-04-02 23:47:02.106922: step 8660, loss = 2.30 (312.3 examples/sec; 0.410 sec/batch)
2017-04-02 23:47:06.200074: step 8670, loss = 2.31 (312.7 examples/sec; 0.409 sec/batch)
2017-04-02 23:47:10.315042: step 8680, loss = 2.30 (311.1 examples/sec; 0.411 sec/batch)
2017-04-02 23:47:14.407902: step 8690, loss = 2.31 (312.7 examples/sec; 0.409 sec/batch)
2017-04-02 23:47:18.594426: step 8700, loss = 2.30 (305.7 examples/sec; 0.419 sec/batch)
2017-04-02 23:47:22.735672: step 8710, loss = 2.31 (309.1 examples/sec; 0.414 sec/batch)
2017-04-02 23:47:26.842805: step 8720, loss = 2.31 (311.7 examples/sec; 0.411 sec/batch)
2017-04-02 23:47:30.972553: step 8730, loss = 2.31 (309.9 examples/sec; 0.413 sec/batch)
2017-04-02 23:47:35.095431: step 8740, loss = 2.30 (310.5 examples/sec; 0.412 sec/batch)
2017-04-02 23:47:39.213303: step 8750, loss = 2.30 (310.8 examples/sec; 0.412 sec/batch)
2017-04-02 23:47:43.358230: step 8760, loss = 2.31 (308.8 examples/sec; 0.414 sec/batch)
2017-04-02 23:47:47.501193: step 8770, loss = 2.30 (309.0 examples/sec; 0.414 sec/batch)
2017-04-02 23:47:51.606718: step 8780, loss = 2.31 (311.8 examples/sec; 0.411 sec/batch)
2017-04-02 23:47:55.725475: step 8790, loss = 2.30 (310.8 examples/sec; 0.412 sec/batch)
2017-04-02 23:47:59.891113: step 8800, loss = 2.31 (307.3 examples/sec; 0.417 sec/batch)
2017-04-02 23:48:04.049473: step 8810, loss = 2.30 (307.8 examples/sec; 0.416 sec/batch)
2017-04-02 23:48:08.212465: step 8820, loss = 2.31 (307.5 examples/sec; 0.416 sec/batch)
2017-04-02 23:48:12.348931: step 8830, loss = 2.31 (309.4 examples/sec; 0.414 sec/batch)
2017-04-02 23:48:16.484325: step 8840, loss = 2.31 (309.5 examples/sec; 0.414 sec/batch)
2017-04-02 23:48:20.637946: step 8850, loss = 2.30 (308.2 examples/sec; 0.415 sec/batch)
2017-04-02 23:48:24.752731: step 8860, loss = 2.31 (311.1 examples/sec; 0.411 sec/batch)
2017-04-02 23:48:28.881714: step 8870, loss = 2.31 (310.0 examples/sec; 0.413 sec/batch)
2017-04-02 23:48:32.974898: step 8880, loss = 2.30 (312.7 examples/sec; 0.409 sec/batch)
2017-04-02 23:48:37.057348: step 8890, loss = 2.31 (313.5 examples/sec; 0.408 sec/batch)
2017-04-02 23:48:41.251637: step 8900, loss = 2.30 (305.2 examples/sec; 0.419 sec/batch)
2017-04-02 23:48:45.382066: step 8910, loss = 2.31 (309.9 examples/sec; 0.413 sec/batch)
2017-04-02 23:48:49.472782: step 8920, loss = 2.30 (312.9 examples/sec; 0.409 sec/batch)
2017-04-02 23:48:53.590357: step 8930, loss = 2.31 (310.9 examples/sec; 0.412 sec/batch)
2017-04-02 23:48:57.727366: step 8940, loss = 2.30 (309.4 examples/sec; 0.414 sec/batch)
2017-04-02 23:49:01.869355: step 8950, loss = 2.31 (309.0 examples/sec; 0.414 sec/batch)
2017-04-02 23:49:06.006008: step 8960, loss = 2.31 (309.4 examples/sec; 0.414 sec/batch)
2017-04-02 23:49:10.136240: step 8970, loss = 2.31 (309.9 examples/sec; 0.413 sec/batch)
2017-04-02 23:49:14.296930: step 8980, loss = 2.30 (307.6 examples/sec; 0.416 sec/batch)
2017-04-02 23:49:18.415203: step 8990, loss = 2.31 (310.8 examples/sec; 0.412 sec/batch)
2017-04-02 23:49:22.616500: step 9000, loss = 2.31 (304.7 examples/sec; 0.420 sec/batch)
2017-04-02 23:49:26.740240: step 9010, loss = 2.30 (310.4 examples/sec; 0.412 sec/batch)
2017-04-02 23:49:30.880011: step 9020, loss = 2.31 (309.2 examples/sec; 0.414 sec/batch)
2017-04-02 23:49:35.011664: step 9030, loss = 2.31 (309.8 examples/sec; 0.413 sec/batch)
2017-04-02 23:49:39.139766: step 9040, loss = 2.31 (310.1 examples/sec; 0.413 sec/batch)
2017-04-02 23:49:43.244130: step 9050, loss = 2.30 (311.9 examples/sec; 0.410 sec/batch)
2017-04-02 23:49:47.371313: step 9060, loss = 2.30 (310.1 examples/sec; 0.413 sec/batch)
2017-04-02 23:49:51.500352: step 9070, loss = 2.31 (310.0 examples/sec; 0.413 sec/batch)
2017-04-02 23:49:55.625371: step 9080, loss = 2.30 (310.3 examples/sec; 0.413 sec/batch)
2017-04-02 23:49:59.711505: step 9090, loss = 2.30 (313.3 examples/sec; 0.409 sec/batch)
2017-04-02 23:50:03.913887: step 9100, loss = 2.30 (304.6 examples/sec; 0.420 sec/batch)
2017-04-02 23:50:08.059853: step 9110, loss = 2.31 (308.7 examples/sec; 0.415 sec/batch)
2017-04-02 23:50:12.193686: step 9120, loss = 2.30 (309.6 examples/sec; 0.413 sec/batch)
2017-04-02 23:50:16.317829: step 9130, loss = 2.30 (310.4 examples/sec; 0.412 sec/batch)
2017-04-02 23:50:20.430556: step 9140, loss = 2.31 (311.2 examples/sec; 0.411 sec/batch)
2017-04-02 23:50:24.542036: step 9150, loss = 2.30 (311.3 examples/sec; 0.411 sec/batch)
2017-04-02 23:50:28.684389: step 9160, loss = 2.30 (309.0 examples/sec; 0.414 sec/batch)
2017-04-02 23:50:32.808329: step 9170, loss = 2.31 (310.4 examples/sec; 0.412 sec/batch)
2017-04-02 23:50:36.918961: step 9180, loss = 2.30 (311.4 examples/sec; 0.411 sec/batch)
2017-04-02 23:50:41.045423: step 9190, loss = 2.30 (310.2 examples/sec; 0.413 sec/batch)
2017-04-02 23:50:45.195734: step 9200, loss = 2.30 (308.4 examples/sec; 0.415 sec/batch)
2017-04-02 23:50:49.346167: step 9210, loss = 2.30 (308.4 examples/sec; 0.415 sec/batch)
2017-04-02 23:50:53.477996: step 9220, loss = 2.30 (309.8 examples/sec; 0.413 sec/batch)
2017-04-02 23:50:57.606460: step 9230, loss = 2.31 (310.0 examples/sec; 0.413 sec/batch)
2017-04-02 23:51:01.755149: step 9240, loss = 2.30 (308.5 examples/sec; 0.415 sec/batch)
2017-04-02 23:51:05.887599: step 9250, loss = 2.30 (309.7 examples/sec; 0.413 sec/batch)
2017-04-02 23:51:09.997031: step 9260, loss = 2.31 (311.5 examples/sec; 0.411 sec/batch)
2017-04-02 23:51:14.112726: step 9270, loss = 2.31 (311.0 examples/sec; 0.412 sec/batch)
2017-04-02 23:51:18.217781: step 9280, loss = 2.30 (311.8 examples/sec; 0.411 sec/batch)
2017-04-02 23:51:22.350334: step 9290, loss = 2.30 (309.7 examples/sec; 0.413 sec/batch)
2017-04-02 23:51:26.550338: step 9300, loss = 2.30 (304.8 examples/sec; 0.420 sec/batch)
2017-04-02 23:51:30.670384: step 9310, loss = 2.31 (310.7 examples/sec; 0.412 sec/batch)
2017-04-02 23:51:34.766041: step 9320, loss = 2.30 (312.5 examples/sec; 0.410 sec/batch)
2017-04-02 23:51:38.901078: step 9330, loss = 2.30 (309.5 examples/sec; 0.414 sec/batch)
2017-04-02 23:51:43.036863: step 9340, loss = 2.30 (309.5 examples/sec; 0.414 sec/batch)
2017-04-02 23:51:47.130928: step 9350, loss = 2.30 (312.6 examples/sec; 0.409 sec/batch)
2017-04-02 23:51:51.289720: step 9360, loss = 2.30 (307.8 examples/sec; 0.416 sec/batch)
2017-04-02 23:51:55.373511: step 9370, loss = 2.31 (313.4 examples/sec; 0.408 sec/batch)
2017-04-02 23:51:59.491587: step 9380, loss = 2.30 (310.8 examples/sec; 0.412 sec/batch)
2017-04-02 23:52:03.572639: step 9390, loss = 2.31 (313.6 examples/sec; 0.408 sec/batch)
2017-04-02 23:52:07.724013: step 9400, loss = 2.30 (308.3 examples/sec; 0.415 sec/batch)
2017-04-02 23:52:11.829658: step 9410, loss = 2.30 (311.8 examples/sec; 0.411 sec/batch)
2017-04-02 23:52:15.943963: step 9420, loss = 2.30 (311.1 examples/sec; 0.411 sec/batch)
2017-04-02 23:52:20.041621: step 9430, loss = 2.31 (312.4 examples/sec; 0.410 sec/batch)
2017-04-02 23:52:24.170520: step 9440, loss = 2.31 (310.0 examples/sec; 0.413 sec/batch)
2017-04-02 23:52:28.302712: step 9450, loss = 2.30 (309.8 examples/sec; 0.413 sec/batch)
2017-04-02 23:52:33.066846: step 9460, loss = 2.30 (268.7 examples/sec; 0.476 sec/batch)
2017-04-02 23:52:37.196306: step 9470, loss = 2.30 (310.0 examples/sec; 0.413 sec/batch)
2017-04-02 23:52:41.310583: step 9480, loss = 2.31 (311.1 examples/sec; 0.411 sec/batch)
2017-04-02 23:52:45.436161: step 9490, loss = 2.30 (310.3 examples/sec; 0.413 sec/batch)
2017-04-02 23:52:49.625630: step 9500, loss = 2.30 (305.5 examples/sec; 0.419 sec/batch)
2017-04-02 23:52:53.706424: step 9510, loss = 2.31 (313.7 examples/sec; 0.408 sec/batch)
2017-04-02 23:52:57.797905: step 9520, loss = 2.30 (312.8 examples/sec; 0.409 sec/batch)
2017-04-02 23:53:01.946886: step 9530, loss = 2.30 (308.5 examples/sec; 0.415 sec/batch)
2017-04-02 23:53:06.063977: step 9540, loss = 2.30 (310.9 examples/sec; 0.412 sec/batch)
2017-04-02 23:53:10.205778: step 9550, loss = 2.30 (309.0 examples/sec; 0.414 sec/batch)
2017-04-02 23:53:14.309386: step 9560, loss = 2.30 (311.9 examples/sec; 0.410 sec/batch)
2017-04-02 23:53:18.429438: step 9570, loss = 2.31 (310.7 examples/sec; 0.412 sec/batch)
2017-04-02 23:53:22.569572: step 9580, loss = 2.31 (309.2 examples/sec; 0.414 sec/batch)
2017-04-02 23:53:26.695004: step 9590, loss = 2.30 (310.3 examples/sec; 0.413 sec/batch)
2017-04-02 23:53:30.912581: step 9600, loss = 2.31 (303.5 examples/sec; 0.422 sec/batch)
2017-04-02 23:53:35.050553: step 9610, loss = 2.30 (309.3 examples/sec; 0.414 sec/batch)
2017-04-02 23:53:39.173403: step 9620, loss = 2.31 (310.5 examples/sec; 0.412 sec/batch)
2017-04-02 23:53:43.303464: step 9630, loss = 2.30 (309.9 examples/sec; 0.413 sec/batch)
2017-04-02 23:53:47.443528: step 9640, loss = 2.31 (309.2 examples/sec; 0.414 sec/batch)
2017-04-02 23:53:51.546782: step 9650, loss = 2.30 (311.9 examples/sec; 0.410 sec/batch)
2017-04-02 23:53:55.677993: step 9660, loss = 2.31 (309.8 examples/sec; 0.413 sec/batch)
2017-04-02 23:53:59.778179: step 9670, loss = 2.30 (312.2 examples/sec; 0.410 sec/batch)
2017-04-02 23:54:03.867468: step 9680, loss = 2.30 (313.0 examples/sec; 0.409 sec/batch)
2017-04-02 23:54:07.988833: step 9690, loss = 2.31 (310.6 examples/sec; 0.412 sec/batch)
2017-04-02 23:54:12.176111: step 9700, loss = 2.31 (305.7 examples/sec; 0.419 sec/batch)
2017-04-02 23:54:16.268698: step 9710, loss = 2.30 (312.8 examples/sec; 0.409 sec/batch)
2017-04-02 23:54:20.374065: step 9720, loss = 2.30 (311.8 examples/sec; 0.411 sec/batch)
2017-04-02 23:54:24.503430: step 9730, loss = 2.30 (310.0 examples/sec; 0.413 sec/batch)
2017-04-02 23:54:28.626943: step 9740, loss = 2.31 (310.4 examples/sec; 0.412 sec/batch)
2017-04-02 23:54:32.732882: step 9750, loss = 2.31 (311.7 examples/sec; 0.411 sec/batch)
2017-04-02 23:54:36.837650: step 9760, loss = 2.31 (311.8 examples/sec; 0.410 sec/batch)
2017-04-02 23:54:40.953889: step 9770, loss = 2.30 (311.0 examples/sec; 0.412 sec/batch)
2017-04-02 23:54:45.082314: step 9780, loss = 2.30 (310.0 examples/sec; 0.413 sec/batch)
2017-04-02 23:54:49.197947: step 9790, loss = 2.30 (311.0 examples/sec; 0.412 sec/batch)
2017-04-02 23:54:53.516610: step 9800, loss = 2.30 (296.4 examples/sec; 0.432 sec/batch)
2017-04-02 23:54:57.622836: step 9810, loss = 2.30 (311.7 examples/sec; 0.411 sec/batch)
2017-04-02 23:55:01.709847: step 9820, loss = 2.30 (313.2 examples/sec; 0.409 sec/batch)
2017-04-02 23:55:05.856256: step 9830, loss = 2.31 (308.7 examples/sec; 0.415 sec/batch)
2017-04-02 23:55:09.983371: step 9840, loss = 2.31 (310.1 examples/sec; 0.413 sec/batch)
2017-04-02 23:55:14.108017: step 9850, loss = 2.30 (310.3 examples/sec; 0.412 sec/batch)
2017-04-02 23:55:18.224155: step 9860, loss = 2.30 (311.0 examples/sec; 0.412 sec/batch)
2017-04-02 23:55:22.306749: step 9870, loss = 2.30 (313.5 examples/sec; 0.408 sec/batch)
2017-04-02 23:55:26.402755: step 9880, loss = 2.30 (312.5 examples/sec; 0.410 sec/batch)
2017-04-02 23:55:30.524696: step 9890, loss = 2.30 (310.5 examples/sec; 0.412 sec/batch)
2017-04-02 23:55:34.702145: step 9900, loss = 2.30 (306.4 examples/sec; 0.418 sec/batch)
2017-04-02 23:55:38.804374: step 9910, loss = 2.30 (312.0 examples/sec; 0.410 sec/batch)
2017-04-02 23:55:42.932331: step 9920, loss = 2.30 (310.1 examples/sec; 0.413 sec/batch)
2017-04-02 23:55:47.063725: step 9930, loss = 2.31 (309.8 examples/sec; 0.413 sec/batch)
2017-04-02 23:55:51.177999: step 9940, loss = 2.30 (311.1 examples/sec; 0.411 sec/batch)
2017-04-02 23:55:55.313929: step 9950, loss = 2.30 (309.5 examples/sec; 0.414 sec/batch)
2017-04-02 23:55:59.414714: step 9960, loss = 2.30 (312.1 examples/sec; 0.410 sec/batch)
2017-04-02 23:56:03.534336: step 9970, loss = 2.30 (310.7 examples/sec; 0.412 sec/batch)
2017-04-02 23:56:07.682440: step 9980, loss = 2.30 (308.6 examples/sec; 0.415 sec/batch)
2017-04-02 23:56:11.843647: step 9990, loss = 2.30 (307.6 examples/sec; 0.416 sec/batch)
2017-04-02 23:56:15.991208: step 10000, loss = 2.30 (308.6 examples/sec; 0.415 sec/batch)
2017-04-02 23:57:11.085325: precision @ 1 = 0.100

2017-04-02 23:57:55.141722: step 10010, loss = 2.30 (12.9 examples/sec; 9.915 sec/batch)
2017-04-02 23:57:59.312573: step 10020, loss = 2.30 (306.9 examples/sec; 0.417 sec/batch)
2017-04-02 23:58:03.434510: step 10030, loss = 2.31 (310.5 examples/sec; 0.412 sec/batch)
2017-04-02 23:58:07.562526: step 10040, loss = 2.30 (310.1 examples/sec; 0.413 sec/batch)
2017-04-02 23:58:11.696033: step 10050, loss = 2.30 (309.7 examples/sec; 0.413 sec/batch)
2017-04-02 23:58:15.808985: step 10060, loss = 2.31 (311.2 examples/sec; 0.411 sec/batch)
2017-04-02 23:58:19.894875: step 10070, loss = 2.30 (313.3 examples/sec; 0.409 sec/batch)
2017-04-02 23:58:24.044668: step 10080, loss = 2.30 (308.4 examples/sec; 0.415 sec/batch)
2017-04-02 23:58:28.189884: step 10090, loss = 2.30 (308.8 examples/sec; 0.415 sec/batch)
2017-04-02 23:58:32.384115: step 10100, loss = 2.31 (305.2 examples/sec; 0.419 sec/batch)
2017-04-02 23:58:36.499332: step 10110, loss = 2.30 (311.0 examples/sec; 0.412 sec/batch)
2017-04-02 23:58:40.621066: step 10120, loss = 2.31 (310.5 examples/sec; 0.412 sec/batch)
2017-04-02 23:58:44.721788: step 10130, loss = 2.30 (312.1 examples/sec; 0.410 sec/batch)
2017-04-02 23:58:48.836403: step 10140, loss = 2.31 (311.1 examples/sec; 0.411 sec/batch)
2017-04-02 23:58:53.012582: step 10150, loss = 2.30 (306.5 examples/sec; 0.418 sec/batch)
2017-04-02 23:58:57.149655: step 10160, loss = 2.30 (309.4 examples/sec; 0.414 sec/batch)
2017-04-02 23:59:01.285574: step 10170, loss = 2.30 (309.5 examples/sec; 0.414 sec/batch)
2017-04-02 23:59:05.420451: step 10180, loss = 2.30 (309.6 examples/sec; 0.413 sec/batch)
2017-04-02 23:59:09.560027: step 10190, loss = 2.31 (309.2 examples/sec; 0.414 sec/batch)
2017-04-02 23:59:13.758981: step 10200, loss = 2.30 (304.8 examples/sec; 0.420 sec/batch)
2017-04-02 23:59:17.924947: step 10210, loss = 2.30 (307.3 examples/sec; 0.417 sec/batch)
2017-04-02 23:59:22.075270: step 10220, loss = 2.30 (308.4 examples/sec; 0.415 sec/batch)
2017-04-02 23:59:26.206777: step 10230, loss = 2.30 (309.8 examples/sec; 0.413 sec/batch)
2017-04-02 23:59:30.357873: step 10240, loss = 2.30 (308.4 examples/sec; 0.415 sec/batch)
2017-04-02 23:59:34.502056: step 10250, loss = 2.30 (308.9 examples/sec; 0.414 sec/batch)
2017-04-02 23:59:38.634898: step 10260, loss = 2.30 (309.7 examples/sec; 0.413 sec/batch)
2017-04-02 23:59:42.734081: step 10270, loss = 2.30 (312.3 examples/sec; 0.410 sec/batch)
2017-04-02 23:59:46.837700: step 10280, loss = 2.30 (311.9 examples/sec; 0.410 sec/batch)
2017-04-02 23:59:50.960523: step 10290, loss = 2.30 (310.5 examples/sec; 0.412 sec/batch)
2017-04-02 23:59:55.149574: step 10300, loss = 2.30 (305.6 examples/sec; 0.419 sec/batch)
2017-04-02 23:59:59.313835: step 10310, loss = 2.30 (307.4 examples/sec; 0.416 sec/batch)
2017-04-03 00:00:03.432173: step 10320, loss = 2.30 (310.8 examples/sec; 0.412 sec/batch)
2017-04-03 00:00:07.532061: step 10330, loss = 2.30 (312.2 examples/sec; 0.410 sec/batch)
2017-04-03 00:00:11.672747: step 10340, loss = 2.30 (309.1 examples/sec; 0.414 sec/batch)
2017-04-03 00:00:15.807476: step 10350, loss = 2.30 (309.6 examples/sec; 0.413 sec/batch)
2017-04-03 00:00:19.906815: step 10360, loss = 2.30 (312.2 examples/sec; 0.410 sec/batch)
2017-04-03 00:00:24.022320: step 10370, loss = 2.31 (311.0 examples/sec; 0.412 sec/batch)
2017-04-03 00:00:28.165082: step 10380, loss = 2.30 (309.0 examples/sec; 0.414 sec/batch)
2017-04-03 00:00:32.286717: step 10390, loss = 2.30 (310.6 examples/sec; 0.412 sec/batch)
2017-04-03 00:00:36.532321: step 10400, loss = 2.30 (301.5 examples/sec; 0.425 sec/batch)
2017-04-03 00:00:40.686324: step 10410, loss = 2.31 (308.1 examples/sec; 0.415 sec/batch)
2017-04-03 00:00:44.833578: step 10420, loss = 2.30 (308.6 examples/sec; 0.415 sec/batch)
2017-04-03 00:00:48.957986: step 10430, loss = 2.30 (310.3 examples/sec; 0.412 sec/batch)
2017-04-03 00:00:53.078027: step 10440, loss = 2.30 (310.7 examples/sec; 0.412 sec/batch)
2017-04-03 00:00:57.197767: step 10450, loss = 2.30 (310.7 examples/sec; 0.412 sec/batch)
2017-04-03 00:01:01.355412: step 10460, loss = 2.31 (307.9 examples/sec; 0.416 sec/batch)
2017-04-03 00:01:05.472134: step 10470, loss = 2.30 (310.9 examples/sec; 0.412 sec/batch)
2017-04-03 00:01:09.559768: step 10480, loss = 2.30 (313.1 examples/sec; 0.409 sec/batch)
2017-04-03 00:01:13.677567: step 10490, loss = 2.30 (310.8 examples/sec; 0.412 sec/batch)
2017-04-03 00:01:17.875468: step 10500, loss = 2.30 (304.9 examples/sec; 0.420 sec/batch)
2017-04-03 00:01:21.989998: step 10510, loss = 2.30 (311.1 examples/sec; 0.411 sec/batch)
2017-04-03 00:01:26.138718: step 10520, loss = 2.30 (308.5 examples/sec; 0.415 sec/batch)
2017-04-03 00:01:30.322565: step 10530, loss = 2.30 (305.9 examples/sec; 0.418 sec/batch)
2017-04-03 00:01:34.435365: step 10540, loss = 2.30 (311.2 examples/sec; 0.411 sec/batch)
2017-04-03 00:01:38.559155: step 10550, loss = 2.30 (310.4 examples/sec; 0.412 sec/batch)
2017-04-03 00:01:42.690272: step 10560, loss = 2.30 (309.8 examples/sec; 0.413 sec/batch)
2017-04-03 00:01:46.822907: step 10570, loss = 2.30 (309.7 examples/sec; 0.413 sec/batch)
2017-04-03 00:01:50.940937: step 10580, loss = 2.30 (310.8 examples/sec; 0.412 sec/batch)
2017-04-03 00:01:55.056446: step 10590, loss = 2.31 (311.0 examples/sec; 0.412 sec/batch)
2017-04-03 00:01:59.330922: step 10600, loss = 2.30 (299.5 examples/sec; 0.427 sec/batch)
2017-04-03 00:02:03.464317: step 10610, loss = 2.30 (309.7 examples/sec; 0.413 sec/batch)
2017-04-03 00:02:07.603220: step 10620, loss = 2.30 (309.3 examples/sec; 0.414 sec/batch)
2017-04-03 00:02:11.722854: step 10630, loss = 2.30 (310.7 examples/sec; 0.412 sec/batch)
2017-04-03 00:02:15.814661: step 10640, loss = 2.31 (312.8 examples/sec; 0.409 sec/batch)
2017-04-03 00:02:19.947026: step 10650, loss = 2.30 (309.7 examples/sec; 0.413 sec/batch)
2017-04-03 00:02:24.035383: step 10660, loss = 2.30 (313.1 examples/sec; 0.409 sec/batch)
2017-04-03 00:02:28.158627: step 10670, loss = 2.30 (310.4 examples/sec; 0.412 sec/batch)
2017-04-03 00:02:33.052035: step 10680, loss = 2.30 (261.6 examples/sec; 0.489 sec/batch)
2017-04-03 00:02:37.150204: step 10690, loss = 2.30 (312.3 examples/sec; 0.410 sec/batch)
2017-04-03 00:02:41.328646: step 10700, loss = 2.30 (306.3 examples/sec; 0.418 sec/batch)
2017-04-03 00:02:45.433925: step 10710, loss = 2.30 (311.8 examples/sec; 0.411 sec/batch)
2017-04-03 00:02:49.571693: step 10720, loss = 2.30 (309.3 examples/sec; 0.414 sec/batch)
2017-04-03 00:02:53.706533: step 10730, loss = 2.30 (309.6 examples/sec; 0.413 sec/batch)
2017-04-03 00:02:57.842823: step 10740, loss = 2.30 (309.5 examples/sec; 0.414 sec/batch)
2017-04-03 00:03:01.951156: step 10750, loss = 2.30 (311.6 examples/sec; 0.411 sec/batch)
2017-04-03 00:03:06.086771: step 10760, loss = 2.30 (309.5 examples/sec; 0.414 sec/batch)
2017-04-03 00:03:10.232964: step 10770, loss = 2.30 (308.7 examples/sec; 0.415 sec/batch)
2017-04-03 00:03:14.347552: step 10780, loss = 2.30 (311.1 examples/sec; 0.411 sec/batch)
2017-04-03 00:03:18.474922: step 10790, loss = 2.30 (310.1 examples/sec; 0.413 sec/batch)
2017-04-03 00:03:22.640354: step 10800, loss = 2.30 (307.3 examples/sec; 0.417 sec/batch)
2017-04-03 00:03:26.767266: step 10810, loss = 2.30 (310.2 examples/sec; 0.413 sec/batch)
2017-04-03 00:03:30.878080: step 10820, loss = 2.31 (311.4 examples/sec; 0.411 sec/batch)
2017-04-03 00:03:34.982063: step 10830, loss = 2.30 (311.9 examples/sec; 0.410 sec/batch)
2017-04-03 00:03:39.097754: step 10840, loss = 2.31 (311.0 examples/sec; 0.412 sec/batch)
2017-04-03 00:03:43.205741: step 10850, loss = 2.31 (311.6 examples/sec; 0.411 sec/batch)
2017-04-03 00:03:47.317969: step 10860, loss = 2.30 (311.3 examples/sec; 0.411 sec/batch)
2017-04-03 00:03:51.433715: step 10870, loss = 2.30 (311.0 examples/sec; 0.412 sec/batch)
2017-04-03 00:03:55.552250: step 10880, loss = 2.30 (310.8 examples/sec; 0.412 sec/batch)
2017-04-03 00:03:59.685726: step 10890, loss = 2.31 (309.7 examples/sec; 0.413 sec/batch)
2017-04-03 00:04:03.866731: step 10900, loss = 2.30 (306.1 examples/sec; 0.418 sec/batch)
2017-04-03 00:04:07.978211: step 10910, loss = 2.31 (311.3 examples/sec; 0.411 sec/batch)
2017-04-03 00:04:12.093604: step 10920, loss = 2.30 (311.0 examples/sec; 0.412 sec/batch)
2017-04-03 00:04:16.217330: step 10930, loss = 2.30 (310.4 examples/sec; 0.412 sec/batch)
2017-04-03 00:04:20.330528: step 10940, loss = 2.30 (311.2 examples/sec; 0.411 sec/batch)
2017-04-03 00:04:24.437303: step 10950, loss = 2.30 (311.7 examples/sec; 0.411 sec/batch)
2017-04-03 00:04:28.543406: step 10960, loss = 2.30 (311.7 examples/sec; 0.411 sec/batch)
2017-04-03 00:04:32.673077: step 10970, loss = 2.30 (310.0 examples/sec; 0.413 sec/batch)
2017-04-03 00:04:36.810499: step 10980, loss = 2.30 (309.4 examples/sec; 0.414 sec/batch)
2017-04-03 00:04:40.903573: step 10990, loss = 2.30 (312.7 examples/sec; 0.409 sec/batch)
2017-04-03 00:04:45.077171: step 11000, loss = 2.30 (306.7 examples/sec; 0.417 sec/batch)
2017-04-03 00:04:49.194197: step 11010, loss = 2.31 (310.9 examples/sec; 0.412 sec/batch)
2017-04-03 00:04:53.324188: step 11020, loss = 2.30 (309.9 examples/sec; 0.413 sec/batch)
2017-04-03 00:04:57.462911: step 11030, loss = 2.30 (309.3 examples/sec; 0.414 sec/batch)
2017-04-03 00:05:01.583408: step 11040, loss = 2.30 (310.6 examples/sec; 0.412 sec/batch)
2017-04-03 00:05:05.708338: step 11050, loss = 2.30 (310.3 examples/sec; 0.412 sec/batch)
2017-04-03 00:05:09.801103: step 11060, loss = 2.30 (312.7 examples/sec; 0.409 sec/batch)
2017-04-03 00:05:13.909086: step 11070, loss = 2.30 (311.6 examples/sec; 0.411 sec/batch)
2017-04-03 00:05:18.018554: step 11080, loss = 2.30 (311.5 examples/sec; 0.411 sec/batch)
2017-04-03 00:05:22.078797: step 11090, loss = 2.30 (315.3 examples/sec; 0.406 sec/batch)
2017-04-03 00:05:26.246327: step 11100, loss = 2.30 (307.1 examples/sec; 0.417 sec/batch)
2017-04-03 00:05:30.337318: step 11110, loss = 2.30 (312.9 examples/sec; 0.409 sec/batch)
2017-04-03 00:05:34.457388: step 11120, loss = 2.30 (310.7 examples/sec; 0.412 sec/batch)
2017-04-03 00:05:38.559410: step 11130, loss = 2.30 (312.0 examples/sec; 0.410 sec/batch)
2017-04-03 00:05:42.678445: step 11140, loss = 2.30 (310.8 examples/sec; 0.412 sec/batch)
2017-04-03 00:05:46.818752: step 11150, loss = 2.30 (309.2 examples/sec; 0.414 sec/batch)
2017-04-03 00:05:50.959182: step 11160, loss = 2.30 (309.1 examples/sec; 0.414 sec/batch)
2017-04-03 00:05:55.081718: step 11170, loss = 2.30 (310.5 examples/sec; 0.412 sec/batch)
2017-04-03 00:05:59.198314: step 11180, loss = 2.30 (310.9 examples/sec; 0.412 sec/batch)
2017-04-03 00:06:03.389422: step 11190, loss = 2.30 (305.4 examples/sec; 0.419 sec/batch)
2017-04-03 00:06:07.561469: step 11200, loss = 2.30 (306.8 examples/sec; 0.417 sec/batch)
2017-04-03 00:06:11.696347: step 11210, loss = 2.30 (309.6 examples/sec; 0.413 sec/batch)
2017-04-03 00:06:15.802497: step 11220, loss = 2.30 (311.7 examples/sec; 0.411 sec/batch)
2017-04-03 00:06:19.919127: step 11230, loss = 2.30 (310.9 examples/sec; 0.412 sec/batch)
2017-04-03 00:06:24.062772: step 11240, loss = 2.30 (308.9 examples/sec; 0.414 sec/batch)
2017-04-03 00:06:28.151264: step 11250, loss = 2.30 (313.1 examples/sec; 0.409 sec/batch)
2017-04-03 00:06:32.240574: step 11260, loss = 2.30 (313.0 examples/sec; 0.409 sec/batch)
2017-04-03 00:06:36.361624: step 11270, loss = 2.30 (310.6 examples/sec; 0.412 sec/batch)
2017-04-03 00:06:40.767901: step 11280, loss = 2.30 (290.5 examples/sec; 0.441 sec/batch)
2017-04-03 00:06:44.877170: step 11290, loss = 2.30 (311.5 examples/sec; 0.411 sec/batch)
2017-04-03 00:06:49.052588: step 11300, loss = 2.30 (306.6 examples/sec; 0.418 sec/batch)
2017-04-03 00:06:53.167242: step 11310, loss = 2.30 (311.1 examples/sec; 0.411 sec/batch)
2017-04-03 00:06:57.322586: step 11320, loss = 2.31 (308.0 examples/sec; 0.416 sec/batch)
2017-04-03 00:07:01.442514: step 11330, loss = 2.30 (310.7 examples/sec; 0.412 sec/batch)
2017-04-03 00:07:05.578355: step 11340, loss = 2.30 (309.5 examples/sec; 0.414 sec/batch)
2017-04-03 00:07:09.746096: step 11350, loss = 2.30 (307.1 examples/sec; 0.417 sec/batch)
2017-04-03 00:07:13.882356: step 11360, loss = 2.30 (309.5 examples/sec; 0.414 sec/batch)
2017-04-03 00:07:18.026512: step 11370, loss = 2.30 (308.9 examples/sec; 0.414 sec/batch)
2017-04-03 00:07:22.138186: step 11380, loss = 2.30 (311.3 examples/sec; 0.411 sec/batch)
2017-04-03 00:07:26.237658: step 11390, loss = 2.31 (312.2 examples/sec; 0.410 sec/batch)
2017-04-03 00:07:30.498721: step 11400, loss = 2.30 (300.4 examples/sec; 0.426 sec/batch)
2017-04-03 00:07:34.651176: step 11410, loss = 2.30 (308.3 examples/sec; 0.415 sec/batch)
2017-04-03 00:07:38.787702: step 11420, loss = 2.30 (309.4 examples/sec; 0.414 sec/batch)
2017-04-03 00:07:42.928796: step 11430, loss = 2.30 (309.1 examples/sec; 0.414 sec/batch)
2017-04-03 00:07:47.058206: step 11440, loss = 2.30 (310.0 examples/sec; 0.413 sec/batch)
2017-04-03 00:07:51.198928: step 11450, loss = 2.31 (309.1 examples/sec; 0.414 sec/batch)
2017-04-03 00:07:55.349444: step 11460, loss = 2.30 (308.4 examples/sec; 0.415 sec/batch)
2017-04-03 00:07:59.444325: step 11470, loss = 2.30 (312.6 examples/sec; 0.409 sec/batch)
2017-04-03 00:08:03.574591: step 11480, loss = 2.31 (309.9 examples/sec; 0.413 sec/batch)
2017-04-03 00:08:07.712026: step 11490, loss = 2.30 (309.4 examples/sec; 0.414 sec/batch)
2017-04-03 00:08:11.892522: step 11500, loss = 2.30 (306.2 examples/sec; 0.418 sec/batch)
2017-04-03 00:08:16.026522: step 11510, loss = 2.30 (309.6 examples/sec; 0.413 sec/batch)
2017-04-03 00:08:20.137781: step 11520, loss = 2.31 (311.3 examples/sec; 0.411 sec/batch)
2017-04-03 00:08:24.279167: step 11530, loss = 2.30 (309.1 examples/sec; 0.414 sec/batch)
2017-04-03 00:08:28.390429: step 11540, loss = 2.31 (311.3 examples/sec; 0.411 sec/batch)
2017-04-03 00:08:32.528543: step 11550, loss = 2.30 (309.3 examples/sec; 0.414 sec/batch)
2017-04-03 00:08:36.616845: step 11560, loss = 2.30 (313.1 examples/sec; 0.409 sec/batch)
2017-04-03 00:08:40.758318: step 11570, loss = 2.30 (309.1 examples/sec; 0.414 sec/batch)
2017-04-03 00:08:44.875618: step 11580, loss = 2.30 (310.9 examples/sec; 0.412 sec/batch)
2017-04-03 00:08:49.029802: step 11590, loss = 2.31 (308.1 examples/sec; 0.415 sec/batch)
2017-04-03 00:08:53.277045: step 11600, loss = 2.31 (301.4 examples/sec; 0.425 sec/batch)
2017-04-03 00:08:57.408501: step 11610, loss = 2.30 (309.8 examples/sec; 0.413 sec/batch)
2017-04-03 00:09:01.560323: step 11620, loss = 2.30 (308.3 examples/sec; 0.415 sec/batch)
2017-04-03 00:09:05.686614: step 11630, loss = 2.30 (310.2 examples/sec; 0.413 sec/batch)
2017-04-03 00:09:09.859655: step 11640, loss = 2.30 (306.7 examples/sec; 0.417 sec/batch)
2017-04-03 00:09:14.011280: step 11650, loss = 2.30 (308.3 examples/sec; 0.415 sec/batch)
2017-04-03 00:09:18.191099: step 11660, loss = 2.31 (306.2 examples/sec; 0.418 sec/batch)
2017-04-03 00:09:22.330572: step 11670, loss = 2.30 (309.2 examples/sec; 0.414 sec/batch)
2017-04-03 00:09:26.481762: step 11680, loss = 2.30 (308.3 examples/sec; 0.415 sec/batch)
2017-04-03 00:09:30.608514: step 11690, loss = 2.31 (310.2 examples/sec; 0.413 sec/batch)
2017-04-03 00:09:34.835391: step 11700, loss = 2.30 (302.8 examples/sec; 0.423 sec/batch)
2017-04-03 00:09:38.962655: step 11710, loss = 2.30 (310.1 examples/sec; 0.413 sec/batch)
2017-04-03 00:09:43.112452: step 11720, loss = 2.30 (308.4 examples/sec; 0.415 sec/batch)
2017-04-03 00:09:47.243194: step 11730, loss = 2.30 (309.9 examples/sec; 0.413 sec/batch)
2017-04-03 00:09:51.370123: step 11740, loss = 2.30 (310.2 examples/sec; 0.413 sec/batch)
2017-04-03 00:09:55.498044: step 11750, loss = 2.31 (310.1 examples/sec; 0.413 sec/batch)
2017-04-03 00:09:59.649764: step 11760, loss = 2.30 (308.3 examples/sec; 0.415 sec/batch)
2017-04-03 00:10:03.778155: step 11770, loss = 2.30 (310.0 examples/sec; 0.413 sec/batch)
2017-04-03 00:10:07.910559: step 11780, loss = 2.30 (309.7 examples/sec; 0.413 sec/batch)
2017-04-03 00:10:12.015075: step 11790, loss = 2.30 (311.9 examples/sec; 0.410 sec/batch)
2017-04-03 00:10:16.259526: step 11800, loss = 2.30 (301.6 examples/sec; 0.424 sec/batch)
2017-04-03 00:10:20.362602: step 11810, loss = 2.30 (312.0 examples/sec; 0.410 sec/batch)
2017-04-03 00:10:24.506857: step 11820, loss = 2.30 (308.9 examples/sec; 0.414 sec/batch)
2017-04-03 00:10:28.613737: step 11830, loss = 2.31 (311.7 examples/sec; 0.411 sec/batch)
2017-04-03 00:10:32.745882: step 11840, loss = 2.30 (309.8 examples/sec; 0.413 sec/batch)
2017-04-03 00:10:36.858259: step 11850, loss = 2.30 (311.3 examples/sec; 0.411 sec/batch)
2017-04-03 00:10:40.983704: step 11860, loss = 2.30 (310.3 examples/sec; 0.413 sec/batch)
2017-04-03 00:10:45.083423: step 11870, loss = 2.30 (312.2 examples/sec; 0.410 sec/batch)
2017-04-03 00:10:49.210876: step 11880, loss = 2.31 (310.1 examples/sec; 0.413 sec/batch)
2017-04-03 00:10:53.323200: step 11890, loss = 2.30 (311.3 examples/sec; 0.411 sec/batch)
2017-04-03 00:10:57.514565: step 11900, loss = 2.30 (305.4 examples/sec; 0.419 sec/batch)
2017-04-03 00:11:01.614830: step 11910, loss = 2.30 (312.2 examples/sec; 0.410 sec/batch)
2017-04-03 00:11:05.707322: step 11920, loss = 2.30 (312.8 examples/sec; 0.409 sec/batch)
2017-04-03 00:11:09.824851: step 11930, loss = 2.30 (310.9 examples/sec; 0.412 sec/batch)
2017-04-03 00:11:13.966097: step 11940, loss = 2.30 (309.1 examples/sec; 0.414 sec/batch)
2017-04-03 00:11:18.127736: step 11950, loss = 2.30 (307.6 examples/sec; 0.416 sec/batch)
2017-04-03 00:11:22.247687: step 11960, loss = 2.31 (310.7 examples/sec; 0.412 sec/batch)
2017-04-03 00:11:26.384974: step 11970, loss = 2.30 (309.4 examples/sec; 0.414 sec/batch)
2017-04-03 00:11:30.523121: step 11980, loss = 2.30 (309.3 examples/sec; 0.414 sec/batch)
2017-04-03 00:11:34.674068: step 11990, loss = 2.30 (308.4 examples/sec; 0.415 sec/batch)
2017-04-03 00:11:38.862912: step 12000, loss = 2.31 (305.6 examples/sec; 0.419 sec/batch)
2017-04-03 00:12:28.536056: precision @ 1 = 0.098

2017-04-03 00:16:14.144230: step 12010, loss = 2.30 (4.6 examples/sec; 27.528 sec/batch)
2017-04-03 00:16:18.338265: step 12020, loss = 2.30 (305.2 examples/sec; 0.419 sec/batch)
2017-04-03 00:16:22.460055: step 12030, loss = 2.30 (310.5 examples/sec; 0.412 sec/batch)
2017-04-03 00:16:26.591773: step 12040, loss = 2.30 (309.8 examples/sec; 0.413 sec/batch)
2017-04-03 00:16:30.707228: step 12050, loss = 2.30 (311.0 examples/sec; 0.412 sec/batch)
2017-04-03 00:16:34.854070: step 12060, loss = 2.30 (308.7 examples/sec; 0.415 sec/batch)
2017-04-03 00:16:38.992366: step 12070, loss = 2.30 (309.3 examples/sec; 0.414 sec/batch)
2017-04-03 00:16:43.147531: step 12080, loss = 2.30 (308.1 examples/sec; 0.416 sec/batch)
2017-04-03 00:16:47.267258: step 12090, loss = 2.31 (310.7 examples/sec; 0.412 sec/batch)
2017-04-03 00:16:51.489266: step 12100, loss = 2.30 (303.2 examples/sec; 0.422 sec/batch)
2017-04-03 00:16:55.610446: step 12110, loss = 2.30 (310.6 examples/sec; 0.412 sec/batch)
2017-04-03 00:16:59.779146: step 12120, loss = 2.30 (307.1 examples/sec; 0.417 sec/batch)
2017-04-03 00:17:03.944702: step 12130, loss = 2.30 (307.3 examples/sec; 0.417 sec/batch)
2017-04-03 00:17:08.081821: step 12140, loss = 2.30 (309.4 examples/sec; 0.414 sec/batch)
2017-04-03 00:17:12.234516: step 12150, loss = 2.30 (308.2 examples/sec; 0.415 sec/batch)
2017-04-03 00:17:16.352315: step 12160, loss = 2.31 (310.8 examples/sec; 0.412 sec/batch)
2017-04-03 00:17:20.469315: step 12170, loss = 2.30 (310.9 examples/sec; 0.412 sec/batch)
2017-04-03 00:17:24.607312: step 12180, loss = 2.30 (309.3 examples/sec; 0.414 sec/batch)
2017-04-03 00:17:28.749893: step 12190, loss = 2.30 (309.0 examples/sec; 0.414 sec/batch)
2017-04-03 00:17:32.964457: step 12200, loss = 2.31 (303.7 examples/sec; 0.421 sec/batch)
2017-04-03 00:17:37.122352: step 12210, loss = 2.31 (307.8 examples/sec; 0.416 sec/batch)
2017-04-03 00:17:41.287868: step 12220, loss = 2.30 (307.3 examples/sec; 0.417 sec/batch)
2017-04-03 00:17:45.419910: step 12230, loss = 2.30 (309.8 examples/sec; 0.413 sec/batch)
2017-04-03 00:17:49.547040: step 12240, loss = 2.31 (310.1 examples/sec; 0.413 sec/batch)
2017-04-03 00:17:53.702549: step 12250, loss = 2.30 (308.0 examples/sec; 0.416 sec/batch)
2017-04-03 00:17:57.835708: step 12260, loss = 2.30 (309.7 examples/sec; 0.413 sec/batch)
2017-04-03 00:18:01.964784: step 12270, loss = 2.30 (310.0 examples/sec; 0.413 sec/batch)
2017-04-03 00:18:06.145442: step 12280, loss = 2.30 (306.2 examples/sec; 0.418 sec/batch)
2017-04-03 00:18:10.274454: step 12290, loss = 2.30 (310.0 examples/sec; 0.413 sec/batch)
2017-04-03 00:18:14.496782: step 12300, loss = 2.30 (303.2 examples/sec; 0.422 sec/batch)
2017-04-03 00:18:18.645586: step 12310, loss = 2.31 (308.5 examples/sec; 0.415 sec/batch)
2017-04-03 00:18:22.778139: step 12320, loss = 2.31 (309.7 examples/sec; 0.413 sec/batch)
2017-04-03 00:18:26.918168: step 12330, loss = 2.30 (309.2 examples/sec; 0.414 sec/batch)
2017-04-03 00:18:31.096305: step 12340, loss = 2.30 (306.4 examples/sec; 0.418 sec/batch)
2017-04-03 00:18:35.245926: step 12350, loss = 2.31 (308.5 examples/sec; 0.415 sec/batch)
2017-04-03 00:18:39.381710: step 12360, loss = 2.30 (309.5 examples/sec; 0.414 sec/batch)
2017-04-03 00:18:43.579034: step 12370, loss = 2.30 (305.0 examples/sec; 0.420 sec/batch)
2017-04-03 00:18:47.686862: step 12380, loss = 2.30 (311.6 examples/sec; 0.411 sec/batch)
2017-04-03 00:18:51.837569: step 12390, loss = 2.30 (308.4 examples/sec; 0.415 sec/batch)
2017-04-03 00:18:56.052407: step 12400, loss = 2.30 (303.7 examples/sec; 0.421 sec/batch)
2017-04-03 00:19:00.194845: step 12410, loss = 2.30 (309.0 examples/sec; 0.414 sec/batch)
2017-04-03 00:19:04.349919: step 12420, loss = 2.30 (308.1 examples/sec; 0.416 sec/batch)
2017-04-03 00:19:08.497277: step 12430, loss = 2.30 (308.6 examples/sec; 0.415 sec/batch)
2017-04-03 00:19:12.633035: step 12440, loss = 2.30 (309.5 examples/sec; 0.414 sec/batch)
2017-04-03 00:19:16.757148: step 12450, loss = 2.30 (310.4 examples/sec; 0.412 sec/batch)
2017-04-03 00:19:20.918404: step 12460, loss = 2.30 (307.6 examples/sec; 0.416 sec/batch)
2017-04-03 00:19:25.060212: step 12470, loss = 2.30 (309.0 examples/sec; 0.414 sec/batch)
2017-04-03 00:19:29.190646: step 12480, loss = 2.30 (309.9 examples/sec; 0.413 sec/batch)
2017-04-03 00:19:33.331752: step 12490, loss = 2.30 (309.1 examples/sec; 0.414 sec/batch)
2017-04-03 00:19:37.512440: step 12500, loss = 2.30 (306.2 examples/sec; 0.418 sec/batch)
2017-04-03 00:19:41.673283: step 12510, loss = 2.30 (307.6 examples/sec; 0.416 sec/batch)
2017-04-03 00:19:45.821978: step 12520, loss = 2.30 (308.5 examples/sec; 0.415 sec/batch)
2017-04-03 00:19:49.976715: step 12530, loss = 2.30 (308.1 examples/sec; 0.415 sec/batch)
2017-04-03 00:19:54.115922: step 12540, loss = 2.31 (309.2 examples/sec; 0.414 sec/batch)
2017-04-03 00:19:58.268958: step 12550, loss = 2.30 (308.2 examples/sec; 0.415 sec/batch)
2017-04-03 00:20:02.426945: step 12560, loss = 2.30 (307.8 examples/sec; 0.416 sec/batch)
2017-04-03 00:20:06.531913: step 12570, loss = 2.30 (311.8 examples/sec; 0.410 sec/batch)
2017-04-03 00:20:10.672468: step 12580, loss = 2.31 (309.1 examples/sec; 0.414 sec/batch)
2017-04-03 00:20:14.803652: step 12590, loss = 2.30 (309.8 examples/sec; 0.413 sec/batch)
2017-04-03 00:20:19.006847: step 12600, loss = 2.30 (304.5 examples/sec; 0.420 sec/batch)
2017-04-03 00:20:23.146695: step 12610, loss = 2.30 (309.2 examples/sec; 0.414 sec/batch)
2017-04-03 00:20:27.314708: step 12620, loss = 2.30 (307.1 examples/sec; 0.417 sec/batch)
2017-04-03 00:20:31.435299: step 12630, loss = 2.30 (310.6 examples/sec; 0.412 sec/batch)
2017-04-03 00:20:35.581326: step 12640, loss = 2.30 (308.7 examples/sec; 0.415 sec/batch)
2017-04-03 00:20:39.716145: step 12650, loss = 2.30 (309.6 examples/sec; 0.413 sec/batch)
2017-04-03 00:20:43.881352: step 12660, loss = 2.30 (307.3 examples/sec; 0.417 sec/batch)
2017-04-03 00:20:48.034000: step 12670, loss = 2.30 (308.2 examples/sec; 0.415 sec/batch)
2017-04-03 00:20:52.196399: step 12680, loss = 2.30 (307.5 examples/sec; 0.416 sec/batch)
2017-04-03 00:20:56.328625: step 12690, loss = 2.30 (309.8 examples/sec; 0.413 sec/batch)
2017-04-03 00:21:00.550777: step 12700, loss = 2.30 (303.2 examples/sec; 0.422 sec/batch)
2017-04-03 00:21:04.721513: step 12710, loss = 2.30 (306.9 examples/sec; 0.417 sec/batch)
2017-04-03 00:21:08.907358: step 12720, loss = 2.30 (305.8 examples/sec; 0.419 sec/batch)
2017-04-03 00:21:13.020365: step 12730, loss = 2.30 (311.2 examples/sec; 0.411 sec/batch)
2017-04-03 00:21:17.193448: step 12740, loss = 2.30 (306.7 examples/sec; 0.417 sec/batch)
2017-04-03 00:21:21.332273: step 12750, loss = 2.30 (309.3 examples/sec; 0.414 sec/batch)
2017-04-03 00:21:25.496829: step 12760, loss = 2.30 (307.4 examples/sec; 0.416 sec/batch)
2017-04-03 00:21:29.647623: step 12770, loss = 2.30 (308.4 examples/sec; 0.415 sec/batch)
2017-04-03 00:21:33.814715: step 12780, loss = 2.30 (307.2 examples/sec; 0.417 sec/batch)
2017-04-03 00:21:37.981457: step 12790, loss = 2.30 (307.2 examples/sec; 0.417 sec/batch)
2017-04-03 00:21:42.189571: step 12800, loss = 2.30 (304.2 examples/sec; 0.421 sec/batch)
2017-04-03 00:21:46.330824: step 12810, loss = 2.30 (309.1 examples/sec; 0.414 sec/batch)
2017-04-03 00:21:50.493285: step 12820, loss = 2.30 (307.5 examples/sec; 0.416 sec/batch)
2017-04-03 00:21:54.686770: step 12830, loss = 2.30 (305.2 examples/sec; 0.419 sec/batch)
2017-04-03 00:21:58.862044: step 12840, loss = 2.30 (306.6 examples/sec; 0.418 sec/batch)
2017-04-03 00:22:03.018485: step 12850, loss = 2.31 (308.0 examples/sec; 0.416 sec/batch)
2017-04-03 00:22:07.187220: step 12860, loss = 2.30 (307.0 examples/sec; 0.417 sec/batch)
2017-04-03 00:22:11.340232: step 12870, loss = 2.30 (308.2 examples/sec; 0.415 sec/batch)
2017-04-03 00:22:15.487775: step 12880, loss = 2.30 (308.6 examples/sec; 0.415 sec/batch)
2017-04-03 00:22:19.626964: step 12890, loss = 2.30 (309.2 examples/sec; 0.414 sec/batch)
2017-04-03 00:22:23.888424: step 12900, loss = 2.30 (300.4 examples/sec; 0.426 sec/batch)
2017-04-03 00:22:28.028858: step 12910, loss = 2.30 (309.1 examples/sec; 0.414 sec/batch)
2017-04-03 00:22:32.209160: step 12920, loss = 2.30 (306.2 examples/sec; 0.418 sec/batch)
2017-04-03 00:22:36.369369: step 12930, loss = 2.30 (307.7 examples/sec; 0.416 sec/batch)
2017-04-03 00:22:40.526941: step 12940, loss = 2.30 (307.9 examples/sec; 0.416 sec/batch)
2017-04-03 00:22:44.701931: step 12950, loss = 2.30 (306.6 examples/sec; 0.417 sec/batch)
2017-04-03 00:22:48.817439: step 12960, loss = 2.30 (311.0 examples/sec; 0.412 sec/batch)
2017-04-03 00:22:52.982151: step 12970, loss = 2.30 (307.3 examples/sec; 0.416 sec/batch)
2017-04-03 00:22:57.141654: step 12980, loss = 2.30 (307.7 examples/sec; 0.416 sec/batch)
2017-04-03 00:23:01.265379: step 12990, loss = 2.30 (310.4 examples/sec; 0.412 sec/batch)
2017-04-03 00:23:05.457343: step 13000, loss = 2.30 (305.3 examples/sec; 0.419 sec/batch)
2017-04-03 00:23:09.619705: step 13010, loss = 2.30 (307.5 examples/sec; 0.416 sec/batch)
2017-04-03 00:23:13.797231: step 13020, loss = 2.30 (306.4 examples/sec; 0.418 sec/batch)
2017-04-03 00:23:17.950455: step 13030, loss = 2.30 (308.2 examples/sec; 0.415 sec/batch)
2017-04-03 00:23:22.090729: step 13040, loss = 2.30 (309.2 examples/sec; 0.414 sec/batch)
2017-04-03 00:23:26.225973: step 13050, loss = 2.30 (309.5 examples/sec; 0.414 sec/batch)
2017-04-03 00:23:30.374135: step 13060, loss = 2.30 (308.6 examples/sec; 0.415 sec/batch)
2017-04-03 00:23:34.513100: step 13070, loss = 2.30 (309.3 examples/sec; 0.414 sec/batch)
2017-04-03 00:23:38.651156: step 13080, loss = 2.30 (309.3 examples/sec; 0.414 sec/batch)
2017-04-03 00:23:42.787492: step 13090, loss = 2.30 (309.5 examples/sec; 0.414 sec/batch)
2017-04-03 00:23:47.000303: step 13100, loss = 2.30 (303.8 examples/sec; 0.421 sec/batch)
2017-04-03 00:23:51.146211: step 13110, loss = 2.31 (308.7 examples/sec; 0.415 sec/batch)
2017-04-03 00:23:55.270088: step 13120, loss = 2.30 (310.4 examples/sec; 0.412 sec/batch)
2017-04-03 00:23:59.419702: step 13130, loss = 2.30 (308.5 examples/sec; 0.415 sec/batch)
2017-04-03 00:24:03.574779: step 13140, loss = 2.30 (308.1 examples/sec; 0.416 sec/batch)
2017-04-03 00:24:07.717207: step 13150, loss = 2.30 (309.0 examples/sec; 0.414 sec/batch)
2017-04-03 00:24:11.857222: step 13160, loss = 2.30 (309.2 examples/sec; 0.414 sec/batch)
2017-04-03 00:24:15.975195: step 13170, loss = 2.30 (310.8 examples/sec; 0.412 sec/batch)
2017-04-03 00:24:20.155360: step 13180, loss = 2.30 (306.2 examples/sec; 0.418 sec/batch)
2017-04-03 00:24:24.284515: step 13190, loss = 2.30 (310.0 examples/sec; 0.413 sec/batch)
2017-04-03 00:24:28.491402: step 13200, loss = 2.30 (304.3 examples/sec; 0.421 sec/batch)
2017-04-03 00:24:32.677532: step 13210, loss = 2.30 (305.8 examples/sec; 0.419 sec/batch)
2017-04-03 00:24:36.792794: step 13220, loss = 2.31 (311.0 examples/sec; 0.412 sec/batch)
2017-04-03 00:24:40.917578: step 13230, loss = 2.31 (310.3 examples/sec; 0.412 sec/batch)
2017-04-03 00:24:45.031722: step 13240, loss = 2.30 (311.1 examples/sec; 0.411 sec/batch)
2017-04-03 00:24:49.144196: step 13250, loss = 2.30 (311.2 examples/sec; 0.411 sec/batch)
2017-04-03 00:24:53.263420: step 13260, loss = 2.30 (310.7 examples/sec; 0.412 sec/batch)
2017-04-03 00:24:57.393384: step 13270, loss = 2.30 (309.9 examples/sec; 0.413 sec/batch)
2017-04-03 00:25:01.554540: step 13280, loss = 2.30 (307.6 examples/sec; 0.416 sec/batch)
2017-04-03 00:25:05.674828: step 13290, loss = 2.30 (310.7 examples/sec; 0.412 sec/batch)
2017-04-03 00:25:09.995168: step 13300, loss = 2.30 (296.3 examples/sec; 0.432 sec/batch)
2017-04-03 00:25:14.074370: step 13310, loss = 2.30 (313.8 examples/sec; 0.408 sec/batch)
2017-04-03 00:25:18.169143: step 13320, loss = 2.30 (312.6 examples/sec; 0.409 sec/batch)
2017-04-03 00:25:22.330103: step 13330, loss = 2.30 (307.6 examples/sec; 0.416 sec/batch)
2017-04-03 00:25:26.478194: step 13340, loss = 2.30 (308.6 examples/sec; 0.415 sec/batch)
2017-04-03 00:25:30.609805: step 13350, loss = 2.30 (309.8 examples/sec; 0.413 sec/batch)
2017-04-03 00:25:34.787558: step 13360, loss = 2.30 (306.4 examples/sec; 0.418 sec/batch)
2017-04-03 00:25:38.903370: step 13370, loss = 2.30 (311.0 examples/sec; 0.412 sec/batch)
2017-04-03 00:25:43.051911: step 13380, loss = 2.30 (308.5 examples/sec; 0.415 sec/batch)
2017-04-03 00:25:47.164254: step 13390, loss = 2.30 (311.3 examples/sec; 0.411 sec/batch)
2017-04-03 00:25:51.353968: step 13400, loss = 2.31 (305.5 examples/sec; 0.419 sec/batch)
2017-04-03 00:25:55.471453: step 13410, loss = 2.30 (310.9 examples/sec; 0.412 sec/batch)
2017-04-03 00:25:59.634905: step 13420, loss = 2.30 (307.4 examples/sec; 0.416 sec/batch)
2017-04-03 00:26:03.758008: step 13430, loss = 2.30 (310.4 examples/sec; 0.412 sec/batch)
2017-04-03 00:26:07.902252: step 13440, loss = 2.30 (308.9 examples/sec; 0.414 sec/batch)
2017-04-03 00:26:12.066542: step 13450, loss = 2.31 (307.4 examples/sec; 0.416 sec/batch)
2017-04-03 00:26:16.958180: step 13460, loss = 2.30 (261.7 examples/sec; 0.489 sec/batch)
2017-04-03 00:26:21.067012: step 13470, loss = 2.31 (311.5 examples/sec; 0.411 sec/batch)
2017-04-03 00:26:25.182688: step 13480, loss = 2.30 (311.0 examples/sec; 0.412 sec/batch)
2017-04-03 00:26:29.269226: step 13490, loss = 2.30 (313.2 examples/sec; 0.409 sec/batch)
2017-04-03 00:26:33.427196: step 13500, loss = 2.30 (307.8 examples/sec; 0.416 sec/batch)
2017-04-03 00:26:37.507065: step 13510, loss = 2.30 (313.7 examples/sec; 0.408 sec/batch)
2017-04-03 00:26:41.587521: step 13520, loss = 2.30 (313.7 examples/sec; 0.408 sec/batch)
2017-04-03 00:26:45.704082: step 13530, loss = 2.30 (310.9 examples/sec; 0.412 sec/batch)
2017-04-03 00:26:49.798046: step 13540, loss = 2.30 (312.7 examples/sec; 0.409 sec/batch)
2017-04-03 00:26:53.882140: step 13550, loss = 2.30 (313.4 examples/sec; 0.408 sec/batch)
2017-04-03 00:26:57.970791: step 13560, loss = 2.30 (313.1 examples/sec; 0.409 sec/batch)
2017-04-03 00:27:02.052614: step 13570, loss = 2.30 (313.6 examples/sec; 0.408 sec/batch)
2017-04-03 00:27:06.174231: step 13580, loss = 2.31 (310.6 examples/sec; 0.412 sec/batch)
2017-04-03 00:27:10.286193: step 13590, loss = 2.31 (311.3 examples/sec; 0.411 sec/batch)
2017-04-03 00:27:14.414592: step 13600, loss = 2.30 (310.0 examples/sec; 0.413 sec/batch)
2017-04-03 00:27:18.491249: step 13610, loss = 2.30 (314.0 examples/sec; 0.408 sec/batch)
2017-04-03 00:27:22.591252: step 13620, loss = 2.30 (312.2 examples/sec; 0.410 sec/batch)
2017-04-03 00:27:26.703430: step 13630, loss = 2.30 (311.3 examples/sec; 0.411 sec/batch)
2017-04-03 00:27:30.806718: step 13640, loss = 2.30 (311.9 examples/sec; 0.410 sec/batch)
2017-04-03 00:27:34.915293: step 13650, loss = 2.31 (311.5 examples/sec; 0.411 sec/batch)
2017-04-03 00:27:39.019223: step 13660, loss = 2.30 (311.9 examples/sec; 0.410 sec/batch)
2017-04-03 00:27:43.109369: step 13670, loss = 2.30 (312.9 examples/sec; 0.409 sec/batch)
2017-04-03 00:27:47.212798: step 13680, loss = 2.30 (311.9 examples/sec; 0.410 sec/batch)
2017-04-03 00:27:51.282564: step 13690, loss = 2.30 (314.5 examples/sec; 0.407 sec/batch)
2017-04-03 00:27:55.467670: step 13700, loss = 2.31 (305.8 examples/sec; 0.419 sec/batch)
2017-04-03 00:27:59.584339: step 13710, loss = 2.30 (310.9 examples/sec; 0.412 sec/batch)
2017-04-03 00:28:03.693232: step 13720, loss = 2.31 (311.5 examples/sec; 0.411 sec/batch)
2017-04-03 00:28:07.829500: step 13730, loss = 2.31 (309.5 examples/sec; 0.414 sec/batch)
2017-04-03 00:28:11.917194: step 13740, loss = 2.31 (313.1 examples/sec; 0.409 sec/batch)
2017-04-03 00:28:16.036199: step 13750, loss = 2.30 (310.8 examples/sec; 0.412 sec/batch)
2017-04-03 00:28:20.141883: step 13760, loss = 2.30 (311.8 examples/sec; 0.411 sec/batch)
2017-04-03 00:28:24.249652: step 13770, loss = 2.30 (311.6 examples/sec; 0.411 sec/batch)
2017-04-03 00:28:28.338831: step 13780, loss = 2.30 (313.0 examples/sec; 0.409 sec/batch)
2017-04-03 00:28:32.445481: step 13790, loss = 2.30 (311.7 examples/sec; 0.411 sec/batch)
2017-04-03 00:28:36.608405: step 13800, loss = 2.30 (307.5 examples/sec; 0.416 sec/batch)
2017-04-03 00:28:40.702404: step 13810, loss = 2.30 (312.7 examples/sec; 0.409 sec/batch)
2017-04-03 00:28:44.779300: step 13820, loss = 2.30 (314.0 examples/sec; 0.408 sec/batch)
2017-04-03 00:28:48.896806: step 13830, loss = 2.30 (310.9 examples/sec; 0.412 sec/batch)
2017-04-03 00:28:53.051536: step 13840, loss = 2.30 (308.1 examples/sec; 0.415 sec/batch)
2017-04-03 00:28:57.180223: step 13850, loss = 2.30 (310.0 examples/sec; 0.413 sec/batch)
2017-04-03 00:29:01.291962: step 13860, loss = 2.30 (311.3 examples/sec; 0.411 sec/batch)
2017-04-03 00:29:05.383115: step 13870, loss = 2.30 (312.9 examples/sec; 0.409 sec/batch)
2017-04-03 00:29:09.471537: step 13880, loss = 2.30 (313.1 examples/sec; 0.409 sec/batch)
2017-04-03 00:29:13.531406: step 13890, loss = 2.31 (315.3 examples/sec; 0.406 sec/batch)
2017-04-03 00:29:17.718793: step 13900, loss = 2.30 (305.7 examples/sec; 0.419 sec/batch)
2017-04-03 00:29:21.826971: step 13910, loss = 2.30 (311.6 examples/sec; 0.411 sec/batch)
2017-04-03 00:29:25.935201: step 13920, loss = 2.30 (311.6 examples/sec; 0.411 sec/batch)
2017-04-03 00:29:30.043812: step 13930, loss = 2.30 (311.5 examples/sec; 0.411 sec/batch)
2017-04-03 00:29:34.121389: step 13940, loss = 2.30 (313.9 examples/sec; 0.408 sec/batch)
2017-04-03 00:29:38.214551: step 13950, loss = 2.30 (312.7 examples/sec; 0.409 sec/batch)
2017-04-03 00:29:42.313682: step 13960, loss = 2.30 (312.3 examples/sec; 0.410 sec/batch)
2017-04-03 00:29:46.421503: step 13970, loss = 2.30 (311.6 examples/sec; 0.411 sec/batch)
2017-04-03 00:29:50.492434: step 13980, loss = 2.30 (314.4 examples/sec; 0.407 sec/batch)
2017-04-03 00:29:54.582914: step 13990, loss = 2.30 (312.9 examples/sec; 0.409 sec/batch)
2017-04-03 00:29:58.757838: step 14000, loss = 2.31 (306.6 examples/sec; 0.417 sec/batch)
2017-04-03 00:30:45.384687: precision @ 1 = 0.101

2017-04-03 00:31:18.203146: step 14010, loss = 2.30 (16.1 examples/sec; 7.945 sec/batch)
2017-04-03 00:31:22.345164: step 14020, loss = 2.30 (309.0 examples/sec; 0.414 sec/batch)
2017-04-03 00:31:26.471018: step 14030, loss = 2.30 (310.2 examples/sec; 0.413 sec/batch)
2017-04-03 00:31:30.621565: step 14040, loss = 2.30 (308.4 examples/sec; 0.415 sec/batch)
2017-04-03 00:31:34.742479: step 14050, loss = 2.30 (310.6 examples/sec; 0.412 sec/batch)
2017-04-03 00:31:38.859637: step 14060, loss = 2.30 (310.9 examples/sec; 0.412 sec/batch)
2017-04-03 00:31:43.011528: step 14070, loss = 2.30 (308.3 examples/sec; 0.415 sec/batch)
2017-04-03 00:31:47.113717: step 14080, loss = 2.30 (312.0 examples/sec; 0.410 sec/batch)
2017-04-03 00:31:51.236478: step 14090, loss = 2.30 (310.5 examples/sec; 0.412 sec/batch)
2017-04-03 00:31:55.539724: step 14100, loss = 2.31 (297.4 examples/sec; 0.430 sec/batch)
2017-04-03 00:31:59.670550: step 14110, loss = 2.30 (309.9 examples/sec; 0.413 sec/batch)
2017-04-03 00:32:03.833675: step 14120, loss = 2.30 (307.5 examples/sec; 0.416 sec/batch)
2017-04-03 00:32:07.973562: step 14130, loss = 2.30 (309.2 examples/sec; 0.414 sec/batch)
2017-04-03 00:32:12.115732: step 14140, loss = 2.30 (309.0 examples/sec; 0.414 sec/batch)
2017-04-03 00:32:16.231752: step 14150, loss = 2.30 (311.0 examples/sec; 0.412 sec/batch)
2017-04-03 00:32:20.379701: step 14160, loss = 2.30 (308.6 examples/sec; 0.415 sec/batch)
2017-04-03 00:32:24.501301: step 14170, loss = 2.30 (310.6 examples/sec; 0.412 sec/batch)
2017-04-03 00:32:28.640861: step 14180, loss = 2.30 (309.2 examples/sec; 0.414 sec/batch)
2017-04-03 00:32:32.755343: step 14190, loss = 2.30 (311.1 examples/sec; 0.411 sec/batch)
2017-04-03 00:32:36.936847: step 14200, loss = 2.30 (306.1 examples/sec; 0.418 sec/batch)
2017-04-03 00:32:41.058509: step 14210, loss = 2.30 (310.6 examples/sec; 0.412 sec/batch)
2017-04-03 00:32:45.199211: step 14220, loss = 2.30 (309.1 examples/sec; 0.414 sec/batch)
2017-04-03 00:32:49.329978: step 14230, loss = 2.30 (309.9 examples/sec; 0.413 sec/batch)
2017-04-03 00:32:53.471482: step 14240, loss = 2.30 (309.1 examples/sec; 0.414 sec/batch)
2017-04-03 00:32:57.590592: step 14250, loss = 2.30 (310.7 examples/sec; 0.412 sec/batch)
2017-04-03 00:33:01.712316: step 14260, loss = 2.30 (310.5 examples/sec; 0.412 sec/batch)
2017-04-03 00:33:05.821732: step 14270, loss = 2.30 (311.5 examples/sec; 0.411 sec/batch)
2017-04-03 00:33:09.958164: step 14280, loss = 2.31 (309.4 examples/sec; 0.414 sec/batch)
2017-04-03 00:33:14.039288: step 14290, loss = 2.30 (313.6 examples/sec; 0.408 sec/batch)
2017-04-03 00:33:18.205792: step 14300, loss = 2.30 (307.2 examples/sec; 0.417 sec/batch)
2017-04-03 00:33:22.338313: step 14310, loss = 2.30 (309.7 examples/sec; 0.413 sec/batch)
2017-04-03 00:33:26.455429: step 14320, loss = 2.30 (310.9 examples/sec; 0.412 sec/batch)
2017-04-03 00:33:30.644209: step 14330, loss = 2.30 (305.6 examples/sec; 0.419 sec/batch)
2017-04-03 00:33:34.750437: step 14340, loss = 2.30 (311.7 examples/sec; 0.411 sec/batch)
2017-04-03 00:33:38.883794: step 14350, loss = 2.30 (309.7 examples/sec; 0.413 sec/batch)
2017-04-03 00:33:43.014149: step 14360, loss = 2.31 (309.9 examples/sec; 0.413 sec/batch)
2017-04-03 00:33:47.110643: step 14370, loss = 2.30 (312.5 examples/sec; 0.410 sec/batch)
2017-04-03 00:33:51.221983: step 14380, loss = 2.30 (311.3 examples/sec; 0.411 sec/batch)
2017-04-03 00:33:55.325362: step 14390, loss = 2.31 (311.9 examples/sec; 0.410 sec/batch)
2017-04-03 00:33:59.503613: step 14400, loss = 2.30 (306.3 examples/sec; 0.418 sec/batch)
2017-04-03 00:34:03.670078: step 14410, loss = 2.30 (307.2 examples/sec; 0.417 sec/batch)
2017-04-03 00:34:07.794802: step 14420, loss = 2.30 (310.3 examples/sec; 0.412 sec/batch)
2017-04-03 00:34:11.899395: step 14430, loss = 2.30 (311.8 examples/sec; 0.410 sec/batch)
2017-04-03 00:34:16.048119: step 14440, loss = 2.30 (308.5 examples/sec; 0.415 sec/batch)
2017-04-03 00:34:20.151325: step 14450, loss = 2.30 (312.0 examples/sec; 0.410 sec/batch)
2017-04-03 00:34:24.261262: step 14460, loss = 2.31 (311.4 examples/sec; 0.411 sec/batch)
2017-04-03 00:34:28.371589: step 14470, loss = 2.30 (311.4 examples/sec; 0.411 sec/batch)
2017-04-03 00:34:32.482169: step 14480, loss = 2.30 (311.4 examples/sec; 0.411 sec/batch)
2017-04-03 00:34:36.602495: step 14490, loss = 2.30 (310.7 examples/sec; 0.412 sec/batch)
2017-04-03 00:34:40.771655: step 14500, loss = 2.30 (307.0 examples/sec; 0.417 sec/batch)
2017-04-03 00:34:44.921711: step 14510, loss = 2.30 (308.4 examples/sec; 0.415 sec/batch)
2017-04-03 00:34:49.066132: step 14520, loss = 2.30 (308.8 examples/sec; 0.414 sec/batch)
2017-04-03 00:34:53.208179: step 14530, loss = 2.31 (309.0 examples/sec; 0.414 sec/batch)
2017-04-03 00:34:57.386453: step 14540, loss = 2.30 (306.3 examples/sec; 0.418 sec/batch)
2017-04-03 00:35:01.492880: step 14550, loss = 2.30 (311.7 examples/sec; 0.411 sec/batch)
2017-04-03 00:35:05.601489: step 14560, loss = 2.30 (311.5 examples/sec; 0.411 sec/batch)
2017-04-03 00:35:09.760620: step 14570, loss = 2.30 (307.8 examples/sec; 0.416 sec/batch)
2017-04-03 00:35:13.891208: step 14580, loss = 2.30 (309.9 examples/sec; 0.413 sec/batch)
2017-04-03 00:35:18.031131: step 14590, loss = 2.30 (309.2 examples/sec; 0.414 sec/batch)
2017-04-03 00:35:22.247112: step 14600, loss = 2.30 (303.6 examples/sec; 0.422 sec/batch)
2017-04-03 00:35:26.419229: step 14610, loss = 2.30 (306.8 examples/sec; 0.417 sec/batch)
2017-04-03 00:35:30.559508: step 14620, loss = 2.30 (309.2 examples/sec; 0.414 sec/batch)
2017-04-03 00:35:34.688814: step 14630, loss = 2.30 (310.0 examples/sec; 0.413 sec/batch)
2017-04-03 00:35:38.833364: step 14640, loss = 2.30 (308.8 examples/sec; 0.414 sec/batch)
2017-04-03 00:35:42.960533: step 14650, loss = 2.30 (310.1 examples/sec; 0.413 sec/batch)
2017-04-03 00:35:47.098232: step 14660, loss = 2.30 (309.4 examples/sec; 0.414 sec/batch)
2017-04-03 00:35:51.240585: step 14670, loss = 2.30 (309.0 examples/sec; 0.414 sec/batch)
2017-04-03 00:35:55.367257: step 14680, loss = 2.30 (310.2 examples/sec; 0.413 sec/batch)
2017-04-03 00:35:59.537952: step 14690, loss = 2.30 (306.9 examples/sec; 0.417 sec/batch)
2017-04-03 00:36:03.768196: step 14700, loss = 2.31 (302.6 examples/sec; 0.423 sec/batch)
2017-04-03 00:36:07.917858: step 14710, loss = 2.30 (308.5 examples/sec; 0.415 sec/batch)
2017-04-03 00:36:12.060966: step 14720, loss = 2.30 (308.9 examples/sec; 0.414 sec/batch)
2017-04-03 00:36:17.127281: step 14730, loss = 2.30 (252.6 examples/sec; 0.507 sec/batch)
2017-04-03 00:36:21.290279: step 14740, loss = 2.30 (307.5 examples/sec; 0.416 sec/batch)
2017-04-03 00:36:25.404137: step 14750, loss = 2.30 (311.1 examples/sec; 0.411 sec/batch)
2017-04-03 00:36:29.539690: step 14760, loss = 2.30 (309.5 examples/sec; 0.414 sec/batch)
2017-04-03 00:36:33.693944: step 14770, loss = 2.30 (308.1 examples/sec; 0.415 sec/batch)
2017-04-03 00:36:37.807668: step 14780, loss = 2.30 (311.2 examples/sec; 0.411 sec/batch)
2017-04-03 00:36:41.928799: step 14790, loss = 2.30 (310.6 examples/sec; 0.412 sec/batch)
2017-04-03 00:36:46.140241: step 14800, loss = 2.30 (303.9 examples/sec; 0.421 sec/batch)
2017-04-03 00:36:50.294674: step 14810, loss = 2.30 (308.1 examples/sec; 0.415 sec/batch)
2017-04-03 00:36:54.437115: step 14820, loss = 2.31 (309.0 examples/sec; 0.414 sec/batch)
2017-04-03 00:36:58.585697: step 14830, loss = 2.30 (308.5 examples/sec; 0.415 sec/batch)
2017-04-03 00:37:02.729675: step 14840, loss = 2.30 (308.9 examples/sec; 0.414 sec/batch)
2017-04-03 00:37:06.893202: step 14850, loss = 2.30 (307.4 examples/sec; 0.416 sec/batch)
2017-04-03 00:37:11.017503: step 14860, loss = 2.30 (310.4 examples/sec; 0.412 sec/batch)
2017-04-03 00:37:15.197027: step 14870, loss = 2.30 (306.3 examples/sec; 0.418 sec/batch)
2017-04-03 00:37:19.357784: step 14880, loss = 2.30 (307.6 examples/sec; 0.416 sec/batch)
2017-04-03 00:37:23.516287: step 14890, loss = 2.30 (307.8 examples/sec; 0.416 sec/batch)
2017-04-03 00:37:27.720688: step 14900, loss = 2.30 (304.4 examples/sec; 0.420 sec/batch)
2017-04-03 00:37:31.834439: step 14910, loss = 2.30 (311.2 examples/sec; 0.411 sec/batch)
2017-04-03 00:37:35.991804: step 14920, loss = 2.30 (307.9 examples/sec; 0.416 sec/batch)
2017-04-03 00:37:40.150249: step 14930, loss = 2.30 (307.8 examples/sec; 0.416 sec/batch)
2017-04-03 00:37:44.314340: step 14940, loss = 2.30 (307.4 examples/sec; 0.416 sec/batch)
2017-04-03 00:37:48.486923: step 14950, loss = 2.30 (306.8 examples/sec; 0.417 sec/batch)
2017-04-03 00:37:52.654163: step 14960, loss = 2.30 (307.2 examples/sec; 0.417 sec/batch)
2017-04-03 00:37:56.809061: step 14970, loss = 2.30 (308.1 examples/sec; 0.415 sec/batch)
2017-04-03 00:38:00.936497: step 14980, loss = 2.30 (310.1 examples/sec; 0.413 sec/batch)
2017-04-03 00:38:05.080817: step 14990, loss = 2.30 (308.9 examples/sec; 0.414 sec/batch)
2017-04-03 00:38:09.286409: step 15000, loss = 2.30 (304.4 examples/sec; 0.421 sec/batch)
2017-04-03 00:38:13.417989: step 15010, loss = 2.30 (309.8 examples/sec; 0.413 sec/batch)
2017-04-03 00:38:17.541908: step 15020, loss = 2.30 (310.4 examples/sec; 0.412 sec/batch)
2017-04-03 00:38:21.671848: step 15030, loss = 2.30 (309.9 examples/sec; 0.413 sec/batch)
2017-04-03 00:38:25.811079: step 15040, loss = 2.30 (309.2 examples/sec; 0.414 sec/batch)
2017-04-03 00:38:29.947001: step 15050, loss = 2.30 (309.5 examples/sec; 0.414 sec/batch)
2017-04-03 00:38:34.073956: step 15060, loss = 2.30 (310.2 examples/sec; 0.413 sec/batch)
2017-04-03 00:38:38.235951: step 15070, loss = 2.30 (307.5 examples/sec; 0.416 sec/batch)
2017-04-03 00:38:42.645174: step 15080, loss = 2.30 (290.3 examples/sec; 0.441 sec/batch)
2017-04-03 00:38:46.779730: step 15090, loss = 2.30 (309.6 examples/sec; 0.413 sec/batch)
2017-04-03 00:38:50.960146: step 15100, loss = 2.31 (306.2 examples/sec; 0.418 sec/batch)
2017-04-03 00:38:55.081886: step 15110, loss = 2.30 (310.5 examples/sec; 0.412 sec/batch)
2017-04-03 00:38:59.191321: step 15120, loss = 2.30 (311.5 examples/sec; 0.411 sec/batch)
2017-04-03 00:39:03.342157: step 15130, loss = 2.31 (308.4 examples/sec; 0.415 sec/batch)
2017-04-03 00:39:07.477564: step 15140, loss = 2.30 (309.5 examples/sec; 0.414 sec/batch)
2017-04-03 00:39:11.634385: step 15150, loss = 2.30 (307.9 examples/sec; 0.416 sec/batch)
2017-04-03 00:39:15.752081: step 15160, loss = 2.30 (310.9 examples/sec; 0.412 sec/batch)
2017-04-03 00:39:19.962285: step 15170, loss = 2.30 (304.0 examples/sec; 0.421 sec/batch)
2017-04-03 00:39:24.070032: step 15180, loss = 2.30 (311.6 examples/sec; 0.411 sec/batch)
2017-04-03 00:39:28.218230: step 15190, loss = 2.30 (308.6 examples/sec; 0.415 sec/batch)
2017-04-03 00:39:32.372315: step 15200, loss = 2.30 (308.1 examples/sec; 0.415 sec/batch)
2017-04-03 00:39:36.511956: step 15210, loss = 2.30 (309.2 examples/sec; 0.414 sec/batch)
2017-04-03 00:39:40.603263: step 15220, loss = 2.30 (312.9 examples/sec; 0.409 sec/batch)
2017-04-03 00:39:44.715693: step 15230, loss = 2.30 (311.3 examples/sec; 0.411 sec/batch)
2017-04-03 00:39:48.851808: step 15240, loss = 2.30 (309.5 examples/sec; 0.414 sec/batch)
2017-04-03 00:39:53.080376: step 15250, loss = 2.30 (302.7 examples/sec; 0.423 sec/batch)
2017-04-03 00:39:57.169404: step 15260, loss = 2.31 (313.0 examples/sec; 0.409 sec/batch)
2017-04-03 00:40:01.281724: step 15270, loss = 2.30 (311.3 examples/sec; 0.411 sec/batch)
2017-04-03 00:40:05.386612: step 15280, loss = 2.30 (311.8 examples/sec; 0.410 sec/batch)
2017-04-03 00:40:09.551290: step 15290, loss = 2.31 (307.3 examples/sec; 0.416 sec/batch)
2017-04-03 00:40:13.787625: step 15300, loss = 2.30 (302.1 examples/sec; 0.424 sec/batch)
2017-04-03 00:40:17.953915: step 15310, loss = 2.30 (307.2 examples/sec; 0.417 sec/batch)
2017-04-03 00:40:22.072679: step 15320, loss = 2.30 (310.8 examples/sec; 0.412 sec/batch)
2017-04-03 00:40:26.186822: step 15330, loss = 2.30 (311.1 examples/sec; 0.411 sec/batch)
2017-04-03 00:40:30.328132: step 15340, loss = 2.30 (309.1 examples/sec; 0.414 sec/batch)
2017-04-03 00:40:34.465849: step 15350, loss = 2.30 (309.3 examples/sec; 0.414 sec/batch)
2017-04-03 00:40:38.600979: step 15360, loss = 2.30 (309.5 examples/sec; 0.414 sec/batch)
2017-04-03 00:40:42.730010: step 15370, loss = 2.30 (310.0 examples/sec; 0.413 sec/batch)
2017-04-03 00:40:46.892082: step 15380, loss = 2.30 (307.5 examples/sec; 0.416 sec/batch)
2017-04-03 00:40:51.022656: step 15390, loss = 2.30 (309.9 examples/sec; 0.413 sec/batch)
2017-04-03 00:40:55.210270: step 15400, loss = 2.31 (305.7 examples/sec; 0.419 sec/batch)
2017-04-03 00:40:59.315162: step 15410, loss = 2.31 (311.8 examples/sec; 0.410 sec/batch)
2017-04-03 00:41:03.464991: step 15420, loss = 2.30 (308.4 examples/sec; 0.415 sec/batch)
2017-04-03 00:41:07.598071: step 15430, loss = 2.30 (309.7 examples/sec; 0.413 sec/batch)
2017-04-03 00:41:11.706288: step 15440, loss = 2.30 (311.6 examples/sec; 0.411 sec/batch)
2017-04-03 00:41:15.879621: step 15450, loss = 2.31 (306.7 examples/sec; 0.417 sec/batch)
2017-04-03 00:41:20.009975: step 15460, loss = 2.30 (309.9 examples/sec; 0.413 sec/batch)
2017-04-03 00:41:24.139330: step 15470, loss = 2.30 (310.0 examples/sec; 0.413 sec/batch)
2017-04-03 00:41:28.260754: step 15480, loss = 2.30 (310.6 examples/sec; 0.412 sec/batch)
2017-04-03 00:41:32.403290: step 15490, loss = 2.31 (309.0 examples/sec; 0.414 sec/batch)
2017-04-03 00:41:36.601462: step 15500, loss = 2.31 (304.9 examples/sec; 0.420 sec/batch)
2017-04-03 00:41:40.731214: step 15510, loss = 2.30 (309.9 examples/sec; 0.413 sec/batch)
2017-04-03 00:41:44.851812: step 15520, loss = 2.30 (310.6 examples/sec; 0.412 sec/batch)
2017-04-03 00:41:48.970094: step 15530, loss = 2.30 (310.8 examples/sec; 0.412 sec/batch)
2017-04-03 00:41:53.063065: step 15540, loss = 2.31 (312.7 examples/sec; 0.409 sec/batch)
2017-04-03 00:41:57.185676: step 15550, loss = 2.31 (310.5 examples/sec; 0.412 sec/batch)
2017-04-03 00:42:01.279557: step 15560, loss = 2.30 (312.7 examples/sec; 0.409 sec/batch)
2017-04-03 00:42:05.443940: step 15570, loss = 2.30 (307.4 examples/sec; 0.416 sec/batch)
2017-04-03 00:42:09.558604: step 15580, loss = 2.31 (311.1 examples/sec; 0.411 sec/batch)
2017-04-03 00:42:13.681470: step 15590, loss = 2.30 (310.5 examples/sec; 0.412 sec/batch)
2017-04-03 00:42:17.900779: step 15600, loss = 2.30 (303.4 examples/sec; 0.422 sec/batch)
2017-04-03 00:42:22.004044: step 15610, loss = 2.30 (311.9 examples/sec; 0.410 sec/batch)
2017-04-03 00:42:26.123137: step 15620, loss = 2.30 (310.7 examples/sec; 0.412 sec/batch)
2017-04-03 00:42:30.249537: step 15630, loss = 2.30 (310.2 examples/sec; 0.413 sec/batch)
2017-04-03 00:42:34.383771: step 15640, loss = 2.30 (309.6 examples/sec; 0.413 sec/batch)
2017-04-03 00:42:38.481424: step 15650, loss = 2.30 (312.4 examples/sec; 0.410 sec/batch)
2017-04-03 00:42:42.599914: step 15660, loss = 2.30 (310.8 examples/sec; 0.412 sec/batch)
2017-04-03 00:42:46.699298: step 15670, loss = 2.30 (312.2 examples/sec; 0.410 sec/batch)
2017-04-03 00:42:50.808361: step 15680, loss = 2.30 (311.5 examples/sec; 0.411 sec/batch)
2017-04-03 00:42:54.942035: step 15690, loss = 2.30 (309.7 examples/sec; 0.413 sec/batch)
2017-04-03 00:42:59.118677: step 15700, loss = 2.30 (306.5 examples/sec; 0.418 sec/batch)
2017-04-03 00:43:03.235922: step 15710, loss = 2.30 (310.9 examples/sec; 0.412 sec/batch)
2017-04-03 00:43:07.370801: step 15720, loss = 2.30 (309.6 examples/sec; 0.413 sec/batch)
2017-04-03 00:43:11.514802: step 15730, loss = 2.30 (308.9 examples/sec; 0.414 sec/batch)
2017-04-03 00:43:15.626480: step 15740, loss = 2.30 (311.3 examples/sec; 0.411 sec/batch)
2017-04-03 00:43:19.761923: step 15750, loss = 2.30 (309.5 examples/sec; 0.414 sec/batch)
2017-04-03 00:43:23.886937: step 15760, loss = 2.30 (310.3 examples/sec; 0.413 sec/batch)
2017-04-03 00:43:28.023363: step 15770, loss = 2.30 (309.4 examples/sec; 0.414 sec/batch)
2017-04-03 00:43:32.154796: step 15780, loss = 2.30 (309.8 examples/sec; 0.413 sec/batch)
2017-04-03 00:43:36.278522: step 15790, loss = 2.30 (310.4 examples/sec; 0.412 sec/batch)
2017-04-03 00:43:40.487233: step 15800, loss = 2.31 (304.1 examples/sec; 0.421 sec/batch)
2017-04-03 00:43:44.606320: step 15810, loss = 2.30 (310.7 examples/sec; 0.412 sec/batch)
2017-04-03 00:43:48.728020: step 15820, loss = 2.30 (310.6 examples/sec; 0.412 sec/batch)
2017-04-03 00:43:52.849417: step 15830, loss = 2.30 (310.6 examples/sec; 0.412 sec/batch)
2017-04-03 00:43:56.998203: step 15840, loss = 2.30 (308.5 examples/sec; 0.415 sec/batch)
2017-04-03 00:44:01.125436: step 15850, loss = 2.30 (310.1 examples/sec; 0.413 sec/batch)
2017-04-03 00:44:05.254045: step 15860, loss = 2.30 (310.0 examples/sec; 0.413 sec/batch)
2017-04-03 00:44:09.373456: step 15870, loss = 2.30 (310.7 examples/sec; 0.412 sec/batch)
2017-04-03 00:44:13.483903: step 15880, loss = 2.30 (311.4 examples/sec; 0.411 sec/batch)
2017-04-03 00:44:17.587396: step 15890, loss = 2.30 (311.9 examples/sec; 0.410 sec/batch)
2017-04-03 00:44:21.870110: step 15900, loss = 2.30 (298.9 examples/sec; 0.428 sec/batch)
2017-04-03 00:44:25.983068: step 15910, loss = 2.30 (311.2 examples/sec; 0.411 sec/batch)
2017-04-03 00:44:30.094037: step 15920, loss = 2.30 (311.4 examples/sec; 0.411 sec/batch)
2017-04-03 00:44:34.207753: step 15930, loss = 2.30 (311.2 examples/sec; 0.411 sec/batch)
2017-04-03 00:44:38.341909: step 15940, loss = 2.30 (309.6 examples/sec; 0.413 sec/batch)
2017-04-03 00:44:42.522792: step 15950, loss = 2.30 (306.2 examples/sec; 0.418 sec/batch)
2017-04-03 00:44:46.654113: step 15960, loss = 2.30 (309.8 examples/sec; 0.413 sec/batch)
2017-04-03 00:44:50.751627: step 15970, loss = 2.30 (312.4 examples/sec; 0.410 sec/batch)
2017-04-03 00:44:54.875369: step 15980, loss = 2.30 (310.4 examples/sec; 0.412 sec/batch)
2017-04-03 00:44:58.998588: step 15990, loss = 2.30 (310.4 examples/sec; 0.412 sec/batch)
2017-04-03 00:45:03.138362: step 16000, loss = 2.30 (309.2 examples/sec; 0.414 sec/batch)
2017-04-03 00:45:59.722121: precision @ 1 = 0.092

