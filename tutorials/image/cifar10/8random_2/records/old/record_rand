2017-04-02 15:35:28.652716: step 6230, loss = 2.32 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 15:35:32.607672: step 6240, loss = 2.32 (323.6 examples/sec; 0.395 sec/batch)
2017-04-02 15:35:36.586103: step 6250, loss = 2.32 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 15:35:40.585240: step 6260, loss = 2.32 (320.1 examples/sec; 0.400 sec/batch)
2017-04-02 15:35:44.593610: step 6270, loss = 2.32 (319.3 examples/sec; 0.401 sec/batch)
2017-04-02 15:35:48.602453: step 6280, loss = 2.32 (319.3 examples/sec; 0.401 sec/batch)
2017-04-02 15:35:52.556235: step 6290, loss = 2.32 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 15:35:56.602807: step 6300, loss = 2.32 (316.3 examples/sec; 0.405 sec/batch)
2017-04-02 15:36:00.590881: step 6310, loss = 2.31 (321.0 examples/sec; 0.399 sec/batch)
2017-04-02 15:36:04.594067: step 6320, loss = 2.32 (319.7 examples/sec; 0.400 sec/batch)
2017-04-02 15:36:08.628849: step 6330, loss = 2.32 (317.2 examples/sec; 0.403 sec/batch)
2017-04-02 15:36:12.588319: step 6340, loss = 2.32 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 15:36:16.577363: step 6350, loss = 2.32 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 15:36:20.587176: step 6360, loss = 2.32 (319.2 examples/sec; 0.401 sec/batch)
2017-04-02 15:36:24.581682: step 6370, loss = 2.32 (320.4 examples/sec; 0.399 sec/batch)
2017-04-02 15:36:28.556667: step 6380, loss = 2.32 (322.0 examples/sec; 0.397 sec/batch)
2017-04-02 15:36:32.533334: step 6390, loss = 2.32 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 15:36:36.591629: step 6400, loss = 2.32 (315.4 examples/sec; 0.406 sec/batch)
2017-04-02 15:36:40.570195: step 6410, loss = 2.32 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 15:36:44.553216: step 6420, loss = 2.32 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 15:36:48.512476: step 6430, loss = 2.32 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 15:36:52.498380: step 6440, loss = 2.32 (321.1 examples/sec; 0.399 sec/batch)
2017-04-02 15:36:56.445777: step 6450, loss = 2.32 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 15:37:00.452433: step 6460, loss = 2.32 (319.5 examples/sec; 0.401 sec/batch)
2017-04-02 15:37:04.444914: step 6470, loss = 2.31 (320.6 examples/sec; 0.399 sec/batch)
2017-04-02 15:37:08.427315: step 6480, loss = 2.31 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 15:37:12.389000: step 6490, loss = 2.32 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 15:37:16.447453: step 6500, loss = 2.32 (315.4 examples/sec; 0.406 sec/batch)
2017-04-02 15:37:20.487957: step 6510, loss = 2.31 (316.8 examples/sec; 0.404 sec/batch)
2017-04-02 15:37:24.616648: step 6520, loss = 2.31 (310.0 examples/sec; 0.413 sec/batch)
2017-04-02 15:37:28.611932: step 6530, loss = 2.31 (320.4 examples/sec; 0.400 sec/batch)
2017-04-02 15:37:32.585076: step 6540, loss = 2.31 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 15:37:36.593675: step 6550, loss = 2.32 (319.3 examples/sec; 0.401 sec/batch)
2017-04-02 15:37:40.586064: step 6560, loss = 2.32 (320.6 examples/sec; 0.399 sec/batch)
2017-04-02 15:37:44.575241: step 6570, loss = 2.31 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 15:37:48.575145: step 6580, loss = 2.31 (320.0 examples/sec; 0.400 sec/batch)
2017-04-02 15:37:52.574033: step 6590, loss = 2.32 (320.1 examples/sec; 0.400 sec/batch)
2017-04-02 15:37:56.626510: step 6600, loss = 2.31 (315.9 examples/sec; 0.405 sec/batch)
2017-04-02 15:38:00.633999: step 6610, loss = 2.31 (319.4 examples/sec; 0.401 sec/batch)
2017-04-02 15:38:04.638453: step 6620, loss = 2.31 (319.6 examples/sec; 0.400 sec/batch)
2017-04-02 15:38:08.632268: step 6630, loss = 2.31 (320.5 examples/sec; 0.399 sec/batch)
2017-04-02 15:38:12.641568: step 6640, loss = 2.32 (319.3 examples/sec; 0.401 sec/batch)
2017-04-02 15:38:16.665466: step 6650, loss = 2.32 (318.1 examples/sec; 0.402 sec/batch)
2017-04-02 15:38:20.694045: step 6660, loss = 2.31 (317.7 examples/sec; 0.403 sec/batch)
2017-04-02 15:38:25.678786: step 6670, loss = 2.31 (256.8 examples/sec; 0.498 sec/batch)
2017-04-02 15:38:30.175490: step 6680, loss = 2.31 (284.7 examples/sec; 0.450 sec/batch)
2017-04-02 15:38:34.675012: step 6690, loss = 2.32 (284.5 examples/sec; 0.450 sec/batch)
2017-04-02 15:38:38.873802: step 6700, loss = 2.31 (304.8 examples/sec; 0.420 sec/batch)
2017-04-02 15:38:42.936550: step 6710, loss = 2.31 (315.1 examples/sec; 0.406 sec/batch)
2017-04-02 15:38:46.933121: step 6720, loss = 2.31 (320.3 examples/sec; 0.400 sec/batch)
2017-04-02 15:38:50.957707: step 6730, loss = 2.31 (318.0 examples/sec; 0.402 sec/batch)
2017-04-02 15:38:54.965763: step 6740, loss = 2.31 (319.4 examples/sec; 0.401 sec/batch)
2017-04-02 15:38:59.012845: step 6750, loss = 2.31 (316.3 examples/sec; 0.405 sec/batch)
2017-04-02 15:39:03.001486: step 6760, loss = 2.32 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 15:39:06.978444: step 6770, loss = 2.31 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 15:39:11.003457: step 6780, loss = 2.31 (318.0 examples/sec; 0.403 sec/batch)
2017-04-02 15:39:14.987040: step 6790, loss = 2.31 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 15:39:19.038626: step 6800, loss = 2.31 (315.9 examples/sec; 0.405 sec/batch)
2017-04-02 15:39:23.015183: step 6810, loss = 2.31 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 15:39:27.080603: step 6820, loss = 2.31 (314.9 examples/sec; 0.407 sec/batch)
2017-04-02 15:39:31.098857: step 6830, loss = 2.32 (318.5 examples/sec; 0.402 sec/batch)
2017-04-02 15:39:35.137713: step 6840, loss = 2.32 (316.9 examples/sec; 0.404 sec/batch)
2017-04-02 15:39:39.179270: step 6850, loss = 2.31 (316.7 examples/sec; 0.404 sec/batch)
2017-04-02 15:39:43.207086: step 6860, loss = 2.31 (317.8 examples/sec; 0.403 sec/batch)
2017-04-02 15:39:47.220301: step 6870, loss = 2.31 (318.9 examples/sec; 0.401 sec/batch)
2017-04-02 15:39:51.236257: step 6880, loss = 2.31 (318.7 examples/sec; 0.402 sec/batch)
2017-04-02 15:39:55.295018: step 6890, loss = 2.31 (315.4 examples/sec; 0.406 sec/batch)
2017-04-02 15:39:59.352412: step 6900, loss = 2.31 (315.5 examples/sec; 0.406 sec/batch)
2017-04-02 15:40:03.386829: step 6910, loss = 2.32 (317.3 examples/sec; 0.403 sec/batch)
2017-04-02 15:40:07.436314: step 6920, loss = 2.31 (316.1 examples/sec; 0.405 sec/batch)
2017-04-02 15:40:11.577609: step 6930, loss = 2.31 (309.1 examples/sec; 0.414 sec/batch)
2017-04-02 15:40:15.650838: step 6940, loss = 2.32 (314.2 examples/sec; 0.407 sec/batch)
2017-04-02 15:40:19.722362: step 6950, loss = 2.31 (314.4 examples/sec; 0.407 sec/batch)
2017-04-02 15:40:23.804856: step 6960, loss = 2.31 (313.5 examples/sec; 0.408 sec/batch)
2017-04-02 15:40:28.729057: step 6970, loss = 2.31 (259.9 examples/sec; 0.492 sec/batch)
2017-04-02 15:40:33.739766: step 6980, loss = 2.31 (255.5 examples/sec; 0.501 sec/batch)
2017-04-02 15:40:38.483022: step 6990, loss = 2.31 (269.9 examples/sec; 0.474 sec/batch)
2017-04-02 15:40:43.085815: step 7000, loss = 2.31 (278.1 examples/sec; 0.460 sec/batch)
2017-04-02 15:40:47.551927: step 7010, loss = 2.31 (286.6 examples/sec; 0.447 sec/batch)
2017-04-02 15:40:51.840783: step 7020, loss = 2.31 (298.4 examples/sec; 0.429 sec/batch)
2017-04-02 15:40:55.962308: step 7030, loss = 2.31 (310.6 examples/sec; 0.412 sec/batch)
2017-04-02 15:41:00.384350: step 7040, loss = 2.31 (289.5 examples/sec; 0.442 sec/batch)
2017-04-02 15:41:04.713403: step 7050, loss = 2.31 (295.7 examples/sec; 0.433 sec/batch)
2017-04-02 15:41:08.775169: step 7060, loss = 2.31 (315.1 examples/sec; 0.406 sec/batch)
2017-04-02 15:41:12.880640: step 7070, loss = 2.31 (311.8 examples/sec; 0.411 sec/batch)
2017-04-02 15:41:17.124981: step 7080, loss = 2.31 (301.6 examples/sec; 0.424 sec/batch)
2017-04-02 15:41:21.765819: step 7090, loss = 2.31 (275.8 examples/sec; 0.464 sec/batch)
2017-04-02 15:41:26.566393: step 7100, loss = 2.31 (266.6 examples/sec; 0.480 sec/batch)
2017-04-02 15:41:30.998602: step 7110, loss = 2.31 (288.8 examples/sec; 0.443 sec/batch)
2017-04-02 15:41:35.397286: step 7120, loss = 2.31 (291.0 examples/sec; 0.440 sec/batch)
2017-04-02 15:41:40.013535: step 7130, loss = 2.31 (277.3 examples/sec; 0.462 sec/batch)
2017-04-02 15:41:44.032532: step 7140, loss = 2.31 (318.5 examples/sec; 0.402 sec/batch)
2017-04-02 15:41:48.030354: step 7150, loss = 2.31 (320.2 examples/sec; 0.400 sec/batch)
2017-04-02 15:41:51.993071: step 7160, loss = 2.31 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 15:41:55.986407: step 7170, loss = 2.31 (320.5 examples/sec; 0.399 sec/batch)
2017-04-02 15:41:59.988581: step 7180, loss = 2.31 (319.8 examples/sec; 0.400 sec/batch)
2017-04-02 15:42:03.970480: step 7190, loss = 2.31 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 15:42:08.328076: step 7200, loss = 2.31 (293.7 examples/sec; 0.436 sec/batch)
2017-04-02 15:42:12.763028: step 7210, loss = 2.31 (288.6 examples/sec; 0.443 sec/batch)
2017-04-02 15:42:16.906965: step 7220, loss = 2.31 (308.9 examples/sec; 0.414 sec/batch)
2017-04-02 15:42:21.008162: step 7230, loss = 2.31 (312.1 examples/sec; 0.410 sec/batch)
2017-04-02 15:42:25.044677: step 7240, loss = 2.31 (317.1 examples/sec; 0.404 sec/batch)
2017-04-02 15:42:29.026768: step 7250, loss = 2.31 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 15:42:33.030569: step 7260, loss = 2.31 (319.7 examples/sec; 0.400 sec/batch)
2017-04-02 15:42:37.013492: step 7270, loss = 2.31 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 15:42:41.023394: step 7280, loss = 2.31 (319.2 examples/sec; 0.401 sec/batch)
2017-04-02 15:42:45.019176: step 7290, loss = 2.31 (320.3 examples/sec; 0.400 sec/batch)
2017-04-02 15:42:49.050590: step 7300, loss = 2.31 (317.5 examples/sec; 0.403 sec/batch)
2017-04-02 15:42:53.021416: step 7310, loss = 2.31 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 15:42:57.002488: step 7320, loss = 2.31 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 15:43:00.965872: step 7330, loss = 2.31 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 15:43:04.929411: step 7340, loss = 2.31 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 15:43:08.890783: step 7350, loss = 2.31 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 15:43:12.817048: step 7360, loss = 2.31 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 15:43:16.812279: step 7370, loss = 2.31 (320.4 examples/sec; 0.400 sec/batch)
2017-04-02 15:43:20.768210: step 7380, loss = 2.31 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 15:43:24.748195: step 7390, loss = 2.31 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 15:43:28.851406: step 7400, loss = 2.30 (312.0 examples/sec; 0.410 sec/batch)
2017-04-02 15:43:32.896235: step 7410, loss = 2.31 (316.5 examples/sec; 0.404 sec/batch)
2017-04-02 15:43:36.873412: step 7420, loss = 2.31 (321.8 examples/sec; 0.398 sec/batch)
2017-04-02 15:43:40.821444: step 7430, loss = 2.31 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 15:43:44.744882: step 7440, loss = 2.30 (326.2 examples/sec; 0.392 sec/batch)
2017-04-02 15:43:48.746425: step 7450, loss = 2.31 (319.9 examples/sec; 0.400 sec/batch)
2017-04-02 15:43:52.714622: step 7460, loss = 2.31 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 15:43:56.677961: step 7470, loss = 2.31 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 15:44:01.066698: step 7480, loss = 2.31 (291.7 examples/sec; 0.439 sec/batch)
2017-04-02 15:44:05.028263: step 7490, loss = 2.31 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 15:44:09.036624: step 7500, loss = 2.31 (319.3 examples/sec; 0.401 sec/batch)
2017-04-02 15:44:12.987575: step 7510, loss = 2.31 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 15:44:16.955803: step 7520, loss = 2.31 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 15:44:20.911662: step 7530, loss = 2.31 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 15:44:24.920141: step 7540, loss = 2.31 (319.3 examples/sec; 0.401 sec/batch)
2017-04-02 15:44:28.882354: step 7550, loss = 2.31 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 15:44:32.836492: step 7560, loss = 2.31 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 15:44:36.801864: step 7570, loss = 2.30 (322.8 examples/sec; 0.397 sec/batch)
2017-04-02 15:44:40.762051: step 7580, loss = 2.31 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 15:44:44.703157: step 7590, loss = 2.31 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 15:44:48.731854: step 7600, loss = 2.31 (317.7 examples/sec; 0.403 sec/batch)
2017-04-02 15:44:52.689172: step 7610, loss = 2.31 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 15:44:56.621462: step 7620, loss = 2.31 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 15:45:00.589939: step 7630, loss = 2.31 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 15:45:04.550381: step 7640, loss = 2.31 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 15:45:08.516929: step 7650, loss = 2.31 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 15:45:12.464403: step 7660, loss = 2.31 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 15:45:16.400554: step 7670, loss = 2.31 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 15:45:20.391037: step 7680, loss = 2.31 (320.8 examples/sec; 0.399 sec/batch)
2017-04-02 15:45:24.342673: step 7690, loss = 2.31 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 15:45:28.322765: step 7700, loss = 2.31 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 15:45:32.294151: step 7710, loss = 2.31 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 15:45:36.238665: step 7720, loss = 2.31 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 15:45:40.220021: step 7730, loss = 2.30 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 15:45:44.181083: step 7740, loss = 2.30 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 15:45:48.142446: step 7750, loss = 2.31 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 15:45:52.098505: step 7760, loss = 2.30 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 15:45:56.039442: step 7770, loss = 2.31 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 15:46:00.008116: step 7780, loss = 2.30 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 15:46:03.957259: step 7790, loss = 2.31 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 15:46:07.944680: step 7800, loss = 2.31 (321.0 examples/sec; 0.399 sec/batch)
2017-04-02 15:46:11.890235: step 7810, loss = 2.31 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 15:46:15.861080: step 7820, loss = 2.31 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 15:46:19.817176: step 7830, loss = 2.31 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 15:46:23.774054: step 7840, loss = 2.31 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 15:46:27.736028: step 7850, loss = 2.31 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 15:46:31.682626: step 7860, loss = 2.31 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 15:46:35.632341: step 7870, loss = 2.31 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 15:46:39.581712: step 7880, loss = 2.31 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 15:46:43.519047: step 7890, loss = 2.31 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 15:46:47.540528: step 7900, loss = 2.31 (318.3 examples/sec; 0.402 sec/batch)
2017-04-02 15:46:51.513237: step 7910, loss = 2.31 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 15:46:55.474466: step 7920, loss = 2.30 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 15:46:59.417442: step 7930, loss = 2.31 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 15:47:03.381529: step 7940, loss = 2.31 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 15:47:07.342968: step 7950, loss = 2.31 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 15:47:11.340765: step 7960, loss = 2.31 (320.2 examples/sec; 0.400 sec/batch)
2017-04-02 15:47:15.293725: step 7970, loss = 2.31 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 15:47:19.212905: step 7980, loss = 2.31 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 15:47:23.167077: step 7990, loss = 2.31 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 15:47:27.196025: step 8000, loss = 2.31 (317.7 examples/sec; 0.403 sec/batch)
2017-04-02 15:47:31.144795: step 8010, loss = 2.31 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 15:47:35.105073: step 8020, loss = 2.31 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 15:47:39.051123: step 8030, loss = 2.31 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 15:47:43.019585: step 8040, loss = 2.30 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 15:47:46.976981: step 8050, loss = 2.31 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 15:47:50.954461: step 8060, loss = 2.31 (321.8 examples/sec; 0.398 sec/batch)
2017-04-02 15:47:54.927558: step 8070, loss = 2.31 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 15:47:58.881758: step 8080, loss = 2.31 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 15:48:02.855932: step 8090, loss = 2.31 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 15:48:06.886287: step 8100, loss = 2.31 (317.6 examples/sec; 0.403 sec/batch)
2017-04-02 15:48:10.853175: step 8110, loss = 2.31 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 15:48:14.832486: step 8120, loss = 2.31 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 15:48:18.791204: step 8130, loss = 2.31 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 15:48:22.732091: step 8140, loss = 2.31 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 15:48:26.696337: step 8150, loss = 2.31 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 15:48:30.665534: step 8160, loss = 2.30 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 15:48:34.650685: step 8170, loss = 2.31 (321.2 examples/sec; 0.399 sec/batch)
2017-04-02 15:48:38.568495: step 8180, loss = 2.31 (326.7 examples/sec; 0.392 sec/batch)
2017-04-02 15:48:42.507528: step 8190, loss = 2.31 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 15:48:46.517433: step 8200, loss = 2.31 (319.2 examples/sec; 0.401 sec/batch)
2017-04-02 15:48:50.496153: step 8210, loss = 2.31 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 15:48:54.471118: step 8220, loss = 2.31 (322.0 examples/sec; 0.397 sec/batch)
2017-04-02 15:48:58.463440: step 8230, loss = 2.30 (320.6 examples/sec; 0.399 sec/batch)
2017-04-02 15:49:02.423650: step 8240, loss = 2.30 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 15:49:06.394832: step 8250, loss = 2.30 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 15:49:10.341708: step 8260, loss = 2.31 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 15:49:14.298523: step 8270, loss = 2.31 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 15:49:18.251245: step 8280, loss = 2.30 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 15:49:22.161585: step 8290, loss = 2.31 (327.3 examples/sec; 0.391 sec/batch)
2017-04-02 15:49:26.244237: step 8300, loss = 2.31 (313.5 examples/sec; 0.408 sec/batch)
2017-04-02 15:49:30.213019: step 8310, loss = 2.31 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 15:49:34.221708: step 8320, loss = 2.31 (319.3 examples/sec; 0.401 sec/batch)
2017-04-02 15:49:38.185676: step 8330, loss = 2.31 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 15:49:42.137921: step 8340, loss = 2.31 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 15:49:46.097015: step 8350, loss = 2.30 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 15:49:50.059672: step 8360, loss = 2.31 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 15:49:54.019625: step 8370, loss = 2.31 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 15:49:57.970627: step 8380, loss = 2.30 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 15:50:01.910664: step 8390, loss = 2.30 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 15:50:05.945024: step 8400, loss = 2.31 (317.3 examples/sec; 0.403 sec/batch)
2017-04-02 15:50:09.896897: step 8410, loss = 2.31 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 15:50:13.854055: step 8420, loss = 2.31 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 15:50:17.839431: step 8430, loss = 2.30 (321.2 examples/sec; 0.399 sec/batch)
2017-04-02 15:50:21.803994: step 8440, loss = 2.30 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 15:50:25.739655: step 8450, loss = 2.31 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 15:50:29.690398: step 8460, loss = 2.31 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 15:50:33.660653: step 8470, loss = 2.31 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 15:50:37.607496: step 8480, loss = 2.31 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 15:50:41.573153: step 8490, loss = 2.31 (322.8 examples/sec; 0.397 sec/batch)
2017-04-02 15:50:45.589006: step 8500, loss = 2.30 (318.7 examples/sec; 0.402 sec/batch)
2017-04-02 15:50:49.577765: step 8510, loss = 2.30 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 15:50:53.547757: step 8520, loss = 2.30 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 15:50:57.503732: step 8530, loss = 2.30 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 15:51:01.466087: step 8540, loss = 2.30 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 15:51:05.407541: step 8550, loss = 2.31 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 15:51:09.369885: step 8560, loss = 2.31 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 15:51:13.302827: step 8570, loss = 2.30 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 15:51:17.267587: step 8580, loss = 2.31 (322.8 examples/sec; 0.396 sec/batch)
2017-04-02 15:51:21.223611: step 8590, loss = 2.31 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 15:51:25.248426: step 8600, loss = 2.31 (318.0 examples/sec; 0.402 sec/batch)
2017-04-02 15:51:29.203448: step 8610, loss = 2.31 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 15:51:33.182651: step 8620, loss = 2.30 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 15:51:37.165969: step 8630, loss = 2.30 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 15:51:41.109619: step 8640, loss = 2.30 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 15:51:45.088669: step 8650, loss = 2.31 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 15:51:49.038197: step 8660, loss = 2.31 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 15:51:52.953503: step 8670, loss = 2.31 (326.9 examples/sec; 0.392 sec/batch)
2017-04-02 15:51:56.911951: step 8680, loss = 2.30 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 15:52:00.890287: step 8690, loss = 2.30 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 15:52:04.899120: step 8700, loss = 2.31 (319.3 examples/sec; 0.401 sec/batch)
2017-04-02 15:52:08.889851: step 8710, loss = 2.31 (320.7 examples/sec; 0.399 sec/batch)
2017-04-02 15:52:12.872897: step 8720, loss = 2.31 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 15:52:16.822743: step 8730, loss = 2.30 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 15:52:20.788872: step 8740, loss = 2.31 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 15:52:24.727489: step 8750, loss = 2.31 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 15:52:28.688681: step 8760, loss = 2.30 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 15:52:32.677240: step 8770, loss = 2.30 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 15:52:36.647430: step 8780, loss = 2.31 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 15:52:40.576609: step 8790, loss = 2.30 (325.8 examples/sec; 0.393 sec/batch)
2017-04-02 15:52:44.609499: step 8800, loss = 2.31 (317.4 examples/sec; 0.403 sec/batch)
2017-04-02 15:52:48.581914: step 8810, loss = 2.31 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 15:52:52.550649: step 8820, loss = 2.31 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 15:52:56.538334: step 8830, loss = 2.30 (321.0 examples/sec; 0.399 sec/batch)
2017-04-02 15:53:00.525603: step 8840, loss = 2.31 (321.0 examples/sec; 0.399 sec/batch)
2017-04-02 15:53:04.487250: step 8850, loss = 2.31 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 15:53:08.481515: step 8860, loss = 2.30 (320.5 examples/sec; 0.399 sec/batch)
2017-04-02 15:53:12.432845: step 8870, loss = 2.31 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 15:53:16.399227: step 8880, loss = 2.31 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 15:53:20.394676: step 8890, loss = 2.30 (320.4 examples/sec; 0.400 sec/batch)
2017-04-02 15:53:24.427540: step 8900, loss = 2.30 (317.4 examples/sec; 0.403 sec/batch)
2017-04-02 15:53:28.370635: step 8910, loss = 2.31 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 15:53:32.322352: step 8920, loss = 2.30 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 15:53:36.274065: step 8930, loss = 2.30 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 15:53:40.230781: step 8940, loss = 2.30 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 15:53:44.204008: step 8950, loss = 2.31 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 15:53:48.177872: step 8960, loss = 2.30 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 15:53:52.186713: step 8970, loss = 2.30 (319.3 examples/sec; 0.401 sec/batch)
2017-04-02 15:53:56.202660: step 8980, loss = 2.31 (318.7 examples/sec; 0.402 sec/batch)
2017-04-02 15:54:00.189417: step 8990, loss = 2.30 (321.1 examples/sec; 0.399 sec/batch)
2017-04-02 15:54:04.213722: step 9000, loss = 2.31 (318.1 examples/sec; 0.402 sec/batch)
2017-04-02 15:54:08.167693: step 9010, loss = 2.30 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 15:54:12.133512: step 9020, loss = 2.30 (322.8 examples/sec; 0.397 sec/batch)
2017-04-02 15:54:16.110997: step 9030, loss = 2.31 (321.8 examples/sec; 0.398 sec/batch)
2017-04-02 15:54:20.094501: step 9040, loss = 2.30 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 15:54:24.040084: step 9050, loss = 2.31 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 15:54:28.003817: step 9060, loss = 2.30 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 15:54:31.999564: step 9070, loss = 2.31 (320.3 examples/sec; 0.400 sec/batch)
2017-04-02 15:54:35.990471: step 9080, loss = 2.30 (320.7 examples/sec; 0.399 sec/batch)
2017-04-02 15:54:39.962230: step 9090, loss = 2.30 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 15:54:43.995586: step 9100, loss = 2.31 (317.4 examples/sec; 0.403 sec/batch)
2017-04-02 15:54:47.961604: step 9110, loss = 2.30 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 15:54:51.970967: step 9120, loss = 2.31 (319.3 examples/sec; 0.401 sec/batch)
2017-04-02 15:54:55.965759: step 9130, loss = 2.30 (320.4 examples/sec; 0.399 sec/batch)
2017-04-02 15:54:59.938382: step 9140, loss = 2.30 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 15:55:03.925970: step 9150, loss = 2.31 (321.0 examples/sec; 0.399 sec/batch)
2017-04-02 15:55:07.942385: step 9160, loss = 2.30 (318.7 examples/sec; 0.402 sec/batch)
2017-04-02 15:55:11.906460: step 9170, loss = 2.30 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 15:55:15.897825: step 9180, loss = 2.31 (320.7 examples/sec; 0.399 sec/batch)
2017-04-02 15:55:19.886722: step 9190, loss = 2.30 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 15:55:23.915161: step 9200, loss = 2.31 (317.7 examples/sec; 0.403 sec/batch)
2017-04-02 15:55:27.863581: step 9210, loss = 2.30 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 15:55:31.835018: step 9220, loss = 2.31 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 15:55:35.794842: step 9230, loss = 2.30 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 15:55:39.756638: step 9240, loss = 2.31 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 15:55:43.738782: step 9250, loss = 2.30 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 15:55:47.687334: step 9260, loss = 2.30 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 15:55:51.637753: step 9270, loss = 2.30 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 15:55:55.637044: step 9280, loss = 2.31 (320.1 examples/sec; 0.400 sec/batch)
2017-04-02 15:55:59.695498: step 9290, loss = 2.31 (315.4 examples/sec; 0.406 sec/batch)
2017-04-02 15:56:03.803460: step 9300, loss = 2.31 (311.6 examples/sec; 0.411 sec/batch)
2017-04-02 15:56:07.797384: step 9310, loss = 2.30 (320.5 examples/sec; 0.399 sec/batch)
2017-04-02 15:56:11.786164: step 9320, loss = 2.30 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 15:56:15.780630: step 9330, loss = 2.31 (320.4 examples/sec; 0.399 sec/batch)
2017-04-02 15:56:19.749786: step 9340, loss = 2.31 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 15:56:23.766692: step 9350, loss = 2.31 (318.7 examples/sec; 0.402 sec/batch)
2017-04-02 15:56:27.740635: step 9360, loss = 2.30 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 15:56:31.741732: step 9370, loss = 2.31 (319.9 examples/sec; 0.400 sec/batch)
2017-04-02 15:56:35.723417: step 9380, loss = 2.31 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 15:56:39.718075: step 9390, loss = 2.30 (320.4 examples/sec; 0.399 sec/batch)
2017-04-02 15:56:43.763227: step 9400, loss = 2.30 (316.4 examples/sec; 0.405 sec/batch)
2017-04-02 15:56:47.747785: step 9410, loss = 2.31 (321.2 examples/sec; 0.398 sec/batch)
2017-04-02 15:56:51.740355: step 9420, loss = 2.30 (320.6 examples/sec; 0.399 sec/batch)
2017-04-02 15:56:55.733618: step 9430, loss = 2.30 (320.5 examples/sec; 0.399 sec/batch)
2017-04-02 15:56:59.729956: step 9440, loss = 2.30 (320.3 examples/sec; 0.400 sec/batch)
2017-04-02 15:57:03.725345: step 9450, loss = 2.30 (320.4 examples/sec; 0.400 sec/batch)
2017-04-02 15:57:07.713123: step 9460, loss = 2.31 (321.0 examples/sec; 0.399 sec/batch)
2017-04-02 15:57:11.677386: step 9470, loss = 2.30 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 15:57:15.662537: step 9480, loss = 2.30 (321.2 examples/sec; 0.399 sec/batch)
2017-04-02 15:57:19.642292: step 9490, loss = 2.30 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 15:57:23.755065: step 9500, loss = 2.31 (311.2 examples/sec; 0.411 sec/batch)
2017-04-02 15:57:27.755265: step 9510, loss = 2.31 (320.0 examples/sec; 0.400 sec/batch)
2017-04-02 15:57:31.712858: step 9520, loss = 2.30 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 15:57:35.682458: step 9530, loss = 2.30 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 15:57:39.675126: step 9540, loss = 2.30 (320.6 examples/sec; 0.399 sec/batch)
2017-04-02 15:57:43.639024: step 9550, loss = 2.30 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 15:57:47.615344: step 9560, loss = 2.31 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 15:57:51.595401: step 9570, loss = 2.31 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 15:57:55.550870: step 9580, loss = 2.31 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 15:57:59.494330: step 9590, loss = 2.31 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 15:58:03.556701: step 9600, loss = 2.30 (315.1 examples/sec; 0.406 sec/batch)
2017-04-02 15:58:07.579031: step 9610, loss = 2.30 (318.2 examples/sec; 0.402 sec/batch)
2017-04-02 15:58:11.675431: step 9620, loss = 2.30 (312.5 examples/sec; 0.410 sec/batch)
2017-04-02 15:58:15.667901: step 9630, loss = 2.30 (320.6 examples/sec; 0.399 sec/batch)
2017-04-02 15:58:19.652425: step 9640, loss = 2.30 (321.2 examples/sec; 0.398 sec/batch)
2017-04-02 15:58:23.794555: step 9650, loss = 2.30 (309.0 examples/sec; 0.414 sec/batch)
2017-04-02 15:58:28.654734: step 9660, loss = 2.30 (263.4 examples/sec; 0.486 sec/batch)
2017-04-02 15:58:33.497263: step 9670, loss = 2.31 (264.3 examples/sec; 0.484 sec/batch)
2017-04-02 15:58:37.992353: step 9680, loss = 2.31 (284.8 examples/sec; 0.450 sec/batch)
2017-04-02 15:58:41.958928: step 9690, loss = 2.31 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 15:58:46.016281: step 9700, loss = 2.30 (315.5 examples/sec; 0.406 sec/batch)
2017-04-02 15:58:49.998547: step 9710, loss = 2.30 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 15:58:53.979507: step 9720, loss = 2.30 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 15:58:57.966763: step 9730, loss = 2.30 (321.0 examples/sec; 0.399 sec/batch)
2017-04-02 15:59:01.936640: step 9740, loss = 2.30 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 15:59:05.942295: step 9750, loss = 2.30 (319.5 examples/sec; 0.401 sec/batch)
2017-04-02 15:59:09.924143: step 9760, loss = 2.30 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 15:59:13.846175: step 9770, loss = 2.30 (326.4 examples/sec; 0.392 sec/batch)
2017-04-02 15:59:17.856587: step 9780, loss = 2.31 (319.2 examples/sec; 0.401 sec/batch)
2017-04-02 15:59:21.821221: step 9790, loss = 2.30 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 15:59:25.833525: step 9800, loss = 2.30 (319.0 examples/sec; 0.401 sec/batch)
2017-04-02 15:59:29.886770: step 9810, loss = 2.31 (315.8 examples/sec; 0.405 sec/batch)
2017-04-02 15:59:33.846165: step 9820, loss = 2.31 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 15:59:37.809585: step 9830, loss = 2.30 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 15:59:41.798195: step 9840, loss = 2.31 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 15:59:45.764863: step 9850, loss = 2.30 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 15:59:49.750594: step 9860, loss = 2.30 (321.1 examples/sec; 0.399 sec/batch)
2017-04-02 15:59:53.709589: step 9870, loss = 2.30 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 15:59:57.677534: step 9880, loss = 2.30 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 16:00:01.694404: step 9890, loss = 2.30 (318.7 examples/sec; 0.402 sec/batch)
2017-04-02 16:00:05.760838: step 9900, loss = 2.30 (314.8 examples/sec; 0.407 sec/batch)
2017-04-02 16:00:09.742677: step 9910, loss = 2.30 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 16:00:13.753796: step 9920, loss = 2.30 (319.1 examples/sec; 0.401 sec/batch)
2017-04-02 16:00:17.758038: step 9930, loss = 2.30 (319.7 examples/sec; 0.400 sec/batch)
2017-04-02 16:00:21.745672: step 9940, loss = 2.30 (321.0 examples/sec; 0.399 sec/batch)
2017-04-02 16:00:25.772018: step 9950, loss = 2.30 (317.9 examples/sec; 0.403 sec/batch)
2017-04-02 16:00:29.758971: step 9960, loss = 2.30 (321.0 examples/sec; 0.399 sec/batch)
2017-04-02 16:00:33.755784: step 9970, loss = 2.30 (320.3 examples/sec; 0.400 sec/batch)
2017-04-02 16:00:37.736037: step 9980, loss = 2.30 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 16:00:41.709602: step 9990, loss = 2.30 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 16:00:45.764696: step 10000, loss = 2.30 (315.7 examples/sec; 0.406 sec/batch)
2017-04-02 16:00:49.782100: step 10010, loss = 2.30 (318.6 examples/sec; 0.402 sec/batch)
2017-04-02 16:00:53.743835: step 10020, loss = 2.30 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 16:00:57.750840: step 10030, loss = 2.30 (319.4 examples/sec; 0.401 sec/batch)
2017-04-02 16:01:01.716999: step 10040, loss = 2.30 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 16:01:05.670845: step 10050, loss = 2.30 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 16:01:09.645128: step 10060, loss = 2.31 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 16:01:13.617085: step 10070, loss = 2.30 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 16:01:17.579550: step 10080, loss = 2.30 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 16:01:21.865644: step 10090, loss = 2.30 (298.6 examples/sec; 0.429 sec/batch)
2017-04-02 16:01:25.912966: step 10100, loss = 2.30 (316.3 examples/sec; 0.405 sec/batch)
2017-04-02 16:01:29.883689: step 10110, loss = 2.30 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 16:01:33.871615: step 10120, loss = 2.30 (321.0 examples/sec; 0.399 sec/batch)
2017-04-02 16:01:37.822241: step 10130, loss = 2.30 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 16:01:41.789502: step 10140, loss = 2.30 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 16:01:45.791676: step 10150, loss = 2.31 (319.8 examples/sec; 0.400 sec/batch)
2017-04-02 16:01:49.786586: step 10160, loss = 2.30 (320.4 examples/sec; 0.399 sec/batch)
2017-04-02 16:01:53.789467: step 10170, loss = 2.30 (319.8 examples/sec; 0.400 sec/batch)
2017-04-02 16:01:57.782212: step 10180, loss = 2.30 (320.6 examples/sec; 0.399 sec/batch)
2017-04-02 16:02:01.749627: step 10190, loss = 2.30 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 16:02:05.815118: step 10200, loss = 2.30 (314.8 examples/sec; 0.407 sec/batch)
2017-04-02 16:02:09.789843: step 10210, loss = 2.30 (322.0 examples/sec; 0.397 sec/batch)
2017-04-02 16:02:13.760357: step 10220, loss = 2.30 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 16:02:17.740888: step 10230, loss = 2.30 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 16:02:21.740263: step 10240, loss = 2.30 (320.1 examples/sec; 0.400 sec/batch)
2017-04-02 16:02:25.738412: step 10250, loss = 2.30 (320.1 examples/sec; 0.400 sec/batch)
2017-04-02 16:02:29.697636: step 10260, loss = 2.30 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 16:02:33.679669: step 10270, loss = 2.30 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 16:02:37.664656: step 10280, loss = 2.30 (321.2 examples/sec; 0.398 sec/batch)
2017-04-02 16:02:41.665930: step 10290, loss = 2.30 (319.9 examples/sec; 0.400 sec/batch)
2017-04-02 16:02:45.703699: step 10300, loss = 2.30 (317.0 examples/sec; 0.404 sec/batch)
2017-04-02 16:02:49.665928: step 10310, loss = 2.30 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 16:02:53.628616: step 10320, loss = 2.31 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 16:02:57.605946: step 10330, loss = 2.30 (321.8 examples/sec; 0.398 sec/batch)
2017-04-02 16:03:01.605318: step 10340, loss = 2.30 (320.1 examples/sec; 0.400 sec/batch)
2017-04-02 16:03:05.603985: step 10350, loss = 2.30 (320.1 examples/sec; 0.400 sec/batch)
2017-04-02 16:03:09.574177: step 10360, loss = 2.30 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 16:03:13.556330: step 10370, loss = 2.30 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 16:03:17.547527: step 10380, loss = 2.30 (320.7 examples/sec; 0.399 sec/batch)
2017-04-02 16:03:21.564270: step 10390, loss = 2.30 (318.7 examples/sec; 0.402 sec/batch)
2017-04-02 16:03:25.600950: step 10400, loss = 2.30 (317.1 examples/sec; 0.404 sec/batch)
2017-04-02 16:03:29.576472: step 10410, loss = 2.30 (322.0 examples/sec; 0.398 sec/batch)
2017-04-02 16:03:33.572473: step 10420, loss = 2.30 (320.3 examples/sec; 0.400 sec/batch)
2017-04-02 16:03:37.540730: step 10430, loss = 2.30 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 16:03:41.507866: step 10440, loss = 2.30 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 16:03:45.449795: step 10450, loss = 2.31 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 16:03:49.402902: step 10460, loss = 2.30 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 16:03:53.381717: step 10470, loss = 2.31 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 16:03:57.316783: step 10480, loss = 2.30 (325.3 examples/sec; 0.394 sec/batch)
2017-04-02 16:04:01.270821: step 10490, loss = 2.30 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 16:04:05.273761: step 10500, loss = 2.30 (319.8 examples/sec; 0.400 sec/batch)
2017-04-02 16:04:09.238525: step 10510, loss = 2.30 (322.8 examples/sec; 0.396 sec/batch)
2017-04-02 16:04:13.204637: step 10520, loss = 2.30 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 16:04:17.142603: step 10530, loss = 2.30 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 16:04:21.104532: step 10540, loss = 2.31 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 16:04:25.065827: step 10550, loss = 2.30 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 16:04:29.026472: step 10560, loss = 2.30 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 16:04:33.004912: step 10570, loss = 2.30 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 16:04:37.021573: step 10580, loss = 2.30 (318.7 examples/sec; 0.402 sec/batch)
2017-04-02 16:04:40.978640: step 10590, loss = 2.30 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 16:04:45.033343: step 10600, loss = 2.30 (315.7 examples/sec; 0.405 sec/batch)
2017-04-02 16:04:48.998220: step 10610, loss = 2.30 (322.8 examples/sec; 0.396 sec/batch)
2017-04-02 16:04:52.947821: step 10620, loss = 2.30 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 16:04:56.893384: step 10630, loss = 2.31 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 16:05:00.859503: step 10640, loss = 2.30 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 16:05:04.814694: step 10650, loss = 2.31 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 16:05:08.783252: step 10660, loss = 2.31 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 16:05:12.745167: step 10670, loss = 2.30 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 16:05:16.704472: step 10680, loss = 2.31 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 16:05:20.662126: step 10690, loss = 2.30 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 16:05:24.704594: step 10700, loss = 2.30 (316.6 examples/sec; 0.404 sec/batch)
2017-04-02 16:05:28.698521: step 10710, loss = 2.31 (320.5 examples/sec; 0.399 sec/batch)
2017-04-02 16:05:32.674559: step 10720, loss = 2.30 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 16:05:36.620530: step 10730, loss = 2.30 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 16:05:40.596745: step 10740, loss = 2.30 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 16:05:44.520688: step 10750, loss = 2.31 (326.2 examples/sec; 0.392 sec/batch)
2017-04-02 16:05:48.486269: step 10760, loss = 2.30 (322.8 examples/sec; 0.397 sec/batch)
2017-04-02 16:05:52.462726: step 10770, loss = 2.30 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 16:05:56.426504: step 10780, loss = 2.30 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 16:06:00.414400: step 10790, loss = 2.30 (321.0 examples/sec; 0.399 sec/batch)
2017-04-02 16:06:04.416436: step 10800, loss = 2.30 (319.8 examples/sec; 0.400 sec/batch)
2017-04-02 16:06:08.381072: step 10810, loss = 2.30 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 16:06:12.395055: step 10820, loss = 2.30 (318.9 examples/sec; 0.401 sec/batch)
2017-04-02 16:06:16.336181: step 10830, loss = 2.30 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 16:06:20.335981: step 10840, loss = 2.31 (320.0 examples/sec; 0.400 sec/batch)
2017-04-02 16:06:24.320198: step 10850, loss = 2.30 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 16:06:28.317235: step 10860, loss = 2.30 (320.2 examples/sec; 0.400 sec/batch)
2017-04-02 16:06:32.251282: step 10870, loss = 2.30 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 16:06:36.223513: step 10880, loss = 2.30 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 16:06:40.187925: step 10890, loss = 2.30 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 16:06:44.204667: step 10900, loss = 2.30 (318.7 examples/sec; 0.402 sec/batch)
2017-04-02 16:06:48.208276: step 10910, loss = 2.30 (319.7 examples/sec; 0.400 sec/batch)
2017-04-02 16:06:52.170828: step 10920, loss = 2.30 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 16:06:56.114161: step 10930, loss = 2.30 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 16:07:00.066301: step 10940, loss = 2.30 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 16:07:04.055201: step 10950, loss = 2.30 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 16:07:08.038455: step 10960, loss = 2.30 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 16:07:12.013019: step 10970, loss = 2.30 (322.0 examples/sec; 0.397 sec/batch)
2017-04-02 16:07:15.962863: step 10980, loss = 2.31 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 16:07:19.941426: step 10990, loss = 2.30 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 16:07:24.025164: step 11000, loss = 2.30 (313.4 examples/sec; 0.408 sec/batch)
2017-04-02 16:07:28.042003: step 11010, loss = 2.30 (318.7 examples/sec; 0.402 sec/batch)
2017-04-02 16:07:31.991580: step 11020, loss = 2.31 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 16:07:35.966492: step 11030, loss = 2.30 (322.0 examples/sec; 0.397 sec/batch)
2017-04-02 16:07:39.926427: step 11040, loss = 2.30 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 16:07:43.931150: step 11050, loss = 2.30 (319.6 examples/sec; 0.400 sec/batch)
2017-04-02 16:07:47.925509: step 11060, loss = 2.30 (320.5 examples/sec; 0.399 sec/batch)
2017-04-02 16:07:51.910109: step 11070, loss = 2.30 (321.2 examples/sec; 0.398 sec/batch)
2017-04-02 16:07:55.890154: step 11080, loss = 2.30 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 16:07:59.889774: step 11090, loss = 2.30 (320.0 examples/sec; 0.400 sec/batch)
2017-04-02 16:08:03.926021: step 11100, loss = 2.30 (317.1 examples/sec; 0.404 sec/batch)
2017-04-02 16:08:07.910784: step 11110, loss = 2.30 (321.2 examples/sec; 0.398 sec/batch)
2017-04-02 16:08:11.906869: step 11120, loss = 2.30 (320.3 examples/sec; 0.400 sec/batch)
2017-04-02 16:08:15.892730: step 11130, loss = 2.30 (321.1 examples/sec; 0.399 sec/batch)
2017-04-02 16:08:19.883978: step 11140, loss = 2.30 (320.7 examples/sec; 0.399 sec/batch)
2017-04-02 16:08:23.874573: step 11150, loss = 2.30 (320.8 examples/sec; 0.399 sec/batch)
2017-04-02 16:08:27.859471: step 11160, loss = 2.30 (321.2 examples/sec; 0.398 sec/batch)
2017-04-02 16:08:31.823348: step 11170, loss = 2.30 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 16:08:35.789453: step 11180, loss = 2.30 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 16:08:39.763721: step 11190, loss = 2.31 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 16:08:43.833007: step 11200, loss = 2.31 (314.6 examples/sec; 0.407 sec/batch)
2017-04-02 16:08:47.838053: step 11210, loss = 2.30 (319.6 examples/sec; 0.401 sec/batch)
^Z
[1]  + 17433 suspended  python cifar10_train.py
(tensorflow) ➜  cifar10 git:(master) ✗ python cifar10_eval.py                 
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-04-02 16:09:17.181800: precision @ 1 = 0.100
^Z
[3]  + 20327 suspended  python cifar10_eval.py
(tensorflow) ➜  cifar10 git:(master) ✗ 

