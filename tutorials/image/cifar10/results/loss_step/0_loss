2017-04-02 21:09:21.820484: step 0, loss = 4.68 (359.4 examples/sec; 0.356 sec/batch)
2017-04-02 21:09:25.897787: step 10, loss = 4.61 (313.9 examples/sec; 0.408 sec/batch)
2017-04-02 21:09:29.970243: step 20, loss = 4.46 (314.3 examples/sec; 0.407 sec/batch)
2017-04-02 21:09:34.088932: step 30, loss = 4.43 (310.8 examples/sec; 0.412 sec/batch)
2017-04-02 21:09:38.148911: step 40, loss = 4.32 (315.3 examples/sec; 0.406 sec/batch)
2017-04-02 21:09:42.194457: step 50, loss = 4.27 (316.4 examples/sec; 0.405 sec/batch)
2017-04-02 21:09:46.235271: step 60, loss = 4.41 (316.8 examples/sec; 0.404 sec/batch)
2017-04-02 21:09:50.261064: step 70, loss = 4.38 (317.9 examples/sec; 0.403 sec/batch)
2017-04-02 21:09:54.354417: step 80, loss = 4.24 (312.7 examples/sec; 0.409 sec/batch)
2017-04-02 21:09:58.380842: step 90, loss = 4.02 (317.9 examples/sec; 0.403 sec/batch)
2017-04-02 21:10:02.460941: step 100, loss = 4.04 (313.7 examples/sec; 0.408 sec/batch)
2017-04-02 21:10:06.474278: step 110, loss = 4.07 (318.9 examples/sec; 0.401 sec/batch)
2017-04-02 21:10:10.485237: step 120, loss = 4.06 (319.1 examples/sec; 0.401 sec/batch)
2017-04-02 21:10:14.518848: step 130, loss = 3.88 (317.3 examples/sec; 0.403 sec/batch)
2017-04-02 21:10:18.537035: step 140, loss = 3.84 (318.6 examples/sec; 0.402 sec/batch)
2017-04-02 21:10:22.536747: step 150, loss = 4.01 (320.0 examples/sec; 0.400 sec/batch)
2017-04-02 21:10:26.541137: step 160, loss = 3.94 (319.6 examples/sec; 0.400 sec/batch)
2017-04-02 21:10:30.561066: step 170, loss = 3.86 (318.4 examples/sec; 0.402 sec/batch)
2017-04-02 21:10:34.582685: step 180, loss = 3.86 (318.3 examples/sec; 0.402 sec/batch)
2017-04-02 21:10:38.590148: step 190, loss = 3.92 (319.4 examples/sec; 0.401 sec/batch)
2017-04-02 21:10:42.694076: step 200, loss = 3.76 (311.9 examples/sec; 0.410 sec/batch)
2017-04-02 21:10:46.762958: step 210, loss = 3.81 (314.6 examples/sec; 0.407 sec/batch)
2017-04-02 21:10:50.812712: step 220, loss = 3.58 (316.1 examples/sec; 0.405 sec/batch)
2017-04-02 21:10:54.839724: step 230, loss = 3.82 (317.9 examples/sec; 0.403 sec/batch)
2017-04-02 21:10:58.850563: step 240, loss = 3.81 (319.1 examples/sec; 0.401 sec/batch)
2017-04-02 21:11:02.901833: step 250, loss = 3.75 (316.0 examples/sec; 0.405 sec/batch)
2017-04-02 21:11:06.896682: step 260, loss = 3.79 (320.4 examples/sec; 0.399 sec/batch)
2017-04-02 21:11:10.899111: step 270, loss = 3.65 (319.8 examples/sec; 0.400 sec/batch)
2017-04-02 21:11:14.903165: step 280, loss = 3.69 (319.7 examples/sec; 0.400 sec/batch)
2017-04-02 21:11:18.949942: step 290, loss = 3.68 (316.3 examples/sec; 0.405 sec/batch)
2017-04-02 21:11:23.024292: step 300, loss = 3.84 (314.2 examples/sec; 0.407 sec/batch)
2017-04-02 21:11:27.043436: step 310, loss = 3.68 (318.5 examples/sec; 0.402 sec/batch)
2017-04-02 21:11:31.048010: step 320, loss = 3.53 (319.6 examples/sec; 0.400 sec/batch)
2017-04-02 21:11:35.081871: step 330, loss = 3.47 (317.3 examples/sec; 0.403 sec/batch)
2017-04-02 21:11:39.112109: step 340, loss = 3.48 (317.6 examples/sec; 0.403 sec/batch)
2017-04-02 21:11:43.129360: step 350, loss = 3.63 (318.6 examples/sec; 0.402 sec/batch)
2017-04-02 21:11:47.211756: step 360, loss = 3.26 (313.5 examples/sec; 0.408 sec/batch)
2017-04-02 21:11:51.236134: step 370, loss = 3.41 (318.1 examples/sec; 0.402 sec/batch)
2017-04-02 21:11:55.262290: step 380, loss = 3.44 (317.9 examples/sec; 0.403 sec/batch)
2017-04-02 21:11:59.265282: step 390, loss = 3.36 (319.8 examples/sec; 0.400 sec/batch)
2017-04-02 21:12:03.432318: step 400, loss = 3.48 (307.2 examples/sec; 0.417 sec/batch)
2017-04-02 21:12:07.431081: step 410, loss = 3.32 (320.1 examples/sec; 0.400 sec/batch)
2017-04-02 21:12:11.454510: step 420, loss = 3.44 (318.1 examples/sec; 0.402 sec/batch)
2017-04-02 21:12:15.486825: step 430, loss = 3.30 (317.4 examples/sec; 0.403 sec/batch)
2017-04-02 21:12:19.500628: step 440, loss = 3.27 (318.9 examples/sec; 0.401 sec/batch)
2017-04-02 21:12:23.535011: step 450, loss = 3.12 (317.3 examples/sec; 0.403 sec/batch)
2017-04-02 21:12:27.572740: step 460, loss = 3.23 (317.0 examples/sec; 0.404 sec/batch)
2017-04-02 21:12:31.604486: step 470, loss = 3.77 (317.5 examples/sec; 0.403 sec/batch)
2017-04-02 21:12:35.612829: step 480, loss = 3.34 (319.3 examples/sec; 0.401 sec/batch)
2017-04-02 21:12:39.632811: step 490, loss = 3.10 (318.4 examples/sec; 0.402 sec/batch)
2017-04-02 21:12:43.745605: step 500, loss = 3.17 (311.2 examples/sec; 0.411 sec/batch)
2017-04-02 21:12:47.820420: step 510, loss = 3.17 (314.1 examples/sec; 0.407 sec/batch)
2017-04-02 21:12:51.906661: step 520, loss = 3.33 (313.2 examples/sec; 0.409 sec/batch)
2017-04-02 21:12:55.917760: step 530, loss = 3.03 (319.1 examples/sec; 0.401 sec/batch)
2017-04-02 21:12:59.945218: step 540, loss = 3.04 (317.8 examples/sec; 0.403 sec/batch)
2017-04-02 21:13:03.965421: step 550, loss = 2.92 (318.4 examples/sec; 0.402 sec/batch)
2017-04-02 21:13:07.960420: step 560, loss = 3.11 (320.4 examples/sec; 0.399 sec/batch)
2017-04-02 21:13:12.000890: step 570, loss = 3.01 (316.8 examples/sec; 0.404 sec/batch)
2017-04-02 21:13:15.984349: step 580, loss = 3.02 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 21:13:19.989470: step 590, loss = 3.15 (319.6 examples/sec; 0.401 sec/batch)
2017-04-02 21:13:24.077525: step 600, loss = 3.00 (313.1 examples/sec; 0.409 sec/batch)
2017-04-02 21:13:28.273899: step 610, loss = 3.00 (305.0 examples/sec; 0.420 sec/batch)
2017-04-02 21:13:32.367014: step 620, loss = 3.12 (312.7 examples/sec; 0.409 sec/batch)
2017-04-02 21:13:36.550428: step 630, loss = 3.12 (306.0 examples/sec; 0.418 sec/batch)
2017-04-02 21:13:40.752931: step 640, loss = 2.91 (304.6 examples/sec; 0.420 sec/batch)
2017-04-02 21:13:44.988566: step 650, loss = 2.93 (302.2 examples/sec; 0.424 sec/batch)
2017-04-02 21:13:49.225369: step 660, loss = 2.71 (302.1 examples/sec; 0.424 sec/batch)
2017-04-02 21:13:53.259447: step 670, loss = 2.93 (317.3 examples/sec; 0.403 sec/batch)
2017-04-02 21:13:57.300308: step 680, loss = 2.80 (316.8 examples/sec; 0.404 sec/batch)
2017-04-02 21:14:01.291529: step 690, loss = 2.80 (320.7 examples/sec; 0.399 sec/batch)
2017-04-02 21:14:05.455719: step 700, loss = 3.12 (307.4 examples/sec; 0.416 sec/batch)
2017-04-02 21:14:09.513070: step 710, loss = 2.76 (315.5 examples/sec; 0.406 sec/batch)
2017-04-02 21:14:13.496011: step 720, loss = 2.79 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 21:14:17.497562: step 730, loss = 2.84 (319.9 examples/sec; 0.400 sec/batch)
2017-04-02 21:14:21.496246: step 740, loss = 2.69 (320.1 examples/sec; 0.400 sec/batch)
2017-04-02 21:14:25.507103: step 750, loss = 2.74 (319.1 examples/sec; 0.401 sec/batch)
2017-04-02 21:14:29.472273: step 760, loss = 2.83 (322.8 examples/sec; 0.397 sec/batch)
2017-04-02 21:14:33.497642: step 770, loss = 2.78 (318.0 examples/sec; 0.403 sec/batch)
2017-04-02 21:14:37.490218: step 780, loss = 2.78 (320.6 examples/sec; 0.399 sec/batch)
2017-04-02 21:14:41.538087: step 790, loss = 2.70 (316.2 examples/sec; 0.405 sec/batch)
2017-04-02 21:14:45.607379: step 800, loss = 2.63 (314.6 examples/sec; 0.407 sec/batch)
2017-04-02 21:14:49.619283: step 810, loss = 2.73 (319.1 examples/sec; 0.401 sec/batch)
2017-04-02 21:14:53.616623: step 820, loss = 2.65 (320.2 examples/sec; 0.400 sec/batch)
2017-04-02 21:14:58.043355: step 830, loss = 2.57 (289.2 examples/sec; 0.443 sec/batch)
2017-04-02 21:15:02.192032: step 840, loss = 2.73 (308.5 examples/sec; 0.415 sec/batch)
2017-04-02 21:15:06.246652: step 850, loss = 2.67 (315.7 examples/sec; 0.405 sec/batch)
2017-04-02 21:15:10.310873: step 860, loss = 2.53 (314.9 examples/sec; 0.406 sec/batch)
2017-04-02 21:15:14.349470: step 870, loss = 2.61 (316.9 examples/sec; 0.404 sec/batch)
2017-04-02 21:15:18.395930: step 880, loss = 2.61 (316.3 examples/sec; 0.405 sec/batch)
2017-04-02 21:15:22.426105: step 890, loss = 2.65 (317.6 examples/sec; 0.403 sec/batch)
2017-04-02 21:15:26.459937: step 900, loss = 2.47 (317.3 examples/sec; 0.403 sec/batch)
2017-04-02 21:15:30.442502: step 910, loss = 2.53 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 21:15:34.407283: step 920, loss = 2.52 (322.8 examples/sec; 0.396 sec/batch)
2017-04-02 21:15:38.424292: step 930, loss = 2.35 (318.6 examples/sec; 0.402 sec/batch)
2017-04-02 21:15:42.418683: step 940, loss = 2.72 (320.4 examples/sec; 0.399 sec/batch)
2017-04-02 21:15:46.410562: step 950, loss = 2.59 (320.7 examples/sec; 0.399 sec/batch)
2017-04-02 21:15:50.414978: step 960, loss = 2.72 (319.6 examples/sec; 0.400 sec/batch)
2017-04-02 21:15:54.442840: step 970, loss = 2.54 (317.8 examples/sec; 0.403 sec/batch)
2017-04-02 21:15:58.437631: step 980, loss = 2.31 (320.4 examples/sec; 0.399 sec/batch)
2017-04-02 21:16:02.444879: step 990, loss = 2.39 (319.4 examples/sec; 0.401 sec/batch)
2017-04-02 21:16:06.505340: step 1000, loss = 2.44 (315.2 examples/sec; 0.406 sec/batch)
2017-04-02 21:16:10.507313: step 1010, loss = 2.35 (319.8 examples/sec; 0.400 sec/batch)
2017-04-02 21:16:14.549697: step 1020, loss = 2.39 (316.6 examples/sec; 0.404 sec/batch)
2017-04-02 21:16:18.590123: step 1030, loss = 2.42 (316.8 examples/sec; 0.404 sec/batch)
2017-04-02 21:16:22.632915: step 1040, loss = 2.38 (316.6 examples/sec; 0.404 sec/batch)
2017-04-02 21:16:27.065187: step 1050, loss = 2.41 (288.8 examples/sec; 0.443 sec/batch)
2017-04-02 21:16:31.104319: step 1060, loss = 2.26 (316.9 examples/sec; 0.404 sec/batch)
2017-04-02 21:16:35.172830: step 1070, loss = 2.49 (314.6 examples/sec; 0.407 sec/batch)
2017-04-02 21:16:39.301897: step 1080, loss = 2.32 (310.0 examples/sec; 0.413 sec/batch)
2017-04-02 21:16:43.390423: step 1090, loss = 2.42 (313.1 examples/sec; 0.409 sec/batch)
2017-04-02 21:16:47.501445: step 1100, loss = 2.66 (311.4 examples/sec; 0.411 sec/batch)
2017-04-02 21:16:51.585384: step 1110, loss = 2.20 (313.4 examples/sec; 0.408 sec/batch)
2017-04-02 21:16:55.690677: step 1120, loss = 2.38 (311.8 examples/sec; 0.411 sec/batch)
2017-04-02 21:16:59.770646: step 1130, loss = 2.24 (313.7 examples/sec; 0.408 sec/batch)
2017-04-02 21:17:03.830006: step 1140, loss = 2.08 (315.3 examples/sec; 0.406 sec/batch)
2017-04-02 21:17:07.844598: step 1150, loss = 2.28 (318.8 examples/sec; 0.401 sec/batch)
2017-04-02 21:17:11.862586: step 1160, loss = 2.35 (318.6 examples/sec; 0.402 sec/batch)
2017-04-02 21:17:15.856667: step 1170, loss = 2.08 (320.5 examples/sec; 0.399 sec/batch)
2017-04-02 21:17:19.889849: step 1180, loss = 2.23 (317.4 examples/sec; 0.403 sec/batch)
2017-04-02 21:17:23.919862: step 1190, loss = 2.26 (317.6 examples/sec; 0.403 sec/batch)
2017-04-02 21:17:27.993992: step 1200, loss = 2.52 (314.2 examples/sec; 0.407 sec/batch)
2017-04-02 21:17:32.000449: step 1210, loss = 2.26 (319.5 examples/sec; 0.401 sec/batch)
2017-04-02 21:17:36.066695: step 1220, loss = 2.33 (314.8 examples/sec; 0.407 sec/batch)
2017-04-02 21:17:40.124805: step 1230, loss = 2.35 (315.4 examples/sec; 0.406 sec/batch)
2017-04-02 21:17:44.168372: step 1240, loss = 2.20 (316.6 examples/sec; 0.404 sec/batch)
2017-04-02 21:17:48.237781: step 1250, loss = 2.20 (314.5 examples/sec; 0.407 sec/batch)
2017-04-02 21:17:52.316473: step 1260, loss = 2.18 (313.8 examples/sec; 0.408 sec/batch)
2017-04-02 21:17:56.387475: step 1270, loss = 2.26 (314.4 examples/sec; 0.407 sec/batch)
2017-04-02 21:18:00.406019: step 1280, loss = 2.16 (318.5 examples/sec; 0.402 sec/batch)
2017-04-02 21:18:04.437774: step 1290, loss = 2.15 (317.5 examples/sec; 0.403 sec/batch)
2017-04-02 21:18:08.559514: step 1300, loss = 2.01 (310.5 examples/sec; 0.412 sec/batch)
2017-04-02 21:18:12.610654: step 1310, loss = 2.02 (316.0 examples/sec; 0.405 sec/batch)
2017-04-02 21:18:16.675893: step 1320, loss = 1.93 (314.9 examples/sec; 0.407 sec/batch)
2017-04-02 21:18:20.730701: step 1330, loss = 2.13 (315.7 examples/sec; 0.405 sec/batch)
2017-04-02 21:18:24.753218: step 1340, loss = 2.20 (318.2 examples/sec; 0.402 sec/batch)
2017-04-02 21:18:28.778949: step 1350, loss = 1.99 (318.0 examples/sec; 0.403 sec/batch)
2017-04-02 21:18:32.788534: step 1360, loss = 1.98 (319.2 examples/sec; 0.401 sec/batch)
2017-04-02 21:18:36.811817: step 1370, loss = 2.00 (318.1 examples/sec; 0.402 sec/batch)
2017-04-02 21:18:40.812939: step 1380, loss = 2.09 (319.9 examples/sec; 0.400 sec/batch)
2017-04-02 21:18:44.784049: step 1390, loss = 2.13 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 21:18:48.856535: step 1400, loss = 2.00 (314.3 examples/sec; 0.407 sec/batch)
2017-04-02 21:18:52.894476: step 1410, loss = 1.91 (317.0 examples/sec; 0.404 sec/batch)
2017-04-02 21:18:56.918373: step 1420, loss = 1.94 (318.1 examples/sec; 0.402 sec/batch)
2017-04-02 21:19:00.947061: step 1430, loss = 2.05 (317.7 examples/sec; 0.403 sec/batch)
2017-04-02 21:19:04.986029: step 1440, loss = 1.89 (316.9 examples/sec; 0.404 sec/batch)
2017-04-02 21:19:09.013518: step 1450, loss = 2.16 (317.8 examples/sec; 0.403 sec/batch)
2017-04-02 21:19:13.006448: step 1460, loss = 1.89 (320.6 examples/sec; 0.399 sec/batch)
2017-04-02 21:19:17.042636: step 1470, loss = 1.93 (317.1 examples/sec; 0.404 sec/batch)
2017-04-02 21:19:21.090787: step 1480, loss = 1.90 (316.2 examples/sec; 0.405 sec/batch)
2017-04-02 21:19:25.855071: step 1490, loss = 1.78 (268.7 examples/sec; 0.476 sec/batch)
2017-04-02 21:19:29.936492: step 1500, loss = 1.96 (313.6 examples/sec; 0.408 sec/batch)
2017-04-02 21:19:33.964081: step 1510, loss = 1.85 (317.8 examples/sec; 0.403 sec/batch)
2017-04-02 21:19:37.944853: step 1520, loss = 2.07 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 21:19:41.969064: step 1530, loss = 1.98 (318.1 examples/sec; 0.402 sec/batch)
2017-04-02 21:19:45.937033: step 1540, loss = 1.80 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 21:19:50.036765: step 1550, loss = 1.94 (312.2 examples/sec; 0.410 sec/batch)
2017-04-02 21:19:54.170932: step 1560, loss = 1.91 (309.6 examples/sec; 0.413 sec/batch)
2017-04-02 21:19:58.235088: step 1570, loss = 1.94 (314.9 examples/sec; 0.406 sec/batch)
2017-04-02 21:20:02.343326: step 1580, loss = 1.75 (311.6 examples/sec; 0.411 sec/batch)
2017-04-02 21:20:06.452928: step 1590, loss = 1.88 (311.5 examples/sec; 0.411 sec/batch)
2017-04-02 21:20:10.600733: step 1600, loss = 1.82 (308.6 examples/sec; 0.415 sec/batch)
2017-04-02 21:20:14.661340: step 1610, loss = 2.13 (315.2 examples/sec; 0.406 sec/batch)
2017-04-02 21:20:18.709706: step 1620, loss = 1.90 (316.2 examples/sec; 0.405 sec/batch)
2017-04-02 21:20:22.717026: step 1630, loss = 1.87 (319.4 examples/sec; 0.401 sec/batch)
2017-04-02 21:20:26.783053: step 1640, loss = 2.05 (314.8 examples/sec; 0.407 sec/batch)
2017-04-02 21:20:30.757457: step 1650, loss = 1.78 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 21:20:34.796369: step 1660, loss = 1.88 (316.9 examples/sec; 0.404 sec/batch)
2017-04-02 21:20:38.843692: step 1670, loss = 1.77 (316.3 examples/sec; 0.405 sec/batch)
2017-04-02 21:20:42.837619: step 1680, loss = 1.68 (320.5 examples/sec; 0.399 sec/batch)
2017-04-02 21:20:46.929162: step 1690, loss = 1.82 (312.8 examples/sec; 0.409 sec/batch)
2017-04-02 21:20:51.024226: step 1700, loss = 1.78 (312.6 examples/sec; 0.410 sec/batch)
2017-04-02 21:20:55.055055: step 1710, loss = 1.77 (317.6 examples/sec; 0.403 sec/batch)
2017-04-02 21:20:59.060051: step 1720, loss = 1.79 (319.6 examples/sec; 0.400 sec/batch)
2017-04-02 21:21:03.076834: step 1730, loss = 1.78 (318.7 examples/sec; 0.402 sec/batch)
2017-04-02 21:21:07.117097: step 1740, loss = 1.84 (316.8 examples/sec; 0.404 sec/batch)
2017-04-02 21:21:11.173461: step 1750, loss = 1.81 (315.6 examples/sec; 0.406 sec/batch)
2017-04-02 21:21:15.267163: step 1760, loss = 1.90 (312.7 examples/sec; 0.409 sec/batch)
2017-04-02 21:21:19.272329: step 1770, loss = 1.65 (319.6 examples/sec; 0.401 sec/batch)
2017-04-02 21:21:23.334509: step 1780, loss = 1.75 (315.1 examples/sec; 0.406 sec/batch)
2017-04-02 21:21:27.402606: step 1790, loss = 1.84 (314.6 examples/sec; 0.407 sec/batch)
2017-04-02 21:21:31.490617: step 1800, loss = 1.66 (313.1 examples/sec; 0.409 sec/batch)
2017-04-02 21:21:35.553679: step 1810, loss = 1.84 (315.0 examples/sec; 0.406 sec/batch)
2017-04-02 21:21:39.613809: step 1820, loss = 1.86 (315.3 examples/sec; 0.406 sec/batch)
2017-04-02 21:21:43.652087: step 1830, loss = 1.77 (317.0 examples/sec; 0.404 sec/batch)
2017-04-02 21:21:47.758305: step 1840, loss = 1.75 (311.7 examples/sec; 0.411 sec/batch)
2017-04-02 21:21:51.741748: step 1850, loss = 1.69 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 21:21:55.721518: step 1860, loss = 1.66 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 21:21:59.703235: step 1870, loss = 1.73 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 21:22:03.707011: step 1880, loss = 1.68 (319.7 examples/sec; 0.400 sec/batch)
2017-04-02 21:22:07.689534: step 1890, loss = 1.69 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 21:22:11.722276: step 1900, loss = 1.66 (317.4 examples/sec; 0.403 sec/batch)
2017-04-02 21:22:15.734682: step 1910, loss = 1.78 (319.0 examples/sec; 0.401 sec/batch)
2017-04-02 21:22:19.716246: step 1920, loss = 1.72 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 21:22:23.745292: step 1930, loss = 1.74 (317.7 examples/sec; 0.403 sec/batch)
2017-04-02 21:22:27.737483: step 1940, loss = 1.61 (320.6 examples/sec; 0.399 sec/batch)
2017-04-02 21:22:31.723334: step 1950, loss = 1.63 (321.1 examples/sec; 0.399 sec/batch)
2017-04-02 21:22:35.722901: step 1960, loss = 1.61 (320.0 examples/sec; 0.400 sec/batch)
2017-04-02 21:22:39.743189: step 1970, loss = 1.68 (318.4 examples/sec; 0.402 sec/batch)
2017-04-02 21:22:43.716962: step 1980, loss = 1.62 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 21:22:47.676956: step 1990, loss = 1.79 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 21:22:51.782361: step 2000, loss = 1.64 (311.8 examples/sec; 0.411 sec/batch)
2017-04-02 21:22:55.790215: step 2010, loss = 1.90 (319.4 examples/sec; 0.401 sec/batch)
2017-04-02 21:22:59.793426: step 2020, loss = 1.63 (319.7 examples/sec; 0.400 sec/batch)
2017-04-02 21:23:03.846249: step 2030, loss = 1.47 (315.8 examples/sec; 0.405 sec/batch)
2017-04-02 21:23:07.821483: step 2040, loss = 1.60 (322.0 examples/sec; 0.398 sec/batch)
2017-04-02 21:23:11.830522: step 2050, loss = 1.75 (319.3 examples/sec; 0.401 sec/batch)
2017-04-02 21:23:15.851910: step 2060, loss = 1.70 (318.3 examples/sec; 0.402 sec/batch)
2017-04-02 21:23:19.862926: step 2070, loss = 1.61 (319.1 examples/sec; 0.401 sec/batch)
2017-04-02 21:27:15.551730: step 2080, loss = 1.54 (5.4 examples/sec; 23.569 sec/batch)
2017-04-02 21:27:19.586438: step 2090, loss = 1.57 (317.2 examples/sec; 0.403 sec/batch)
2017-04-02 21:27:23.636404: step 2100, loss = 1.45 (316.1 examples/sec; 0.405 sec/batch)
2017-04-02 21:27:27.620479: step 2110, loss = 1.74 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 21:27:31.612069: step 2120, loss = 1.59 (320.7 examples/sec; 0.399 sec/batch)
2017-04-02 21:27:35.580797: step 2130, loss = 1.60 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 21:27:39.527025: step 2140, loss = 1.63 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 21:27:43.491223: step 2150, loss = 1.73 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 21:27:47.445155: step 2160, loss = 1.50 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 21:27:51.408553: step 2170, loss = 1.50 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 21:27:55.346524: step 2180, loss = 1.54 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 21:27:59.300854: step 2190, loss = 1.35 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 21:28:03.367938: step 2200, loss = 1.71 (314.7 examples/sec; 0.407 sec/batch)
2017-04-02 21:28:07.322700: step 2210, loss = 1.66 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 21:28:11.327754: step 2220, loss = 1.59 (319.6 examples/sec; 0.401 sec/batch)
2017-04-02 21:28:15.319652: step 2230, loss = 1.40 (320.6 examples/sec; 0.399 sec/batch)
2017-04-02 21:28:19.265013: step 2240, loss = 1.56 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 21:28:23.263630: step 2250, loss = 1.60 (320.1 examples/sec; 0.400 sec/batch)
2017-04-02 21:28:27.222522: step 2260, loss = 1.90 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 21:28:31.202569: step 2270, loss = 1.32 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 21:28:35.209127: step 2280, loss = 1.59 (319.5 examples/sec; 0.401 sec/batch)
2017-04-02 21:28:39.201349: step 2290, loss = 1.49 (320.6 examples/sec; 0.399 sec/batch)
2017-04-02 21:28:43.273052: step 2300, loss = 1.38 (314.4 examples/sec; 0.407 sec/batch)
2017-04-02 21:28:47.294633: step 2310, loss = 1.69 (318.3 examples/sec; 0.402 sec/batch)
2017-04-02 21:28:51.289053: step 2320, loss = 1.38 (320.4 examples/sec; 0.399 sec/batch)
2017-04-02 21:28:55.295495: step 2330, loss = 1.40 (319.5 examples/sec; 0.401 sec/batch)
2017-04-02 21:28:59.320338: step 2340, loss = 1.57 (318.0 examples/sec; 0.402 sec/batch)
2017-04-02 21:29:03.330266: step 2350, loss = 1.63 (319.2 examples/sec; 0.401 sec/batch)
2017-04-02 21:29:07.336986: step 2360, loss = 1.52 (319.5 examples/sec; 0.401 sec/batch)
2017-04-02 21:29:11.347002: step 2370, loss = 1.70 (319.2 examples/sec; 0.401 sec/batch)
2017-04-02 21:29:15.362852: step 2380, loss = 1.58 (318.7 examples/sec; 0.402 sec/batch)
2017-04-02 21:29:19.356436: step 2390, loss = 1.53 (320.5 examples/sec; 0.399 sec/batch)
2017-04-02 21:29:24.134157: step 2400, loss = 1.69 (267.9 examples/sec; 0.478 sec/batch)
2017-04-02 21:29:28.155579: step 2410, loss = 1.39 (318.3 examples/sec; 0.402 sec/batch)
2017-04-02 21:29:32.164074: step 2420, loss = 1.46 (319.3 examples/sec; 0.401 sec/batch)
2017-04-02 21:29:36.171571: step 2430, loss = 1.47 (319.4 examples/sec; 0.401 sec/batch)
2017-04-02 21:29:40.186161: step 2440, loss = 1.51 (318.8 examples/sec; 0.401 sec/batch)
2017-04-02 21:29:44.162700: step 2450, loss = 1.35 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 21:29:48.232986: step 2460, loss = 1.43 (314.5 examples/sec; 0.407 sec/batch)
2017-04-02 21:29:52.293667: step 2470, loss = 1.32 (315.2 examples/sec; 0.406 sec/batch)
2017-04-02 21:29:56.444261: step 2480, loss = 1.32 (308.4 examples/sec; 0.415 sec/batch)
2017-04-02 21:30:00.552107: step 2490, loss = 1.50 (311.6 examples/sec; 0.411 sec/batch)
2017-04-02 21:30:04.653159: step 2500, loss = 1.39 (312.1 examples/sec; 0.410 sec/batch)
2017-04-02 21:30:08.720139: step 2510, loss = 1.42 (314.7 examples/sec; 0.407 sec/batch)
2017-04-02 21:30:12.839517: step 2520, loss = 1.22 (310.7 examples/sec; 0.412 sec/batch)
2017-04-02 21:30:16.866480: step 2530, loss = 1.43 (317.9 examples/sec; 0.403 sec/batch)
2017-04-02 21:30:20.921596: step 2540, loss = 1.52 (315.7 examples/sec; 0.406 sec/batch)
2017-04-02 21:30:24.989992: step 2550, loss = 1.54 (314.6 examples/sec; 0.407 sec/batch)
2017-04-02 21:30:28.968172: step 2560, loss = 1.42 (321.8 examples/sec; 0.398 sec/batch)
2017-04-02 21:30:32.957327: step 2570, loss = 1.41 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 21:30:36.968274: step 2580, loss = 1.31 (319.1 examples/sec; 0.401 sec/batch)
2017-04-02 21:30:40.980511: step 2590, loss = 1.53 (319.0 examples/sec; 0.401 sec/batch)
2017-04-02 21:30:45.079175: step 2600, loss = 1.40 (312.3 examples/sec; 0.410 sec/batch)
2017-04-02 21:30:49.129471: step 2610, loss = 1.29 (316.0 examples/sec; 0.405 sec/batch)
2017-04-02 21:30:53.134992: step 2620, loss = 1.30 (319.6 examples/sec; 0.401 sec/batch)
2017-04-02 21:30:57.312919: step 2630, loss = 1.45 (306.4 examples/sec; 0.418 sec/batch)
2017-04-02 21:31:02.368470: step 2640, loss = 1.42 (253.2 examples/sec; 0.506 sec/batch)
2017-04-02 21:31:06.773419: step 2650, loss = 1.21 (290.6 examples/sec; 0.440 sec/batch)
2017-04-02 21:31:11.027629: step 2660, loss = 1.39 (300.9 examples/sec; 0.425 sec/batch)
2017-04-02 21:31:15.233359: step 2670, loss = 1.28 (304.3 examples/sec; 0.421 sec/batch)
2017-04-02 21:31:19.363424: step 2680, loss = 1.44 (309.9 examples/sec; 0.413 sec/batch)
2017-04-02 21:31:23.367667: step 2690, loss = 1.20 (319.7 examples/sec; 0.400 sec/batch)
2017-04-02 21:31:27.448132: step 2700, loss = 1.20 (313.7 examples/sec; 0.408 sec/batch)
2017-04-02 21:31:31.466085: step 2710, loss = 1.44 (318.6 examples/sec; 0.402 sec/batch)
2017-04-02 21:31:35.441334: step 2720, loss = 1.30 (322.0 examples/sec; 0.398 sec/batch)
2017-04-02 21:31:39.405227: step 2730, loss = 1.35 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 21:31:43.348927: step 2740, loss = 1.59 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 21:31:47.333697: step 2750, loss = 1.33 (321.2 examples/sec; 0.398 sec/batch)
2017-04-02 21:31:51.288799: step 2760, loss = 1.39 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 21:31:55.260632: step 2770, loss = 1.33 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 21:31:59.212786: step 2780, loss = 1.09 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 21:32:03.212982: step 2790, loss = 1.37 (320.0 examples/sec; 0.400 sec/batch)
2017-04-02 21:32:07.223474: step 2800, loss = 1.24 (319.2 examples/sec; 0.401 sec/batch)
2017-04-02 21:32:11.177314: step 2810, loss = 1.27 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 21:32:15.105527: step 2820, loss = 1.28 (325.8 examples/sec; 0.393 sec/batch)
2017-04-02 21:32:19.030277: step 2830, loss = 1.27 (326.1 examples/sec; 0.392 sec/batch)
2017-04-02 21:32:22.929129: step 2840, loss = 1.41 (328.3 examples/sec; 0.390 sec/batch)
2017-04-02 21:32:26.852965: step 2850, loss = 1.35 (326.2 examples/sec; 0.392 sec/batch)
2017-04-02 21:32:30.803338: step 2860, loss = 1.29 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 21:32:34.760415: step 2870, loss = 1.38 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 21:32:38.733014: step 2880, loss = 1.18 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 21:32:42.716778: step 2890, loss = 1.31 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 21:32:46.725508: step 2900, loss = 1.26 (319.3 examples/sec; 0.401 sec/batch)
2017-04-02 21:32:50.621161: step 2910, loss = 1.18 (328.6 examples/sec; 0.390 sec/batch)
2017-04-02 21:32:54.571077: step 2920, loss = 1.25 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 21:32:58.554062: step 2930, loss = 1.15 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 21:33:02.533009: step 2940, loss = 1.27 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 21:33:06.479942: step 2950, loss = 1.35 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 21:33:10.445425: step 2960, loss = 1.19 (322.8 examples/sec; 0.397 sec/batch)
2017-04-02 21:33:14.386221: step 2970, loss = 1.38 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 21:33:18.350289: step 2980, loss = 1.31 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 21:33:22.359169: step 2990, loss = 1.38 (319.3 examples/sec; 0.401 sec/batch)
2017-04-02 21:33:26.419656: step 3000, loss = 1.29 (315.2 examples/sec; 0.406 sec/batch)
2017-04-02 21:33:30.353435: step 3010, loss = 1.22 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 21:33:34.313100: step 3020, loss = 1.22 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 21:33:38.324401: step 3030, loss = 1.23 (319.1 examples/sec; 0.401 sec/batch)
2017-04-02 21:33:42.348017: step 3040, loss = 1.45 (318.1 examples/sec; 0.402 sec/batch)
2017-04-02 21:33:46.302649: step 3050, loss = 1.30 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 21:33:50.261474: step 3060, loss = 1.17 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 21:33:54.208368: step 3070, loss = 1.12 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 21:33:58.219855: step 3080, loss = 1.09 (319.1 examples/sec; 0.401 sec/batch)
2017-04-02 21:34:02.238863: step 3090, loss = 1.34 (318.5 examples/sec; 0.402 sec/batch)
2017-04-02 21:34:06.307770: step 3100, loss = 1.22 (314.6 examples/sec; 0.407 sec/batch)
2017-04-02 21:34:10.266737: step 3110, loss = 1.38 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 21:34:14.278710: step 3120, loss = 1.38 (319.0 examples/sec; 0.401 sec/batch)
2017-04-02 21:34:18.279360: step 3130, loss = 1.25 (319.9 examples/sec; 0.400 sec/batch)
2017-04-02 21:34:22.334877: step 3140, loss = 1.24 (315.6 examples/sec; 0.406 sec/batch)
2017-04-02 21:34:26.304028: step 3150, loss = 1.25 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 21:34:30.335502: step 3160, loss = 1.18 (317.5 examples/sec; 0.403 sec/batch)
2017-04-02 21:34:34.391182: step 3170, loss = 1.38 (315.6 examples/sec; 0.406 sec/batch)
2017-04-02 21:34:38.353713: step 3180, loss = 1.26 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 21:34:42.380902: step 3190, loss = 1.27 (317.8 examples/sec; 0.403 sec/batch)
2017-04-02 21:34:46.473321: step 3200, loss = 1.25 (312.8 examples/sec; 0.409 sec/batch)
2017-04-02 21:34:50.521381: step 3210, loss = 1.24 (316.2 examples/sec; 0.405 sec/batch)
2017-04-02 21:34:54.566071: step 3220, loss = 1.29 (316.5 examples/sec; 0.404 sec/batch)
2017-04-02 21:34:58.557785: step 3230, loss = 1.36 (320.7 examples/sec; 0.399 sec/batch)
2017-04-02 21:35:02.574554: step 3240, loss = 1.38 (318.7 examples/sec; 0.402 sec/batch)
2017-04-02 21:35:06.587679: step 3250, loss = 1.26 (319.0 examples/sec; 0.401 sec/batch)
2017-04-02 21:35:10.518125: step 3260, loss = 1.19 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 21:35:14.478342: step 3270, loss = 1.14 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 21:35:18.429659: step 3280, loss = 1.13 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 21:35:22.388568: step 3290, loss = 1.31 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 21:35:26.401471: step 3300, loss = 1.30 (319.0 examples/sec; 0.401 sec/batch)
2017-04-02 21:35:30.351107: step 3310, loss = 1.37 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 21:35:34.363939: step 3320, loss = 1.14 (319.0 examples/sec; 0.401 sec/batch)
2017-04-02 21:35:38.388143: step 3330, loss = 1.30 (318.1 examples/sec; 0.402 sec/batch)
2017-04-02 21:35:42.371691: step 3340, loss = 1.22 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 21:35:46.356583: step 3350, loss = 1.30 (321.2 examples/sec; 0.398 sec/batch)
2017-04-02 21:35:50.325116: step 3360, loss = 1.10 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 21:35:54.338052: step 3370, loss = 1.20 (319.0 examples/sec; 0.401 sec/batch)
2017-04-02 21:35:58.338958: step 3380, loss = 1.07 (319.9 examples/sec; 0.400 sec/batch)
2017-04-02 21:36:02.310217: step 3390, loss = 1.10 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 21:36:06.381865: step 3400, loss = 0.93 (314.4 examples/sec; 0.407 sec/batch)
2017-04-02 21:36:10.365686: step 3410, loss = 1.16 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 21:36:14.349305: step 3420, loss = 1.18 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 21:36:18.360112: step 3430, loss = 1.18 (319.1 examples/sec; 0.401 sec/batch)
2017-04-02 21:36:22.563922: step 3440, loss = 1.28 (304.5 examples/sec; 0.420 sec/batch)
2017-04-02 21:36:26.659205: step 3450, loss = 1.14 (312.6 examples/sec; 0.410 sec/batch)
2017-04-02 21:36:30.730193: step 3460, loss = 1.17 (314.4 examples/sec; 0.407 sec/batch)
2017-04-02 21:36:34.859379: step 3470, loss = 1.17 (310.0 examples/sec; 0.413 sec/batch)
2017-04-02 21:36:39.083241: step 3480, loss = 1.14 (303.0 examples/sec; 0.422 sec/batch)
2017-04-02 21:36:43.305736: step 3490, loss = 1.11 (303.1 examples/sec; 0.422 sec/batch)
2017-04-02 21:36:47.677888: step 3500, loss = 1.20 (292.8 examples/sec; 0.437 sec/batch)
2017-04-02 21:36:51.951133: step 3510, loss = 1.21 (299.5 examples/sec; 0.427 sec/batch)
2017-04-02 21:36:56.116705: step 3520, loss = 1.18 (307.3 examples/sec; 0.417 sec/batch)
2017-04-02 21:37:00.198309: step 3530, loss = 1.10 (313.6 examples/sec; 0.408 sec/batch)
2017-04-02 21:37:04.165019: step 3540, loss = 1.35 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 21:37:08.182754: step 3550, loss = 1.23 (318.6 examples/sec; 0.402 sec/batch)
2017-04-02 21:37:12.154862: step 3560, loss = 1.17 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 21:37:16.122395: step 3570, loss = 1.21 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 21:37:20.102638: step 3580, loss = 1.39 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 21:37:24.039256: step 3590, loss = 1.16 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 21:37:28.065615: step 3600, loss = 1.02 (317.9 examples/sec; 0.403 sec/batch)
2017-04-02 21:37:32.043006: step 3610, loss = 1.26 (321.8 examples/sec; 0.398 sec/batch)
2017-04-02 21:37:36.345932: step 3620, loss = 1.18 (297.5 examples/sec; 0.430 sec/batch)
2017-04-02 21:37:40.352952: step 3630, loss = 1.07 (319.4 examples/sec; 0.401 sec/batch)
2017-04-02 21:37:44.326655: step 3640, loss = 1.17 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 21:37:48.310760: step 3650, loss = 1.02 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 21:37:52.525771: step 3660, loss = 1.07 (303.7 examples/sec; 0.422 sec/batch)
2017-04-02 21:37:56.582087: step 3670, loss = 1.15 (315.6 examples/sec; 0.406 sec/batch)
2017-04-02 21:38:00.590215: step 3680, loss = 1.12 (319.4 examples/sec; 0.401 sec/batch)
2017-04-02 21:38:04.588454: step 3690, loss = 1.03 (320.1 examples/sec; 0.400 sec/batch)
2017-04-02 21:38:08.626330: step 3700, loss = 1.07 (317.0 examples/sec; 0.404 sec/batch)
2017-04-02 21:38:12.603659: step 3710, loss = 1.16 (321.8 examples/sec; 0.398 sec/batch)
2017-04-02 21:38:16.588042: step 3720, loss = 1.33 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 21:38:20.570118: step 3730, loss = 1.16 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 21:38:24.547200: step 3740, loss = 1.06 (321.8 examples/sec; 0.398 sec/batch)
2017-04-02 21:38:28.520416: step 3750, loss = 1.33 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 21:38:32.471702: step 3760, loss = 1.15 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 21:38:36.426491: step 3770, loss = 1.02 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 21:38:40.440093: step 3780, loss = 1.04 (318.9 examples/sec; 0.401 sec/batch)
2017-04-02 21:38:44.467177: step 3790, loss = 1.11 (317.8 examples/sec; 0.403 sec/batch)
2017-04-02 21:38:48.528389: step 3800, loss = 1.44 (315.2 examples/sec; 0.406 sec/batch)
2017-04-02 21:38:52.522696: step 3810, loss = 1.17 (320.5 examples/sec; 0.399 sec/batch)
2017-04-02 21:38:56.517231: step 3820, loss = 1.03 (320.4 examples/sec; 0.399 sec/batch)
2017-04-02 21:39:00.553542: step 3830, loss = 1.05 (317.1 examples/sec; 0.404 sec/batch)
2017-04-02 21:39:04.549070: step 3840, loss = 1.25 (320.4 examples/sec; 0.400 sec/batch)
2017-04-02 21:39:08.511519: step 3850, loss = 1.28 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 21:39:12.464594: step 3860, loss = 1.43 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 21:39:16.462231: step 3870, loss = 1.31 (320.2 examples/sec; 0.400 sec/batch)
2017-04-02 21:39:20.514462: step 3880, loss = 1.18 (315.9 examples/sec; 0.405 sec/batch)
2017-04-02 21:39:25.817572: step 3890, loss = 1.09 (241.4 examples/sec; 0.530 sec/batch)
2017-04-02 21:39:29.880214: step 3900, loss = 1.23 (315.1 examples/sec; 0.406 sec/batch)
2017-04-02 21:39:33.895471: step 3910, loss = 1.23 (318.8 examples/sec; 0.402 sec/batch)
2017-04-02 21:39:37.929089: step 3920, loss = 1.12 (317.3 examples/sec; 0.403 sec/batch)
2017-04-02 21:39:41.916211: step 3930, loss = 1.28 (321.0 examples/sec; 0.399 sec/batch)
2017-04-02 21:39:45.920492: step 3940, loss = 1.18 (319.7 examples/sec; 0.400 sec/batch)
2017-04-02 21:39:49.913698: step 3950, loss = 1.24 (320.5 examples/sec; 0.399 sec/batch)
2017-04-02 21:39:53.917196: step 3960, loss = 0.96 (319.7 examples/sec; 0.400 sec/batch)
2017-04-02 21:39:57.930212: step 3970, loss = 1.29 (319.0 examples/sec; 0.401 sec/batch)
2017-04-02 21:40:01.895035: step 3980, loss = 1.18 (322.8 examples/sec; 0.396 sec/batch)
2017-04-02 21:40:05.876439: step 3990, loss = 0.99 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 21:40:09.876862: step 4000, loss = 1.17 (320.0 examples/sec; 0.400 sec/batch)
2017-04-02 21:42:22.450531: step 4010, loss = 1.26 (9.7 examples/sec; 13.257 sec/batch)
2017-04-02 21:42:26.488456: step 4020, loss = 1.13 (317.0 examples/sec; 0.404 sec/batch)
2017-04-02 21:42:30.443725: step 4030, loss = 1.03 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 21:42:34.469602: step 4040, loss = 1.54 (317.9 examples/sec; 0.403 sec/batch)
2017-04-02 21:42:38.569520: step 4050, loss = 1.18 (312.2 examples/sec; 0.410 sec/batch)
2017-04-02 21:42:42.585752: step 4060, loss = 1.13 (318.7 examples/sec; 0.402 sec/batch)
2017-04-02 21:42:46.614553: step 4070, loss = 0.98 (317.7 examples/sec; 0.403 sec/batch)
2017-04-02 21:42:50.684271: step 4080, loss = 1.06 (314.5 examples/sec; 0.407 sec/batch)
2017-04-02 21:42:54.716336: step 4090, loss = 1.21 (317.5 examples/sec; 0.403 sec/batch)
2017-04-02 21:42:58.761559: step 4100, loss = 0.98 (316.4 examples/sec; 0.405 sec/batch)
2017-04-02 21:43:02.775889: step 4110, loss = 1.03 (318.9 examples/sec; 0.401 sec/batch)
2017-04-02 21:43:06.745188: step 4120, loss = 1.18 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 21:43:10.726986: step 4130, loss = 1.08 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 21:43:14.711384: step 4140, loss = 1.02 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 21:43:18.671173: step 4150, loss = 0.99 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 21:43:22.685004: step 4160, loss = 1.10 (318.9 examples/sec; 0.401 sec/batch)
2017-04-02 21:43:26.664504: step 4170, loss = 1.21 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 21:43:30.626367: step 4180, loss = 1.18 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 21:43:34.588396: step 4190, loss = 1.04 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 21:43:38.597933: step 4200, loss = 1.24 (319.2 examples/sec; 0.401 sec/batch)
2017-04-02 21:43:42.601681: step 4210, loss = 1.06 (319.7 examples/sec; 0.400 sec/batch)
2017-04-02 21:43:46.603578: step 4220, loss = 1.09 (319.8 examples/sec; 0.400 sec/batch)
2017-04-02 21:43:50.575371: step 4230, loss = 1.10 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 21:43:54.557209: step 4240, loss = 0.99 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 21:43:58.537443: step 4250, loss = 1.11 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 21:44:02.537973: step 4260, loss = 1.20 (320.0 examples/sec; 0.400 sec/batch)
2017-04-02 21:44:06.509899: step 4270, loss = 1.02 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 21:44:10.457610: step 4280, loss = 0.92 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 21:44:14.398908: step 4290, loss = 0.98 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 21:44:18.385318: step 4300, loss = 0.94 (321.1 examples/sec; 0.399 sec/batch)
2017-04-02 21:44:22.344225: step 4310, loss = 1.10 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 21:44:26.292674: step 4320, loss = 1.13 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 21:44:30.413198: step 4330, loss = 1.06 (310.6 examples/sec; 0.412 sec/batch)
2017-04-02 21:44:34.477662: step 4340, loss = 1.13 (314.9 examples/sec; 0.406 sec/batch)
2017-04-02 21:44:38.566366: step 4350, loss = 1.33 (313.1 examples/sec; 0.409 sec/batch)
2017-04-02 21:44:42.691578: step 4360, loss = 1.15 (310.3 examples/sec; 0.413 sec/batch)
2017-04-02 21:44:46.802098: step 4370, loss = 1.09 (311.4 examples/sec; 0.411 sec/batch)
2017-04-02 21:44:50.953290: step 4380, loss = 1.07 (308.3 examples/sec; 0.415 sec/batch)
2017-04-02 21:44:55.065107: step 4390, loss = 0.99 (311.3 examples/sec; 0.411 sec/batch)
2017-04-02 21:44:59.286143: step 4400, loss = 1.02 (303.2 examples/sec; 0.422 sec/batch)
2017-04-02 21:45:03.462618: step 4410, loss = 0.97 (306.5 examples/sec; 0.418 sec/batch)
2017-04-02 21:45:07.579372: step 4420, loss = 0.99 (310.9 examples/sec; 0.412 sec/batch)
2017-04-02 21:45:11.504315: step 4430, loss = 1.16 (326.1 examples/sec; 0.392 sec/batch)
2017-04-02 21:45:15.439486: step 4440, loss = 1.35 (325.3 examples/sec; 0.394 sec/batch)
2017-04-02 21:45:19.345254: step 4450, loss = 1.12 (327.7 examples/sec; 0.391 sec/batch)
2017-04-02 21:45:23.264507: step 4460, loss = 0.96 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 21:45:27.153995: step 4470, loss = 0.96 (329.1 examples/sec; 0.389 sec/batch)
2017-04-02 21:45:31.075384: step 4480, loss = 1.07 (326.4 examples/sec; 0.392 sec/batch)
2017-04-02 21:45:35.020165: step 4490, loss = 1.04 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 21:45:39.009228: step 4500, loss = 1.26 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 21:45:42.911870: step 4510, loss = 1.14 (328.0 examples/sec; 0.390 sec/batch)
2017-04-02 21:45:46.828789: step 4520, loss = 1.33 (326.8 examples/sec; 0.392 sec/batch)
2017-04-02 21:45:50.728542: step 4530, loss = 0.81 (328.2 examples/sec; 0.390 sec/batch)
2017-04-02 21:45:54.674715: step 4540, loss = 1.10 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 21:45:58.617119: step 4550, loss = 1.13 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 21:46:02.568741: step 4560, loss = 0.98 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 21:46:06.483481: step 4570, loss = 0.95 (327.0 examples/sec; 0.391 sec/batch)
2017-04-02 21:46:10.396772: step 4580, loss = 1.16 (327.1 examples/sec; 0.391 sec/batch)
2017-04-02 21:46:14.309034: step 4590, loss = 0.95 (327.2 examples/sec; 0.391 sec/batch)
2017-04-02 21:46:18.298565: step 4600, loss = 0.97 (320.8 examples/sec; 0.399 sec/batch)
2017-04-02 21:46:22.225257: step 4610, loss = 1.31 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 21:46:26.175901: step 4620, loss = 1.04 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 21:46:30.108702: step 4630, loss = 1.10 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 21:46:34.040595: step 4640, loss = 1.00 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 21:46:37.992665: step 4650, loss = 0.94 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 21:46:41.938489: step 4660, loss = 1.01 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 21:46:45.873937: step 4670, loss = 0.99 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 21:46:49.787460: step 4680, loss = 1.02 (327.1 examples/sec; 0.391 sec/batch)
2017-04-02 21:46:53.778582: step 4690, loss = 1.01 (320.7 examples/sec; 0.399 sec/batch)
2017-04-02 21:46:57.798898: step 4700, loss = 1.20 (318.4 examples/sec; 0.402 sec/batch)
2017-04-02 21:47:01.797031: step 4710, loss = 0.95 (320.1 examples/sec; 0.400 sec/batch)
2017-04-02 21:47:05.751870: step 4720, loss = 1.20 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 21:47:09.718978: step 4730, loss = 0.97 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 21:47:13.640397: step 4740, loss = 0.91 (326.4 examples/sec; 0.392 sec/batch)
2017-04-02 21:47:17.601651: step 4750, loss = 0.96 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 21:47:21.603119: step 4760, loss = 1.08 (319.9 examples/sec; 0.400 sec/batch)
2017-04-02 21:47:25.523001: step 4770, loss = 0.95 (326.5 examples/sec; 0.392 sec/batch)
2017-04-02 21:47:29.483173: step 4780, loss = 0.93 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 21:47:33.405650: step 4790, loss = 1.01 (326.3 examples/sec; 0.392 sec/batch)
2017-04-02 21:47:37.785146: step 4800, loss = 1.14 (292.3 examples/sec; 0.438 sec/batch)
2017-04-02 21:47:41.688332: step 4810, loss = 1.09 (327.9 examples/sec; 0.390 sec/batch)
2017-04-02 21:47:45.679061: step 4820, loss = 1.06 (320.7 examples/sec; 0.399 sec/batch)
2017-04-02 21:47:49.578331: step 4830, loss = 0.97 (328.3 examples/sec; 0.390 sec/batch)
2017-04-02 21:47:53.465970: step 4840, loss = 1.08 (329.2 examples/sec; 0.389 sec/batch)
2017-04-02 21:47:57.372909: step 4850, loss = 0.92 (327.6 examples/sec; 0.391 sec/batch)
2017-04-02 21:48:01.291126: step 4860, loss = 1.19 (326.7 examples/sec; 0.392 sec/batch)
2017-04-02 21:48:05.210062: step 4870, loss = 1.06 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 21:48:09.300927: step 4880, loss = 0.97 (312.9 examples/sec; 0.409 sec/batch)
2017-04-02 21:48:13.387495: step 4890, loss = 1.03 (313.2 examples/sec; 0.409 sec/batch)
2017-04-02 21:48:17.428652: step 4900, loss = 1.13 (316.7 examples/sec; 0.404 sec/batch)
2017-04-02 21:48:21.392722: step 4910, loss = 1.12 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 21:48:25.335231: step 4920, loss = 1.15 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 21:48:29.269509: step 4930, loss = 0.88 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 21:48:33.207443: step 4940, loss = 0.96 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 21:48:37.128587: step 4950, loss = 1.13 (326.4 examples/sec; 0.392 sec/batch)
2017-04-02 21:48:41.032455: step 4960, loss = 0.97 (327.9 examples/sec; 0.390 sec/batch)
2017-04-02 21:48:44.949087: step 4970, loss = 1.06 (326.8 examples/sec; 0.392 sec/batch)
2017-04-02 21:48:48.857873: step 4980, loss = 0.91 (327.5 examples/sec; 0.391 sec/batch)
2017-04-02 21:48:52.758177: step 4990, loss = 1.41 (328.2 examples/sec; 0.390 sec/batch)
2017-04-02 21:48:56.710787: step 5000, loss = 1.14 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 21:49:00.611078: step 5010, loss = 0.99 (328.2 examples/sec; 0.390 sec/batch)
2017-04-02 21:49:04.571214: step 5020, loss = 1.16 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 21:49:08.506331: step 5030, loss = 0.90 (325.3 examples/sec; 0.394 sec/batch)
2017-04-02 21:49:12.510634: step 5040, loss = 0.98 (319.7 examples/sec; 0.400 sec/batch)
2017-04-02 21:49:16.502790: step 5050, loss = 1.06 (320.6 examples/sec; 0.399 sec/batch)
2017-04-02 21:49:20.448538: step 5060, loss = 1.02 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 21:49:25.038314: step 5070, loss = 0.94 (278.9 examples/sec; 0.459 sec/batch)
2017-04-02 21:49:28.971221: step 5080, loss = 0.93 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 21:49:32.915621: step 5090, loss = 1.02 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 21:49:36.947382: step 5100, loss = 1.16 (317.5 examples/sec; 0.403 sec/batch)
2017-04-02 21:49:40.938240: step 5110, loss = 0.93 (320.7 examples/sec; 0.399 sec/batch)
2017-04-02 21:49:44.862779: step 5120, loss = 1.02 (326.2 examples/sec; 0.392 sec/batch)
2017-04-02 21:49:48.850333: step 5130, loss = 1.06 (321.0 examples/sec; 0.399 sec/batch)
2017-04-02 21:49:52.749531: step 5140, loss = 1.00 (328.3 examples/sec; 0.390 sec/batch)
2017-04-02 21:49:56.674449: step 5150, loss = 1.06 (326.1 examples/sec; 0.392 sec/batch)
2017-04-02 21:50:00.669478: step 5160, loss = 0.95 (320.4 examples/sec; 0.400 sec/batch)
2017-04-02 21:50:04.607456: step 5170, loss = 0.99 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 21:50:08.553500: step 5180, loss = 1.02 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 21:50:12.509223: step 5190, loss = 0.97 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 21:50:16.490926: step 5200, loss = 1.14 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 21:50:20.442776: step 5210, loss = 0.97 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 21:50:24.381960: step 5220, loss = 0.92 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 21:50:28.303655: step 5230, loss = 0.90 (326.4 examples/sec; 0.392 sec/batch)
2017-04-02 21:50:32.198463: step 5240, loss = 1.00 (328.6 examples/sec; 0.389 sec/batch)
2017-04-02 21:50:36.121638: step 5250, loss = 0.94 (326.3 examples/sec; 0.392 sec/batch)
2017-04-02 21:50:40.099913: step 5260, loss = 1.09 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 21:50:44.035870: step 5270, loss = 1.17 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 21:50:48.004472: step 5280, loss = 1.09 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 21:50:51.939219: step 5290, loss = 0.83 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 21:50:55.960245: step 5300, loss = 0.92 (318.3 examples/sec; 0.402 sec/batch)
2017-04-02 21:50:59.911332: step 5310, loss = 0.84 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 21:51:03.885108: step 5320, loss = 1.30 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 21:51:07.809613: step 5330, loss = 0.86 (326.2 examples/sec; 0.392 sec/batch)
2017-04-02 21:51:11.741966: step 5340, loss = 1.03 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 21:51:15.659015: step 5350, loss = 0.93 (326.8 examples/sec; 0.392 sec/batch)
2017-04-02 21:51:19.580536: step 5360, loss = 0.98 (326.4 examples/sec; 0.392 sec/batch)
2017-04-02 21:51:23.524151: step 5370, loss = 1.07 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 21:51:27.440332: step 5380, loss = 0.95 (326.8 examples/sec; 0.392 sec/batch)
2017-04-02 21:51:31.381706: step 5390, loss = 1.00 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 21:51:35.397719: step 5400, loss = 1.13 (318.7 examples/sec; 0.402 sec/batch)
2017-04-02 21:51:39.342642: step 5410, loss = 1.04 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 21:51:43.255724: step 5420, loss = 0.98 (327.1 examples/sec; 0.391 sec/batch)
2017-04-02 21:51:47.158902: step 5430, loss = 1.03 (327.9 examples/sec; 0.390 sec/batch)
2017-04-02 21:51:51.095322: step 5440, loss = 0.90 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 21:51:55.050448: step 5450, loss = 0.93 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 21:51:59.009967: step 5460, loss = 1.02 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 21:52:02.914194: step 5470, loss = 0.93 (327.8 examples/sec; 0.390 sec/batch)
2017-04-02 21:52:06.850966: step 5480, loss = 1.03 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 21:52:10.801828: step 5490, loss = 0.99 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 21:52:14.780397: step 5500, loss = 0.90 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 21:52:18.725909: step 5510, loss = 1.08 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 21:52:22.678239: step 5520, loss = 0.98 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 21:52:26.609188: step 5530, loss = 0.89 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 21:52:30.569735: step 5540, loss = 1.02 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 21:52:34.525656: step 5550, loss = 0.96 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 21:52:38.473393: step 5560, loss = 0.95 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 21:52:42.427402: step 5570, loss = 1.04 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 21:52:46.565515: step 5580, loss = 0.86 (309.3 examples/sec; 0.414 sec/batch)
2017-04-02 21:52:50.543947: step 5590, loss = 0.99 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 21:52:54.599373: step 5600, loss = 1.23 (315.6 examples/sec; 0.406 sec/batch)
2017-04-02 21:52:58.535255: step 5610, loss = 0.87 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 21:53:02.476324: step 5620, loss = 1.21 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 21:53:06.505686: step 5630, loss = 1.27 (317.7 examples/sec; 0.403 sec/batch)
2017-04-02 21:53:10.497303: step 5640, loss = 0.99 (320.7 examples/sec; 0.399 sec/batch)
2017-04-02 21:53:14.414115: step 5650, loss = 0.96 (326.8 examples/sec; 0.392 sec/batch)
2017-04-02 21:53:18.369569: step 5660, loss = 0.92 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 21:53:22.330528: step 5670, loss = 0.94 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 21:53:26.374444: step 5680, loss = 1.04 (316.5 examples/sec; 0.404 sec/batch)
2017-04-02 21:53:30.334818: step 5690, loss = 0.80 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 21:53:34.375290: step 5700, loss = 0.99 (316.8 examples/sec; 0.404 sec/batch)
2017-04-02 21:53:38.271358: step 5710, loss = 0.87 (328.5 examples/sec; 0.390 sec/batch)
2017-04-02 21:53:42.197401: step 5720, loss = 1.03 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 21:53:46.124549: step 5730, loss = 0.98 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 21:53:50.081870: step 5740, loss = 1.00 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 21:53:54.041805: step 5750, loss = 1.08 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 21:53:58.028809: step 5760, loss = 0.99 (321.0 examples/sec; 0.399 sec/batch)
2017-04-02 21:54:02.030445: step 5770, loss = 1.05 (319.9 examples/sec; 0.400 sec/batch)
2017-04-02 21:54:06.020454: step 5780, loss = 0.99 (320.8 examples/sec; 0.399 sec/batch)
2017-04-02 21:54:09.930297: step 5790, loss = 1.08 (327.4 examples/sec; 0.391 sec/batch)
2017-04-02 21:54:14.000243: step 5800, loss = 1.03 (314.5 examples/sec; 0.407 sec/batch)
2017-04-02 21:54:17.981726: step 5810, loss = 0.95 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 21:54:21.977149: step 5820, loss = 0.81 (320.4 examples/sec; 0.400 sec/batch)
2017-04-02 21:54:26.046150: step 5830, loss = 0.89 (314.6 examples/sec; 0.407 sec/batch)
2017-04-02 21:54:30.040155: step 5840, loss = 1.13 (320.5 examples/sec; 0.399 sec/batch)
2017-04-02 21:54:34.017725: step 5850, loss = 0.95 (321.8 examples/sec; 0.398 sec/batch)
2017-04-02 21:54:38.010142: step 5860, loss = 1.04 (320.6 examples/sec; 0.399 sec/batch)
2017-04-02 21:54:42.049756: step 5870, loss = 1.16 (316.9 examples/sec; 0.404 sec/batch)
2017-04-02 21:54:46.028388: step 5880, loss = 1.01 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 21:54:50.032863: step 5890, loss = 0.96 (319.6 examples/sec; 0.400 sec/batch)
2017-04-02 21:54:54.084141: step 5900, loss = 0.86 (315.9 examples/sec; 0.405 sec/batch)
2017-04-02 21:54:58.041258: step 5910, loss = 0.90 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 21:55:01.964287: step 5920, loss = 0.95 (326.3 examples/sec; 0.392 sec/batch)
2017-04-02 21:55:05.922843: step 5930, loss = 0.99 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 21:55:09.908459: step 5940, loss = 0.90 (321.2 examples/sec; 0.399 sec/batch)
2017-04-02 21:55:13.907498: step 5950, loss = 0.96 (320.1 examples/sec; 0.400 sec/batch)
2017-04-02 21:55:17.894560: step 5960, loss = 1.06 (321.0 examples/sec; 0.399 sec/batch)
2017-04-02 21:55:21.912629: step 5970, loss = 0.82 (318.6 examples/sec; 0.402 sec/batch)
2017-04-02 21:55:25.921386: step 5980, loss = 0.90 (319.3 examples/sec; 0.401 sec/batch)
2017-04-02 21:55:29.883127: step 5990, loss = 0.99 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 21:55:33.934350: step 6000, loss = 0.99 (316.0 examples/sec; 0.405 sec/batch)
2017-04-02 21:55:37.925259: step 6010, loss = 0.97 (320.7 examples/sec; 0.399 sec/batch)
2017-04-02 21:55:41.965717: step 6020, loss = 0.86 (316.8 examples/sec; 0.404 sec/batch)
2017-04-02 21:58:42.301668: step 6030, loss = 1.16 (7.1 examples/sec; 18.034 sec/batch)
2017-04-02 21:58:46.199870: step 6040, loss = 0.99 (328.4 examples/sec; 0.390 sec/batch)
2017-04-02 21:58:50.178161: step 6050, loss = 1.03 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 21:58:54.146634: step 6060, loss = 0.72 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 21:58:58.098203: step 6070, loss = 1.07 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 21:59:02.250244: step 6080, loss = 0.93 (308.3 examples/sec; 0.415 sec/batch)
2017-04-02 21:59:06.311702: step 6090, loss = 0.85 (315.2 examples/sec; 0.406 sec/batch)
2017-04-02 21:59:10.386488: step 6100, loss = 1.05 (314.1 examples/sec; 0.407 sec/batch)
2017-04-02 21:59:14.444958: step 6110, loss = 1.01 (315.4 examples/sec; 0.406 sec/batch)
2017-04-02 21:59:18.433986: step 6120, loss = 0.91 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 21:59:23.125911: step 6130, loss = 1.04 (272.8 examples/sec; 0.469 sec/batch)
2017-04-02 21:59:27.144560: step 6140, loss = 1.04 (318.5 examples/sec; 0.402 sec/batch)
2017-04-02 21:59:31.266659: step 6150, loss = 0.95 (310.5 examples/sec; 0.412 sec/batch)
2017-04-02 21:59:35.287065: step 6160, loss = 1.06 (318.4 examples/sec; 0.402 sec/batch)
2017-04-02 21:59:39.277880: step 6170, loss = 1.12 (320.7 examples/sec; 0.399 sec/batch)
2017-04-02 21:59:43.313268: step 6180, loss = 0.82 (317.2 examples/sec; 0.404 sec/batch)
2017-04-02 21:59:47.377764: step 6190, loss = 0.80 (314.9 examples/sec; 0.406 sec/batch)
2017-04-02 21:59:51.530331: step 6200, loss = 1.11 (308.2 examples/sec; 0.415 sec/batch)
2017-04-02 21:59:55.623867: step 6210, loss = 0.84 (312.7 examples/sec; 0.409 sec/batch)
2017-04-02 21:59:59.675064: step 6220, loss = 0.87 (316.0 examples/sec; 0.405 sec/batch)
2017-04-02 22:00:03.719714: step 6230, loss = 0.94 (316.5 examples/sec; 0.404 sec/batch)
2017-04-02 22:00:07.738678: step 6240, loss = 0.94 (318.5 examples/sec; 0.402 sec/batch)
2017-04-02 22:00:11.739874: step 6250, loss = 0.99 (319.9 examples/sec; 0.400 sec/batch)
2017-04-02 22:00:15.790010: step 6260, loss = 0.79 (316.0 examples/sec; 0.405 sec/batch)
2017-04-02 22:00:19.847576: step 6270, loss = 0.97 (315.5 examples/sec; 0.406 sec/batch)
2017-04-02 22:00:23.812740: step 6280, loss = 0.97 (322.8 examples/sec; 0.397 sec/batch)
2017-04-02 22:00:27.833291: step 6290, loss = 0.91 (318.4 examples/sec; 0.402 sec/batch)
2017-04-02 22:00:31.961141: step 6300, loss = 1.00 (310.1 examples/sec; 0.413 sec/batch)
2017-04-02 22:00:36.052732: step 6310, loss = 1.00 (312.8 examples/sec; 0.409 sec/batch)
2017-04-02 22:00:40.109912: step 6320, loss = 0.99 (315.5 examples/sec; 0.406 sec/batch)
2017-04-02 22:00:44.121408: step 6330, loss = 0.82 (319.1 examples/sec; 0.401 sec/batch)
2017-04-02 22:00:48.186662: step 6340, loss = 0.92 (314.9 examples/sec; 0.407 sec/batch)
2017-04-02 22:00:52.223312: step 6350, loss = 1.06 (317.1 examples/sec; 0.404 sec/batch)
2017-04-02 22:00:56.271330: step 6360, loss = 0.93 (316.2 examples/sec; 0.405 sec/batch)
2017-04-02 22:01:00.268320: step 6370, loss = 0.87 (320.2 examples/sec; 0.400 sec/batch)
2017-04-02 22:01:04.364793: step 6380, loss = 1.08 (312.5 examples/sec; 0.410 sec/batch)
2017-04-02 22:01:08.371446: step 6390, loss = 1.13 (319.5 examples/sec; 0.401 sec/batch)
2017-04-02 22:01:12.491522: step 6400, loss = 0.78 (310.7 examples/sec; 0.412 sec/batch)
2017-04-02 22:01:16.574422: step 6410, loss = 1.17 (313.5 examples/sec; 0.408 sec/batch)
2017-04-02 22:01:20.712624: step 6420, loss = 0.97 (309.3 examples/sec; 0.414 sec/batch)
2017-04-02 22:01:24.864388: step 6430, loss = 0.91 (308.3 examples/sec; 0.415 sec/batch)
2017-04-02 22:01:28.777714: step 6440, loss = 0.97 (327.1 examples/sec; 0.391 sec/batch)
2017-04-02 22:01:32.689209: step 6450, loss = 0.88 (327.2 examples/sec; 0.391 sec/batch)
2017-04-02 22:01:36.635647: step 6460, loss = 0.84 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 22:01:40.560367: step 6470, loss = 1.21 (326.1 examples/sec; 0.392 sec/batch)
2017-04-02 22:01:44.501923: step 6480, loss = 0.90 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 22:01:48.431984: step 6490, loss = 1.00 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 22:01:52.539266: step 6500, loss = 0.91 (311.6 examples/sec; 0.411 sec/batch)
2017-04-02 22:01:56.489925: step 6510, loss = 0.88 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 22:02:00.409594: step 6520, loss = 1.01 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 22:02:04.350774: step 6530, loss = 0.92 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 22:02:08.301815: step 6540, loss = 0.92 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 22:02:12.260377: step 6550, loss = 0.95 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 22:02:16.193445: step 6560, loss = 0.97 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 22:02:20.182097: step 6570, loss = 1.06 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 22:02:24.129361: step 6580, loss = 1.17 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 22:02:28.065300: step 6590, loss = 0.95 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 22:02:32.066491: step 6600, loss = 0.92 (319.9 examples/sec; 0.400 sec/batch)
2017-04-02 22:02:36.091740: step 6610, loss = 0.96 (318.0 examples/sec; 0.403 sec/batch)
2017-04-02 22:02:40.131997: step 6620, loss = 1.00 (316.8 examples/sec; 0.404 sec/batch)
2017-04-02 22:02:44.087982: step 6630, loss = 0.99 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 22:02:48.057887: step 6640, loss = 1.06 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 22:02:51.967632: step 6650, loss = 0.96 (327.4 examples/sec; 0.391 sec/batch)
2017-04-02 22:02:55.924326: step 6660, loss = 0.84 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:02:59.902053: step 6670, loss = 0.98 (321.8 examples/sec; 0.398 sec/batch)
2017-04-02 22:03:03.872112: step 6680, loss = 1.08 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 22:03:07.807317: step 6690, loss = 0.85 (325.3 examples/sec; 0.394 sec/batch)
2017-04-02 22:03:11.788867: step 6700, loss = 1.02 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 22:03:15.711055: step 6710, loss = 0.83 (326.3 examples/sec; 0.392 sec/batch)
2017-04-02 22:03:19.656919: step 6720, loss = 0.85 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 22:03:23.555513: step 6730, loss = 0.94 (328.3 examples/sec; 0.390 sec/batch)
2017-04-02 22:03:27.474523: step 6740, loss = 0.70 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 22:03:31.430127: step 6750, loss = 0.93 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 22:03:35.334655: step 6760, loss = 1.19 (327.8 examples/sec; 0.390 sec/batch)
2017-04-02 22:03:39.268954: step 6770, loss = 1.05 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 22:03:43.182168: step 6780, loss = 0.91 (327.1 examples/sec; 0.391 sec/batch)
2017-04-02 22:03:47.135063: step 6790, loss = 0.93 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 22:03:51.151128: step 6800, loss = 0.92 (318.7 examples/sec; 0.402 sec/batch)
2017-04-02 22:03:55.114270: step 6810, loss = 0.85 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 22:03:59.039265: step 6820, loss = 1.14 (326.1 examples/sec; 0.392 sec/batch)
2017-04-02 22:04:02.971733: step 6830, loss = 0.90 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 22:04:06.891033: step 6840, loss = 0.92 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 22:04:10.876073: step 6850, loss = 0.82 (321.2 examples/sec; 0.399 sec/batch)
2017-04-02 22:04:14.842930: step 6860, loss = 0.96 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 22:04:18.806009: step 6870, loss = 0.99 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 22:04:22.786754: step 6880, loss = 0.97 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 22:04:26.732850: step 6890, loss = 0.83 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 22:04:30.719944: step 6900, loss = 0.81 (321.0 examples/sec; 0.399 sec/batch)
2017-04-02 22:04:34.707543: step 6910, loss = 0.88 (321.0 examples/sec; 0.399 sec/batch)
2017-04-02 22:04:38.656695: step 6920, loss = 0.99 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 22:04:42.604284: step 6930, loss = 0.93 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 22:04:46.551269: step 6940, loss = 0.89 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 22:04:50.532056: step 6950, loss = 0.92 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 22:04:54.470294: step 6960, loss = 0.92 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 22:04:58.436955: step 6970, loss = 1.07 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 22:05:02.401813: step 6980, loss = 0.98 (322.8 examples/sec; 0.396 sec/batch)
2017-04-02 22:05:06.334097: step 6990, loss = 0.92 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 22:05:10.327998: step 7000, loss = 0.87 (320.5 examples/sec; 0.399 sec/batch)
2017-04-02 22:05:14.234915: step 7010, loss = 0.99 (327.6 examples/sec; 0.391 sec/batch)
2017-04-02 22:05:18.160745: step 7020, loss = 1.02 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 22:05:22.118574: step 7030, loss = 0.94 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 22:05:26.086487: step 7040, loss = 0.99 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 22:05:30.032391: step 7050, loss = 1.00 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 22:05:33.969573: step 7060, loss = 0.82 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 22:05:37.930506: step 7070, loss = 0.78 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 22:05:41.878555: step 7080, loss = 0.92 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 22:05:45.867672: step 7090, loss = 0.98 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 22:05:49.877407: step 7100, loss = 0.84 (319.2 examples/sec; 0.401 sec/batch)
2017-04-02 22:05:53.880647: step 7110, loss = 0.93 (319.7 examples/sec; 0.400 sec/batch)
2017-04-02 22:05:57.849550: step 7120, loss = 0.89 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 22:06:01.860935: step 7130, loss = 1.05 (319.1 examples/sec; 0.401 sec/batch)
2017-04-02 22:06:05.840480: step 7140, loss = 0.80 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 22:06:09.867422: step 7150, loss = 1.05 (317.9 examples/sec; 0.403 sec/batch)
2017-04-02 22:06:13.841464: step 7160, loss = 1.00 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 22:06:17.841650: step 7170, loss = 0.72 (320.0 examples/sec; 0.400 sec/batch)
2017-04-02 22:06:21.803455: step 7180, loss = 0.92 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 22:06:25.739938: step 7190, loss = 0.86 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 22:06:29.752453: step 7200, loss = 0.77 (319.0 examples/sec; 0.401 sec/batch)
2017-04-02 22:06:33.729374: step 7210, loss = 0.80 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 22:06:37.692589: step 7220, loss = 0.74 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 22:06:41.625490: step 7230, loss = 1.04 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 22:06:45.552008: step 7240, loss = 1.05 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 22:06:49.501687: step 7250, loss = 0.96 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 22:06:53.435384: step 7260, loss = 0.85 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 22:06:57.359162: step 7270, loss = 1.04 (326.2 examples/sec; 0.392 sec/batch)
2017-04-02 22:07:01.292355: step 7280, loss = 0.91 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 22:07:05.218152: step 7290, loss = 0.85 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 22:07:09.211082: step 7300, loss = 0.96 (320.6 examples/sec; 0.399 sec/batch)
2017-04-02 22:07:13.129681: step 7310, loss = 0.82 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 22:07:17.075482: step 7320, loss = 0.88 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 22:07:21.034262: step 7330, loss = 1.03 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 22:07:24.924817: step 7340, loss = 0.95 (329.0 examples/sec; 0.389 sec/batch)
2017-04-02 22:07:28.835272: step 7350, loss = 0.90 (327.3 examples/sec; 0.391 sec/batch)
2017-04-02 22:07:32.795648: step 7360, loss = 1.16 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 22:07:36.738293: step 7370, loss = 1.04 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 22:07:40.682544: step 7380, loss = 0.99 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 22:07:44.858205: step 7390, loss = 0.83 (306.5 examples/sec; 0.418 sec/batch)
2017-04-02 22:07:48.846519: step 7400, loss = 0.98 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 22:07:52.812586: step 7410, loss = 0.92 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 22:07:56.750440: step 7420, loss = 0.80 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 22:08:00.743449: step 7430, loss = 1.11 (320.6 examples/sec; 0.399 sec/batch)
2017-04-02 22:08:04.679253: step 7440, loss = 0.89 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 22:08:08.622516: step 7450, loss = 0.85 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 22:08:12.588975: step 7460, loss = 1.04 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 22:08:16.506882: step 7470, loss = 0.94 (326.7 examples/sec; 0.392 sec/batch)
2017-04-02 22:08:20.469488: step 7480, loss = 0.95 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 22:08:24.437484: step 7490, loss = 0.91 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 22:08:28.427005: step 7500, loss = 0.98 (320.8 examples/sec; 0.399 sec/batch)
2017-04-02 22:08:32.351417: step 7510, loss = 0.88 (326.2 examples/sec; 0.392 sec/batch)
2017-04-02 22:08:36.304391: step 7520, loss = 1.05 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 22:08:40.210190: step 7530, loss = 0.99 (327.7 examples/sec; 0.391 sec/batch)
2017-04-02 22:08:44.123375: step 7540, loss = 0.88 (327.1 examples/sec; 0.391 sec/batch)
2017-04-02 22:08:48.129410: step 7550, loss = 0.78 (319.5 examples/sec; 0.401 sec/batch)
2017-04-02 22:08:52.105342: step 7560, loss = 0.73 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 22:08:56.029778: step 7570, loss = 0.94 (326.2 examples/sec; 0.392 sec/batch)
2017-04-02 22:09:00.029252: step 7580, loss = 0.99 (320.0 examples/sec; 0.400 sec/batch)
2017-04-02 22:09:04.113052: step 7590, loss = 0.87 (313.4 examples/sec; 0.408 sec/batch)
2017-04-02 22:09:08.140499: step 7600, loss = 0.99 (317.8 examples/sec; 0.403 sec/batch)
2017-04-02 22:09:12.043486: step 7610, loss = 0.80 (328.0 examples/sec; 0.390 sec/batch)
2017-04-02 22:09:15.923234: step 7620, loss = 1.06 (329.9 examples/sec; 0.388 sec/batch)
2017-04-02 22:09:19.806222: step 7630, loss = 0.99 (329.6 examples/sec; 0.388 sec/batch)
2017-04-02 22:09:24.755869: step 7640, loss = 1.03 (258.6 examples/sec; 0.495 sec/batch)
2017-04-02 22:09:28.714289: step 7650, loss = 0.89 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 22:09:32.642068: step 7660, loss = 0.92 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 22:09:36.627021: step 7670, loss = 0.98 (321.2 examples/sec; 0.398 sec/batch)
2017-04-02 22:09:40.600831: step 7680, loss = 0.91 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 22:09:44.597279: step 7690, loss = 0.86 (320.3 examples/sec; 0.400 sec/batch)
2017-04-02 22:09:48.625610: step 7700, loss = 0.93 (317.7 examples/sec; 0.403 sec/batch)
2017-04-02 22:09:52.593692: step 7710, loss = 0.85 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 22:09:56.544528: step 7720, loss = 1.06 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 22:10:00.498221: step 7730, loss = 0.85 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 22:10:04.429937: step 7740, loss = 1.07 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 22:10:08.337940: step 7750, loss = 0.82 (327.5 examples/sec; 0.391 sec/batch)
2017-04-02 22:10:12.287181: step 7760, loss = 0.90 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 22:10:16.204726: step 7770, loss = 0.97 (326.7 examples/sec; 0.392 sec/batch)
2017-04-02 22:10:20.184762: step 7780, loss = 1.11 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 22:10:24.157490: step 7790, loss = 0.90 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 22:10:28.178757: step 7800, loss = 0.73 (318.3 examples/sec; 0.402 sec/batch)
2017-04-02 22:10:32.083243: step 7810, loss = 0.92 (327.8 examples/sec; 0.390 sec/batch)
2017-04-02 22:10:36.018527: step 7820, loss = 0.99 (325.3 examples/sec; 0.394 sec/batch)
2017-04-02 22:10:40.053606: step 7830, loss = 0.97 (317.2 examples/sec; 0.404 sec/batch)
2017-04-02 22:10:44.010613: step 7840, loss = 0.74 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:10:47.900774: step 7850, loss = 0.85 (329.0 examples/sec; 0.389 sec/batch)
2017-04-02 22:10:51.821691: step 7860, loss = 0.92 (326.5 examples/sec; 0.392 sec/batch)
2017-04-02 22:10:55.726076: step 7870, loss = 0.96 (327.8 examples/sec; 0.390 sec/batch)
2017-04-02 22:10:59.663533: step 7880, loss = 0.84 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 22:11:03.585093: step 7890, loss = 1.00 (326.4 examples/sec; 0.392 sec/batch)
2017-04-02 22:11:07.561447: step 7900, loss = 1.02 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 22:11:11.488068: step 7910, loss = 0.94 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 22:11:15.423138: step 7920, loss = 0.86 (325.3 examples/sec; 0.394 sec/batch)
2017-04-02 22:11:19.382399: step 7930, loss = 1.10 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 22:11:23.342741: step 7940, loss = 0.92 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 22:11:27.280439: step 7950, loss = 0.92 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 22:11:31.248337: step 7960, loss = 0.88 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 22:11:35.184659: step 7970, loss = 0.89 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 22:11:39.120370: step 7980, loss = 0.94 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 22:11:43.065616: step 7990, loss = 0.83 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 22:11:47.101962: step 8000, loss = 0.86 (317.1 examples/sec; 0.404 sec/batch)
2017-04-02 22:11:51.082041: step 8010, loss = 0.90 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 22:13:49.822906: step 8020, loss = 1.01 (10.8 examples/sec; 11.874 sec/batch)
2017-04-02 22:13:53.806666: step 8030, loss = 0.84 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 22:13:57.792111: step 8040, loss = 0.90 (321.2 examples/sec; 0.399 sec/batch)
2017-04-02 22:14:01.726629: step 8050, loss = 0.73 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 22:14:05.677251: step 8060, loss = 0.95 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 22:14:09.696945: step 8070, loss = 0.96 (318.4 examples/sec; 0.402 sec/batch)
2017-04-02 22:14:13.704591: step 8080, loss = 1.06 (319.4 examples/sec; 0.401 sec/batch)
2017-04-02 22:14:17.794728: step 8090, loss = 0.92 (312.9 examples/sec; 0.409 sec/batch)
2017-04-02 22:14:21.807431: step 8100, loss = 0.85 (319.0 examples/sec; 0.401 sec/batch)
2017-04-02 22:14:25.986295: step 8110, loss = 0.84 (306.3 examples/sec; 0.418 sec/batch)
2017-04-02 22:14:29.947302: step 8120, loss = 0.93 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 22:14:33.937080: step 8130, loss = 0.88 (320.8 examples/sec; 0.399 sec/batch)
2017-04-02 22:14:37.894343: step 8140, loss = 0.74 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:14:41.891168: step 8150, loss = 0.86 (320.3 examples/sec; 0.400 sec/batch)
2017-04-02 22:14:45.826783: step 8160, loss = 1.10 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 22:14:49.801812: step 8170, loss = 1.12 (322.0 examples/sec; 0.398 sec/batch)
2017-04-02 22:14:53.718828: step 8180, loss = 0.94 (326.8 examples/sec; 0.392 sec/batch)
2017-04-02 22:14:57.686680: step 8190, loss = 0.95 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 22:15:01.718196: step 8200, loss = 0.81 (317.5 examples/sec; 0.403 sec/batch)
2017-04-02 22:15:05.704372: step 8210, loss = 0.81 (321.1 examples/sec; 0.399 sec/batch)
2017-04-02 22:15:09.661445: step 8220, loss = 1.07 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:15:13.617581: step 8230, loss = 0.83 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:15:17.613091: step 8240, loss = 1.00 (320.4 examples/sec; 0.400 sec/batch)
2017-04-02 22:15:21.626405: step 8250, loss = 0.93 (318.9 examples/sec; 0.401 sec/batch)
2017-04-02 22:15:25.634314: step 8260, loss = 1.01 (319.4 examples/sec; 0.401 sec/batch)
2017-04-02 22:15:29.633378: step 8270, loss = 0.88 (320.1 examples/sec; 0.400 sec/batch)
2017-04-02 22:15:33.577931: step 8280, loss = 0.83 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 22:15:37.497490: step 8290, loss = 0.98 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 22:15:41.582269: step 8300, loss = 0.92 (313.4 examples/sec; 0.408 sec/batch)
2017-04-02 22:15:45.598747: step 8310, loss = 0.81 (318.7 examples/sec; 0.402 sec/batch)
2017-04-02 22:15:49.546081: step 8320, loss = 0.76 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 22:15:53.542869: step 8330, loss = 0.91 (320.3 examples/sec; 0.400 sec/batch)
2017-04-02 22:15:57.601902: step 8340, loss = 0.92 (315.3 examples/sec; 0.406 sec/batch)
2017-04-02 22:16:02.033414: step 8350, loss = 0.88 (288.8 examples/sec; 0.443 sec/batch)
2017-04-02 22:16:06.071831: step 8360, loss = 0.93 (317.0 examples/sec; 0.404 sec/batch)
2017-04-02 22:16:10.071775: step 8370, loss = 1.05 (320.0 examples/sec; 0.400 sec/batch)
2017-04-02 22:16:14.114643: step 8380, loss = 0.82 (316.6 examples/sec; 0.404 sec/batch)
2017-04-02 22:16:18.191139: step 8390, loss = 0.68 (314.0 examples/sec; 0.408 sec/batch)
2017-04-02 22:16:22.277586: step 8400, loss = 0.84 (313.2 examples/sec; 0.409 sec/batch)
2017-04-02 22:16:26.362182: step 8410, loss = 0.98 (313.4 examples/sec; 0.408 sec/batch)
2017-04-02 22:16:30.576732: step 8420, loss = 0.87 (303.7 examples/sec; 0.421 sec/batch)
2017-04-02 22:16:34.707600: step 8430, loss = 1.03 (309.9 examples/sec; 0.413 sec/batch)
2017-04-02 22:16:38.828012: step 8440, loss = 0.86 (310.6 examples/sec; 0.412 sec/batch)
2017-04-02 22:16:42.846873: step 8450, loss = 0.77 (318.5 examples/sec; 0.402 sec/batch)
2017-04-02 22:16:46.842686: step 8460, loss = 0.97 (320.3 examples/sec; 0.400 sec/batch)
2017-04-02 22:16:50.828472: step 8470, loss = 0.93 (321.1 examples/sec; 0.399 sec/batch)
2017-04-02 22:16:54.824417: step 8480, loss = 0.89 (320.3 examples/sec; 0.400 sec/batch)
2017-04-02 22:16:58.756773: step 8490, loss = 0.83 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 22:17:02.812888: step 8500, loss = 1.09 (315.6 examples/sec; 0.406 sec/batch)
2017-04-02 22:17:06.765203: step 8510, loss = 0.89 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 22:17:10.733260: step 8520, loss = 0.88 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 22:17:14.728244: step 8530, loss = 0.91 (320.4 examples/sec; 0.399 sec/batch)
2017-04-02 22:17:18.700930: step 8540, loss = 0.98 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 22:17:22.701877: step 8550, loss = 0.96 (319.9 examples/sec; 0.400 sec/batch)
2017-04-02 22:17:26.719152: step 8560, loss = 0.97 (318.6 examples/sec; 0.402 sec/batch)
2017-04-02 22:17:30.762308: step 8570, loss = 0.92 (316.6 examples/sec; 0.404 sec/batch)
2017-04-02 22:17:34.751631: step 8580, loss = 0.77 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 22:17:38.699494: step 8590, loss = 0.88 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 22:17:42.711132: step 8600, loss = 0.93 (319.1 examples/sec; 0.401 sec/batch)
2017-04-02 22:17:46.689935: step 8610, loss = 0.87 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 22:17:50.642919: step 8620, loss = 0.92 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 22:17:54.587134: step 8630, loss = 0.78 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 22:17:58.590060: step 8640, loss = 0.79 (319.8 examples/sec; 0.400 sec/batch)
2017-04-02 22:18:02.561251: step 8650, loss = 0.87 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 22:18:06.548081: step 8660, loss = 1.03 (321.1 examples/sec; 0.399 sec/batch)
2017-04-02 22:18:10.508267: step 8670, loss = 0.99 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 22:18:14.408935: step 8680, loss = 0.89 (328.1 examples/sec; 0.390 sec/batch)
2017-04-02 22:18:18.324670: step 8690, loss = 1.07 (326.9 examples/sec; 0.392 sec/batch)
2017-04-02 22:18:22.344144: step 8700, loss = 0.84 (318.4 examples/sec; 0.402 sec/batch)
2017-04-02 22:18:26.318939: step 8710, loss = 0.87 (322.0 examples/sec; 0.397 sec/batch)
2017-04-02 22:18:30.250104: step 8720, loss = 0.96 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 22:18:34.244105: step 8730, loss = 0.93 (320.5 examples/sec; 0.399 sec/batch)
2017-04-02 22:18:38.230744: step 8740, loss = 0.81 (321.1 examples/sec; 0.399 sec/batch)
2017-04-02 22:18:42.207443: step 8750, loss = 0.87 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 22:18:46.128471: step 8760, loss = 0.93 (326.4 examples/sec; 0.392 sec/batch)
2017-04-02 22:18:50.234673: step 8770, loss = 0.96 (311.7 examples/sec; 0.411 sec/batch)
2017-04-02 22:18:54.240517: step 8780, loss = 0.93 (319.5 examples/sec; 0.401 sec/batch)
2017-04-02 22:18:58.198284: step 8790, loss = 0.89 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 22:19:02.238231: step 8800, loss = 0.85 (316.8 examples/sec; 0.404 sec/batch)
2017-04-02 22:19:06.196472: step 8810, loss = 0.77 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 22:19:10.158172: step 8820, loss = 1.08 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 22:19:14.126083: step 8830, loss = 0.85 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 22:19:18.126445: step 8840, loss = 1.07 (320.0 examples/sec; 0.400 sec/batch)
2017-04-02 22:19:22.075525: step 8850, loss = 0.89 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 22:19:26.806092: step 8860, loss = 0.97 (270.6 examples/sec; 0.473 sec/batch)
2017-04-02 22:19:30.776803: step 8870, loss = 1.07 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 22:19:34.730747: step 8880, loss = 0.82 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 22:19:38.687135: step 8890, loss = 1.01 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:19:42.729755: step 8900, loss = 0.91 (316.6 examples/sec; 0.404 sec/batch)
2017-04-02 22:19:46.712805: step 8910, loss = 0.77 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 22:19:50.695319: step 8920, loss = 1.01 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 22:19:54.666542: step 8930, loss = 0.79 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 22:19:58.684687: step 8940, loss = 0.78 (318.6 examples/sec; 0.402 sec/batch)
2017-04-02 22:20:02.658629: step 8950, loss = 0.94 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 22:20:06.623353: step 8960, loss = 0.91 (322.8 examples/sec; 0.396 sec/batch)
2017-04-02 22:20:10.555596: step 8970, loss = 0.72 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 22:20:14.464932: step 8980, loss = 1.03 (327.4 examples/sec; 0.391 sec/batch)
2017-04-02 22:20:18.482718: step 8990, loss = 0.69 (318.6 examples/sec; 0.402 sec/batch)
2017-04-02 22:20:22.555886: step 9000, loss = 0.97 (314.3 examples/sec; 0.407 sec/batch)
2017-04-02 22:20:26.526929: step 9010, loss = 0.81 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 22:20:30.457218: step 9020, loss = 0.89 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 22:20:34.384313: step 9030, loss = 0.81 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 22:20:38.317896: step 9040, loss = 0.92 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 22:20:42.248032: step 9050, loss = 0.87 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 22:20:46.174422: step 9060, loss = 0.85 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 22:20:50.099761: step 9070, loss = 0.87 (326.1 examples/sec; 0.393 sec/batch)
2017-04-02 22:20:54.052811: step 9080, loss = 0.80 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 22:20:57.992154: step 9090, loss = 0.93 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 22:21:02.051496: step 9100, loss = 0.82 (315.3 examples/sec; 0.406 sec/batch)
2017-04-02 22:21:06.040681: step 9110, loss = 1.08 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 22:21:10.001550: step 9120, loss = 0.96 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 22:21:13.942334: step 9130, loss = 1.00 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 22:21:17.904550: step 9140, loss = 0.89 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 22:21:21.841887: step 9150, loss = 0.92 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 22:21:25.823120: step 9160, loss = 0.89 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 22:21:29.826866: step 9170, loss = 0.87 (319.7 examples/sec; 0.400 sec/batch)
2017-04-02 22:21:33.796334: step 9180, loss = 0.88 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 22:21:37.749348: step 9190, loss = 1.02 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 22:21:41.753896: step 9200, loss = 0.92 (319.6 examples/sec; 0.400 sec/batch)
2017-04-02 22:21:45.697464: step 9210, loss = 1.00 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 22:21:49.651120: step 9220, loss = 0.87 (323.8 examples/sec; 0.395 sec/batch)
2017-04-02 22:21:53.598647: step 9230, loss = 0.92 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 22:21:57.611547: step 9240, loss = 0.83 (319.0 examples/sec; 0.401 sec/batch)
2017-04-02 22:22:01.584995: step 9250, loss = 1.04 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 22:22:05.573882: step 9260, loss = 0.84 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 22:22:09.532501: step 9270, loss = 0.92 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 22:22:13.476928: step 9280, loss = 0.95 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 22:22:17.446837: step 9290, loss = 0.93 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 22:22:21.465715: step 9300, loss = 0.79 (318.5 examples/sec; 0.402 sec/batch)
2017-04-02 22:22:25.458004: step 9310, loss = 0.87 (320.6 examples/sec; 0.399 sec/batch)
2017-04-02 22:22:29.431596: step 9320, loss = 0.98 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 22:22:33.373825: step 9330, loss = 1.06 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 22:22:37.314542: step 9340, loss = 1.05 (324.8 examples/sec; 0.394 sec/batch)
2017-04-02 22:22:41.420008: step 9350, loss = 0.88 (311.8 examples/sec; 0.411 sec/batch)
2017-04-02 22:22:45.423472: step 9360, loss = 1.02 (319.7 examples/sec; 0.400 sec/batch)
2017-04-02 22:22:49.357042: step 9370, loss = 0.75 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 22:22:53.349950: step 9380, loss = 0.94 (320.6 examples/sec; 0.399 sec/batch)
2017-04-02 22:22:57.287342: step 9390, loss = 0.93 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 22:23:01.268667: step 9400, loss = 0.85 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 22:23:05.184629: step 9410, loss = 1.05 (326.9 examples/sec; 0.392 sec/batch)
2017-04-02 22:23:09.098543: step 9420, loss = 0.84 (327.0 examples/sec; 0.391 sec/batch)
2017-04-02 22:23:13.060359: step 9430, loss = 0.90 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 22:23:16.999096: step 9440, loss = 0.98 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 22:23:20.955223: step 9450, loss = 0.89 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:23:24.919156: step 9460, loss = 0.97 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 22:23:28.852392: step 9470, loss = 1.14 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 22:23:32.859788: step 9480, loss = 0.81 (319.4 examples/sec; 0.401 sec/batch)
2017-04-02 22:23:36.816323: step 9490, loss = 0.93 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:23:40.775436: step 9500, loss = 0.81 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 22:23:44.720592: step 9510, loss = 0.77 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 22:23:48.606386: step 9520, loss = 0.87 (329.4 examples/sec; 0.389 sec/batch)
2017-04-02 22:23:52.545698: step 9530, loss = 0.83 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 22:23:56.514176: step 9540, loss = 0.76 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 22:24:00.492618: step 9550, loss = 0.70 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 22:24:04.469198: step 9560, loss = 0.74 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 22:24:08.395748: step 9570, loss = 0.83 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 22:24:12.356189: step 9580, loss = 0.84 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 22:24:16.312986: step 9590, loss = 1.15 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:24:20.475642: step 9600, loss = 0.96 (307.5 examples/sec; 0.416 sec/batch)
2017-04-02 22:24:24.532749: step 9610, loss = 0.91 (315.5 examples/sec; 0.406 sec/batch)
2017-04-02 22:24:28.588538: step 9620, loss = 0.88 (315.6 examples/sec; 0.406 sec/batch)
2017-04-02 22:24:32.542814: step 9630, loss = 0.91 (323.7 examples/sec; 0.395 sec/batch)
2017-04-02 22:24:36.469358: step 9640, loss = 0.76 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 22:24:40.392080: step 9650, loss = 0.90 (326.3 examples/sec; 0.392 sec/batch)
2017-04-02 22:24:44.355883: step 9660, loss = 0.73 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 22:24:48.323316: step 9670, loss = 1.06 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 22:24:52.415276: step 9680, loss = 0.83 (312.8 examples/sec; 0.409 sec/batch)
2017-04-02 22:24:56.359107: step 9690, loss = 0.70 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 22:25:00.350898: step 9700, loss = 0.87 (320.7 examples/sec; 0.399 sec/batch)
2017-04-02 22:25:04.263217: step 9710, loss = 0.65 (327.2 examples/sec; 0.391 sec/batch)
2017-04-02 22:25:08.201447: step 9720, loss = 0.90 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 22:25:12.163215: step 9730, loss = 1.02 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 22:25:16.079722: step 9740, loss = 0.87 (326.8 examples/sec; 0.392 sec/batch)
2017-04-02 22:25:20.028716: step 9750, loss = 0.90 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 22:25:23.937447: step 9760, loss = 1.06 (327.5 examples/sec; 0.391 sec/batch)
2017-04-02 22:25:27.827352: step 9770, loss = 1.01 (329.1 examples/sec; 0.389 sec/batch)
2017-04-02 22:25:31.791978: step 9780, loss = 0.91 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 22:25:35.742258: step 9790, loss = 1.01 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 22:25:39.698615: step 9800, loss = 0.77 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:25:43.626404: step 9810, loss = 1.01 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 22:25:47.538892: step 9820, loss = 0.83 (327.2 examples/sec; 0.391 sec/batch)
2017-04-02 22:25:51.502257: step 9830, loss = 0.97 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 22:25:55.453110: step 9840, loss = 0.93 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 22:25:59.367366: step 9850, loss = 0.84 (327.0 examples/sec; 0.391 sec/batch)
2017-04-02 22:26:03.252589: step 9860, loss = 0.75 (329.5 examples/sec; 0.389 sec/batch)
2017-04-02 22:26:07.153588: step 9870, loss = 1.07 (328.1 examples/sec; 0.390 sec/batch)
2017-04-02 22:26:11.073236: step 9880, loss = 0.87 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 22:26:15.044359: step 9890, loss = 0.80 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 22:26:19.054926: step 9900, loss = 0.84 (319.2 examples/sec; 0.401 sec/batch)
2017-04-02 22:26:23.088965: step 9910, loss = 0.80 (317.3 examples/sec; 0.403 sec/batch)
2017-04-02 22:26:27.179344: step 9920, loss = 1.03 (312.9 examples/sec; 0.409 sec/batch)
2017-04-02 22:26:31.275213: step 9930, loss = 0.79 (312.5 examples/sec; 0.410 sec/batch)
2017-04-02 22:26:35.376154: step 9940, loss = 0.87 (312.1 examples/sec; 0.410 sec/batch)
2017-04-02 22:26:39.423732: step 9950, loss = 0.88 (316.2 examples/sec; 0.405 sec/batch)
2017-04-02 22:26:43.426419: step 9960, loss = 0.91 (319.8 examples/sec; 0.400 sec/batch)
2017-04-02 22:26:47.432187: step 9970, loss = 0.89 (319.5 examples/sec; 0.401 sec/batch)
2017-04-02 22:26:51.402270: step 9980, loss = 0.78 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 22:26:55.466810: step 9990, loss = 0.91 (314.9 examples/sec; 0.406 sec/batch)
2017-04-02 22:26:59.581641: step 10000, loss = 0.88 (311.1 examples/sec; 0.411 sec/batch)
2017-04-02 22:28:52.363848: step 10010, loss = 0.89 (11.3 examples/sec; 11.278 sec/batch)
2017-04-02 22:28:56.362595: step 10020, loss = 0.96 (320.1 examples/sec; 0.400 sec/batch)
2017-04-02 22:29:00.311381: step 10030, loss = 0.94 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 22:29:04.240884: step 10040, loss = 0.90 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 22:29:08.273880: step 10050, loss = 0.81 (317.4 examples/sec; 0.403 sec/batch)
2017-04-02 22:29:12.244880: step 10060, loss = 0.82 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 22:29:16.187837: step 10070, loss = 0.81 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 22:29:20.167136: step 10080, loss = 0.85 (321.7 examples/sec; 0.398 sec/batch)
2017-04-02 22:29:24.868982: step 10090, loss = 0.82 (272.2 examples/sec; 0.470 sec/batch)
2017-04-02 22:29:28.786288: step 10100, loss = 1.03 (326.8 examples/sec; 0.392 sec/batch)
2017-04-02 22:29:32.676297: step 10110, loss = 0.91 (329.0 examples/sec; 0.389 sec/batch)
2017-04-02 22:29:36.580906: step 10120, loss = 0.96 (327.8 examples/sec; 0.390 sec/batch)
2017-04-02 22:29:40.467849: step 10130, loss = 0.89 (329.3 examples/sec; 0.389 sec/batch)
2017-04-02 22:29:44.348837: step 10140, loss = 0.89 (329.8 examples/sec; 0.388 sec/batch)
2017-04-02 22:29:48.276914: step 10150, loss = 1.13 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 22:29:52.209353: step 10160, loss = 0.81 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 22:29:56.096143: step 10170, loss = 0.92 (329.3 examples/sec; 0.389 sec/batch)
2017-04-02 22:30:00.056703: step 10180, loss = 0.85 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 22:30:04.000640: step 10190, loss = 0.82 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 22:30:08.013113: step 10200, loss = 0.85 (319.0 examples/sec; 0.401 sec/batch)
2017-04-02 22:30:11.935154: step 10210, loss = 1.01 (326.4 examples/sec; 0.392 sec/batch)
2017-04-02 22:30:15.906860: step 10220, loss = 1.03 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 22:30:19.874503: step 10230, loss = 0.79 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 22:30:23.900558: step 10240, loss = 0.96 (317.9 examples/sec; 0.403 sec/batch)
2017-04-02 22:30:27.788552: step 10250, loss = 0.76 (329.2 examples/sec; 0.389 sec/batch)
2017-04-02 22:30:31.694890: step 10260, loss = 0.85 (327.7 examples/sec; 0.391 sec/batch)
2017-04-02 22:30:35.582496: step 10270, loss = 0.95 (329.3 examples/sec; 0.389 sec/batch)
2017-04-02 22:30:39.479151: step 10280, loss = 0.89 (328.5 examples/sec; 0.390 sec/batch)
2017-04-02 22:30:43.398465: step 10290, loss = 0.89 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 22:30:47.363161: step 10300, loss = 0.93 (322.8 examples/sec; 0.396 sec/batch)
2017-04-02 22:30:51.256643: step 10310, loss = 0.99 (328.8 examples/sec; 0.389 sec/batch)
2017-04-02 22:30:55.170558: step 10320, loss = 0.81 (327.0 examples/sec; 0.391 sec/batch)
2017-04-02 22:30:59.089186: step 10330, loss = 0.86 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 22:31:03.029135: step 10340, loss = 0.78 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 22:31:06.984935: step 10350, loss = 0.88 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 22:31:10.909838: step 10360, loss = 0.90 (326.1 examples/sec; 0.392 sec/batch)
2017-04-02 22:31:14.927933: step 10370, loss = 0.90 (318.6 examples/sec; 0.402 sec/batch)
2017-04-02 22:31:18.835383: step 10380, loss = 0.93 (327.6 examples/sec; 0.391 sec/batch)
2017-04-02 22:31:22.753924: step 10390, loss = 0.93 (326.7 examples/sec; 0.392 sec/batch)
2017-04-02 22:31:26.739642: step 10400, loss = 0.70 (321.1 examples/sec; 0.399 sec/batch)
2017-04-02 22:31:30.628836: step 10410, loss = 1.00 (329.1 examples/sec; 0.389 sec/batch)
2017-04-02 22:31:34.600474: step 10420, loss = 0.84 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 22:31:38.471570: step 10430, loss = 0.73 (330.7 examples/sec; 0.387 sec/batch)
2017-04-02 22:31:42.391381: step 10440, loss = 0.86 (326.5 examples/sec; 0.392 sec/batch)
2017-04-02 22:31:46.305643: step 10450, loss = 0.73 (327.0 examples/sec; 0.391 sec/batch)
2017-04-02 22:31:50.232153: step 10460, loss = 1.02 (326.0 examples/sec; 0.393 sec/batch)
2017-04-02 22:31:54.127646: step 10470, loss = 1.03 (328.6 examples/sec; 0.390 sec/batch)
2017-04-02 22:31:58.077077: step 10480, loss = 0.89 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 22:32:01.969367: step 10490, loss = 0.78 (328.9 examples/sec; 0.389 sec/batch)
2017-04-02 22:32:05.994071: step 10500, loss = 0.73 (318.0 examples/sec; 0.402 sec/batch)
2017-04-02 22:32:09.941552: step 10510, loss = 0.99 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 22:32:13.851088: step 10520, loss = 0.97 (327.4 examples/sec; 0.391 sec/batch)
2017-04-02 22:32:17.732742: step 10530, loss = 1.01 (329.8 examples/sec; 0.388 sec/batch)
2017-04-02 22:32:21.619325: step 10540, loss = 0.86 (329.3 examples/sec; 0.389 sec/batch)
2017-04-02 22:32:25.532870: step 10550, loss = 0.89 (327.1 examples/sec; 0.391 sec/batch)
2017-04-02 22:32:29.442372: step 10560, loss = 0.78 (327.4 examples/sec; 0.391 sec/batch)
2017-04-02 22:32:33.373874: step 10570, loss = 0.74 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 22:32:37.305653: step 10580, loss = 0.99 (325.6 examples/sec; 0.393 sec/batch)
2017-04-02 22:32:41.194321: step 10590, loss = 0.71 (329.2 examples/sec; 0.389 sec/batch)
2017-04-02 22:32:45.205302: step 10600, loss = 0.99 (319.1 examples/sec; 0.401 sec/batch)
2017-04-02 22:32:49.129295: step 10610, loss = 0.86 (326.2 examples/sec; 0.392 sec/batch)
2017-04-02 22:32:53.022380: step 10620, loss = 1.16 (328.8 examples/sec; 0.389 sec/batch)
2017-04-02 22:32:56.928415: step 10630, loss = 0.88 (327.7 examples/sec; 0.391 sec/batch)
2017-04-02 22:33:00.795934: step 10640, loss = 0.93 (331.0 examples/sec; 0.387 sec/batch)
2017-04-02 22:33:04.689956: step 10650, loss = 0.97 (328.7 examples/sec; 0.389 sec/batch)
2017-04-02 22:33:08.594514: step 10660, loss = 0.77 (327.8 examples/sec; 0.390 sec/batch)
2017-04-02 22:33:12.514864: step 10670, loss = 0.70 (326.5 examples/sec; 0.392 sec/batch)
2017-04-02 22:33:16.411155: step 10680, loss = 0.82 (328.5 examples/sec; 0.390 sec/batch)
2017-04-02 22:33:20.308476: step 10690, loss = 0.87 (328.4 examples/sec; 0.390 sec/batch)
2017-04-02 22:33:24.319831: step 10700, loss = 0.85 (319.1 examples/sec; 0.401 sec/batch)
2017-04-02 22:33:28.194174: step 10710, loss = 0.96 (330.4 examples/sec; 0.387 sec/batch)
2017-04-02 22:33:32.121349: step 10720, loss = 0.99 (325.9 examples/sec; 0.393 sec/batch)
2017-04-02 22:33:36.080782: step 10730, loss = 1.01 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 22:33:39.973198: step 10740, loss = 0.87 (328.8 examples/sec; 0.389 sec/batch)
2017-04-02 22:33:43.895750: step 10750, loss = 0.78 (326.3 examples/sec; 0.392 sec/batch)
2017-04-02 22:33:47.796971: step 10760, loss = 0.87 (328.1 examples/sec; 0.390 sec/batch)
2017-04-02 22:33:51.716350: step 10770, loss = 0.74 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 22:33:55.631199: step 10780, loss = 0.94 (327.0 examples/sec; 0.391 sec/batch)
2017-04-02 22:33:59.499090: step 10790, loss = 0.66 (330.9 examples/sec; 0.387 sec/batch)
2017-04-02 22:34:03.438058: step 10800, loss = 1.02 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 22:34:07.389065: step 10810, loss = 0.91 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 22:34:11.281998: step 10820, loss = 0.79 (328.8 examples/sec; 0.389 sec/batch)
2017-04-02 22:34:15.173929: step 10830, loss = 0.98 (328.9 examples/sec; 0.389 sec/batch)
2017-04-02 22:34:19.140584: step 10840, loss = 0.89 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 22:34:23.217452: step 10850, loss = 0.85 (314.0 examples/sec; 0.408 sec/batch)
2017-04-02 22:34:27.300470: step 10860, loss = 0.80 (313.5 examples/sec; 0.408 sec/batch)
2017-04-02 22:34:31.335185: step 10870, loss = 0.83 (317.2 examples/sec; 0.403 sec/batch)
2017-04-02 22:34:35.286097: step 10880, loss = 0.99 (324.0 examples/sec; 0.395 sec/batch)
2017-04-02 22:34:39.266198: step 10890, loss = 0.88 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 22:34:43.346634: step 10900, loss = 0.94 (313.7 examples/sec; 0.408 sec/batch)
2017-04-02 22:34:47.391186: step 10910, loss = 0.83 (316.5 examples/sec; 0.404 sec/batch)
2017-04-02 22:34:51.382938: step 10920, loss = 0.75 (320.7 examples/sec; 0.399 sec/batch)
2017-04-02 22:34:55.347233: step 10930, loss = 1.03 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 22:34:59.271146: step 10940, loss = 0.89 (326.2 examples/sec; 0.392 sec/batch)
2017-04-02 22:35:03.189993: step 10950, loss = 0.90 (326.6 examples/sec; 0.392 sec/batch)
2017-04-02 22:35:07.080652: step 10960, loss = 0.90 (329.0 examples/sec; 0.389 sec/batch)
2017-04-02 22:35:10.993549: step 10970, loss = 0.74 (327.1 examples/sec; 0.391 sec/batch)
2017-04-02 22:35:14.926930: step 10980, loss = 0.95 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 22:35:18.806378: step 10990, loss = 0.78 (329.9 examples/sec; 0.388 sec/batch)
2017-04-02 22:35:22.774120: step 11000, loss = 0.81 (322.6 examples/sec; 0.397 sec/batch)
2017-04-02 22:35:26.707991: step 11010, loss = 0.91 (325.4 examples/sec; 0.393 sec/batch)
2017-04-02 22:35:30.609365: step 11020, loss = 0.80 (328.1 examples/sec; 0.390 sec/batch)
2017-04-02 22:35:34.539309: step 11030, loss = 0.83 (325.7 examples/sec; 0.393 sec/batch)
2017-04-02 22:35:38.494361: step 11040, loss = 0.93 (323.6 examples/sec; 0.396 sec/batch)
2017-04-02 22:35:42.382980: step 11050, loss = 0.98 (329.2 examples/sec; 0.389 sec/batch)
2017-04-02 22:35:46.326881: step 11060, loss = 0.78 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 22:35:50.266746: step 11070, loss = 0.90 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 22:35:54.178625: step 11080, loss = 0.86 (327.2 examples/sec; 0.391 sec/batch)
2017-04-02 22:35:58.068098: step 11090, loss = 0.78 (329.1 examples/sec; 0.389 sec/batch)
2017-04-02 22:36:02.025042: step 11100, loss = 0.93 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:36:05.962445: step 11110, loss = 0.62 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 22:36:09.897040: step 11120, loss = 0.92 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 22:36:13.801599: step 11130, loss = 0.83 (327.8 examples/sec; 0.390 sec/batch)
2017-04-02 22:36:17.678281: step 11140, loss = 0.77 (330.2 examples/sec; 0.388 sec/batch)
2017-04-02 22:36:21.570686: step 11150, loss = 0.85 (328.8 examples/sec; 0.389 sec/batch)
2017-04-02 22:36:25.505731: step 11160, loss = 0.88 (325.3 examples/sec; 0.394 sec/batch)
2017-04-02 22:36:29.451572: step 11170, loss = 0.89 (324.4 examples/sec; 0.395 sec/batch)
2017-04-02 22:36:33.374659: step 11180, loss = 0.86 (326.3 examples/sec; 0.392 sec/batch)
2017-04-02 22:36:37.257063: step 11190, loss = 0.74 (329.7 examples/sec; 0.388 sec/batch)
2017-04-02 22:36:41.226175: step 11200, loss = 0.94 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 22:36:45.165862: step 11210, loss = 0.91 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 22:36:49.066719: step 11220, loss = 0.89 (328.1 examples/sec; 0.390 sec/batch)
2017-04-02 22:36:52.989501: step 11230, loss = 1.10 (326.3 examples/sec; 0.392 sec/batch)
2017-04-02 22:36:56.925689: step 11240, loss = 0.81 (325.2 examples/sec; 0.394 sec/batch)
2017-04-02 22:37:00.829317: step 11250, loss = 0.88 (327.9 examples/sec; 0.390 sec/batch)
2017-04-02 22:37:04.794703: step 11260, loss = 0.68 (322.8 examples/sec; 0.397 sec/batch)
2017-04-02 22:37:08.746433: step 11270, loss = 1.12 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 22:37:12.633914: step 11280, loss = 0.93 (329.3 examples/sec; 0.389 sec/batch)
2017-04-02 22:37:16.578511: step 11290, loss = 0.88 (324.5 examples/sec; 0.394 sec/batch)
2017-04-02 22:37:20.579518: step 11300, loss = 0.93 (319.9 examples/sec; 0.400 sec/batch)
2017-04-02 22:37:24.521039: step 11310, loss = 0.84 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 22:37:28.410774: step 11320, loss = 0.86 (329.1 examples/sec; 0.389 sec/batch)
2017-04-02 22:37:32.301561: step 11330, loss = 0.83 (329.0 examples/sec; 0.389 sec/batch)
2017-04-02 22:37:36.244886: step 11340, loss = 0.85 (324.6 examples/sec; 0.394 sec/batch)
2017-04-02 22:37:40.158393: step 11350, loss = 0.85 (327.1 examples/sec; 0.391 sec/batch)
2017-04-02 22:37:44.046297: step 11360, loss = 0.76 (329.2 examples/sec; 0.389 sec/batch)
2017-04-02 22:37:48.144078: step 11370, loss = 0.86 (312.4 examples/sec; 0.410 sec/batch)
2017-04-02 22:37:52.351362: step 11380, loss = 0.84 (304.2 examples/sec; 0.421 sec/batch)
2017-04-02 22:37:56.359600: step 11390, loss = 0.77 (319.3 examples/sec; 0.401 sec/batch)
2017-04-02 22:38:00.364800: step 11400, loss = 0.84 (319.6 examples/sec; 0.401 sec/batch)
2017-04-02 22:38:04.358256: step 11410, loss = 0.88 (320.5 examples/sec; 0.399 sec/batch)
2017-04-02 22:38:08.307048: step 11420, loss = 0.97 (324.1 examples/sec; 0.395 sec/batch)
2017-04-02 22:38:12.239786: step 11430, loss = 0.89 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 22:38:16.186622: step 11440, loss = 0.87 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 22:38:20.124852: step 11450, loss = 0.83 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 22:38:24.097787: step 11460, loss = 0.91 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 22:38:28.088056: step 11470, loss = 0.85 (320.8 examples/sec; 0.399 sec/batch)
2017-04-02 22:38:32.062326: step 11480, loss = 0.99 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 22:38:36.056570: step 11490, loss = 1.02 (320.5 examples/sec; 0.399 sec/batch)
2017-04-02 22:38:40.053222: step 11500, loss = 0.84 (320.3 examples/sec; 0.400 sec/batch)
2017-04-02 22:38:43.986153: step 11510, loss = 0.80 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 22:38:47.962886: step 11520, loss = 1.01 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 22:38:51.942731: step 11530, loss = 0.91 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 22:38:55.917212: step 11540, loss = 0.86 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 22:38:59.906266: step 11550, loss = 0.84 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 22:39:03.894206: step 11560, loss = 0.88 (321.0 examples/sec; 0.399 sec/batch)
2017-04-02 22:39:07.861306: step 11570, loss = 0.85 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 22:39:11.834816: step 11580, loss = 0.86 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 22:39:15.797053: step 11590, loss = 0.98 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 22:39:19.822056: step 11600, loss = 0.88 (318.0 examples/sec; 0.403 sec/batch)
2017-04-02 22:39:24.903939: step 11610, loss = 0.90 (251.9 examples/sec; 0.508 sec/batch)
2017-04-02 22:39:28.899614: step 11620, loss = 0.81 (320.3 examples/sec; 0.400 sec/batch)
2017-04-02 22:39:32.880908: step 11630, loss = 0.88 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 22:39:36.854443: step 11640, loss = 0.87 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 22:39:40.819713: step 11650, loss = 0.93 (322.8 examples/sec; 0.397 sec/batch)
2017-04-02 22:39:44.853486: step 11660, loss = 0.77 (317.3 examples/sec; 0.403 sec/batch)
2017-04-02 22:39:48.787761: step 11670, loss = 0.90 (325.3 examples/sec; 0.393 sec/batch)
2017-04-02 22:39:52.812466: step 11680, loss = 0.81 (318.0 examples/sec; 0.402 sec/batch)
2017-04-02 22:39:56.779306: step 11690, loss = 0.90 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 22:40:00.821871: step 11700, loss = 0.86 (316.6 examples/sec; 0.404 sec/batch)
2017-04-02 22:40:04.815511: step 11710, loss = 0.79 (320.5 examples/sec; 0.399 sec/batch)
2017-04-02 22:40:08.767762: step 11720, loss = 0.90 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 22:40:12.793269: step 11730, loss = 1.13 (318.0 examples/sec; 0.403 sec/batch)
2017-04-02 22:40:16.757361: step 11740, loss = 0.79 (322.9 examples/sec; 0.396 sec/batch)
2017-04-02 22:40:20.746137: step 11750, loss = 0.78 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 22:40:24.740272: step 11760, loss = 1.02 (320.5 examples/sec; 0.399 sec/batch)
2017-04-02 22:40:28.777561: step 11770, loss = 0.92 (317.0 examples/sec; 0.404 sec/batch)
2017-04-02 22:40:32.725307: step 11780, loss = 0.91 (324.2 examples/sec; 0.395 sec/batch)
2017-04-02 22:40:36.683268: step 11790, loss = 0.93 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 22:40:40.731945: step 11800, loss = 0.83 (316.2 examples/sec; 0.405 sec/batch)
2017-04-02 22:40:44.925992: step 11810, loss = 0.98 (305.2 examples/sec; 0.419 sec/batch)
2017-04-02 22:40:48.952323: step 11820, loss = 0.65 (317.9 examples/sec; 0.403 sec/batch)
2017-04-02 22:40:52.921965: step 11830, loss = 0.91 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 22:40:56.868447: step 11840, loss = 0.84 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 22:41:00.864966: step 11850, loss = 0.74 (320.3 examples/sec; 0.400 sec/batch)
2017-04-02 22:41:04.811491: step 11860, loss = 0.79 (324.3 examples/sec; 0.395 sec/batch)
2017-04-02 22:41:08.774082: step 11870, loss = 0.91 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 22:41:12.745384: step 11880, loss = 0.99 (322.3 examples/sec; 0.397 sec/batch)
2017-04-02 22:41:16.708526: step 11890, loss = 1.01 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 22:41:20.704580: step 11900, loss = 0.88 (320.3 examples/sec; 0.400 sec/batch)
2017-04-02 22:41:24.642566: step 11910, loss = 0.78 (325.0 examples/sec; 0.394 sec/batch)
2017-04-02 22:41:28.603242: step 11920, loss = 0.96 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 22:41:32.621017: step 11930, loss = 0.89 (318.6 examples/sec; 0.402 sec/batch)
2017-04-02 22:41:36.622539: step 11940, loss = 0.80 (319.9 examples/sec; 0.400 sec/batch)
2017-04-02 22:41:40.595887: step 11950, loss = 0.79 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 22:41:44.594923: step 11960, loss = 0.85 (320.1 examples/sec; 0.400 sec/batch)
2017-04-02 22:41:48.609734: step 11970, loss = 0.83 (318.8 examples/sec; 0.401 sec/batch)
2017-04-02 22:41:52.619640: step 11980, loss = 0.80 (319.2 examples/sec; 0.401 sec/batch)
2017-04-02 22:41:56.631171: step 11990, loss = 0.88 (319.1 examples/sec; 0.401 sec/batch)
2017-04-02 22:42:00.697888: step 12000, loss = 0.91 (314.8 examples/sec; 0.407 sec/batch)
2017-04-02 22:42:04.664807: step 12010, loss = 0.77 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 22:42:08.624193: step 12020, loss = 0.90 (323.3 examples/sec; 0.396 sec/batch)
2017-04-02 22:42:12.673242: step 12030, loss = 0.87 (316.1 examples/sec; 0.405 sec/batch)
2017-04-02 22:42:16.676752: step 12040, loss = 0.97 (319.7 examples/sec; 0.400 sec/batch)
2017-04-02 22:42:20.650738: step 12050, loss = 0.95 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 22:42:24.582566: step 12060, loss = 0.83 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 22:42:28.511805: step 12070, loss = 0.97 (325.8 examples/sec; 0.393 sec/batch)
2017-04-02 22:42:32.453551: step 12080, loss = 0.76 (324.7 examples/sec; 0.394 sec/batch)
2017-04-02 22:42:36.410913: step 12090, loss = 0.85 (323.4 examples/sec; 0.396 sec/batch)
2017-04-02 22:42:40.423208: step 12100, loss = 0.81 (319.0 examples/sec; 0.401 sec/batch)
2017-04-02 22:42:44.408382: step 12110, loss = 0.92 (321.2 examples/sec; 0.399 sec/batch)
2017-04-02 22:42:48.368622: step 12120, loss = 1.01 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 22:42:52.308296: step 12130, loss = 0.90 (324.9 examples/sec; 0.394 sec/batch)
2017-04-02 22:42:56.270380: step 12140, loss = 1.02 (323.1 examples/sec; 0.396 sec/batch)
2017-04-02 22:43:00.257727: step 12150, loss = 0.81 (321.0 examples/sec; 0.399 sec/batch)
2017-04-02 22:43:04.250783: step 12160, loss = 0.84 (320.6 examples/sec; 0.399 sec/batch)
2017-04-02 22:43:08.250935: step 12170, loss = 0.93 (320.0 examples/sec; 0.400 sec/batch)
2017-04-02 22:43:12.220417: step 12180, loss = 0.86 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 22:45:08.819175: step 12190, loss = 0.92 (11.0 examples/sec; 11.660 sec/batch)
2017-04-02 22:45:12.894645: step 12200, loss = 0.77 (314.1 examples/sec; 0.408 sec/batch)
2017-04-02 22:45:16.967159: step 12210, loss = 0.94 (314.3 examples/sec; 0.407 sec/batch)
2017-04-02 22:45:21.133427: step 12220, loss = 0.84 (307.2 examples/sec; 0.417 sec/batch)
2017-04-02 22:45:25.090743: step 12230, loss = 0.82 (323.5 examples/sec; 0.396 sec/batch)
2017-04-02 22:45:29.061277: step 12240, loss = 0.94 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 22:45:33.050563: step 12250, loss = 0.84 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 22:45:36.982513: step 12260, loss = 0.67 (325.5 examples/sec; 0.393 sec/batch)
2017-04-02 22:45:40.955722: step 12270, loss = 0.67 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 22:45:44.938983: step 12280, loss = 1.03 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 22:45:48.942200: step 12290, loss = 1.08 (319.7 examples/sec; 0.400 sec/batch)
2017-04-02 22:45:53.047033: step 12300, loss = 0.90 (311.8 examples/sec; 0.410 sec/batch)
2017-04-02 22:45:57.022663: step 12310, loss = 0.88 (322.0 examples/sec; 0.398 sec/batch)
2017-04-02 22:46:01.065950: step 12320, loss = 0.91 (316.6 examples/sec; 0.404 sec/batch)
2017-04-02 22:46:05.079108: step 12330, loss = 0.98 (319.0 examples/sec; 0.401 sec/batch)
2017-04-02 22:46:09.109353: step 12340, loss = 0.81 (317.6 examples/sec; 0.403 sec/batch)
2017-04-02 22:46:13.081662: step 12350, loss = 0.87 (322.2 examples/sec; 0.397 sec/batch)
2017-04-02 22:46:17.075728: step 12360, loss = 1.04 (320.5 examples/sec; 0.399 sec/batch)
2017-04-02 22:46:21.126198: step 12370, loss = 1.00 (316.0 examples/sec; 0.405 sec/batch)
2017-04-02 22:46:25.150115: step 12380, loss = 0.90 (318.1 examples/sec; 0.402 sec/batch)
2017-04-02 22:46:29.129962: step 12390, loss = 0.87 (321.6 examples/sec; 0.398 sec/batch)
2017-04-02 22:46:33.311804: step 12400, loss = 0.87 (306.1 examples/sec; 0.418 sec/batch)
2017-04-02 22:46:37.395936: step 12410, loss = 0.70 (313.4 examples/sec; 0.408 sec/batch)
2017-04-02 22:46:41.453680: step 12420, loss = 0.89 (315.4 examples/sec; 0.406 sec/batch)
2017-04-02 22:46:45.430335: step 12430, loss = 0.83 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 22:46:49.427450: step 12440, loss = 0.87 (320.2 examples/sec; 0.400 sec/batch)
2017-04-02 22:46:53.441442: step 12450, loss = 0.96 (318.9 examples/sec; 0.401 sec/batch)
2017-04-02 22:46:57.410221: step 12460, loss = 0.96 (322.5 examples/sec; 0.397 sec/batch)
2017-04-02 22:47:01.377356: step 12470, loss = 0.73 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 22:47:05.314406: step 12480, loss = 0.82 (325.1 examples/sec; 0.394 sec/batch)
2017-04-02 22:47:09.266040: step 12490, loss = 0.82 (323.9 examples/sec; 0.395 sec/batch)
2017-04-02 22:47:13.331647: step 12500, loss = 0.85 (314.8 examples/sec; 0.407 sec/batch)
2017-04-02 22:47:17.763902: step 12510, loss = 0.68 (288.8 examples/sec; 0.443 sec/batch)
2017-04-02 22:47:21.800591: step 12520, loss = 0.82 (317.1 examples/sec; 0.404 sec/batch)
2017-04-02 22:47:25.811452: step 12530, loss = 1.00 (319.1 examples/sec; 0.401 sec/batch)
2017-04-02 22:47:29.817108: step 12540, loss = 0.80 (319.5 examples/sec; 0.401 sec/batch)
2017-04-02 22:47:33.799533: step 12550, loss = 1.01 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 22:47:37.781975: step 12560, loss = 0.92 (321.4 examples/sec; 0.398 sec/batch)
2017-04-02 22:47:41.812234: step 12570, loss = 0.90 (317.6 examples/sec; 0.403 sec/batch)
2017-04-02 22:47:45.876208: step 12580, loss = 0.99 (315.0 examples/sec; 0.406 sec/batch)
2017-04-02 22:47:49.906289: step 12590, loss = 0.84 (317.6 examples/sec; 0.403 sec/batch)
2017-04-02 22:47:53.983411: step 12600, loss = 0.83 (313.9 examples/sec; 0.408 sec/batch)
2017-04-02 22:47:57.953923: step 12610, loss = 0.77 (322.4 examples/sec; 0.397 sec/batch)
2017-04-02 22:48:01.958374: step 12620, loss = 0.68 (319.6 examples/sec; 0.400 sec/batch)
2017-04-02 22:48:05.932164: step 12630, loss = 0.79 (322.1 examples/sec; 0.397 sec/batch)
2017-04-02 22:48:09.913988: step 12640, loss = 0.82 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 22:48:13.880357: step 12650, loss = 0.92 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 22:48:17.866581: step 12660, loss = 0.87 (321.1 examples/sec; 0.399 sec/batch)
2017-04-02 22:48:21.903166: step 12670, loss = 0.85 (317.1 examples/sec; 0.404 sec/batch)
2017-04-02 22:48:26.046747: step 12680, loss = 0.77 (308.9 examples/sec; 0.414 sec/batch)
2017-04-02 22:48:30.087396: step 12690, loss = 0.87 (316.8 examples/sec; 0.404 sec/batch)
2017-04-02 22:48:34.179007: step 12700, loss = 0.92 (312.8 examples/sec; 0.409 sec/batch)
2017-04-02 22:48:38.253435: step 12710, loss = 0.90 (314.2 examples/sec; 0.407 sec/batch)
2017-04-02 22:48:42.275698: step 12720, loss = 0.67 (318.2 examples/sec; 0.402 sec/batch)
2017-04-02 22:48:46.319520: step 12730, loss = 0.72 (316.5 examples/sec; 0.404 sec/batch)
2017-04-02 22:48:50.311107: step 12740, loss = 1.00 (320.7 examples/sec; 0.399 sec/batch)
2017-04-02 22:48:54.313455: step 12750, loss = 0.93 (319.8 examples/sec; 0.400 sec/batch)
2017-04-02 22:48:58.309871: step 12760, loss = 1.05 (320.3 examples/sec; 0.400 sec/batch)
2017-04-02 22:49:02.352434: step 12770, loss = 0.85 (316.6 examples/sec; 0.404 sec/batch)
2017-04-02 22:49:06.350895: step 12780, loss = 0.81 (320.1 examples/sec; 0.400 sec/batch)
2017-04-02 22:49:10.389464: step 12790, loss = 0.83 (316.9 examples/sec; 0.404 sec/batch)
2017-04-02 22:49:14.471441: step 12800, loss = 0.84 (313.6 examples/sec; 0.408 sec/batch)
2017-04-02 22:49:18.466391: step 12810, loss = 0.71 (320.4 examples/sec; 0.399 sec/batch)
2017-04-02 22:49:22.462372: step 12820, loss = 0.75 (320.3 examples/sec; 0.400 sec/batch)
2017-04-02 22:49:27.163498: step 12830, loss = 0.77 (272.3 examples/sec; 0.470 sec/batch)
2017-04-02 22:49:31.163426: step 12840, loss = 0.70 (320.0 examples/sec; 0.400 sec/batch)
2017-04-02 22:49:35.453042: step 12850, loss = 0.79 (298.4 examples/sec; 0.429 sec/batch)
2017-04-02 22:49:39.484503: step 12860, loss = 0.74 (317.5 examples/sec; 0.403 sec/batch)
2017-04-02 22:49:43.480409: step 12870, loss = 0.89 (320.3 examples/sec; 0.400 sec/batch)
2017-04-02 22:49:47.569921: step 12880, loss = 0.81 (313.0 examples/sec; 0.409 sec/batch)
2017-04-02 22:49:51.535048: step 12890, loss = 0.76 (322.8 examples/sec; 0.397 sec/batch)
2017-04-02 22:49:55.657413: step 12900, loss = 0.79 (310.5 examples/sec; 0.412 sec/batch)
2017-04-02 22:49:59.743638: step 12910, loss = 0.83 (313.2 examples/sec; 0.409 sec/batch)
2017-04-02 22:50:03.758680: step 12920, loss = 0.86 (318.8 examples/sec; 0.402 sec/batch)
2017-04-02 22:50:07.746652: step 12930, loss = 0.77 (321.0 examples/sec; 0.399 sec/batch)
2017-04-02 22:50:11.730718: step 12940, loss = 0.83 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 22:50:15.746362: step 12950, loss = 0.86 (318.8 examples/sec; 0.402 sec/batch)
2017-04-02 22:50:19.752201: step 12960, loss = 1.02 (319.5 examples/sec; 0.401 sec/batch)
2017-04-02 22:50:23.799133: step 12970, loss = 0.94 (316.3 examples/sec; 0.405 sec/batch)
2017-04-02 22:50:27.845506: step 12980, loss = 0.89 (316.3 examples/sec; 0.405 sec/batch)
2017-04-02 22:50:31.924705: step 12990, loss = 1.03 (313.8 examples/sec; 0.408 sec/batch)
2017-04-02 22:50:36.014679: step 13000, loss = 0.86 (313.0 examples/sec; 0.409 sec/batch)
2017-04-02 22:50:40.043938: step 13010, loss = 0.69 (317.7 examples/sec; 0.403 sec/batch)
2017-04-02 22:50:44.032716: step 13020, loss = 1.02 (320.9 examples/sec; 0.399 sec/batch)
2017-04-02 22:50:48.095804: step 13030, loss = 1.01 (315.0 examples/sec; 0.406 sec/batch)
2017-04-02 22:50:52.079459: step 13040, loss = 0.72 (321.3 examples/sec; 0.398 sec/batch)
2017-04-02 22:50:56.055497: step 13050, loss = 0.95 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 22:51:00.073751: step 13060, loss = 0.91 (318.5 examples/sec; 0.402 sec/batch)
2017-04-02 22:51:04.146799: step 13070, loss = 0.84 (314.3 examples/sec; 0.407 sec/batch)
2017-04-02 22:51:08.169347: step 13080, loss = 0.88 (318.2 examples/sec; 0.402 sec/batch)
2017-04-02 22:51:12.177315: step 13090, loss = 1.02 (319.4 examples/sec; 0.401 sec/batch)
2017-04-02 22:51:16.314319: step 13100, loss = 0.89 (309.4 examples/sec; 0.414 sec/batch)
2017-04-02 22:51:20.360807: step 13110, loss = 0.70 (316.3 examples/sec; 0.405 sec/batch)
2017-04-02 22:51:24.385706: step 13120, loss = 0.81 (318.0 examples/sec; 0.402 sec/batch)
2017-04-02 22:51:28.378254: step 13130, loss = 0.73 (320.6 examples/sec; 0.399 sec/batch)
2017-04-02 22:51:32.372633: step 13140, loss = 0.90 (320.5 examples/sec; 0.399 sec/batch)
2017-04-02 22:51:36.384513: step 13150, loss = 1.14 (319.1 examples/sec; 0.401 sec/batch)
2017-04-02 22:51:40.492801: step 13160, loss = 0.78 (311.6 examples/sec; 0.411 sec/batch)
2017-04-02 22:51:44.544352: step 13170, loss = 0.83 (315.9 examples/sec; 0.405 sec/batch)
2017-04-02 22:51:48.586081: step 13180, loss = 0.91 (316.7 examples/sec; 0.404 sec/batch)
2017-04-02 22:51:52.618649: step 13190, loss = 0.87 (317.4 examples/sec; 0.403 sec/batch)
2017-04-02 22:51:56.722306: step 13200, loss = 0.92 (311.9 examples/sec; 0.410 sec/batch)
2017-04-02 22:52:00.731653: step 13210, loss = 0.85 (319.3 examples/sec; 0.401 sec/batch)
2017-04-02 22:52:04.750449: step 13220, loss = 0.92 (318.5 examples/sec; 0.402 sec/batch)
2017-04-02 22:52:08.783979: step 13230, loss = 0.92 (317.3 examples/sec; 0.403 sec/batch)
2017-04-02 22:52:12.835128: step 13240, loss = 0.88 (316.0 examples/sec; 0.405 sec/batch)
2017-04-02 22:52:16.883619: step 13250, loss = 0.80 (316.2 examples/sec; 0.405 sec/batch)
2017-04-02 22:52:20.937536: step 13260, loss = 0.90 (315.7 examples/sec; 0.405 sec/batch)
2017-04-02 22:52:25.002968: step 13270, loss = 0.75 (314.8 examples/sec; 0.407 sec/batch)
2017-04-02 22:52:29.065402: step 13280, loss = 0.89 (315.1 examples/sec; 0.406 sec/batch)
2017-04-02 22:52:33.137160: step 13290, loss = 0.96 (314.4 examples/sec; 0.407 sec/batch)
2017-04-02 22:52:37.400895: step 13300, loss = 0.89 (300.2 examples/sec; 0.426 sec/batch)
2017-04-02 22:52:41.435266: step 13310, loss = 0.73 (317.3 examples/sec; 0.403 sec/batch)
2017-04-02 22:52:45.430933: step 13320, loss = 0.80 (320.3 examples/sec; 0.400 sec/batch)
2017-04-02 22:52:49.478210: step 13330, loss = 0.94 (316.3 examples/sec; 0.405 sec/batch)
2017-04-02 22:52:53.498577: step 13340, loss = 0.88 (318.4 examples/sec; 0.402 sec/batch)
2017-04-02 22:52:57.503006: step 13350, loss = 0.97 (319.6 examples/sec; 0.400 sec/batch)
2017-04-02 22:53:01.582994: step 13360, loss = 0.95 (313.7 examples/sec; 0.408 sec/batch)
2017-04-02 22:53:05.662643: step 13370, loss = 0.77 (313.8 examples/sec; 0.408 sec/batch)
2017-04-02 22:53:09.666852: step 13380, loss = 0.92 (319.7 examples/sec; 0.400 sec/batch)
2017-04-02 22:53:13.642945: step 13390, loss = 1.05 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 22:53:17.709797: step 13400, loss = 0.85 (314.7 examples/sec; 0.407 sec/batch)
2017-04-02 22:53:21.732297: step 13410, loss = 0.97 (318.2 examples/sec; 0.402 sec/batch)
2017-04-02 22:53:25.791479: step 13420, loss = 1.05 (315.3 examples/sec; 0.406 sec/batch)
2017-04-02 22:53:29.930128: step 13430, loss = 0.91 (309.3 examples/sec; 0.414 sec/batch)
2017-04-02 22:53:34.081786: step 13440, loss = 0.94 (308.3 examples/sec; 0.415 sec/batch)
2017-04-02 22:53:38.149733: step 13450, loss = 0.85 (314.7 examples/sec; 0.407 sec/batch)
2017-04-02 22:53:42.182508: step 13460, loss = 0.74 (317.4 examples/sec; 0.403 sec/batch)
2017-04-02 22:53:46.271679: step 13470, loss = 1.04 (313.0 examples/sec; 0.409 sec/batch)
2017-04-02 22:53:50.325863: step 13480, loss = 0.71 (315.7 examples/sec; 0.405 sec/batch)
2017-04-02 22:53:54.399464: step 13490, loss = 0.82 (314.2 examples/sec; 0.407 sec/batch)
2017-04-02 22:53:58.486516: step 13500, loss = 0.80 (313.2 examples/sec; 0.409 sec/batch)
2017-04-02 22:54:02.546865: step 13510, loss = 0.97 (315.2 examples/sec; 0.406 sec/batch)
2017-04-02 22:54:06.605689: step 13520, loss = 1.06 (315.4 examples/sec; 0.406 sec/batch)
2017-04-02 22:54:10.610691: step 13530, loss = 0.88 (319.6 examples/sec; 0.400 sec/batch)
2017-04-02 22:54:14.690532: step 13540, loss = 0.76 (313.7 examples/sec; 0.408 sec/batch)
2017-04-02 22:54:18.672343: step 13550, loss = 0.91 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 22:54:22.681905: step 13560, loss = 0.79 (319.2 examples/sec; 0.401 sec/batch)
2017-04-02 22:54:26.679040: step 13570, loss = 0.91 (320.2 examples/sec; 0.400 sec/batch)
2017-04-02 22:54:30.687435: step 13580, loss = 0.91 (319.3 examples/sec; 0.401 sec/batch)
2017-04-02 22:54:34.741366: step 13590, loss = 0.80 (315.7 examples/sec; 0.405 sec/batch)
2017-04-02 22:54:38.827269: step 13600, loss = 0.96 (313.3 examples/sec; 0.409 sec/batch)
2017-04-02 22:54:42.817440: step 13610, loss = 0.73 (320.8 examples/sec; 0.399 sec/batch)
2017-04-02 22:54:46.895283: step 13620, loss = 0.81 (313.9 examples/sec; 0.408 sec/batch)
2017-04-02 22:54:50.862194: step 13630, loss = 0.94 (322.7 examples/sec; 0.397 sec/batch)
2017-04-02 22:54:54.938175: step 13640, loss = 0.74 (314.0 examples/sec; 0.408 sec/batch)
2017-04-02 22:54:59.009528: step 13650, loss = 0.91 (314.4 examples/sec; 0.407 sec/batch)
2017-04-02 22:55:03.013424: step 13660, loss = 0.75 (319.7 examples/sec; 0.400 sec/batch)
2017-04-02 22:55:07.022388: step 13670, loss = 0.80 (319.3 examples/sec; 0.401 sec/batch)
2017-04-02 22:55:11.207995: step 13680, loss = 0.83 (305.8 examples/sec; 0.419 sec/batch)
2017-04-02 22:55:15.300008: step 13690, loss = 0.94 (312.8 examples/sec; 0.409 sec/batch)
2017-04-02 22:55:19.403313: step 13700, loss = 0.97 (311.9 examples/sec; 0.410 sec/batch)
2017-04-02 22:55:23.483584: step 13710, loss = 0.69 (313.7 examples/sec; 0.408 sec/batch)
2017-04-02 22:55:27.515231: step 13720, loss = 0.62 (317.5 examples/sec; 0.403 sec/batch)
2017-04-02 22:55:31.566522: step 13730, loss = 0.90 (315.9 examples/sec; 0.405 sec/batch)
2017-04-02 22:55:35.659184: step 13740, loss = 0.79 (312.8 examples/sec; 0.409 sec/batch)
2017-04-02 22:55:39.702590: step 13750, loss = 0.63 (316.6 examples/sec; 0.404 sec/batch)
2017-04-02 22:55:43.841215: step 13760, loss = 0.79 (309.3 examples/sec; 0.414 sec/batch)
2017-04-02 22:55:47.909858: step 13770, loss = 1.03 (314.6 examples/sec; 0.407 sec/batch)
2017-04-02 22:55:52.004306: step 13780, loss = 0.73 (312.6 examples/sec; 0.409 sec/batch)
2017-04-02 22:55:56.046797: step 13790, loss = 0.80 (316.6 examples/sec; 0.404 sec/batch)
2017-04-02 22:56:00.164560: step 13800, loss = 0.94 (310.8 examples/sec; 0.412 sec/batch)
2017-04-02 22:56:04.224038: step 13810, loss = 1.09 (315.3 examples/sec; 0.406 sec/batch)
2017-04-02 22:56:08.329342: step 13820, loss = 0.97 (311.8 examples/sec; 0.411 sec/batch)
2017-04-02 22:56:12.382436: step 13830, loss = 0.88 (315.8 examples/sec; 0.405 sec/batch)
2017-04-02 22:56:16.456215: step 13840, loss = 0.86 (314.2 examples/sec; 0.407 sec/batch)
2017-04-02 22:56:20.497420: step 13850, loss = 0.96 (316.7 examples/sec; 0.404 sec/batch)
2017-04-02 22:56:24.556891: step 13860, loss = 0.87 (315.3 examples/sec; 0.406 sec/batch)
2017-04-02 22:56:28.654270: step 13870, loss = 0.81 (312.4 examples/sec; 0.410 sec/batch)
2017-04-02 22:56:32.698516: step 13880, loss = 0.84 (316.5 examples/sec; 0.404 sec/batch)
2017-04-02 22:56:36.773207: step 13890, loss = 0.92 (314.1 examples/sec; 0.407 sec/batch)
2017-04-02 22:56:40.854023: step 13900, loss = 0.91 (313.7 examples/sec; 0.408 sec/batch)
2017-04-02 22:56:44.944445: step 13910, loss = 0.83 (312.9 examples/sec; 0.409 sec/batch)
2017-04-02 22:56:48.974342: step 13920, loss = 0.81 (317.6 examples/sec; 0.403 sec/batch)
2017-04-02 22:56:53.014388: step 13930, loss = 0.92 (316.8 examples/sec; 0.404 sec/batch)
2017-04-02 22:56:57.046487: step 13940, loss = 0.86 (317.5 examples/sec; 0.403 sec/batch)
2017-04-02 22:57:01.076101: step 13950, loss = 0.84 (317.6 examples/sec; 0.403 sec/batch)
2017-04-02 22:57:05.140243: step 13960, loss = 0.93 (314.9 examples/sec; 0.406 sec/batch)
2017-04-02 22:57:09.117217: step 13970, loss = 0.96 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 22:57:13.172298: step 13980, loss = 0.89 (315.7 examples/sec; 0.406 sec/batch)
2017-04-02 22:57:17.189227: step 13990, loss = 0.99 (318.7 examples/sec; 0.402 sec/batch)
2017-04-02 22:57:21.232525: step 14000, loss = 0.70 (316.6 examples/sec; 0.404 sec/batch)
2017-04-02 22:58:49.738858: step 14010, loss = 0.75 (14.5 examples/sec; 8.851 sec/batch)
2017-04-02 22:58:53.778164: step 14020, loss = 0.85 (316.9 examples/sec; 0.404 sec/batch)
2017-04-02 22:58:57.849734: step 14030, loss = 0.79 (314.4 examples/sec; 0.407 sec/batch)
2017-04-02 22:59:02.079571: step 14040, loss = 0.75 (302.6 examples/sec; 0.423 sec/batch)
2017-04-02 22:59:06.120661: step 14050, loss = 1.02 (316.7 examples/sec; 0.404 sec/batch)
2017-04-02 22:59:10.168724: step 14060, loss = 0.89 (316.2 examples/sec; 0.405 sec/batch)
2017-04-02 22:59:14.182748: step 14070, loss = 0.84 (318.9 examples/sec; 0.401 sec/batch)
2017-04-02 22:59:18.196141: step 14080, loss = 0.89 (318.9 examples/sec; 0.401 sec/batch)
2017-04-02 22:59:22.278738: step 14090, loss = 0.78 (313.5 examples/sec; 0.408 sec/batch)
2017-04-02 22:59:26.967357: step 14100, loss = 1.07 (273.0 examples/sec; 0.469 sec/batch)
2017-04-02 22:59:31.017713: step 14110, loss = 0.97 (316.0 examples/sec; 0.405 sec/batch)
2017-04-02 22:59:35.064824: step 14120, loss = 0.83 (316.3 examples/sec; 0.405 sec/batch)
2017-04-02 22:59:39.073746: step 14130, loss = 0.89 (319.3 examples/sec; 0.401 sec/batch)
2017-04-02 22:59:43.124550: step 14140, loss = 0.80 (316.0 examples/sec; 0.405 sec/batch)
2017-04-02 22:59:47.153226: step 14150, loss = 0.88 (317.7 examples/sec; 0.403 sec/batch)
2017-04-02 22:59:51.192599: step 14160, loss = 0.76 (316.9 examples/sec; 0.404 sec/batch)
2017-04-02 22:59:55.236800: step 14170, loss = 0.97 (316.5 examples/sec; 0.404 sec/batch)
2017-04-02 22:59:59.306262: step 14180, loss = 0.77 (314.5 examples/sec; 0.407 sec/batch)
2017-04-02 23:00:03.351934: step 14190, loss = 0.75 (316.4 examples/sec; 0.405 sec/batch)
2017-04-02 23:00:07.441977: step 14200, loss = 0.69 (313.0 examples/sec; 0.409 sec/batch)
2017-04-02 23:00:11.594242: step 14210, loss = 0.79 (308.3 examples/sec; 0.415 sec/batch)
2017-04-02 23:00:15.730171: step 14220, loss = 0.93 (309.5 examples/sec; 0.414 sec/batch)
2017-04-02 23:00:19.746756: step 14230, loss = 0.95 (318.7 examples/sec; 0.402 sec/batch)
2017-04-02 23:00:23.844882: step 14240, loss = 0.96 (312.3 examples/sec; 0.410 sec/batch)
2017-04-02 23:00:27.869363: step 14250, loss = 0.71 (318.1 examples/sec; 0.402 sec/batch)
2017-04-02 23:00:31.913083: step 14260, loss = 0.72 (316.5 examples/sec; 0.404 sec/batch)
2017-04-02 23:00:35.960690: step 14270, loss = 0.82 (316.2 examples/sec; 0.405 sec/batch)
2017-04-02 23:00:40.053493: step 14280, loss = 0.86 (312.7 examples/sec; 0.409 sec/batch)
2017-04-02 23:00:44.073649: step 14290, loss = 0.89 (318.4 examples/sec; 0.402 sec/batch)
2017-04-02 23:00:48.388326: step 14300, loss = 0.71 (296.7 examples/sec; 0.431 sec/batch)
2017-04-02 23:00:52.402431: step 14310, loss = 0.85 (318.9 examples/sec; 0.401 sec/batch)
2017-04-02 23:00:56.447819: step 14320, loss = 0.71 (316.4 examples/sec; 0.405 sec/batch)
2017-04-02 23:01:00.456215: step 14330, loss = 0.74 (319.3 examples/sec; 0.401 sec/batch)
2017-04-02 23:01:04.530190: step 14340, loss = 0.71 (314.2 examples/sec; 0.407 sec/batch)
2017-04-02 23:01:08.556718: step 14350, loss = 0.82 (317.9 examples/sec; 0.403 sec/batch)
2017-04-02 23:01:12.576675: step 14360, loss = 0.86 (318.4 examples/sec; 0.402 sec/batch)
2017-04-02 23:01:16.620209: step 14370, loss = 0.88 (316.6 examples/sec; 0.404 sec/batch)
2017-04-02 23:01:20.688078: step 14380, loss = 0.76 (314.7 examples/sec; 0.407 sec/batch)
2017-04-02 23:01:24.745644: step 14390, loss = 0.78 (315.5 examples/sec; 0.406 sec/batch)
2017-04-02 23:01:28.893382: step 14400, loss = 0.87 (308.6 examples/sec; 0.415 sec/batch)
2017-04-02 23:01:32.931225: step 14410, loss = 0.80 (317.0 examples/sec; 0.404 sec/batch)
2017-04-02 23:01:36.944336: step 14420, loss = 0.95 (319.0 examples/sec; 0.401 sec/batch)
2017-04-02 23:01:40.921016: step 14430, loss = 0.84 (321.9 examples/sec; 0.398 sec/batch)
2017-04-02 23:01:44.968433: step 14440, loss = 1.15 (316.3 examples/sec; 0.405 sec/batch)
2017-04-02 23:01:49.073315: step 14450, loss = 0.86 (311.8 examples/sec; 0.410 sec/batch)
2017-04-02 23:01:53.159481: step 14460, loss = 0.84 (313.3 examples/sec; 0.409 sec/batch)
2017-04-02 23:01:57.261268: step 14470, loss = 0.87 (312.1 examples/sec; 0.410 sec/batch)
2017-04-02 23:02:01.306558: step 14480, loss = 0.85 (316.4 examples/sec; 0.405 sec/batch)
2017-04-02 23:02:05.352958: step 14490, loss = 0.79 (316.3 examples/sec; 0.405 sec/batch)
2017-04-02 23:02:09.421578: step 14500, loss = 0.99 (314.6 examples/sec; 0.407 sec/batch)
2017-04-02 23:02:13.402776: step 14510, loss = 0.78 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 23:02:17.459144: step 14520, loss = 0.86 (315.6 examples/sec; 0.406 sec/batch)
2017-04-02 23:02:21.534199: step 14530, loss = 0.87 (314.1 examples/sec; 0.408 sec/batch)
2017-04-02 23:02:25.586332: step 14540, loss = 0.75 (315.9 examples/sec; 0.405 sec/batch)
2017-04-02 23:02:29.590846: step 14550, loss = 0.85 (319.6 examples/sec; 0.400 sec/batch)
2017-04-02 23:02:33.660136: step 14560, loss = 0.89 (314.6 examples/sec; 0.407 sec/batch)
2017-04-02 23:02:37.692284: step 14570, loss = 0.89 (317.4 examples/sec; 0.403 sec/batch)
2017-04-02 23:02:41.779195: step 14580, loss = 0.78 (313.2 examples/sec; 0.409 sec/batch)
2017-04-02 23:02:45.832508: step 14590, loss = 0.94 (315.8 examples/sec; 0.405 sec/batch)
2017-04-02 23:02:49.973074: step 14600, loss = 0.97 (309.1 examples/sec; 0.414 sec/batch)
2017-04-02 23:02:54.038460: step 14610, loss = 0.81 (314.9 examples/sec; 0.407 sec/batch)
2017-04-02 23:02:58.151172: step 14620, loss = 0.79 (311.2 examples/sec; 0.411 sec/batch)
2017-04-02 23:03:02.267476: step 14630, loss = 0.77 (311.0 examples/sec; 0.412 sec/batch)
2017-04-02 23:03:06.358501: step 14640, loss = 0.79 (312.9 examples/sec; 0.409 sec/batch)
2017-04-02 23:03:10.440007: step 14650, loss = 0.78 (313.6 examples/sec; 0.408 sec/batch)
2017-04-02 23:03:14.486309: step 14660, loss = 0.82 (316.3 examples/sec; 0.405 sec/batch)
2017-04-02 23:03:18.551525: step 14670, loss = 0.92 (314.9 examples/sec; 0.407 sec/batch)
2017-04-02 23:03:22.565865: step 14680, loss = 1.04 (318.9 examples/sec; 0.401 sec/batch)
2017-04-02 23:03:26.681592: step 14690, loss = 0.84 (311.0 examples/sec; 0.412 sec/batch)
2017-04-02 23:03:30.765322: step 14700, loss = 0.97 (313.4 examples/sec; 0.408 sec/batch)
2017-04-02 23:03:34.772169: step 14710, loss = 0.91 (319.5 examples/sec; 0.401 sec/batch)
2017-04-02 23:03:38.839089: step 14720, loss = 0.77 (314.7 examples/sec; 0.407 sec/batch)
2017-04-02 23:03:42.887035: step 14730, loss = 0.89 (316.2 examples/sec; 0.405 sec/batch)
2017-04-02 23:03:46.956672: step 14740, loss = 0.72 (314.5 examples/sec; 0.407 sec/batch)
2017-04-02 23:03:50.999024: step 14750, loss = 0.91 (316.6 examples/sec; 0.404 sec/batch)
2017-04-02 23:03:55.034057: step 14760, loss = 0.73 (317.2 examples/sec; 0.404 sec/batch)
2017-04-02 23:03:59.134505: step 14770, loss = 0.67 (312.2 examples/sec; 0.410 sec/batch)
2017-04-02 23:04:03.183513: step 14780, loss = 0.83 (316.1 examples/sec; 0.405 sec/batch)
2017-04-02 23:04:07.266950: step 14790, loss = 0.89 (313.5 examples/sec; 0.408 sec/batch)
2017-04-02 23:04:11.447271: step 14800, loss = 0.86 (306.2 examples/sec; 0.418 sec/batch)
2017-04-02 23:04:15.470584: step 14810, loss = 0.79 (318.1 examples/sec; 0.402 sec/batch)
2017-04-02 23:04:19.505876: step 14820, loss = 0.67 (317.2 examples/sec; 0.404 sec/batch)
2017-04-02 23:04:23.524710: step 14830, loss = 0.96 (318.5 examples/sec; 0.402 sec/batch)
2017-04-02 23:04:27.509821: step 14840, loss = 0.82 (321.2 examples/sec; 0.399 sec/batch)
2017-04-02 23:04:31.516453: step 14850, loss = 0.94 (319.5 examples/sec; 0.401 sec/batch)
2017-04-02 23:04:35.548983: step 14860, loss = 0.80 (317.4 examples/sec; 0.403 sec/batch)
2017-04-02 23:04:39.533756: step 14870, loss = 0.77 (321.2 examples/sec; 0.398 sec/batch)
2017-04-02 23:04:43.524252: step 14880, loss = 1.05 (320.8 examples/sec; 0.399 sec/batch)
2017-04-02 23:04:47.705653: step 14890, loss = 0.88 (306.1 examples/sec; 0.418 sec/batch)
2017-04-02 23:04:51.819915: step 14900, loss = 0.92 (311.1 examples/sec; 0.411 sec/batch)
2017-04-02 23:04:55.866381: step 14910, loss = 0.83 (316.3 examples/sec; 0.405 sec/batch)
2017-04-02 23:04:59.943704: step 14920, loss = 0.93 (313.9 examples/sec; 0.408 sec/batch)
2017-04-02 23:05:03.943245: step 14930, loss = 0.76 (320.0 examples/sec; 0.400 sec/batch)
2017-04-02 23:05:07.934082: step 14940, loss = 0.85 (320.7 examples/sec; 0.399 sec/batch)
2017-04-02 23:05:11.968461: step 14950, loss = 0.85 (317.3 examples/sec; 0.403 sec/batch)
2017-04-02 23:05:15.990859: step 14960, loss = 0.86 (318.2 examples/sec; 0.402 sec/batch)
2017-04-02 23:05:20.023237: step 14970, loss = 0.76 (317.4 examples/sec; 0.403 sec/batch)
2017-04-02 23:05:24.100600: step 14980, loss = 0.85 (313.9 examples/sec; 0.408 sec/batch)
2017-04-02 23:05:28.204899: step 14990, loss = 0.63 (311.9 examples/sec; 0.410 sec/batch)
2017-04-02 23:05:32.322305: step 15000, loss = 0.88 (310.9 examples/sec; 0.412 sec/batch)
2017-04-02 23:05:36.390835: step 15010, loss = 0.88 (314.6 examples/sec; 0.407 sec/batch)
2017-04-02 23:05:40.507422: step 15020, loss = 0.83 (310.9 examples/sec; 0.412 sec/batch)
2017-04-02 23:05:44.578855: step 15030, loss = 0.92 (314.4 examples/sec; 0.407 sec/batch)
2017-04-02 23:05:48.677932: step 15040, loss = 0.90 (312.3 examples/sec; 0.410 sec/batch)
2017-04-02 23:05:52.730090: step 15050, loss = 0.80 (315.9 examples/sec; 0.405 sec/batch)
2017-04-02 23:05:56.805341: step 15060, loss = 1.03 (314.1 examples/sec; 0.408 sec/batch)
2017-04-02 23:06:00.850657: step 15070, loss = 0.77 (316.4 examples/sec; 0.405 sec/batch)
2017-04-02 23:06:04.848703: step 15080, loss = 0.83 (320.2 examples/sec; 0.400 sec/batch)
2017-04-02 23:06:08.974557: step 15090, loss = 0.76 (310.2 examples/sec; 0.413 sec/batch)
2017-04-02 23:06:13.162673: step 15100, loss = 1.09 (305.6 examples/sec; 0.419 sec/batch)
2017-04-02 23:06:17.228991: step 15110, loss = 0.78 (314.8 examples/sec; 0.407 sec/batch)
2017-04-02 23:06:21.271438: step 15120, loss = 0.88 (316.6 examples/sec; 0.404 sec/batch)
2017-04-02 23:06:25.291156: step 15130, loss = 0.79 (318.4 examples/sec; 0.402 sec/batch)
2017-04-02 23:06:29.307489: step 15140, loss = 0.94 (318.7 examples/sec; 0.402 sec/batch)
2017-04-02 23:06:33.313897: step 15150, loss = 0.85 (319.5 examples/sec; 0.401 sec/batch)
2017-04-02 23:06:37.370392: step 15160, loss = 0.67 (315.5 examples/sec; 0.406 sec/batch)
2017-04-02 23:06:41.424536: step 15170, loss = 0.77 (315.7 examples/sec; 0.405 sec/batch)
2017-04-02 23:06:45.491507: step 15180, loss = 1.05 (314.7 examples/sec; 0.407 sec/batch)
2017-04-02 23:06:49.476500: step 15190, loss = 0.88 (321.2 examples/sec; 0.398 sec/batch)
2017-04-02 23:06:53.532290: step 15200, loss = 0.75 (315.6 examples/sec; 0.406 sec/batch)
2017-04-02 23:06:57.519758: step 15210, loss = 0.88 (321.0 examples/sec; 0.399 sec/batch)
2017-04-02 23:07:01.516136: step 15220, loss = 0.69 (320.3 examples/sec; 0.400 sec/batch)
2017-04-02 23:07:05.533314: step 15230, loss = 0.63 (318.6 examples/sec; 0.402 sec/batch)
2017-04-02 23:07:09.559130: step 15240, loss = 0.80 (317.9 examples/sec; 0.403 sec/batch)
2017-04-02 23:07:13.597258: step 15250, loss = 1.01 (317.0 examples/sec; 0.404 sec/batch)
2017-04-02 23:07:17.639863: step 15260, loss = 0.71 (316.6 examples/sec; 0.404 sec/batch)
2017-04-02 23:07:21.657814: step 15270, loss = 0.83 (318.6 examples/sec; 0.402 sec/batch)
2017-04-02 23:07:25.688144: step 15280, loss = 1.12 (317.6 examples/sec; 0.403 sec/batch)
2017-04-02 23:07:29.818711: step 15290, loss = 0.84 (309.9 examples/sec; 0.413 sec/batch)
2017-04-02 23:07:33.956034: step 15300, loss = 0.83 (309.4 examples/sec; 0.414 sec/batch)
2017-04-02 23:07:37.966302: step 15310, loss = 0.89 (319.2 examples/sec; 0.401 sec/batch)
2017-04-02 23:07:41.994899: step 15320, loss = 0.89 (317.7 examples/sec; 0.403 sec/batch)
2017-04-02 23:07:46.025636: step 15330, loss = 0.98 (317.6 examples/sec; 0.403 sec/batch)
2017-04-02 23:07:50.083301: step 15340, loss = 0.91 (315.5 examples/sec; 0.406 sec/batch)
2017-04-02 23:07:54.119255: step 15350, loss = 0.82 (317.1 examples/sec; 0.404 sec/batch)
2017-04-02 23:07:58.309995: step 15360, loss = 0.92 (305.4 examples/sec; 0.419 sec/batch)
2017-04-02 23:08:02.368829: step 15370, loss = 0.70 (315.4 examples/sec; 0.406 sec/batch)
2017-04-02 23:08:06.432938: step 15380, loss = 0.99 (315.0 examples/sec; 0.406 sec/batch)
2017-04-02 23:08:10.494554: step 15390, loss = 0.96 (315.1 examples/sec; 0.406 sec/batch)
2017-04-02 23:08:14.599350: step 15400, loss = 0.97 (311.8 examples/sec; 0.410 sec/batch)
2017-04-02 23:08:18.698908: step 15410, loss = 0.81 (312.2 examples/sec; 0.410 sec/batch)
2017-04-02 23:08:22.779185: step 15420, loss = 0.92 (313.7 examples/sec; 0.408 sec/batch)
2017-04-02 23:08:26.888027: step 15430, loss = 0.83 (311.5 examples/sec; 0.411 sec/batch)
2017-04-02 23:08:30.920853: step 15440, loss = 0.91 (317.4 examples/sec; 0.403 sec/batch)
2017-04-02 23:08:34.972609: step 15450, loss = 1.08 (315.9 examples/sec; 0.405 sec/batch)
2017-04-02 23:08:39.135706: step 15460, loss = 0.78 (307.5 examples/sec; 0.416 sec/batch)
2017-04-02 23:08:43.174137: step 15470, loss = 0.74 (317.0 examples/sec; 0.404 sec/batch)
2017-04-02 23:08:47.261394: step 15480, loss = 0.78 (313.2 examples/sec; 0.409 sec/batch)
2017-04-02 23:08:51.297971: step 15490, loss = 1.00 (317.1 examples/sec; 0.404 sec/batch)
2017-04-02 23:08:55.378365: step 15500, loss = 0.85 (313.7 examples/sec; 0.408 sec/batch)
2017-04-02 23:08:59.418231: step 15510, loss = 0.99 (316.8 examples/sec; 0.404 sec/batch)
2017-04-02 23:09:03.430983: step 15520, loss = 0.70 (319.0 examples/sec; 0.401 sec/batch)
2017-04-02 23:09:07.393700: step 15530, loss = 1.08 (323.0 examples/sec; 0.396 sec/batch)
2017-04-02 23:09:11.497210: step 15540, loss = 0.71 (311.9 examples/sec; 0.410 sec/batch)
2017-04-02 23:09:15.665745: step 15550, loss = 0.91 (307.1 examples/sec; 0.417 sec/batch)
2017-04-02 23:09:19.657191: step 15560, loss = 0.90 (320.7 examples/sec; 0.399 sec/batch)
2017-04-02 23:09:24.298798: step 15570, loss = 0.75 (275.8 examples/sec; 0.464 sec/batch)
2017-04-02 23:09:28.319980: step 15580, loss = 0.74 (318.3 examples/sec; 0.402 sec/batch)
2017-04-02 23:09:32.301254: step 15590, loss = 0.89 (321.5 examples/sec; 0.398 sec/batch)
2017-04-02 23:09:36.381467: step 15600, loss = 0.92 (313.7 examples/sec; 0.408 sec/batch)
2017-04-02 23:09:40.368039: step 15610, loss = 0.74 (321.1 examples/sec; 0.399 sec/batch)
2017-04-02 23:09:44.378837: step 15620, loss = 0.72 (319.1 examples/sec; 0.401 sec/batch)
2017-04-02 23:09:48.421219: step 15630, loss = 0.82 (316.6 examples/sec; 0.404 sec/batch)
2017-04-02 23:09:52.440009: step 15640, loss = 1.03 (318.5 examples/sec; 0.402 sec/batch)
2017-04-02 23:09:56.518700: step 15650, loss = 0.78 (313.8 examples/sec; 0.408 sec/batch)
2017-04-02 23:10:00.569729: step 15660, loss = 0.78 (316.0 examples/sec; 0.405 sec/batch)
2017-04-02 23:10:04.530605: step 15670, loss = 0.85 (323.2 examples/sec; 0.396 sec/batch)
2017-04-02 23:10:08.553978: step 15680, loss = 0.76 (318.1 examples/sec; 0.402 sec/batch)
2017-04-02 23:10:12.600086: step 15690, loss = 0.81 (316.4 examples/sec; 0.405 sec/batch)
2017-04-02 23:10:16.705222: step 15700, loss = 0.69 (311.8 examples/sec; 0.411 sec/batch)
2017-04-02 23:10:20.762424: step 15710, loss = 0.82 (315.5 examples/sec; 0.406 sec/batch)
2017-04-02 23:10:24.857337: step 15720, loss = 0.76 (312.6 examples/sec; 0.409 sec/batch)
2017-04-02 23:10:28.918964: step 15730, loss = 0.82 (315.1 examples/sec; 0.406 sec/batch)
2017-04-02 23:10:32.972439: step 15740, loss = 0.82 (315.8 examples/sec; 0.405 sec/batch)
2017-04-02 23:10:37.069504: step 15750, loss = 0.93 (312.4 examples/sec; 0.410 sec/batch)
2017-04-02 23:10:41.153107: step 15760, loss = 0.87 (313.4 examples/sec; 0.408 sec/batch)
2017-04-02 23:10:45.181471: step 15770, loss = 0.71 (317.7 examples/sec; 0.403 sec/batch)
2017-04-02 23:10:49.184637: step 15780, loss = 0.78 (319.7 examples/sec; 0.400 sec/batch)
2017-04-02 23:10:53.212646: step 15790, loss = 0.83 (317.8 examples/sec; 0.403 sec/batch)
2017-04-02 23:10:57.310124: step 15800, loss = 0.94 (312.4 examples/sec; 0.410 sec/batch)
2017-04-02 23:11:01.387992: step 15810, loss = 0.92 (313.9 examples/sec; 0.408 sec/batch)
2017-04-02 23:11:05.455928: step 15820, loss = 0.91 (314.7 examples/sec; 0.407 sec/batch)
2017-04-02 23:11:09.506056: step 15830, loss = 0.76 (316.0 examples/sec; 0.405 sec/batch)
2017-04-02 23:11:13.513198: step 15840, loss = 0.72 (319.4 examples/sec; 0.401 sec/batch)
2017-04-02 23:11:17.530005: step 15850, loss = 0.80 (318.7 examples/sec; 0.402 sec/batch)
2017-04-02 23:11:21.565856: step 15860, loss = 0.89 (317.2 examples/sec; 0.404 sec/batch)
2017-04-02 23:11:25.598682: step 15870, loss = 0.97 (317.4 examples/sec; 0.403 sec/batch)
2017-04-02 23:11:29.609135: step 15880, loss = 1.16 (319.2 examples/sec; 0.401 sec/batch)
2017-04-02 23:11:33.639359: step 15890, loss = 0.94 (317.6 examples/sec; 0.403 sec/batch)
2017-04-02 23:11:37.763305: step 15900, loss = 0.74 (310.4 examples/sec; 0.412 sec/batch)
2017-04-02 23:11:41.852158: step 15910, loss = 0.70 (313.0 examples/sec; 0.409 sec/batch)
2017-04-02 23:11:45.897117: step 15920, loss = 0.77 (316.4 examples/sec; 0.404 sec/batch)
2017-04-02 23:11:49.934659: step 15930, loss = 1.02 (317.0 examples/sec; 0.404 sec/batch)
2017-04-02 23:11:54.257201: step 15940, loss = 0.75 (296.1 examples/sec; 0.432 sec/batch)
2017-04-02 23:11:58.351085: step 15950, loss = 1.19 (312.7 examples/sec; 0.409 sec/batch)
2017-04-02 23:12:02.401523: step 15960, loss = 0.68 (316.0 examples/sec; 0.405 sec/batch)
2017-04-02 23:12:06.472073: step 15970, loss = 1.02 (314.5 examples/sec; 0.407 sec/batch)
2017-04-02 23:12:10.488953: step 15980, loss = 0.83 (318.7 examples/sec; 0.402 sec/batch)
2017-04-02 23:12:14.563199: step 15990, loss = 0.90 (314.2 examples/sec; 0.407 sec/batch)
2017-04-02 23:12:18.662078: step 16000, loss = 0.75 (312.3 examples/sec; 0.410 sec/batch)
