2017-04-02 21:37:18.685310: step 0, loss = 4.68 (137.7 examples/sec; 0.930 sec/batch)
2017-04-02 21:37:29.494912: step 10, loss = 4.63 (118.4 examples/sec; 1.081 sec/batch)
2017-04-02 21:37:40.272393: step 20, loss = 4.54 (118.8 examples/sec; 1.078 sec/batch)
2017-04-02 21:37:51.988075: step 30, loss = 4.45 (109.3 examples/sec; 1.172 sec/batch)
2017-04-02 21:38:03.039606: step 40, loss = 4.31 (115.8 examples/sec; 1.105 sec/batch)
2017-04-02 21:38:14.011177: step 50, loss = 4.31 (116.7 examples/sec; 1.097 sec/batch)
2017-04-02 21:38:25.267157: step 60, loss = 4.29 (113.7 examples/sec; 1.126 sec/batch)
2017-04-02 21:38:36.166914: step 70, loss = 4.18 (117.4 examples/sec; 1.090 sec/batch)
2017-04-02 21:38:47.414842: step 80, loss = 4.16 (113.8 examples/sec; 1.125 sec/batch)
2017-04-02 21:38:58.502963: step 90, loss = 4.29 (115.4 examples/sec; 1.109 sec/batch)
2017-04-02 21:39:09.620919: step 100, loss = 4.02 (115.1 examples/sec; 1.112 sec/batch)
2017-04-02 21:39:21.285286: step 110, loss = 4.29 (109.7 examples/sec; 1.166 sec/batch)
2017-04-02 21:39:32.955347: step 120, loss = 4.08 (109.7 examples/sec; 1.167 sec/batch)
2017-04-02 21:39:44.412534: step 130, loss = 4.01 (111.7 examples/sec; 1.146 sec/batch)
2017-04-02 21:39:55.698798: step 140, loss = 3.97 (113.4 examples/sec; 1.129 sec/batch)
2017-04-02 21:40:06.817313: step 150, loss = 3.90 (115.1 examples/sec; 1.112 sec/batch)
2017-04-02 21:40:18.574922: step 160, loss = 3.90 (108.9 examples/sec; 1.176 sec/batch)
2017-04-02 21:40:29.785531: step 170, loss = 3.93 (114.2 examples/sec; 1.121 sec/batch)
2017-04-02 21:40:40.399942: step 180, loss = 3.91 (120.6 examples/sec; 1.061 sec/batch)
2017-04-02 21:40:51.514914: step 190, loss = 3.81 (115.2 examples/sec; 1.111 sec/batch)
2017-04-02 21:41:02.376221: step 200, loss = 3.72 (117.8 examples/sec; 1.086 sec/batch)
2017-04-02 21:41:13.482951: step 210, loss = 3.82 (115.2 examples/sec; 1.111 sec/batch)
2017-04-02 21:41:24.662001: step 220, loss = 3.76 (114.5 examples/sec; 1.118 sec/batch)
2017-04-02 21:41:35.246919: step 230, loss = 3.77 (120.9 examples/sec; 1.058 sec/batch)
2017-04-02 21:41:46.487773: step 240, loss = 3.62 (113.9 examples/sec; 1.124 sec/batch)
2017-04-02 21:41:57.174659: step 250, loss = 3.63 (119.8 examples/sec; 1.069 sec/batch)
2017-04-02 21:42:09.250941: step 260, loss = 3.74 (106.0 examples/sec; 1.208 sec/batch)
2017-04-02 21:42:20.933573: step 270, loss = 3.53 (109.6 examples/sec; 1.168 sec/batch)
2017-04-02 21:42:32.856356: step 280, loss = 3.54 (107.4 examples/sec; 1.192 sec/batch)
2017-04-02 21:42:44.031209: step 290, loss = 3.64 (114.5 examples/sec; 1.117 sec/batch)
2017-04-02 21:42:55.182268: step 300, loss = 3.58 (114.8 examples/sec; 1.115 sec/batch)
2017-04-02 21:43:06.090915: step 310, loss = 3.55 (117.3 examples/sec; 1.091 sec/batch)
2017-04-02 21:43:17.166370: step 320, loss = 3.52 (115.6 examples/sec; 1.108 sec/batch)
2017-04-02 21:43:28.266706: step 330, loss = 3.43 (115.3 examples/sec; 1.110 sec/batch)
2017-04-02 21:43:40.220609: step 340, loss = 3.51 (107.1 examples/sec; 1.195 sec/batch)
2017-04-02 21:43:51.374948: step 350, loss = 3.49 (114.8 examples/sec; 1.115 sec/batch)
2017-04-02 21:44:02.854923: step 360, loss = 3.58 (111.5 examples/sec; 1.148 sec/batch)
2017-04-02 21:44:14.023562: step 370, loss = 3.36 (114.6 examples/sec; 1.117 sec/batch)
2017-04-02 21:44:25.094963: step 380, loss = 3.36 (115.6 examples/sec; 1.107 sec/batch)
2017-04-02 21:44:36.049599: step 390, loss = 3.27 (116.8 examples/sec; 1.095 sec/batch)
2017-04-02 21:44:47.028732: step 400, loss = 3.42 (116.6 examples/sec; 1.098 sec/batch)
2017-04-02 21:44:57.807239: step 410, loss = 3.21 (118.8 examples/sec; 1.078 sec/batch)
2017-04-02 21:45:08.562923: step 420, loss = 3.33 (119.0 examples/sec; 1.076 sec/batch)
2017-04-02 21:45:19.876403: step 430, loss = 3.37 (113.1 examples/sec; 1.131 sec/batch)
2017-04-02 21:45:30.678954: step 440, loss = 3.25 (118.5 examples/sec; 1.080 sec/batch)
2017-04-02 21:45:42.486918: step 450, loss = 3.33 (108.4 examples/sec; 1.181 sec/batch)
2017-04-02 21:45:54.034918: step 460, loss = 3.15 (110.8 examples/sec; 1.155 sec/batch)
2017-04-02 21:46:05.245601: step 470, loss = 3.21 (114.2 examples/sec; 1.121 sec/batch)
2017-04-02 21:46:16.245898: step 480, loss = 3.30 (116.4 examples/sec; 1.100 sec/batch)
2017-04-02 21:46:27.674918: step 490, loss = 3.13 (112.0 examples/sec; 1.143 sec/batch)
2017-04-02 21:46:38.800680: step 500, loss = 2.97 (115.0 examples/sec; 1.113 sec/batch)
2017-04-02 21:46:49.910349: step 510, loss = 3.11 (115.2 examples/sec; 1.111 sec/batch)
2017-04-02 21:47:01.074949: step 520, loss = 3.12 (114.6 examples/sec; 1.116 sec/batch)
2017-04-02 21:47:12.322616: step 530, loss = 3.25 (113.8 examples/sec; 1.125 sec/batch)
2017-04-02 21:47:23.880666: step 540, loss = 3.30 (110.7 examples/sec; 1.156 sec/batch)
2017-04-02 21:47:34.481419: step 550, loss = 3.09 (120.7 examples/sec; 1.060 sec/batch)
2017-04-02 21:47:45.526916: step 560, loss = 3.10 (115.9 examples/sec; 1.105 sec/batch)
2017-04-02 21:47:56.494942: step 570, loss = 2.90 (116.7 examples/sec; 1.097 sec/batch)
2017-04-02 21:48:07.754912: step 580, loss = 3.04 (113.7 examples/sec; 1.126 sec/batch)
2017-04-02 21:48:18.838912: step 590, loss = 3.00 (115.5 examples/sec; 1.108 sec/batch)
2017-04-02 21:48:31.782319: step 600, loss = 2.99 (98.9 examples/sec; 1.294 sec/batch)
2017-04-02 21:48:43.240943: step 610, loss = 3.00 (111.7 examples/sec; 1.146 sec/batch)
2017-04-02 21:48:53.590916: step 620, loss = 2.87 (123.7 examples/sec; 1.035 sec/batch)
2017-04-02 21:49:05.774916: step 630, loss = 2.79 (105.1 examples/sec; 1.218 sec/batch)
2017-04-02 21:49:17.194923: step 640, loss = 2.77 (112.1 examples/sec; 1.142 sec/batch)
2017-04-02 21:49:28.347439: step 650, loss = 2.79 (114.8 examples/sec; 1.115 sec/batch)
2017-04-02 21:49:38.823162: step 660, loss = 2.91 (122.2 examples/sec; 1.048 sec/batch)
2017-04-02 21:49:50.279436: step 670, loss = 2.88 (111.7 examples/sec; 1.146 sec/batch)
2017-04-02 21:50:01.238917: step 680, loss = 2.95 (116.8 examples/sec; 1.096 sec/batch)
2017-04-02 21:50:12.312880: step 690, loss = 2.67 (115.6 examples/sec; 1.107 sec/batch)
2017-04-02 21:50:23.329705: step 700, loss = 2.92 (116.2 examples/sec; 1.102 sec/batch)
2017-04-02 21:50:34.433085: step 710, loss = 2.83 (115.3 examples/sec; 1.110 sec/batch)
2017-04-02 21:50:45.002942: step 720, loss = 2.80 (121.1 examples/sec; 1.057 sec/batch)
2017-04-02 21:50:55.711261: step 730, loss = 2.91 (119.5 examples/sec; 1.071 sec/batch)
2017-04-02 21:51:07.250910: step 740, loss = 2.79 (110.9 examples/sec; 1.154 sec/batch)
2017-04-02 21:51:17.711226: step 750, loss = 2.72 (122.4 examples/sec; 1.046 sec/batch)
2017-04-02 21:51:28.914923: step 760, loss = 2.66 (114.2 examples/sec; 1.120 sec/batch)
2017-04-02 21:51:40.490914: step 770, loss = 2.61 (110.6 examples/sec; 1.158 sec/batch)
2017-04-02 21:51:51.749720: step 780, loss = 2.69 (113.7 examples/sec; 1.126 sec/batch)
2017-04-02 21:52:02.670040: step 790, loss = 2.62 (117.2 examples/sec; 1.092 sec/batch)
2017-04-02 21:52:13.922559: step 800, loss = 2.63 (113.8 examples/sec; 1.125 sec/batch)
2017-04-02 21:52:25.663370: step 810, loss = 2.64 (109.0 examples/sec; 1.174 sec/batch)
2017-04-02 21:52:37.313963: step 820, loss = 2.66 (109.9 examples/sec; 1.165 sec/batch)
2017-04-02 21:52:48.070912: step 830, loss = 2.74 (119.0 examples/sec; 1.076 sec/batch)
2017-04-02 21:52:59.146921: step 840, loss = 2.92 (115.6 examples/sec; 1.108 sec/batch)
2017-04-02 21:53:10.034922: step 850, loss = 2.61 (117.6 examples/sec; 1.089 sec/batch)
2017-04-02 21:53:21.185831: step 860, loss = 2.61 (114.8 examples/sec; 1.115 sec/batch)
2017-04-02 21:53:31.851003: step 870, loss = 2.85 (120.0 examples/sec; 1.067 sec/batch)
2017-04-02 21:53:42.645593: step 880, loss = 2.48 (118.6 examples/sec; 1.079 sec/batch)
2017-04-02 21:53:53.563085: step 890, loss = 2.68 (117.2 examples/sec; 1.092 sec/batch)
2017-04-02 21:54:04.937431: step 900, loss = 2.59 (112.5 examples/sec; 1.137 sec/batch)
2017-04-02 21:54:15.990916: step 910, loss = 2.49 (115.8 examples/sec; 1.105 sec/batch)
2017-04-02 21:54:27.138913: step 920, loss = 2.57 (114.8 examples/sec; 1.115 sec/batch)
2017-04-02 21:54:38.220130: step 930, loss = 2.42 (115.5 examples/sec; 1.108 sec/batch)
2017-04-02 21:54:48.999951: step 940, loss = 2.31 (118.7 examples/sec; 1.078 sec/batch)
2017-04-02 21:54:59.314909: step 950, loss = 2.50 (124.1 examples/sec; 1.031 sec/batch)
2017-04-02 21:55:10.510913: step 960, loss = 2.43 (114.3 examples/sec; 1.120 sec/batch)
2017-04-02 21:55:21.383519: step 970, loss = 2.78 (117.7 examples/sec; 1.087 sec/batch)
2017-04-02 21:55:31.724284: step 980, loss = 2.28 (123.8 examples/sec; 1.034 sec/batch)
2017-04-02 21:55:42.514913: step 990, loss = 2.55 (118.6 examples/sec; 1.079 sec/batch)
2017-04-02 21:55:53.346526: step 1000, loss = 2.48 (118.2 examples/sec; 1.083 sec/batch)
2017-04-02 21:56:03.659861: step 1010, loss = 2.59 (124.1 examples/sec; 1.031 sec/batch)
2017-04-02 21:56:14.934947: step 1020, loss = 2.29 (113.5 examples/sec; 1.128 sec/batch)
2017-04-02 21:56:26.338769: step 1030, loss = 2.18 (112.2 examples/sec; 1.140 sec/batch)
2017-04-02 21:56:37.261204: step 1040, loss = 2.29 (117.2 examples/sec; 1.092 sec/batch)
2017-04-02 21:56:48.304208: step 1050, loss = 2.34 (115.9 examples/sec; 1.104 sec/batch)
2017-04-02 21:56:59.450912: step 1060, loss = 2.25 (114.8 examples/sec; 1.115 sec/batch)
2017-04-02 21:57:10.512617: step 1070, loss = 2.32 (115.7 examples/sec; 1.106 sec/batch)
2017-04-02 21:57:22.491722: step 1080, loss = 2.28 (106.9 examples/sec; 1.198 sec/batch)
2017-04-02 21:57:33.748040: step 1090, loss = 2.33 (113.7 examples/sec; 1.126 sec/batch)
2017-04-02 21:57:44.688901: step 1100, loss = 2.34 (117.0 examples/sec; 1.094 sec/batch)
2017-04-02 21:57:55.471602: step 1110, loss = 2.28 (118.7 examples/sec; 1.078 sec/batch)
2017-04-02 21:58:06.659383: step 1120, loss = 2.26 (114.4 examples/sec; 1.119 sec/batch)
2017-04-02 21:58:17.666834: step 1130, loss = 2.27 (116.3 examples/sec; 1.101 sec/batch)
2017-04-02 21:58:28.251192: step 1140, loss = 2.32 (120.9 examples/sec; 1.058 sec/batch)
2017-04-02 21:58:39.685607: step 1150, loss = 2.30 (111.9 examples/sec; 1.143 sec/batch)
2017-04-02 21:58:50.346914: step 1160, loss = 2.33 (120.1 examples/sec; 1.066 sec/batch)
2017-04-02 21:59:01.518914: step 1170, loss = 2.30 (114.6 examples/sec; 1.117 sec/batch)
2017-04-02 21:59:12.394910: step 1180, loss = 2.28 (117.7 examples/sec; 1.088 sec/batch)
2017-04-02 21:59:23.498921: step 1190, loss = 2.39 (115.3 examples/sec; 1.110 sec/batch)
2017-04-02 21:59:34.323368: step 1200, loss = 2.14 (118.3 examples/sec; 1.082 sec/batch)
2017-04-02 21:59:45.330912: step 1210, loss = 2.15 (116.3 examples/sec; 1.101 sec/batch)
2017-04-02 21:59:56.618910: step 1220, loss = 2.19 (113.4 examples/sec; 1.129 sec/batch)
2017-04-02 22:00:07.285819: step 1230, loss = 2.15 (120.0 examples/sec; 1.067 sec/batch)
2017-04-02 22:00:18.174913: step 1240, loss = 2.02 (117.5 examples/sec; 1.089 sec/batch)
2017-04-02 22:00:29.430810: step 1250, loss = 1.94 (113.7 examples/sec; 1.126 sec/batch)
2017-04-02 22:00:40.792716: step 1260, loss = 2.16 (112.7 examples/sec; 1.136 sec/batch)
2017-04-02 22:00:51.854919: step 1270, loss = 2.16 (115.7 examples/sec; 1.106 sec/batch)
2017-04-02 22:01:02.776197: step 1280, loss = 2.09 (117.2 examples/sec; 1.092 sec/batch)
2017-04-02 22:01:13.422909: step 1290, loss = 2.37 (120.2 examples/sec; 1.065 sec/batch)
2017-04-02 22:01:24.843363: step 1300, loss = 2.37 (112.1 examples/sec; 1.142 sec/batch)
2017-04-02 22:01:35.696292: step 1310, loss = 2.15 (117.9 examples/sec; 1.085 sec/batch)
2017-04-02 22:01:47.077015: step 1320, loss = 2.15 (112.5 examples/sec; 1.138 sec/batch)
2017-04-02 22:01:57.954141: step 1330, loss = 1.99 (117.7 examples/sec; 1.088 sec/batch)
2017-04-02 22:02:08.915391: step 1340, loss = 2.02 (116.8 examples/sec; 1.096 sec/batch)
2017-04-02 22:02:19.868344: step 1350, loss = 2.22 (116.9 examples/sec; 1.095 sec/batch)
2017-04-02 22:02:30.986910: step 1360, loss = 2.28 (115.1 examples/sec; 1.112 sec/batch)
2017-04-02 22:02:42.138913: step 1370, loss = 1.98 (114.8 examples/sec; 1.115 sec/batch)
2017-04-02 22:02:52.944260: step 1380, loss = 2.09 (118.5 examples/sec; 1.081 sec/batch)
2017-04-02 22:03:03.630918: step 1390, loss = 2.05 (119.8 examples/sec; 1.069 sec/batch)
2017-04-02 22:03:15.007689: step 1400, loss = 2.19 (112.5 examples/sec; 1.138 sec/batch)
2017-04-02 22:03:26.118944: step 1410, loss = 2.25 (115.2 examples/sec; 1.111 sec/batch)
2017-04-02 22:03:37.194909: step 1420, loss = 2.08 (115.6 examples/sec; 1.108 sec/batch)
2017-04-02 22:03:47.954912: step 1430, loss = 1.90 (119.0 examples/sec; 1.076 sec/batch)
2017-04-02 22:03:58.849613: step 1440, loss = 2.24 (117.5 examples/sec; 1.089 sec/batch)
2017-04-02 22:04:10.726909: step 1450, loss = 2.01 (107.8 examples/sec; 1.188 sec/batch)
2017-04-02 22:04:22.245214: step 1460, loss = 1.81 (111.1 examples/sec; 1.152 sec/batch)
2017-04-02 22:04:33.266909: step 1470, loss = 1.96 (116.1 examples/sec; 1.102 sec/batch)
2017-04-02 22:04:44.406022: step 1480, loss = 1.92 (114.9 examples/sec; 1.114 sec/batch)
2017-04-02 22:04:55.602680: step 1490, loss = 2.05 (114.3 examples/sec; 1.120 sec/batch)
2017-04-02 22:05:06.438287: step 1500, loss = 1.99 (118.1 examples/sec; 1.084 sec/batch)
2017-04-02 22:05:17.704952: step 1510, loss = 2.15 (113.6 examples/sec; 1.127 sec/batch)
2017-04-02 22:05:28.274904: step 1520, loss = 1.82 (121.1 examples/sec; 1.057 sec/batch)
2017-04-02 22:05:40.119715: step 1530, loss = 2.00 (108.1 examples/sec; 1.184 sec/batch)
2017-04-02 22:05:51.338911: step 1540, loss = 1.98 (114.1 examples/sec; 1.122 sec/batch)
2017-04-02 22:06:02.374939: step 1550, loss = 1.94 (116.0 examples/sec; 1.104 sec/batch)
2017-04-02 22:06:13.513401: step 1560, loss = 1.80 (114.9 examples/sec; 1.114 sec/batch)
2017-04-02 22:06:24.890914: step 1570, loss = 1.90 (112.5 examples/sec; 1.138 sec/batch)
2017-04-02 22:06:36.170044: step 1580, loss = 1.77 (113.5 examples/sec; 1.128 sec/batch)
2017-04-02 22:06:46.869119: step 1590, loss = 1.89 (119.6 examples/sec; 1.070 sec/batch)
2017-04-02 22:06:57.801446: step 1600, loss = 1.79 (117.1 examples/sec; 1.093 sec/batch)
2017-04-02 22:07:08.635729: step 1610, loss = 2.04 (118.1 examples/sec; 1.083 sec/batch)
2017-04-02 22:07:19.887351: step 1620, loss = 2.00 (113.8 examples/sec; 1.125 sec/batch)
2017-04-02 22:07:30.934908: step 1630, loss = 1.73 (115.9 examples/sec; 1.105 sec/batch)
2017-04-02 22:07:41.827246: step 1640, loss = 1.63 (117.5 examples/sec; 1.089 sec/batch)
2017-04-02 22:07:52.801653: step 1650, loss = 1.96 (116.6 examples/sec; 1.097 sec/batch)
2017-04-02 22:08:03.612451: step 1660, loss = 1.83 (118.4 examples/sec; 1.081 sec/batch)
2017-04-02 22:08:14.830908: step 1670, loss = 1.94 (114.1 examples/sec; 1.122 sec/batch)
2017-04-02 22:08:25.586911: step 1680, loss = 1.76 (119.0 examples/sec; 1.076 sec/batch)
2017-04-02 22:08:36.539098: step 1690, loss = 1.73 (116.9 examples/sec; 1.095 sec/batch)
2017-04-02 22:08:47.627394: step 1700, loss = 1.81 (115.4 examples/sec; 1.109 sec/batch)
2017-04-02 22:08:58.656044: step 1710, loss = 2.06 (116.1 examples/sec; 1.103 sec/batch)
2017-04-02 22:09:08.859978: step 1720, loss = 1.69 (125.4 examples/sec; 1.020 sec/batch)
2017-04-02 22:09:19.680662: step 1730, loss = 1.87 (118.3 examples/sec; 1.082 sec/batch)
2017-04-02 22:09:31.174942: step 1740, loss = 1.75 (111.4 examples/sec; 1.149 sec/batch)
2017-04-02 22:09:41.362941: step 1750, loss = 1.66 (125.6 examples/sec; 1.019 sec/batch)
2017-04-02 22:09:52.691829: step 1760, loss = 1.71 (113.0 examples/sec; 1.133 sec/batch)
2017-04-02 22:10:03.850578: step 1770, loss = 1.78 (114.7 examples/sec; 1.116 sec/batch)
2017-04-02 22:10:14.706944: step 1780, loss = 1.73 (117.9 examples/sec; 1.086 sec/batch)
2017-04-02 22:10:25.766566: step 1790, loss = 1.80 (115.7 examples/sec; 1.106 sec/batch)
2017-04-02 22:10:36.785615: step 1800, loss = 1.66 (116.2 examples/sec; 1.102 sec/batch)
2017-04-02 22:10:48.214906: step 1810, loss = 1.67 (112.0 examples/sec; 1.143 sec/batch)
2017-04-02 22:11:00.054913: step 1820, loss = 1.52 (108.1 examples/sec; 1.184 sec/batch)
2017-04-02 22:11:10.950911: step 1830, loss = 1.70 (117.5 examples/sec; 1.090 sec/batch)
2017-04-02 22:11:22.122913: step 1840, loss = 1.69 (114.6 examples/sec; 1.117 sec/batch)
2017-04-02 22:11:32.984202: step 1850, loss = 1.79 (117.8 examples/sec; 1.086 sec/batch)
2017-04-02 22:11:43.922909: step 1860, loss = 1.89 (117.0 examples/sec; 1.094 sec/batch)
2017-04-02 22:11:55.396290: step 1870, loss = 1.63 (111.6 examples/sec; 1.147 sec/batch)
2017-04-02 22:12:06.531421: step 1880, loss = 1.60 (115.0 examples/sec; 1.114 sec/batch)
2017-04-02 22:12:17.676609: step 1890, loss = 1.73 (114.8 examples/sec; 1.115 sec/batch)
2017-04-02 22:12:29.098957: step 1900, loss = 1.65 (112.1 examples/sec; 1.142 sec/batch)
2017-04-02 22:12:39.883371: step 1910, loss = 1.73 (118.7 examples/sec; 1.078 sec/batch)
2017-04-02 22:12:50.831868: step 1920, loss = 1.60 (116.9 examples/sec; 1.095 sec/batch)
2017-04-02 22:13:01.746134: step 1930, loss = 1.61 (117.3 examples/sec; 1.091 sec/batch)
2017-04-02 22:13:12.723434: step 1940, loss = 1.83 (116.6 examples/sec; 1.098 sec/batch)
2017-04-02 22:13:23.546150: step 1950, loss = 1.68 (118.3 examples/sec; 1.082 sec/batch)
2017-04-02 22:13:34.020933: step 1960, loss = 1.56 (122.2 examples/sec; 1.047 sec/batch)
2017-04-02 22:13:45.291688: step 1970, loss = 1.46 (113.6 examples/sec; 1.127 sec/batch)
2017-04-02 22:13:56.574920: step 1980, loss = 1.79 (113.4 examples/sec; 1.128 sec/batch)
2017-04-02 22:14:07.886909: step 1990, loss = 1.75 (113.2 examples/sec; 1.131 sec/batch)
2017-04-02 22:14:18.739419: step 2000, loss = 1.47 (117.9 examples/sec; 1.085 sec/batch)
